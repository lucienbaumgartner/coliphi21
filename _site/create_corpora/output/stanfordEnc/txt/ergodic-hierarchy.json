[{"date.published":"2011-04-13","date.changed":"2020-07-16","url":"https://plato.stanford.edu/entries/ergodic-hierarchy/","author1":"Roman Frigg","author2":"Joseph Berkovitz","author1.info":"http://www.lse.ac.uk/collections/philosophyLogicAndScientificMethod/WhosWho/staffhomepages/frigg.htm","entry":"ergodic-hierarchy","body.text":"\n\n\nThe Ergodic Hierarchy (EH) is a central part of ergodic theory. It is\na hierarchy of properties that dynamical systems can possess. Its five\nlevels are ergodicity, weak mixing, strong mixing, Kolmogorov, and\nBernoulli. Although EH is a mathematical theory, its concepts have\nbeen widely used in the foundations of statistical physics, accounts\nof randomness, and discussions about the nature of chaos, as well as\nin other sciences such economics. We introduce EH and discuss its\napplications.\n\nThe object of study in ergodic theory is a dynamical system. We first\nintroduce some basic concepts with a simple example, from which we\nabstract the general definition of a dynamical system. For a brief\nhistory of the modern notion of a dynamical system and the associated\nconcepts of EH see the\n Appendix,\n Section A. \nA lead ball is hanging from the ceiling on a spring. We then pull it\ndown a bit and let it go. The ball begins to oscillate. The mechanical\nstate of the ball is completely determined by a specification of the\nposition \\(x\\) and the momentum \\(p\\) of its center of mass; that is,\nif we know \\(x\\) and \\(p\\), then we know all that there is to know\nabout the mechanical state of the ball. If we now conjoin \\(x\\) and\n\\(p\\) in one vector space we obtain the so-called phase space\n\\(X\\) of the system (sometimes also referred to as ‘state\n space’).[1]\n This is illustrated in Figure 1 for a two-dimensional phase space of\nthe state of a ball moving up and down (i.e., the phase space has one\ndimension for the ball’s position and one for its momentum). \nFigure 1: The motion of a ball on a\nspring. \nEach point of \\(X\\) represents a state of the ball (because it gives\nthe ball’s position and momentum). Accordingly, the time\nevolution of the ball’s state is represented by a line in \\(X\\),\na so-called phase space trajectory (from now on\n‘trajectory’), showing where in phase space the system was\nat each instant of time. For instance, let us assume that at time \\(t\n= 0\\) the ball is located at point \\(x_1\\) and then moves to \\(x_2\\)\nwhere it arrives at time \\(t = 5\\). This motion is represented in\n\\(X\\) by the line segment connecting points \\(\\gamma_1\\) and\n\\(\\gamma_2\\). In other words, the motion of the ball is represented in\n\\(X\\) by the motion of a point representing the ball’s\n(instantaneous) state, and all the states that the ball is in over the\ncourse of a certain period of time jointly form a trajectory. The\nmotion of this point has a name: it is the phase flow\n\\(\\phi_t\\). The phase flow tells us where the ball is at some later\ntime \\(t\\) if we specify where it is at \\(t = 0\\); or, metaphorically\nspeaking, \\(\\phi_t\\) drags the ball’s state around in \\(X\\) so\nthat the movement of the state represents the motion of the real ball.\nIn other words, \\(\\phi_t\\) is a mathematical representation of the\nsystem’s time evolution. The state of the ball at time \\(t = 0\\)\nis commonly referred to as the initial condition. \\(\\phi_t\\)\nthen tells us, for every point in phase space, how this point evolves\nif it is chosen as an initial condition. In our concrete example, point\n\\(\\gamma_1\\) is the initial condition and we have \\(\\gamma_2 =\n\\phi_{t=5}(\\gamma_1)\\). More generally, let us call the ball’s\ninitial condition \\(\\gamma_0\\) and let \\(\\gamma(t)\\) be its state at\nsome later time \\(t\\). Then we have \\(\\gamma(t) = \\phi_t (\\gamma_0)\\).\nThis is illustrated in figure 2a. \nFigure 2: Evolution in Phase space. \nSince \\(\\phi_t\\) tells us for every point in \\(X\\) how it evolves in\ntime, it also tells us how sets of points move around. For instance,\nchoose an arbitrary set \\(A\\) in \\(X\\); then \\(\\phi_t (A)\\) is the\nimage of A after \\(t\\) time units under the dynamics of the\nsystem. This is illustrated in Figure 2b. Considering sets of points\nrather than single points is important when we think about physical\napplications of this mathematical formalism. We can never determine\nthe exact initial condition of a ball bouncing on a spring. No matter\nhow precisely we measure \\(\\gamma_0\\), there will always be some\nmeasurement error. So what we really want to know in practical\napplications is not how a precise mathematical point evolves, but\nrather how a set of points around the initial condition \\(\\gamma_0\\)\nevolves. In our example with the ball the evolution is\n‘tame’, in that the set keeps its original shape. As we\nwill see below, this is not always the case. \nAn important feature of \\(X\\) is that it is endowed with a so-called\nmeasure \\(\\mu\\). We are familiar with measures in many\ncontexts: from a mathematical point of view, the length that we\nattribute to a part of a line, the surface we attribute to a part of a\nplane, and the volume we attribute to a segment of space are measures.\nA measure is simply a device to attribute a ‘size’ to a\npart of a space. Although \\(X\\) is an abstract mathematical space, the\nleading idea of a measure remains the same: it is a tool to quantify\nthe size of a set. So we say that the set \\(A\\) has measure \\(\\mu(A)\\)\nin much the same way as we say that a certain collection of points of\nordinary space (for instance the ones that lie on the inside of a\nbottle) have a certain volume (for instance one litre). \nFrom a more formal point of view, a measure assigns numbers to certain\nsubsets of a set \\(X\\) (see Appendix B for a formal definition). This\ncan be done in different ways and hence there are different measures.\nConsider the example of a plane. There is a measure that simply\nassigns to each appropriate region of a plane the area of that region.\nBut now imagine that we pour a bucket of sugar on the plane. The sugar\nis not evenly distributed; there are little heaps in some places while\nthere is almost no sugar in other places. A measure different from the\narea measure is one that assigns to a region a number that is equal to\nthe amount of sugar on that region. One of these measures is\nparticularly important, namely the so-called Lebesgue\nmeasure. This measure has an intuitive interpretation: it is just\na precise formalisation of the measure we commonly use in geometry.\nThe interval [0, 2] has Lebesgue measure 2 and the interval [3, 4] has\nLebegues measure 1. In two dimensions, a square whose sides have\nLebesgue measure 2 has Lebesgue measure 4; etc. Although this sounds\nsimple, the mathematical theory of measures is rather involved. We\nstate the basics of measure theory in the\n Appendix,\n Section B, and avoid appeal to technical issues in measure theory in\nwhat follows. \nThe essential elements in the discussion so far were the phase space\n\\(X\\), the time evolution \\(\\phi_t\\), and the measure \\(\\mu\\). And\nthese are also the ingredients for the definition of an abstract\ndynamical system. An abstract dynamical system is a triple \\([X, \\mu ,\nT_t]\\), where \\(\\{T_t \\mid t \\text{ is an instant of time}\\}\\) is a\nfamily of automorphisms, i.e., a family of transformations of \\(X\\)\nonto itself with the property that \\(T_{t_1 +t_2} = T_{t_1}(T_{t_2})\\)\nfor all \\(x \\in X\\) (Arnold and Avez 1968, 1); we say more about time\n below.[2]\n In the above example \\(X\\) is the phase space of the ball’s\nmotion, \\(\\mu\\) is the Lebesgue measure, and \\(T_t\\) is\n\\(\\phi_t\\). \nSo far we have described \\(T_t\\) as giving the time evolution of a\nsystem. Now let us look at this from a more mathematical point of\nview: the effect of \\(T_t\\) is that it assigns to every point in \\(X\\)\nanother point in \\(X\\) after \\(t\\) time units have elapsed. In the\nabove example \\(\\gamma_1\\) is mapped onto \\(\\gamma_2\\) under\n\\(\\phi_t\\) after \\(t = 5\\) seconds. Hence, from a mathematical point\nof view the time evolution of a system consists in a mapping of \\(X\\)\nonto itself, which is why the above definition takes \\(T_t\\) to be a\nfamily of mappings of \\(X\\) onto itself. Such a mapping is a\nprescription that tells you for every point \\(x\\) in \\(X\\) on which\nother point in \\(X\\) it is mapped (from now on we use \\(x\\) to denote\nany point in \\(X\\), and it no longer stands, as in the above example,\nfor the position of the ball).  \nThe systems studied in ergodic theory are forward deterministic. This\nmeans that if two identical copies of that system are in the same\nstate at one instant of time, then they must be in the same\nstate at all future instants of time. Intuitively speaking,\nthis means that for any given time there is only one way in which the\nsystem can evolve forward. For a discussion of determinism see Earman\n(1986). \nIt should be pointed out that no particular interpretation is intended\nin an abstract dynamical system. We have motivated the definition with\nan example from mechanics, but dynamical systems are not tied to that\ncontext. They are mathematical objects in their own right, and as such\nthey can be studied independently of particular applications. This\nmakes them a versatile tool in many different domains. In fact,\ndynamical systems are used, among others, in fields as diverse as\nphysics, biology, geology and economics.  \nThere are many different kinds of dynamical systems. The three most\nimportant distinctions are the following. \nDiscrete versus continuous time. We may consider discrete\ninstants of time or a continuum of instants of time. For ease of\npresentation, we shall say in the first case that time is discrete and\nin the second case that time is continuous. This is just a convenient\nterminology that has no implications for whether time is fundamentally\ndiscrete or continuous. In the above example with the ball time was\ncontinuous (it was taken to be a real number). But often it is\nconvenient to regard time as discrete. If time is continuous, then\n\\(t\\) is a real number and the family of automorphisms is \\(\\{T_t \\mid\nt \\in \\mathbb{R} \\}\\), where \\(\\mathbb{R}\\) is the set of real numbers. If time is\ndiscrete, then \\(t\\) is in the set \\(\\mathbb{Z} = \\{\\ldots -2, -1, 0,\n1, 2, \\ldots \\}\\), and the family of automorphisms is \\(\\{T_t \\mid t\n\\in \\mathbb{Z}\\}\\). In order to indicate that we are dealing with a\ndiscrete family rather than a continuous one we sometimes replace\n‘\\(T_t\\)’ with ‘\\(T_n\\)’; this is just a\nnotational convention of no conceptual\n importance.[3]\n In such systems the progression from one instant of time to the next\nis also referred to as a ‘step’. In population biology,\nfor instance, we often want to know how a population grows over a\ntypical breeding time (e.g. one year). In mathematical models of such\na population the points in \\(X\\) represent the size of a population\n(rather than the position and the momentum of a ball, as in the above\nexample), and the transformation \\(T_n\\) represents the growth of the\npopulation after \\(n\\) time units. A simple example would be \\(T_n = x\n+ n\\). \nDiscrete families of automorphisms have the interesting property that\nthey are generated by one mapping. As we have seen above, all\nautomorphisms satisfy \\(T_{t_1 +t_2} = T_{t_1}(T_{t_2})\\). From this\nit follows that \\(T_n (x) = T^{n}_1 (x)\\), that is \\(T_n\\) is the\n\\(n\\)-th iterate of \\(T_1\\). In this sense \\(T_1\\) generates \\(\\{T_t\n\\mid t \\in \\mathbb{Z}\\}\\); or, in other words, \\(\\{T_t \\mid t \\in\n\\mathbb{Z}\\}\\) can be ‘reduced’ to \\(T_1\\). For this\nreason one often drops the subscript ‘1’, simply calls the\nmap ‘\\(T\\)’, and writes the dynamical system as the triple\n\\([X, \\mu , T]\\), where it is understood that \\(T = T_1\\). \nFor ease of presentation we use discrete transformations from now on.\nThe definitions and theorems we formulate below carry over to\ncontinuous transformations without further ado, and where this is not\nthe case we explicitly say so and treat the two cases separately. \nMeasure preserving versus non-measure preserving\ntransformations. Roughly speaking, a transformation is measure\npreserving if the size of a set (like set \\(A\\) in the above example)\ndoes not change over the course of time: a set can change its form but\nit cannot shrink or grow (with respect to the measure). Formally,\n\\(T\\) is a measure-preserving transformation on \\(X\\) if and\nonly if (iff) for all sets \\(A\\) in \\(X: \\mu(A) = \\mu(T^{-1}(A))\\),\nwhere \\(T^{-1}(A)\\) is the set of points that gets mapped onto \\(A\\)\nunder \\(T\\); that is \\(T^{-1}(A) = \\{ x \\in X \\mid T(x) \\in A\n \\}\\).[4]\n From now on we also assume that the transformations we consider are\nmeasure\n preserving.[5] \nIn sum, from now on, unless stated otherwise, we consider discrete\nmeasure preserving transformations. \nIn order to introduce the concept of ergodicity we have to introduce\nthe phase and the time mean of a function \\(f\\) on \\(X\\).\nMathematically speaking, a function assigns each point in \\(X\\) a\nnumber. If the numbers are always real the function is a real-valued\nfunction; and if the numbers may be complex, then it is a\ncomplex-valued function. Intuitively we can think of these numbers as\nrepresenting the physical quantities of interest. Recalling the\nexample of the bouncing ball, \\(f\\) could for instance assign each\npoint in the phase space \\(X\\) the kinetic energy the system has at\nthat point; in this case we would have \\(f = p^2 / 2m\\), where \\(m\\)\nis the mass of the ball. For every function we can take two kinds of\naverages. The first is the infinite time average \\(f^*\\). The general\nidea of a time average is familiar from everyday contexts. You play\nthe lottery on three consecutive Saturdays. On the first you win $10;\non the second you win nothing; and on the third you win $50. Your\naverage gain is ($10 + $0 + $50)/3 = $20. Technically speaking this is\na time average. This simple idea can easily be put to use in a\ndynamical system: follow the system’s evolution over time (and\nremember that we are now talking about an average for discrete points\nof time), take the value of the relevant function at each step, add\nthe values, and then divide by the number of steps. This yields \nwhere \nis just an abbreviation for \nThis is the finite time average for \\(f\\) after \\(k\\) steps. If the\nsystem’s state continues to evolve infinitely and we keep\ntracking the system forever, then we get the infinite time\naverage: \nwhere the symbol ‘lim’ (from latin ‘limes’,\nmeaning border or limit) indicates that we are letting time tend\ntowards infinity (in mathematical symbols: \\(\\infty)\\). One point\ndeserves special attention, since it will become crucial later on: the\npresence of \\(x_0\\) in the above expression. Time averages depend on\nwhere the system starts; i.e., they depend on the initial condition.\nIf the process starts in a different state, the time average may well\nbe different. \nNext we have the space average \\(\\bar{f}\\). Let us again start with a\ncolloquial example: the average height of the students in a particular\nschool. This is easily calculated: just take each student’s\nheight, add up all the numbers, and divide the result by the number of\nstudents. Technically speaking this is a space average. In\nthe example the students in the school correspond to the points in\n\\(X\\); and the fact that we count each students once (we don’t,\nfor instance, take John’s height into account twice and omit\nJim’s) corresponds to the choice of a measure that gives equal\n‘weight’ to each point in \\(X\\). The transformation \\(T\\)\nhas no pendant in our example, and this is deliberate: space averages\nhave nothing to do with the dynamics of the system (that’s what\nsets them off from time averages). The general mathematical definition\nof the space average is as follows: \nwhere \\(\\int_X\\) is the integral over the phase space\n \\(X\\).[6]\n If the space consists of discrete elements, like the students of the\nschool (they are ‘discrete’ in that you can count them),\nthen the integral becomes equivalent to a sum like the one we have\nwhen we determine the average height of a population. If the \\(X\\) is\ncontinuous (as the phase space above) things are a bit more\ninvolved. \nWith these concepts in place, we can now define\n ergodicity.[7]\n A dynamical system \\([X, \\mu , T]\\) is ergodic iff \nfor all complex-valued Lebesgue integrable functions \\(f\\) almost\neverywhere, meaning for almost all initial conditions. The\nqualification ‘almost everywhere’ is non-trivial and is\nthe source of a famous problem in the foundations of statistical\nmechanics, the so-called ‘measure zero problem’ (to which\nwe turn in Section 3). So it is worth unpacking carefully what this\ncondition involves. Not all sets have a finite size. In fact, there\nare sets of measure zero. This may sound abstract but is very natural.\nTake a ruler and measure the length of certain objects. You will find,\nfor instance, that your pencil is 17cm long—in the language of\nmathematics this means that the one dimensional Lebegue measure of the\npencil is 17. Now measure a geometrical point and answer the question:\nhow long is the point? The answer is that such a point has no\nextension and so its length is zero. In mathematical parlance: a set\nconsisting of a geometrical point is a measure zero set. The same goes\nfor a set of two geometrical points: also two geometrical points\ntogether have no extension and hence have measure zero. Another\nexample is the following: you have device to measure the surface of\nobjects in a plane. You find out that an A4 sheet has a surface of\n623.7 square centimetres. Then you are asked what the surface of a\nline is. The answer is: zero. Lines don’t have surfaces. So with\nrespect to the two dimensional Lebesgue measure lines are measure zero\nsets. \nIn the context of ergodic theory, ‘almost everywhere’\nmeans, by definition, ‘everywhere in \\(X\\) except, perhaps, in a\nset of measure zero’. That is, whenever a claim is qualified as\n‘almost everywhere’ it means that it could be false for\nsome points in \\(X\\), but these taken together have measure zero. Now\nwe are in a position to explain what the phrase means in the\ndefinition of ergodicity. As we have seen above, the time average (but\nnot the space average!) depends on the initial condition. If we say\nthat \\(f^* = \\bar{f}\\) almost everywhere we mean that all those\ninitial conditions for which it turns out to be the case that \\(f^*\n\\ne \\bar{f}\\) taken together form a set of measure zero—they are\nlike a line in the plane. \nArmed with this understanding of the definition of ergodicity, we can\nnow discuss some important properties of ergodic systems. Consider a\nsubset \\(A\\) of \\(X\\). For instance, thinking again about the example\nof the oscillating ball, take the left half of the phase space. Then\ndefine the so-called characteristic function of \\(A, f_A\\), as\nfollows: \\(f_A (x) = 1\\) for all \\(x\\) in \\(A\\) and \\(f_A (x) = 0\\)\nfor all \\(x\\) not in \\(A\\). Plugging this function into the definition\nof ergodicity yields: \\(f^{*}_A = \\mu(A)\\). This means that the\nproportion of time that the system’s state spends in set \\(A\\)\nis proportional to the measure of that set. To make this even more\nintuitive, assume that the measure is normalised: \\(\\mu(X) = 1\\) (this\nis a very common and unproblematic assumption). If we then choose\n\\(A\\) so that \\(\\mu(A) = 1\\) ⁄ 2, then we know that the system\nspends half of the time in \\(A\\); if \\(\\mu(A) = 1\\) ⁄ 4, it\nspends a quarter of the time in \\(A\\); etc. As we will see below, this\nproperty of ergodic systems plays a crucial role in certain approaches\nto statistical mechanics. \nSince we are free to choose \\(A\\) as we wish, we immediately get\nanother important result: a system can be ergodic only if its\ntrajectory may access all parts of \\(X\\) of positive measure, i.e., if\nthe trajectory passes arbitrarily close to any point in \\(X\\)\ninfinitely many times as time tends towards infinity. And this implies\nthat the phase space of ergodic systems is called metrically\nindecomposable (or also ‘irreducible’ or\n‘inseparable’): every set invariant under \\(T\\) (i.e.,\nevery set that is mapped onto itself under \\(T)\\) has either measure 0\nor 1. As a consequence, \\(X\\) cannot be divided into two or more\nsubspaces (of non-zero measure) that are invariant under \\(T\\).\nConversely, a non-ergodic system is metrically decomposable. Hence,\nmetric indecomposability and ergodicity are equivalent. A metrically\ndecomposable system is schematically illustrated in Figure 3. \nFigure 3: Reducible system: no point in\nregion \\(P\\) evolves into region \\(Q\\) and vice versa. \nFinally, we would like to state a theorem that will become important\nin Section 4. One can prove that a system is ergodic iff \nholds for all subsets \\(A\\) and \\(B\\) of \\(X\\). Although this\ncondition does not have an immediate intuitive interpretation, we will\nsee below that it is crucial for the understanding of the kind of\nrandomness we find in ergodic systems. \nIt turns out that ergodicity is only the bottom level of an entire\nhierarchy of dynamical properties. This hierarchy is called the\nergodic hierarchy, and the study of this hierarchy is the\ncore task of a mathematical discipline called ergodic theory.\nThis choice of terminology is somewhat misleading, since ergodicity is\nonly the bottom level of this hierarchy and so EH contains much more\nthan ergodicity and the scope of ergodic theory stretches far beyond\nergodicty. Ergodic theory (thus understood) is part of dynamical\nsystems theory, which studies a wider class of dynamical systems\nthan ergodic theory.  \nEH is a nested classification of dynamical properties. The hierarchy\nis typically represented as consisting of the following five\nlevels: \nBernoulli \\(\\subset\\) Kolmogorov \\(\\subset\\) Strong Mixing \\(\\subset\\)\nWeak Mixing \\(\\subset\\) Ergodic  \nThe diagram is intended to indicate that all Bernoulli systems are\nKolmogorov systems, all Kolmogorov systems are strong mixing systems,\nand so on. Hence all systems in EH are ergodic. However, the converse\nrelations do not hold: not all ergodic systems are weak mixing, and so\non. In what follows a system that is ergodic but not weak mixing is\nreferred to as merely ergodic and similarly for the next\nthree\n levels.[8] \nFigure 4: Mixing \nMixing can be intuitively explained by the following example, first\nused by Gibbs in introducing the concept of mixing. Begin with a glass\nof water, then add a shot of scotch; this is illustrated in Fig. 4a.\nThe volume \\(C\\) of the cocktail (scotch + water) is \\(\\mu(C)\\) and\nthe volume of scotch that was added to the water is \\(\\mu(S)\\), so\nthat in \\(C\\)the concentration of scotch is \\(\\mu(S)/\\mu(C)\\). \nNow stir. Mathematically, stirring is represented by the time\nevolution \\(T\\), meaning that \\(T(S)\\) is the region occupied by the\nscotch after one unit of mixing time. Intuitively we say that the\ncocktail is thoroughly mixed, if the concentration of scotch equals\n\\(\\mu(S) / \\mu(C)\\) not only with respect to the whole volume\nof fluid, but with respect to any region \\(V\\) in that\nvolume. Hence, the drink is thoroughly mixed at time \\(n\\) if \nfor any volume \\(V\\) (of non-zero measure). Now assume that the volume\nof the cocktail is one unit: \\(\\mu(C) = 1\\) (which we can do without\nloss of generality since there is always a unit system in which the\nvolume of the glass is one). Then the cocktail is thoroughly mixed\niff \nfor any region \\(V\\) (of non-zero measure). But how large must \\(n\\)\nbe before the stirring ends with the cocktail well stirred? We now\ndon’t require that the drink must be thoroughly mixed at any finite\ntime, but only that it approaches a state of being thoroughly mixed as\ntime tends towards infinity: \nfor any region \\(V\\) (of non-zero measure). If we now associate the\nglass with the phase space \\(X\\) and replace the scotch \\(S\\) and the\nvolume \\(V\\) with two arbitrary subsets \\(A\\) and \\(B\\) of \\(X\\), then\nwe get the general definition of what is called strong mixing\n(often also referred to just as ‘mixing’): a system is\nstrong mixing iff \nfor all subsets \\(A\\) and \\(B\\) of \\(X.\\) This requirement for mixing\ncan be relaxed a bit by allowing for\n fluctuations.[9]\n That is, instead of requiring that the cocktail reach a uniform state\nof being mixed, we now only require that it be mixed on average. In\nother words, we allow that bubbles of either scotch or water may crop\nup every now and then, but they do so in a way that these fluctuations\naverage out as time tends towards infinity. This translates into\nmathematics in a straightforward way. The deviation from the ideally\nmixed state at some time \\(n\\) is \\(\\mu(T_n B \\cap A) -\n\\mu(B)\\mu(A)\\). The requirement that the average of these deviations\nvanishes inspires the notion of weak mixing. A system is weak\nmixing iff \nfor all subsets \\(A\\) and \\(B\\) of \\(X\\). The vertical strokes denote\nthe so-called absolute value; for instance: \\(\\lvert 5 \\rvert = \\lvert\n-5 \\rvert = 5\\). One can prove that there is a strict implication\nrelation between the three dynamical properties we have introduced so\nfar: strong mixing implies weak mixing, but not vice versa; and weak\nmixing implies ergodicity, but not vice versa. Hence, strong mixing is\nstronger condition than weak mixing, and weak mixing is stronger\ncondition than ergodicity. \nThe next higher level in EH are K-systems. Unlike in the cases of\nergodic and mixing systems, there is unfortunately no intuitive way of\nexplaining the standard definition of such systems, and the definition\nis such that one cannot read off from it the characteristics of\nK-systems (we state this definition in the\n Appendix,\n Section C). The least unintuitive way to present K-systems is via a\ntheorem due to Cornfeld et al (1982, 283), who prove that a\ndynamical system is a K-system iff it is K-mixing. A system\nis K-mixing iff for any subsets \\(A_0, A_1 , \\ldots ,A_r\\) of \\(X\\)\n(where \\(r\\) is a natural number of your choice) the following\ncondition holds: \nwhere \\(\\sigma(n, r)\\) is the minimal \\(\\sigma\\)-algebra generated by\nthe set \nIt is far from obvious what this so-called sigma algebra is and hence\nthe content of this condition is not immediately transparent. We will\ncome back to this issue in Section 5 where we provide an intuitive\nreading of this condition. What matters for the time being is its\nsimilarity to the mixing condition. Strong mixing is, trivially,\nequivalent to \nSo we see that K-mixing adds something to strong mixing. \nIn passing we would like to mention another important property of\nK-systems: one can prove that K-systems have positive\nKolmogorov-Sinai entropy (KS-entropy); for details see the\n Appendix,\nSection C. The KS-entropy itself does not have an intuitive\ninterpretation, but it relates to three other concepts of dynamical\nsystems theory in an interesting way, and these do have intuitive\ninterpretations. First, Lyapunov exponents are a measure for\nhow fast two originally nearby trajectories diverge on average, and\nthey are often used in chaos theory to characterise the chaotic nature\nof the dynamics of a system. Under certain circumstances (essentially,\nthe system has to be differentiable and ergodic) one can prove that a\ndynamical system has a positive KS-entropy if and only if it has\npositive Lyapounov exponents (Lichtenberg and Liebermann 1992, 304).\nIn such a system initially arbitrarily close trajectories diverge\nexponentially. This result is known as Pessin’s\ntheorem. Second, the algorithmic complexity of a\nsequence is the length of the shortest computer programme needed to\nreproduce the sequence. Some sequences are simple; e.g. a string of a\nmillion ‘1’ is simple: the programme needed to reproduce\nit basically is ‘write ‘1’ a million times’,\nwhich is very short. Others are complex: there is no pattern in the\nsequence 5%8£yu@*mS!}<74^F that one could exploit, and so a\nprogramme reproducing that sequence essentially reads ‘write\n5%8£yu@*mS!}<74^F’, which is similar in length to the\nsequence itself. In the discrete case a trajectory can be represented\nas a sequence of symbols which corresponds to the states of the system\nalong this trajectory.  It is then the case that if a system is a\nK-system, then its KS-entropy equals the algorithmic complexity of\nalmost all its trajectories (Brudno 1978). This is now known\nas Brudno’s theorem (Alekseev and Yakobson\n1981). Third, the Shannon entropy is a common measure for the\nuncertainty of a future outcome: the higher the entropy the more\nuncertain we are about what is going to happen. One can prove that,\ngiven certain plausible assumptions, the KS-entropy is equivalent to a\ngeneralised version of the Shannon entropy, and can hence be regarded\nas a measure for the uncertainty of future events given past events\n(Frigg 2004). \nBernoulli systems mark the highest level in EH. In order to define\nBernoulli systems we first have to introduce the notion of a partition\nof \\(X\\) (sometimes also called the ‘coarse graining of\n\\(X\\)’). A partition of X is a division of \\(X\\) into\ndifferent parts (the so-called ‘atoms of the partition’)\nso that these parts don’t overlap and jointly cover \\(X\\) (i.e.,\nthey are mutually exclusive and jointly exhaustive). For instance, in\nFigure 1 there is a partition of the phase space that has two atoms\n(the left and the right part). More formally, \\(\\alpha =\n\\{\\alpha_1,\\ldots , \\alpha_n\\}\\) is a partition of \\(X\\) (and the\n\\(\\alpha_i\\) its atoms) iff (i) the intersection of any two atoms of\nthe partition is the empty set, and (ii) the union of all atoms is\n\\(X\\) (up to measure zero). Furthermore it is important to notice that\na partition remains a partition under the dynamics of the system. That\nis, if \\(\\alpha\\) is a partition, then \\(T_n\\alpha = \\{T_n\\alpha_1\n,\\ldots ,T_n\\alpha_n\\}\\) is also a partition for all \\(n\\). \nThere are, of course, many different ways of partitioning a phase\nspace. In what follows we are going to study how different partitions\nrelate to each other. An important concept in this connection is\nindependence. Let \\(\\alpha\\) and \\(\\beta\\) be two partitions of \\(X\\).\nBy definition, these partitions are independent iff\n\\(\\mu(\\alpha_i \\cap \\beta_j) = \\mu(\\alpha_i)\\mu(\\beta_j)\\) for all\natoms \\(\\alpha_i\\) of \\(\\alpha\\) and all atoms \\(\\beta_j\\) of\n\\(\\beta\\). We will explain the intuitive meaning of this definition\n(and justify calling it ‘independence’) in Section 4; for\nthe time being we just use it as a formal definition. \nWith these notions in hand we can now define a Bernoulli\ntransformation: a transformation \\(T\\) is a Bernoulli\ntransformation iff there exists a partition \\(\\alpha\\) of \\(X\\) so\nthat the images of \\(\\alpha\\) under \\(T\\) at different instants of\ntime are independent; that is, the partitions \\(\\ldots ,T_{-1}\\alpha ,\nT_0 \\alpha , T_1 \\alpha ,\\ldots\\) are all\n independent.[10]\n In other words, \\(T\\) is a Bernoulli transformation iff \nfor all atoms \\(\\delta_i\\) of \\(T_k\\alpha\\) and all atoms \\(\\beta_j\\)\nof \\(T_l\\alpha\\) for all \\(k \\ne l\\). We then refer to \\(\\alpha\\) as\nthe Bernoulli partition, and we call a dynamical system \\([X,\n\\mu , T]\\) a Bernoulli system if \\(T\\) is a Bernoulli\nautomorphism, i.e., a Bernoulli transformation mapping \\(X\\) onto\nitself. \nLet us illustrate this with a well-known example, the\nbaker’s transformation (so named because of its\nsimilarity to the kneading of dough). This transformation maps the\nunit square onto itself. Using standard Cartesian coordinates the\ntransformation can be written as follows: \nIn words, for all points \\((x, y)\\) in the unit square that have an\n\\(x\\)-coordinate smaller than \\(1/2\\), the transformation \\(T\\)\ndoubles the value of \\(x\\) and halves the value of \\(y\\). For all the\npoints \\((x, y)\\) that have an \\(x\\)-coordinate greater or equal to\n\\(1/2\\), \\(T\\) transforms \\(x\\) in to \\(2x-1\\) and \\(y\\) into \\(y/2 +\n1/2\\). This is illustrated in Fig. 5a. \nFigure 5a: The Baker’s\ntransformation \nNow regard the two areas shown in the left-hand part of the above\nfigure as the two atoms of a partition \\(\\alpha = \\{\\alpha_1\n,\\alpha_2\\}\\). It is then easy to see that \\(\\alpha\\) and T\\(\\alpha\\)\nare independent: \\(\\mu(\\alpha_1 \\cap T\\alpha_2) =\n\\mu(\\alpha_1)\\mu(T\\alpha_2)\\), and similarly for all other atoms of\n\\(\\alpha\\) and \\(T\\alpha\\). This is illustrated in Figure 5b. \nFigure 5b: The independence of\n\\(\\alpha\\) and \\(T\\alpha\\). \nOne can prove that independence holds for all other iterates of\n\\(\\alpha\\) as well. So the baker’s transformation together with\nthe partition \\(\\alpha\\) is a Bernoulli transformation. \nIn the literature Bernoulli systems are often introduced using\nso-called shift maps (or Bernoulli shifts). We here\nbriefly indicate how shift maps are related to Bernoulli systems with\nthe example of the baker’s transformation; for a more general\ndiscussion see the\n Appendix,\n Section D. Choose a point in the unit square and write its \\(x\\) and\n\\(y\\) coordinates as binary numbers: \\(x = 0.a_1 a_2 a_3\\ldots\\) and\n\\(y = 0.b_1 b_2 b_3\\ldots\\), where all the \\(a_i\\) and \\(b_i\\) are\neither 0 or 1. Now put both strings together back to back with a dot\nin the middle to form one infinite string: \\(S= \\ldots b_3 b_2 b_1\n.a_1 a_2 a_3\\ldots\\), which may represent the state of the system just\nas a ‘standard’ two-dimensional vector does. Some\nstraightforward algebra then shows that \nFrom this we see that in our ‘one string’ representation\nof the point the operation of \\(T\\) amounts to shifting the dot one\nposition to the right: \\(TS= \\ldots b_3 b_2 b_1 a_1 .a_2 a_3\\ldots\\)\nHence, the baker’s transformation is equivalent to a shift on an\ninfinite string of zeros and\n ones.[11] \nThere are two further notions that are crucial to the theory of\nBernoulli systems, the property of being weak Bernoulli and\nvery weak Bernoulli. These properties play a crucial role in\nshowing that certain transformations are in fact Bernoulli. The\nbaker’s transformation is one of the few examples that have a\ngeometrically simple Bernoulli partition, and so one often cannot\nprove directly that a system is a Bernoulli system. One then shows\nthat a certain geometrically simple partition is weak Bernoulli and\nuses a theorem due to Ornstein to the effect that if a system is weak\nBernoulli then there exists a Bernoulli partition for that system. The\nmathematics of these notions and the associated proofs of equivalence\nare intricate and a presentation of them is beyond the scope of this\nentry. The interested reader is referred to Ornstein (1974) or Shields\n(1973). \nThe concepts of EH, and in particular ergodicity itself, play\nimportant roles in the foundation of statistical mechanics (SM). In\nthis section we review what these roles are. \nA discussion of SM faces an immediate problem.\nFoundational debates in many other fields of physics can take as their\npoint of departure a generally accepted formalism. Things are\ndifferent in SM. Unlike, say, relativity theory,\nSM has not yet found a generally accepted theoretical framework, let\nalone a canonical\n formulation.[12]\n What we find in SM is plethora of different approaches and schools,\neach with its own programme and mathematical\n apparatus.[13]\n However, all these schools use (slight variants) of either of two\ntheoretical frameworks, one of which can be associated with Boltzmann\n(1877) and the other with Gibbs (1902), and can thereby be classify\neither as ‘Boltzmannian’ or ‘Gibbsian’. For\nthis reason we divide our presentation of SM into a two parts, one for\neach of these families of approaches. \nBefore delving into a discussion of these theories, let us briefly\nreview the basic tenets of SM by dint of a common example. Consider a\ngas that is confined to the left half of a box. Now remove the barrier\nseparating the two halves of the box. As a result, the gas quickly\ndisperses, and it continues to do so until it uniformly fills the\nentire box. The gas has approached equilibrium. This raises two\nquestions. First, how is equilibrium characterised? That is, what does\nit take for a system to be in equilibrium? Second, how do we\ncharacterise the approach to equilibrium? That is, what are the\nsalient features of the approach to equilibrium and what features of a\nsystem make it behave in this way? These questions are addressed in\ntwo subdisciplines of SM: equilibrium SM and non-equilibrium SM. \nThere are two different ways of describing processes like the\nspreading of a gas. Thermodynamics describes the system using a few\nmacroscopic variables (in the case of the gas pressure, volume and\ntemperature), while disregarding the microscopic\nconstitution of the gas. As far as thermodynamics is concerned matter\ncould be a continuum rather than consisting of particles—it just\nwould not make any difference. For this reason thermodynamics is\ncalled a ‘macro theory’. \nThe cornerstone of thermodynamics is the so-called Second Law of\nthermodynamics. This law describes one of the salient features of the\nabove process: its unidirectionality. We see gases spread—i.e.,\nwe see them evolving towards equilibrium—but we never observe\ngases spontaneously reverting to the left half of a box—i.e., we\nnever see them move away from equilibrium when left alone. And this is\nnot a specific feature of gases. In fact, not only gases but also all\nother macroscopic systems behave in this way, irrespective of their\nspecific makeup. This fact is enshrined in the Second Law of\nthermodynamics, which, roughly, states that transitions from\nequilibrium to non-equilibrium states cannot occur in isolated\nsystems, which is the same as saying that entropy cannot decrease in\nisolated systems (where a system is isolated if it has no interaction\nwith its environment: there is no heat exchange, no one is compressing\nthe gas, etc.). \nBut there is an altogether different way of looking at that same gas;\nthat is, as consisting of a large number of molecules (a vessel on a\nlaboratory table contains something like \\(10^{23}\\) molecules). These\nmolecules bounce around under the influence of the forces exerted onto\nthem when they crash into the walls of the vessel and collide with\neach other. The motion of each molecule is governed by the laws of\nclassical mechanics in the same way as the motion of the bouncing\nball. So rather than attributing some macro variables to the gas and\nfocussing on them, we could try to understand the gas’ behaviour\nby studying the dynamics of its micro constituents. \nThis raises the question of how the two ways of looking at the gas fit\ntogether. Since neither the thermodynamic nor the mechanical approach\nis in any way privileged, both have to lead to the same conclusions.\nStatistical mechanics is the discipline that addresses this task. From\na more abstract point of view we can therefore also say that SM is the\nstudy of the connection between micro-physics and macro-physics: it\naims to account for a system’s macro behaviour in terms of the\ndynamical laws governing its microscopic constituents. The term\n‘statistical’ in its name is owed to the fact that, as we\nwill see, a mechanical explanation can only be given if we also\nintroduce probabilistic elements into the theory. \nWe first introduce the main elements of the Boltzmannian framework and\nthen turn to the use of ergodicity in it. Every system can posses\nvarious macrostates \\(M_1 ,\\ldots ,M_k\\). These macrostates are\ncharacterised by the values of macroscopic variables, in the case of a\ngas pressure, temperature, and\n volume.[14]\n In the introductory example one macro-state corresponds to the gas\nbeing confined to the left half, another one to it being spread out.\nIn fact, these two states have special status: the former is the\ngas’ initial state (also referred to as the ‘past\nstate’); the latter is the gas’ equilibrium state. We\nlabel the states \\(M_p\\) and \\(M_{eq}\\) respectively. \nIt is one of the fundamental posits of the Boltzmann approach that\nmacrostates supervene on microstates, meaning that a change in a\nsystem’s macrostate must be accompanied by a change in its\nmicrostate (for a discussion of supervenience see McLaughlin and\nBennett 2005, and references therein). For instance, it is not\npossible to change the pressure of a system and at the same time keep\nits micro-state constant. Hence, to every given microstate \\(x\\) there\ncorresponds exactly one macrostate. Let us refer to this\nmacrostate as \\(M(x)\\). This determination relation is not one-to-one;\nin fact many different \\(x\\) can correspond to the same macrostate. We\nnow group together all microstates \\(x\\) that correspond to the same\nmacro-state, which yields a partitioning of the phase space in\nnon-overlapping regions, each corresponding to a macro-state. For this\nreason we also use the same letters, \\(M_1 ,\\ldots ,M_k\\), to refer to\nmacro-states and the corresponding regions in phase space. This is\nillustrated in Figure 6a. \nFigure 6: The macrostate structure of\n\\(X\\). \nWe are now in a position to introduce the Boltzmann entropy. To this\nend recall that we have a measure \\(\\mu \\) on the phase space that\nassigns to every set a particular volume, hence a fortiori\nalso to macrostates. With this in mind, the Boltzmann entropy of a\nmacro-state \\(M_j\\) can be defined as \\(S_B = k_B\\log [\\mu(M_j)]\\),\nwhere \\(k_B\\) is the Boltzmann constant. The important feature of the\nlogarithm is that it is a monotonic function: the larger\n\\(M_j\\), the larger its logarithm. From this it follows that the\nlargest macro-state also has the highest entropy! \nOne can show that, at least in the case of dilute gases, the Boltzmann\nentropy coincides with the thermodynamic entropy (in the sense that\nboth have the same functional dependence on the basic state\nvariables), and so it is plausible to say that the equilibrium state\nis the macro-state for which the Boltzmann entropy is maximal (since\nthermodynamics posits that entropy be maximal for equilibrium states).\nBy assumption the system starts off in a low entropy state, the\ninitial state \\(M_p\\) (the gas being squeezed into the left half of\nthe box). The problem of explaining the approach to equilibrium then\namounts to answering the question: why does a system originally in\n\\(M_p\\) eventually move into \\(M_{eq}\\) and then stay there? (See\nFigure 6b.) \nIn the 1870s Boltzmann offered an important answer to this\n question.[15]\n At the heart of his answer lies the idea to assign probabilities to\nmacrostates according to their size. So Boltzmann adopted the\nfollowing postulate: \\(p(M_j) = c\\mu(M_j)\\) for all \\(j = 1,\\ldots\n,k\\), where \\(c\\) is a normalisation constant assuring that the\nprobabilities add up to one. Granted this postulate, it follows\nimmediately that the most likely state is the equilibrium state (since\nthe equilibrium state occupies the largest chunk of the phase space).\nFrom this point of view it seems natural to understand the approach to\nequilibrium as the evolution from an unlikely macrostate to a more\nlikely macrostate and finally to the most likely macro-state. This,\nBoltzmann argued, was a statistical justification of the Second Law of\nthermodynamics. \nBut Boltzmann knew that simply postulating \\(p(M_j) = c\\mu(M_j)\\)\nwould not solve the problem unless the postulate could be justified in\nterms of the dynamics of the system. This is where ergodicity enters\nthe scene. As we have seen above, ergodic systems have the property of\nspending a fraction of time in each part of the phase space that is\nproportional to its size (with respect to \\(\\mu)\\). As we have also\nseen, the equilibrium state is the largest macrostate. In fact, the\nequilibrium state is much larger than the other states. So if\nwe assume that the system is ergodic, then it is in equilibrium most\nof the time! It is then natural to interpret \\(p(M_j)\\) as a time\naverage: \\(p(M_j)\\) is the fraction of time that the system spends in\nstate \\(M_j\\) over the course of time. We now have the main elements\nof Boltzmann’s framework in front of us: (a) partition the phase\nspace of the system in macrostates and show that the equilibrium state\nis by far the largest state; (b) adopt a time average interpretation\nof probability; and (c) assume that the system in question is\nergodic. It then follows that the system is most likely to be found in\nequilibrium, which justifies (a probabilistic version of) the Second\nlaw. \nThree objections have been levelled against this line of thought.\nFirst, it is pointed that assuming ergodicity is too strong in two\nways. The first is that it turns out to be extremely difficult to\nprove that the systems of interest really are ergodic. Contrary to\nwhat is sometimes asserted, not even a system of \\(n\\) elastic hard\nballs moving in a cubic box with hard reflecting walls has been proven\nto be ergodic for arbitrary \\(n\\); it has been proven to be ergodic\nonly for \\(n \\le 4\\). To this charge one could reply that what looks\nlike defeat to some, appears to be a challenge to others. Progress in\nmathematics may eventually resolve the issue, and there is at least\none recent result that justifies optimism: Simanyi (2004) shows that a\nsystem of \\(n\\) hard balls on a torus of dimension 3 or greater is\nergodic, for an arbitrary natural number \\(n\\). \nThe second way in which ergodicity seems to be too strong is that even\nif eventually we can come by proofs of ergodicity for the relevant\nsystems, the assumption is too strong because there are systems that\nare known not to be ergodic and yet they behave in accordance with the\nSecond Law. Bricmont (2001) investigates the Kac Ring Model and a\nsystem of \\(n\\) uncoupled anharmonic oscillators of identical mass,\nand points out that both systems exhibit thermodynamic behaviour and\nyet they fail to be ergodic. Hence, ergodicity is not necessary for\nthermodynamic behaviour. Earman and Redei (1996, p. 70) and van Lith\n(2001, p. 585) argue that if ergodicity is not necessary for\nthermodynamic behaviour, then ergodicity cannot provide a satisfactory\nexplanation for this behaviour. Either there must be properties other\nthan ergodicity that explain thermodynamic behaviour in cases in which\nthe system is not ergodic, or there must be an altogether different\nexplanation for the approach to equilibrium even for systems which are\nergodic. \nIn response to this objection, Vranas (1998) and Frigg and Werndl\n(2011) argue that most systems that fail to be ergodic are\n‘almost ergodic’ in a specifiable way, and this is good\nenough. We discuss Vranas’ approach below when discussing\nGibbsian SM since that is the context in which he has put forward his\nsuggestion. Werndl and Frigg (2015a, 2015b) offer an alternative\ndefinition of Boltzmannian equilibrium and exploit the ergodic\ndecomposition theorem to show that even if a system is not ergodic it\nwill spend most of the time in equilibrium, as envisaged by Boltzmann\n(roughly the ergodic decomposition theorem says that the phase space\nof every measure preserving system can be partitioned into parts so\nthat the dynamics is ergodic on each part; for details see Petersen\n1983). Frigg (2009) suggested exploiting the fact that almost all\nHamiltonian systems are non-integrable, and that these systems have\nso-called Arnold webs, i.e., large regions of phase space on which the\nmotion of the system is ergodic. Lavis (2005) re-examined the Kac ring\nmodel and pointed out that even though the system is not ergodic, it\nhas an ergodic decomposition, which is sufficient to guarantee the\napproach to equilibrium. He also challenged the assumption, implicit\nin the above criticism, that providing an explanation for the approach\nto equilibrium amounts to identifying one (and only one!) property\nthat all systems have in common. In fact, it may be the case that\ndifferent properties are responsible for the approach to equilibrium\nin different systems, and there is no reason to rule out such\nexplanations. In sum, the tenor of all these responses is that even though\nergodicity simpliciter may not have the resources to explain\nthe approach to equilibrium, somewhat qualified properties do. \nThe second objection is that even if ergodicity obtains, this is not\nsufficient to give us what we need. As we have seen above, ergodicity\ncomes with the qualification ‘almost everywhere’. This\nqualification is usually understood as suggesting that sets of measure\nzero can be ignored without detriment. The idea is that points falling\nin a set of measure zero are ‘sparse’ and can therefore be\nneglected. The question of whether or not this move is legitimate is\nknown as the ‘measure zero problem’. \nSimply neglecting sets of measure zero seems to be problematic for\nvarious reasons. First, sets of measure zero can be rather\n‘big’; for instance, the rational numbers have measure\nzero within the real numbers. Moreover, a set of measure zero need not\nbe (or even appear) negligible if sets are compared with respect to\nproperties other than their measures. For instance, we can judge the\n‘size’ of a set by its cardinality or Baire category\nrather than by its measure, which leads us to different conclusions\nabout the set’s size (Sklar 1993, pp. 182–88). It is also\na mistake to assume that an event with measure zero cannot occur. In\nfact, having measure zero and being impossible are distinct notions.\nWhether or not the system at some point was in one of the special\ninitial conditions for which the space and time mean fail to be equal\nis a factual question that cannot be settled by appeal to measures;\npointing out that such points are scarce in the sense of measure\ntheory does resolve the problem because it does not imply that they\nare scarce in the world as well. \nIn response two things can said. First, discounting sets of measure\nzero is standard practice in physics and the problem is not specific\nto ergodic theory. So unless there is a good reason to suspect that\nspecific measure zero states are in fact important, one might argue\nthat the onus of proof is on those who think that discounting them in\nthis case is illegitimate. Second, the fact that SM works in so many\ncases suggests that they indeed are scarce. \nThe third criticism is rarely explicitly articulated, but it is\nclearly in the background of contemporary Boltzmannian approaches to\nSM such as Albert’s (2000), which reject Boltzmann’s\nstarting point, namely the postulate \\(p(M_j) = c\\mu(M_j)\\). Albert\nintroduces an alternative postulate, essentially providing transition\nprobabilities between two macrostates conditional on the so-called\nPast Hypothesis, the posit that the universe came into existence in a\nlow entropy state (the Big Bang). Albert then argues that in such an\naccount erogidicity becomes an idle wheel, and hence he rejects it as\ncompletely irrelevant to the foundations of SM. This, however, may\nwell be too hasty. Although it is true that ergodicity simpliciter\ncannot justify Albert’s probability postulate, another dynamical\nassumption is needed in order for this postulate to be true (Frigg\n2010). \nAt the basis of Gibbs’ approach stands a conceptual shift. The\nobject of study in the Boltzmannian framework is an individual system,\nconsisting of a large but finite number of micro constituents. By\ncontrast, within the Gibbs framework the object of study is a\nso-called ensemble: an imaginary collection of infinitely\nmany copies of the same system (they are the same in that they have\nthe same phase space, dynamics and measure), but who happen to be in\ndifferent states. An ensemble of gases, for instance, consists of\ninfinitely many copies of the same gas bit in different states: one is\nconcentrated in the left corner of the box, one is evenly distributed,\netc. It is important to emphasise that ensembles are fictions, or\n‘mental copies of the one system under consideration’\n(Schrödinger 1952, 3); or alternatively they can be thought of as\ncollections of possible states of the entire system. Hence, it is\nimportant not to confuse ensembles with collections of micro-objects\nsuch as the molecules of a gas! \nThe instantaneous state of one system of the ensemble is specified by\none point in its phase space. The state of the ensemble as a\nwhole is therefore specified by a density function \\(\\varrho\\) on\nthe system’s phase space. From a technical point of view\n\\(\\varrho\\) is a function just like \\(f\\) that we encountered in\nSection 1. We furthermore assume that \\(\\varrho\\) is a probability\ndensity, reflecting the probability density of finding the state of a\nsystem chosen at random from the entire ensemble in region \\(R\\), so\nthat the probability of the state being in \\(R\\) is \\(p(R) = \\int_R\n\\varrho d\\mu\\). To make this more intuitive consider the following\nanalogy. You play a special kind of darts: you fix a plank to the\nwall, which serves as your dart board. For some reason you know that\nthe probability of your dart landing at a particular place on the\nboard is given by the curve shown in Figure 7. You are then asked what\nthe probability is that your next dart lands in the left half of the\nboard. The answer is 1 ⁄ 2 since one half of the surface\nunderneath the curve is on the left side. The dart board then plays\nthe role of the system’s state space, a region of the board\n(here the left half) plays the role of \\(R\\), and throwing a dart\nplays the role of picking a system from the ensemble. \nFigure 7: Dart board \nThe importance of this is that it allows us to calculate expectation\nvalues. Assume that the game is such that you get one Pound if the\ndart hits the left half and three Pounds if it lands on the right\nhalf. What is your expected gain? The answer is 1 ⁄ \\(2 \\times\n1\\) Pound \\(+ 1\\) ⁄ \\(2 \\times 3\\) Pounds \\(= 2\\) Pounds. This\nis the expectation value. The same idea is at work in SM. Physical\nmagnitudes like, for instance, pressure are associated with functions\n\\(f\\) on the phase pace. We then calculate the expectation value of\nthese magnitudes, which, in general is given by \\(\\langle f \\rangle =\n\\int fd\\mu\\). In the context of Gibbsian SM these expectation values\nare also referred to as phase averages or ensemble\naverages. They are of central importance because these values are\nused as predictions for observed values. So if you want to use the\nformalism to predict what will be observed in an experiment, you first\nhave to figure out what the probability density \\(\\varrho\\) is, then\nfind the function \\(f\\) corresponding to the physical quantity you are\ninterested in, and then calculate the phase average. Neither of these\nsteps is easy in practice and working physicists spend most of their\ntime doing these calculations. However, these difficulties need not\noccupy us if we are interested in the conceptual issues underlying\nthis ‘recipe’. \nBy definition, a probability density \\(\\varrho\\) is stationary if it\ndoes not change over time. Given that observable quantities are\nassociated with phase averages and that equilibrium is defined in\nterms of the constancy of the macroscopic parameters characterising\nthe system, it is natural to regard the stationarity of the\ndistribution as a necessary condition for equilibrium because\nstationary distributions yield constant averages. For this reason\nGibbs refers to stationarity as the ‘condition of statistical\nequilibrium’. \nAmong all stationary distributions those satisfying a further\nrequirement, the Gibbsian maximum entropy principle, play a\nspecial role. The Gibbs entropy (sometimes called\n‘ensemble entropy’) is defined as \nThe Gibbsian maximum entropy principle then requires that \\(S_G\n(\\varrho)\\) be maximal, given the constraints that are imposed on the\n system.[16] \nThe last clause is essential because different constraints single out\ndifferent distributions. A common choice is to keep both the energy\nand the particle number in the system fixed. One can prove that under\nthese circumstances \\(S_G (\\varrho)\\) is maximal for the so-called\nmicrocanonical distribution (or microcanonical\nensemble). If we choose to hold the number of particles constant\nwhile allowing for energy fluctuations around a given mean value we\nobtain the so-called canonical distribution; if we also allow\nthe particle number to fluctuate around a given mean value we find the\nso-called grand-canonical\n distribution.[17] \nThis formalism is enormously successful in that correct predictions\ncan be derived for a vast class of systems. But the success of this\nformalism is rather puzzling. The first and most obvious question\nconcerns the relation of systems and ensembles. The probability\ndistribution in the Gibbs approach is defined over an ensemble, the\nformalism provides ensemble averages, and equilibrium is regarded as a\nproperty of an ensemble. But what we are really interested in is the\nbehaviour of a single system! What could the properties of an\nensemble—a fictional entity consisting of infinitely many mental\ncopies of the real system—tell us about the one real system on\nthe laboratory table? And more specifically, why do averages over an\nensemble coincide with the values found in measurements performed on\nan actual physical system in equilibrium? There is no obvious reason\nwhy this should be so, and it turns out that ergodicity plays a\ncentral role in answering these questions. \nCommon textbook wisdom justifies the use of phase averages as follows.\nAs we have seen, the Gibbs formalism associates physical quantities\nwith functions on the system’s phase space. Making an\nexperiment measuring one of these quantities takes time and it is\nassumed that what measurement devices register is not the\ninstantaneous value of the function in question, but rather its time\naverage over the duration of the measurement. Hence, time averages are\nwhat is empirically accessible. Then, so the argument continues,\nalthough measurements take an amount of time that is short by human\nstandards, it is long compared to microscopic time scales on which\ntypical molecular processes take place. For this reason it is assumed\nthat the measured finite time average is approximately equal\nto the infinite time average of the measured function. If we\nnow assume that the system is ergodic, then time averages equal phase\naverages. The latter can easily be obtained from the formalism. Hence\nwe have found the sought-after connection: the Gibbs formalism\nprovides phase averages which, due to ergodicity, are equal to\ninfinite time averages, and these are, to a good approximation, equal\nto the finite time averages obtained from measurements. \nThis argument is problematic for at least two reasons. First, from the\nfact that measurements take some time it does not follow that what is\nactually measured are time averages. For instance, it could be the\ncase that the value provided to us by the measurement device is simply\nthe value assumed by at the last moment of the measurement,\nirrespective of what the previous values of were (e.g.\nit’s simply the last pointer reading registered). So we would\nneed an argument for the conclusion that measurements indeed produce\ntime averages. Second, even if we take for granted that\nmeasurements do produce finite time averages, equating these\naverages with infinite time averages is problematic. Even if the\nduration of the measurement is long by experimental standards (which\nneed not be the case), finite and infinite averages may assume very\ndifferent values. That is not to say that they necessarily have to be\ndifferent; they could coincide. But whether or not they do is\nan empirical question, which depends on the specifics of the system\nunder investigation. So care is needed when replacing finite with\ninfinite time averages, and one cannot identify them without further\nargument.  \nMalament and Zabell (1980) respond to this challenge by suggesting a\nway of explaining the success of equilibrium theory that still invokes\nergodicity, but avoids appeal to time averages. This solves the above\nmentioned problems, but suffers from the difficulty that many systems\nthat are successfully dealt with by the formalism of SM are not\nergodic. To circumvent this difficulty Vranas (1998) suggested\nreplacing ergodicity with what he calls \\(\\varepsilon\\)-ergodicity.\nIntuitively a system is \\(\\varepsilon\\)-ergodic if it is ergodic not\non the entire phase space, but on a very large part of it (those parts\non which it is not ergodic having measure \\(\\varepsilon\\), where\n\\(\\varepsilon\\) is very small). The leading idea behind his approach\nis to challenge the commonly held belief that even if a system is just\na ‘little bit’ non-ergodic, then it behaves in a\ncompletely ‘un-ergodic’ way. Vranas points out that there\nis a middle ground and then argues that this middle ground actually\nprovides us with everything we need. This is a promising proposal, but\nit faces three challenges. First, it needs to be shown that all\nrelevant systems really are \\(\\varepsilon\\)-ergodicity. Second, the\nargument so far has only been developed for the microcanonical\nensemble, but one would like to know whether, and if so how, it works\nfor the canonical and the grandcanonical ensembles. Third, it is still\nbased on the assumption that equilibrium is characterised by a\nstationary distribution, which, as we will see below, is an obstacle\nwhen it comes to formulating a workable Gibbsian non-equilibrium\ntheory. \nThe second response begins with Khinchin’s work. Khinchin (1949)\npointed out that the problems of the ergodic programme are due to the\nfact that it focuses on too general a class of systems. Rather than\nstudying dynamical systems at a general level, we should focus on\nthose cases that are relevant in statistical mechanics. This involves\ntwo restrictions. First, we only have to consider systems with a large\nnumber of degrees of freedom; second, we only need to take into\naccount a special class of phase functions, the so-called ‘sum\nfunctions’. These functions are a sum of one-particle functions,\ni.e., functions that take into account only the position and momentum\nof one particle. Under these assumption Khinchin proved that as \\(n\\)\nbecomes larger, the measure of those regions on the energy\n hypersurface[18]\n where the time and the space means differ by more than a small amount\ntends towards zero. Roughly speaking, this result says that for large\n\\(n\\) the system behaves, for all practical purposes, as if it was\nergodic. \nThe problem with this result is that it is valid only for sum\nfunctions, and in particular only if the energy function of the system\nis itself a sum function, which is not the case when particles\ninteract. So the question is how this result can be generalised to\nmore realistic cases. This problem stands at the starting point of a\nresearch programme now known as the thermodynamic limit,\nchampioned, among others, by Lanford, Mazur, Ruelle, and van der\nLinden (see van Lith (2001) for a survey). Its leading question is\nwhether one can still prove ‘Khinchin-like’ results in the\ncase of energy function with interaction\n terms.[19]\n Results of this kind can be proven in the limit for \\(n \\rightarrow\n\\infty\\), if also the volume \\(V\\) of the system tends towards\ninfinity in such a way that the number-density \\(n/V\\) remains\nconstant. \nSo far we have only dealt with equilibrium, and things get worse once\nwe turn to non-equilibrium. The main problem is that it is a\nconsequence of the formalism that the Gibbs entropy is a constant!\nThis precludes a characterisation of the approach to equilibrium in\nterms of increasing Gibbs entropy, which is what one would expect if\nwe were to treat the Gibbs entropy as the SM counterpart of the\nthermodynamic entropy. The standard way around this problem is to\ncoarse-grain the phase space, and then define the so-called coarse\ngrained Gibbs entropy. Put simply, course-graining the phase space\namounts to putting a grid on the phase space and declare that all\npoints within one cell of the grid are indistinguishable. This\nprocedure turns a continuous phase space into a discrete collection of\ncells, and the state of the system is then specified by saying in\nwhich cell the system’s state is. If we define the Gibbs entropy on\nthis grid, it turns out (for purely mathematical reasons) that the\nentropy is no longer a constant and can actually increase or decrease.\nIf one then assumes that the system is mixing, it follows from the\nso-called convergence theorem of ergodic theory that the\ncoarse-grained Gibbs entropy approaches a maximum. However, this\nsolution is fraught with controversy, the two main bones of contention\nbeing the justification of coarse-graining and the assumption that the\nsystem is mixing. \nIn sum, ergodicity plays a central role in many attempts to justify\nthe posits of SM. And even where a simplistic use of ergodicity is\neventually unsuccessful, somewhat modified notions prove fruitful in\nan analysis of the problem and in the search for better solutions. \nEH is often presented as a hierarchy of increasing degrees of\nrandomness in deterministic systems: the higher up in this hierarchy a\nsystem is placed the more random its\n behaviour.[20]\n However, the definitions of different levels of EH do not make\nexplicit appeal to randomness; nor does the usual way of presenting EH\ninvolve a specification of the notion of randomness that is supposed\nto underlie the hierarchy. So there is a question about what notion of\nrandomness underlies EH and in what sense exactly EH is a hierarchy of\nrandom behaviour. \nBerkovitz, Frigg and Kronz (2006) discuss this problem and argue that\nEH is best understood as a hierarchy of random behaviour if randomness\nis explicated in terms of unpredictability, where unpredictability is\naccounted for in terms of probabilistic relevance. Different patterns\nof probabilistic relevance, in turn, are spelled out in terms of\ndifferent types of decay of correlation between a system’s\nstates at different times. Let us introduce these elements one at a\ntime. \nProperties of systems can be associated with different parts of the\nphase space. In the ball example, for instance, the property\nhaving positive momentum is associated with the right half of\nthe phase space; that is, it is associated with the set \\(\\{x \\in X\n\\mid p \\gt 0\\}\\). Generalising this idea we say that to every subset\n\\(A\\) of a system’s phase space there corresponds a property\n\\(P_A\\) so that the system possesses that property at time \\(t\\) iff\nthe system’s state \\(x\\) is in \\(A\\) at \\(t\\). The subset \\(A\\)\nmay be arbitrary and the property corresponding to \\(A\\) may not be\nintuitive, unlike, for example, the property of having positive\nmomentum. But nothing in the analysis to follow hangs on a\nproperty being ‘intuitive’. We then define the\nevent \\(A^t\\) as the obtaining of \\(P_A\\) at time \\(t\\). \nAt every time \\(t\\) there is a matter of fact whether \\(P_A\\) obtains,\nwhich is determined by the dynamics of the system. However, we may not\nknow whether or not this is the case. We therefore introduce epistemic\nprobabilities expressing our uncertainty about whether \\(P_A\\)\nobtains: \\(p(A^t)\\) reflects an agent’s degree of belief in\n\\(P_A\\)’s obtaining at time \\(t\\). In the same way we can\nintroduce conditional probabilities: \\(p(A^t \\mid B^{t_1})\\) is our\ndegree of belief in the system having \\(P_A\\) at \\(t\\) given that it\nhad \\(P_B\\) at an earlier time \\(t_1\\), where \\(B\\) is also a subset\nof the system’s phase space. By the usual rule of conditional\nprobability we have \\(p(A^t \\mid B^{t_1}) = p(A^t \\amp B^{t_1}) /\n(p(B^{t_1})\\). This can of course be generalised to more then one\nevent: \\(p(A^t \\mid B_{1}^{t_1} \\amp \\ldots \\amp B_{r}^{t_r})\\) is our\ndegree of belief in the system having \\(P_A\\) at \\(t\\) given that it\nhad \\(P_{B_1}\\) at \\(t_1, P_{B_2}\\) at \\(t_2,\\ldots\\), and \\(P_{B_{\nr}}\\) at \\(t_r\\), where \\(B_1 ,\\ldots ,B_r\\) are subsets of the\nsystem’s phase space (and \\(r\\) a natural number), and \\(t_1\n,\\ldots ,t_r\\) are successive instants of time (i.e., \\(t \\gt t_1 \\gt\n\\ldots \\gt t_r)\\).  \nIntuitively, an event in the past is relevant to our making\npredictions if taking the past event into account makes a difference\nto our predictions, or more specifically if it lowers or raises the\nprobability for a future event. In other words, \\(p(A^t \\mid\nB_{1}^{t_1}) - p(A^t)\\) is a measure of the relevance of \\(B^{t_1}\\)\nto predicting \\(A^t : B^{t_1}\\) is positively relevant if the \\(p(A^t\n\\mid B_{1}^{t_1}) - p(A^t) \\gt 0\\), negatively relevant if \\(p(A^t\n\\mid B_{1}^{t_1}) - p(A^t) \\lt 0\\), and irrelevant if \\(p(A^t \\mid\nB_{1}^{t_1}) - p(A^t) = 0\\). For technical reasons it turns out to be\neasier to work with a slightly different but equivalent notion of\nrelevance, which is obtained from the above by multiplying both sides\nof the equation by \\(p(B^{t_1})\\). Therefore we adopt the following\ndefinition. The relevance of \\(B^{t_1}\\) for \\(A^t\\) is \nThe generalisation of this definition to cases with more than one set\n\\(B\\) (as above) is straightforward. \nRelevance serves to explicate unpredictability. Intuitively, the less\nrelevant past events are for \\(A^t\\), the less predictable the system\nis. This basic idea can then be refined in various ways. First, the\ntype of unpredictability we obtain depends on the type of events to\nwhich (R) is applied. For instance, the degree of the unpredictability\nof \\(A^t\\) increases if its probability is independent not only of\n\\(B^{t_1}\\) or other ‘isolated’ past events, but rather\nthe entire past. Second, the unpredictability of an event \\(A^t\\)\nincreases if the probabilistic dependence of that event on past events\n\\(B^{t_1}\\) decreases rapidly with the increase of the temporal\ndistance between the events. Third, the probability of \\(A^t\\) may be\nindependent of past events simpliciter, or it may be\nindependent of such events only on average. These ideas underlie the\nanalysis of EH as a hierarchy of unpredictability. \nBefore we can provide such an analysis, two further steps are needed.\nFirst, if the probabilities are to be useful to understanding\nrandomness in a dynamical system, the probability assignment\nhas to reflect the properties of the system. So we have to connect the\nabove probabilities to features of the system. The natural choice is\nthe system’s measure\n \\(\\mu\\).[21]\n So we postulate that the probability of an event \\(A^t\\) is equal to\nthe measure of the set \\(A: p(A^{t}) = \\mu(A)\\) for all \\(t\\). This\ncan be generalised to joint probabilities as follows: \nfor all instants of time \\(t \\gt t_1\\) and all subsets \\(A\\) and \\(B\\)\nof the system’s phase space. \\(T_{t_1 \\rightarrow t}B\\) is the\nimage of the set \\(B\\) under the dynamics of the system from \\(t_1\\)\nto \\(t\\). We refer to this postulate as the Probability\nPostulate (P), which is illustrated in Figure 8. Again, this\ncondition is naturally generalised to cases of joint probabilities of\n\\(A^t\\) with multiple events \\(B^{t_i}\\). Granted (P) and its\ngeneralization, (R) reflects the dynamical properties of systems.  \nFigure 8: Condition (P). \nBefore briefly introducing the next element of the analysis let us\nmention that there is a question about whether the association of\nprobabilities with the measure of the system is reasonable. Prima\nfacie, a measure on a phase space can have a purely geometrical\ninterpretation and need not necessarily have anything to do with the\nquantification of uncertainty. For instance, we can use a measure to\ndetermine the length of a table, but this measure need not have\nanything to do with uncertainty. Whether or not such an association is\nlegitimate depends on the cases at hand and the interpretation of the\nmeasure. However, for systems of interest in statistical physics it is\nnatural and indeed standard to assume that the probability of the\nsystem’s state to be in a particular subset of the phase space\n\\(X\\) is proportional to the measure of \\(A\\). \nThe last element to be introduced is the notion of the correlation\nbetween two subsets \\(A\\) and \\(B\\) of the system’s phase space,\nwhich is defined as follows: \nIf the value of \\(C(A, B)\\) is positive (negative), there is positive\n(negative) correlation between \\(A\\) and \\(B\\); if it is zero, then\n\\(A\\) and \\(B\\) are uncorrelated. It then follows immediately from the\nabove that \n(RC) constitutes the basis for the interpretation of EH as a hierarchy\nof objective randomness. Granted this equation, the subjective\nprobabilistic relevance of the event \\(B^{t_1}\\) for the event \\(A^t\\)\nreflects objective dynamical properties of the system since for\ndifferent transformations \\(T\\ R(B^{t_1}, A^t)\\) will indicate\ndifferent kinds of probabilistic relevance of \\(B^{t_1}\\) for \\(A^t\\).\n \nTo put (RC) to use, it is important to notice that the equations\ndefining the various levels of EH above can be written in terms of\ncorrelations. Taking into account that we are dealing with discrete\nsystems (and hence we have \\(T_{t_1\\rightarrow t}B = T_k B\\) where\n\\(k\\) is the number of time steps it takes to get from \\(t_1\\) and\n\\(t)\\), these equations read: \nApplying (RC) to these expressions, we can explicate the nature of the\nunpredictability that each of the different levels of EH involves.\n \nLet us start at the top of EH. In Bernoulli systems the probabilities\nof the present state are totally independent of whatever happened in\nthe past, even if the past is only one time step back. So knowing the\npast of the system does not improve our predictive abilities in the\nleast; the past is simply irrelevant to predicting the future. This\nfact is often summarised in the slogan that Bernoulli systems are as\nrandom as a coin toss. We should emphasise, however, that this is true\nonly for events in the Bernoulli partition; the characterisation of a\nBernoulli system is silent about what random properties partitions\nother than the Bernoulli partition have. \nK-mixing is more difficult to analyse. We now have to tackle the\nquestion of how to understand \\(\\sigma(n, r)\\), the minimal\n\\(\\sigma\\)-algebra generated by the set \nthat we sidestepped earlier on. What matters for our analysis is that\nthe following types of sets are members of \nwhere the indices \\(j_i\\) range over \\(1, \\ldots ,r\\). Since we are\nfree to chose the sets A\\(_0\\), A\\(_1,\\ldots\\), A\\(_r\\) as we please,\nwe can always chose them so that they are the past history of the\nsystem: the system was in \\(A_{j_0}\\) \\(k\\) time steps back, in\n\\(A_{j_1}\\) \\( k+1\\) time steps back, etc. Call this the\n(coarse-grained) remote past of the\nsystem—‘remote’ because we only consider states that\nare more than \\(k\\) time steps back. The K-mixing condition then says\nthat the system’s entire remote past history becomes\nirrelevant to predicting what happens in the future as time tends\ntowards infinity. Typically Bernoulli systems are compared with\nK-systems by focussing on the events in the Bernoulli partition. With\nrespect to that partition K is weaker than Bernoulli.  The difference\nis both in the limit and the remote history. In a Bernoulli system the\nfuture is independent of the entire past (not only the remote\npast), and this is true without taking a limit (in the case of\nK-mixing independence only obtains in the limit).  However, this only\nholds for the Bernoulli partition; it may or may not hold for other\npartitions—the definition of a Bernoulli system says nothing\nabout that case.[22] \nThe interpretation of strong mixing is now straightforward. It says\nthat for any two sets \\(A\\) and \\(B\\), having been in \\(B\\) \\(k\\) time\nsteps back becomes irrelevant to the probability of being in \\(A\\)\nsome time in the future if time tends towards infinity (i.e. when n\ntends to infinity). In other words, past events \\(B\\) become\nincreasingly irrelevant for the probability of \\(A\\) as the temporal\ndistance between \\(A\\) and \\(B\\) becomes larger. This condition is\nweaker than K-mixing because it only states that the future is\nindependent of isolated events in the remote past, while K-mixing\nimplies independence of the entire remote past history. \nIn weakly mixing systems the past may be relevant to predicting the\nfuture, even in the remote past. The weak mixing condition only says\nthat this influence has to be weak enough for it to be the case that\nthe absolute value of the correlations between a future event and past\nevents vanishes on average; but this does not mean that all individual\ncorrelations vanish. So in weakly mixing systems events in the past\ncan remain relevant to the future. \nErgodicity, finally, implies no decay of correlation at all. The\nergodicity condition only says that the average of the correlations\n(and this time without an absolute value) of all past events with a \nfuture event is zero. But this is compatible with there being\nstrong correlations between every instant in the past and the future,\nprovided that positive and negative correlations average out. So in\nergodic systems the past does not become irrelevant. For this reason\nergodic system are not random at all (in the sense of random\nintroduced above). \nHow relevant are these insights to understanding the behaviour of\nactual systems? A frequently heard objection (which we have already\nencountered in Section 4) is that EH and more generally ergodic theory\nare irrelevant since most systems (including those that we are\nultimately interested in) are not ergodic at\n all.[23] \nThis charge is less acute than it appears at first glance. First, it\nis important to emphasise that it is not the sheer number of\napplications that make a physical concept important, but whether there\nare some important systems that are ergodic. And there are examples of\nsuch systems. For example, so-called ‘hard-ball systems’\n(and some more sophisticated variants of them) are effective\nidealizations of the dynamics of gas molecules, and these systems seem\nto be ergodic;for details, see Berkovitz, Frigg and Kronz 2006,\nSection 3.2, Vranas (1998) and Frigg and Werndl (2011). \nFurthermore, EH can be used to characterize randomness and chaos in\nboth ergodic and non-ergodic systems. Even if a system as a whole is\nnot ergodic (i.e., if it fails to be ergodic with respect to the\nentire phase space \\(X)\\), there can be (and usually there\nare) subsets of \\(X\\) on which the system is ergodic. This is what\nLichtenberg and Libermann (1992, p. 295) have in mind when they\nobserve that ‘[i]n a sense, ergodicity is universal, and the\ncentral question is to define the subspace over which it\nexists’. In fact, non-ergodic systems may have subsets that are\nnot only ergodic, but even Bernoulli! It then becomes interesting\nto ask what these subsets are, what their measures are, and what\ntopological features they have. These are questions studied in parts\nof dynamical systems theory, most notably KAM theory. Hence, KAM\ntheory does not demonstrate that ergodic theory is not useful in\nanalyzing the dynamical behavior of real physical systems (as is often\nclaimed). Indeed, KAM systems have regions in which the system\nmanifest either merely ergodic or Bernoulli behaviour, and accordingly\nEH is useful for charactering the dynamical properties of such systems\n(Berkovitz, Frigg and Kronz 2006, Section 4). Further, as we have\nmentioned in Section 4.1, almost all Hamiltonian systems are\nnon-integrable, and accordingly they have large regions of the phase\nspace in which their motion is ergodic-like. So EH is a useful tool in\nstudying the dynamical properties of systems even if the system fails\nto be ergodic tout court. \nAnother frequently heard objection is that EH is irrelevant in\npractice because most levels of EH (in fact, all except Bernoulli) are\ndefined in terms of infinite time limits and hence remain\nsilent about what happens in finite time. But all we ever\nobserve are finite times and so EH is irrelevant to physics as\npracticed by actual scientists. \nThis charge can be dispelled by a closer look at the definition of a\nlimit, which shows that infinite limits in fact have\nimportant implications for the dynamical behaviour of the system in\nfinite times. The definition of a limit is as follows (where \\(f\\) is\nan arbitrary function of time): lim\\(_{t\\rightarrow \\infty} f(t) = c\\)\niff for every \\(\\varepsilon \\gt 0\\) there exists an \\(t’ \\gt 0\\) so\nthat for all \\(t \\gt t’\\) we have \\(\\lvert f(t) - c\\rvert \\lt\n\\varepsilon\\). In words, for every number \\(\\varepsilon\\), no matter\nhow small, there is a finite time \\(t’\\) after which the\nvalues of \\(f\\) differ from \\(c\\) by less then \\(\\varepsilon\\). That\nis, once we are past \\(t’\\) the values of \\(f\\) never move more than\n\\(\\varepsilon\\) away from \\(c\\). With this in mind strong mixing, for\ninstance, says that for a given threshold \\(\\varepsilon\\) there exists\na finite time \\(t_n (n\\) units of time after the current\ntime) after which \\(C(T_n B, A)\\) is always smaller than\n\\(\\varepsilon\\). We are free to choose \\(\\varepsilon\\) to be an\nempirically relevant margin, and so we know that if a system is\nmixing, we should expect the correlations between the states of the\nsystem after t\\(_n\\) and its current state to be below\n\\(\\varepsilon\\). The upshot is that in strong mixing systems, being in\na state \\(B\\) at some past time becomes increasingly irrelevant for\nits probability of being in the state \\(A\\) now, as the temporal\ndistance between \\(A\\) and \\(B\\) becomes larger. Thus, the fact that\nsystem is strong mixing clearly has implications for its dynamical\nbehaviour in finite times. Furthermore, often (although not always)\nconvergence proofs provide effective bounds on rates of convergence\nand these bounds can be used to inform expectations about behaviour at\na given time. \nSince different levels of EH correspond to different degrees of\nrandomness, each explicated in terms of a different type of asymptotic\ndecay of correlations between states of systems at different times,\none might suspect that a similar pattern can be found in the rates of\ndecay. That is, one might be tempted to think that EH can equally be\ncharacterized as a hierarchy of increasing rates of decay of\ncorrelations: a K-system, for instance, which exhibits exponential\ndivergence of trajectories would be characterized by an exponential\nrate of decay of correlations, while a SM-system would exhibit a\npolynomial rate of decay. \nThis, unfortunately, does not work. Natural as it may seem, EH cannot\nbe interpreted as a hierarchy of increasing rates of decay of\ncorrelations. It is a mathematical fact that there is no particular\nrate of decay associated with each level of EH. For instance, one can\nconstruct K-systems in which the decay is as slow as one wishes it to\nbe. So the rate of decay is a feature of particular systems rather\nthan of a level of EH. \nThe question of how to characterise chaos has been\ncontroversially discussed ever since the inception of chaos theory;\nfor a survey see Smith (1998, Ch. 10). An important family of\napproaches defines chaos using EH. Belot and Earman (1997, 155) state\nthat being strong mixing is a necessary condition and being a K-system\nis a sufficient condition for a system to be chaotic. The view that\nbeing a K-system is the mark of chaos and that any lower degree of\nrandomness is not chaotic is frequently motivated by two ideas. The\nfirst is the idea that chaotic behaviour involves dynamical\ninstability in the form of exponential divergence of nearby\ntrajectories. Thus, since a system involves an exponential divergence\nof nearby trajectories only if it is a K-system, it is concluded that\n(merely) ergodic and mixing systems are not chaotic whereas K- and\nB-systems are. It is noteworthy, however, that SM is compatible with\nthere being polynomial divergence of nearby trajectories and that such\ndivergence sometimes exceeds exponential divergence in the short run.\nThus, if chaos is to be closely associated with the rate of divergence\nof nearby trajectories, there seems to be no good reason to deny that\nSM systems exhibit chaotic behaviour. \nThe second common motivation for the view that being a K-system is the\nmark of chaos is the idea that the shift from zero to positive\nKS-entropy marks the transition from a ‘regular’ to\n‘chaotic’ behaviour. This may suggest that having positive\nKS-entropy is both necessary and sufficient condition for chaotic\nbehaviour. Thus, since K-systems have positive KS-entropy while SM\nsystems don’t, it is concluded that K-systems are chaotic\nwhereas SM-systems are not. Why is KS-entropy a mark of chaos? There\nare three motivations, corresponding to three different\ninterpretations of KS-entropy. First, KS-entropy could be interpreted\nas entailing dynamical instability in the sense of having nearby\ndivergence of nearby trajectories (see Lichtenberg & Liebermann,\n1992, p. 304). Second, KS-entropy could be connected to algorithmic\ncomplexity (Brudno 1978). Yet, while such a complexity is sometimes\nmentioned as an indication of chaos, it is more difficult to connect\nit to physical intuitions about chaos. Third, KS-entropy could be\ninterpreted as a generalized version of Shannon’s information\ntheoretic entropy (see Frigg 2004). According to this approach,\npositive KS-entropy entails a certain degree of unpredictability,\nwhich is sufficiently high to deserve the title\n chaotic.[24] \nWerndl (2009b) argues that a careful review of all systems that one\ncommonly regards as chaotic shows that strong mixing is the crucial\ncriterion: a system is chaotic just in case it is strong mixing. As\nshe is careful to point out, this claim needs to be qualified: systems\nare rarely mixing on the entire phase space, but neither are they\nchaotic on the entire phase space. The crucial move is to restrict\nattention to those regions of phase space where the system is chaotic,\nand it then turns out that in these same regions the systems are also\nstrong mixing. Hence Werndl concludes that strong mixing is the\nhallmark of chaos. And surprisingly this is true also of dissipative\nsystems (i.e., systems that are not measure preserving). These systems\nhave attractors, and they are chaotic on their attractors rather than\non the entire phase space. The crucial point then is that one can\ndefine an invariant (preserved) measure on the attractor and\nshow that the system is strongly mixing with respect to that measure.\nSo strong mixing can define chaos in both conservative and dissipative\nsystems. \nThe search for necessary and sufficient conditions for chaos\npresupposes that there is a clear-cut divide between chaotic and\nnon-chaotic systems. EH may challenge this view, as every attempt to\ndraw a line somewhere to demarcate the chaotic from non-chaotic\nsystems is bound to be somewhat arbitrary. Ergodic systems are pretty\nregular, mixing systems are less regular and the higher positions in\nthe hierarchy exhibit still more haphazard behaviour. But is there one\nparticular point where the transition from ‘non-chaos’ to\nchaos takes place? Based on the argument that EH is a hierarchy of\nincreasing degrees of randomness and degrees of randomness correspond\nto different degrees of unpredictability (see Section 5), Berkovitz,\nFrigg and Kronz (2006, Section 5.3) suggest that chaos may well be\nviewed as a matter of degree rather than an all-or-nothing affair.\nBernoulli systems are very chaotic, K-systems are slightly less\nchaotic, SM-systems are still less chaotic, and ergodic systems are\nnon-chaotic. This suggestion connects well with the idea that chaos is\nclosely related to unpredictability. \nThe ergodic hierarchy has also been used to understand quantum chaos.\nCastagnino and Lombardi (2007) analyze the problem of quantum chaos as\na particular case of the classical limit of quantum mechanics and\nidentify mixing in the classical limit as the condition that a quantum\nsystem must satisfy to be nonintegrable. Gomez and Castagnino (2014,\n2015) generalize the entire ergodic hierarchy to the quantum context\nand argue that EH thus generalized is a helpful tool to understand\nquantum chaos; Fortin and Lombardi (2018) use EH to understand\ndecoherence; and Gomez (2018) discusses the KS entropy in quantum\nmixing systems. \nMixing, finally, has also been invoked in understanding the effects of\nstructural model error. Frigg, Bradley, Du and Smith (2014) argue that\nthe distinction between parameter error and structural model error is\ncrucial, and that the latter has significant and hitherto\nunappreciated impact on the predictive ability of a model. Wilson-Mayo\n(2015) points out that to put this observation on a solid foundation\nwe need a notion of structural chaos. He proposes such a notion by\nappealing to topological mixing.  \nEH is often regarded as relevant for explicating the nature of\nrandomness in deterministic dynamical systems. It is not clear,\nhowever, what notion of randomness this claim invokes. The formal\ndefinitions of EH do not make explicit appeal to randomness and the\nusual ways of presenting EH do not involve any specification of the\nnotion of randomness that is supposed to underlie EH. As suggested in\nSection 5, EH can be interpreted as a hierarchy of randomness if\ndegrees of randomness are explicated in terms of degrees of\nunpredictability, which in turn are explicated in terms of (coherent)\nconditional degrees of beliefs. In order for these degrees of belief\nto be indicative of the system’s dynamical properties, they have\nto be updated according to a system’s dynamical law. The idea is\nthen that the different levels of EH, except for merely ergodic\nsystems, correspond to different kinds of unpredictability, which\ncorrespond to different patterns of decay of correlations between\nsystems’ past states and their present states. Merely ergodic\nsystems seem to display no randomness, as the correlations between\ntheir past and present states need not decay at all. \nErgodic theory plays an important role in statistical physics, and EH,\nor some modification of it, constitutes an important measure of\nrandomness in both Hamiltonian and dissipative systems. It is\nsometimes argued that EH is by and large irrelevant for physics\nbecause real physical systems are not ergodic. But, this charge is\nunwarranted, and a closer look at non-ergodic systems reveals a rather\ndifferent picture, because EH can be fruitfully be used in the\nfoundations of statistical mechanics, analyses of randomness, and\nchaos theory. More recently it has also played a role in understanding\nlaws of nature (Filomeno 2019, List and Pivato 2019).","contact.mail":"r.p.frigg@lse.ac.uk","contact.domain":"lse.ac.uk"},{"date.published":"2011-04-13","date.changed":"2020-07-16","url":"https://plato.stanford.edu/entries/ergodic-hierarchy/","author1":"Roman Frigg","author2":"Joseph Berkovitz","author1.info":"http://www.lse.ac.uk/collections/philosophyLogicAndScientificMethod/WhosWho/staffhomepages/frigg.htm","entry":"ergodic-hierarchy","body.text":"\n\n\nThe Ergodic Hierarchy (EH) is a central part of ergodic theory. It is\na hierarchy of properties that dynamical systems can possess. Its five\nlevels are ergodicity, weak mixing, strong mixing, Kolmogorov, and\nBernoulli. Although EH is a mathematical theory, its concepts have\nbeen widely used in the foundations of statistical physics, accounts\nof randomness, and discussions about the nature of chaos, as well as\nin other sciences such economics. We introduce EH and discuss its\napplications.\n\nThe object of study in ergodic theory is a dynamical system. We first\nintroduce some basic concepts with a simple example, from which we\nabstract the general definition of a dynamical system. For a brief\nhistory of the modern notion of a dynamical system and the associated\nconcepts of EH see the\n Appendix,\n Section A. \nA lead ball is hanging from the ceiling on a spring. We then pull it\ndown a bit and let it go. The ball begins to oscillate. The mechanical\nstate of the ball is completely determined by a specification of the\nposition \\(x\\) and the momentum \\(p\\) of its center of mass; that is,\nif we know \\(x\\) and \\(p\\), then we know all that there is to know\nabout the mechanical state of the ball. If we now conjoin \\(x\\) and\n\\(p\\) in one vector space we obtain the so-called phase space\n\\(X\\) of the system (sometimes also referred to as ‘state\n space’).[1]\n This is illustrated in Figure 1 for a two-dimensional phase space of\nthe state of a ball moving up and down (i.e., the phase space has one\ndimension for the ball’s position and one for its momentum). \nFigure 1: The motion of a ball on a\nspring. \nEach point of \\(X\\) represents a state of the ball (because it gives\nthe ball’s position and momentum). Accordingly, the time\nevolution of the ball’s state is represented by a line in \\(X\\),\na so-called phase space trajectory (from now on\n‘trajectory’), showing where in phase space the system was\nat each instant of time. For instance, let us assume that at time \\(t\n= 0\\) the ball is located at point \\(x_1\\) and then moves to \\(x_2\\)\nwhere it arrives at time \\(t = 5\\). This motion is represented in\n\\(X\\) by the line segment connecting points \\(\\gamma_1\\) and\n\\(\\gamma_2\\). In other words, the motion of the ball is represented in\n\\(X\\) by the motion of a point representing the ball’s\n(instantaneous) state, and all the states that the ball is in over the\ncourse of a certain period of time jointly form a trajectory. The\nmotion of this point has a name: it is the phase flow\n\\(\\phi_t\\). The phase flow tells us where the ball is at some later\ntime \\(t\\) if we specify where it is at \\(t = 0\\); or, metaphorically\nspeaking, \\(\\phi_t\\) drags the ball’s state around in \\(X\\) so\nthat the movement of the state represents the motion of the real ball.\nIn other words, \\(\\phi_t\\) is a mathematical representation of the\nsystem’s time evolution. The state of the ball at time \\(t = 0\\)\nis commonly referred to as the initial condition. \\(\\phi_t\\)\nthen tells us, for every point in phase space, how this point evolves\nif it is chosen as an initial condition. In our concrete example, point\n\\(\\gamma_1\\) is the initial condition and we have \\(\\gamma_2 =\n\\phi_{t=5}(\\gamma_1)\\). More generally, let us call the ball’s\ninitial condition \\(\\gamma_0\\) and let \\(\\gamma(t)\\) be its state at\nsome later time \\(t\\). Then we have \\(\\gamma(t) = \\phi_t (\\gamma_0)\\).\nThis is illustrated in figure 2a. \nFigure 2: Evolution in Phase space. \nSince \\(\\phi_t\\) tells us for every point in \\(X\\) how it evolves in\ntime, it also tells us how sets of points move around. For instance,\nchoose an arbitrary set \\(A\\) in \\(X\\); then \\(\\phi_t (A)\\) is the\nimage of A after \\(t\\) time units under the dynamics of the\nsystem. This is illustrated in Figure 2b. Considering sets of points\nrather than single points is important when we think about physical\napplications of this mathematical formalism. We can never determine\nthe exact initial condition of a ball bouncing on a spring. No matter\nhow precisely we measure \\(\\gamma_0\\), there will always be some\nmeasurement error. So what we really want to know in practical\napplications is not how a precise mathematical point evolves, but\nrather how a set of points around the initial condition \\(\\gamma_0\\)\nevolves. In our example with the ball the evolution is\n‘tame’, in that the set keeps its original shape. As we\nwill see below, this is not always the case. \nAn important feature of \\(X\\) is that it is endowed with a so-called\nmeasure \\(\\mu\\). We are familiar with measures in many\ncontexts: from a mathematical point of view, the length that we\nattribute to a part of a line, the surface we attribute to a part of a\nplane, and the volume we attribute to a segment of space are measures.\nA measure is simply a device to attribute a ‘size’ to a\npart of a space. Although \\(X\\) is an abstract mathematical space, the\nleading idea of a measure remains the same: it is a tool to quantify\nthe size of a set. So we say that the set \\(A\\) has measure \\(\\mu(A)\\)\nin much the same way as we say that a certain collection of points of\nordinary space (for instance the ones that lie on the inside of a\nbottle) have a certain volume (for instance one litre). \nFrom a more formal point of view, a measure assigns numbers to certain\nsubsets of a set \\(X\\) (see Appendix B for a formal definition). This\ncan be done in different ways and hence there are different measures.\nConsider the example of a plane. There is a measure that simply\nassigns to each appropriate region of a plane the area of that region.\nBut now imagine that we pour a bucket of sugar on the plane. The sugar\nis not evenly distributed; there are little heaps in some places while\nthere is almost no sugar in other places. A measure different from the\narea measure is one that assigns to a region a number that is equal to\nthe amount of sugar on that region. One of these measures is\nparticularly important, namely the so-called Lebesgue\nmeasure. This measure has an intuitive interpretation: it is just\na precise formalisation of the measure we commonly use in geometry.\nThe interval [0, 2] has Lebesgue measure 2 and the interval [3, 4] has\nLebegues measure 1. In two dimensions, a square whose sides have\nLebesgue measure 2 has Lebesgue measure 4; etc. Although this sounds\nsimple, the mathematical theory of measures is rather involved. We\nstate the basics of measure theory in the\n Appendix,\n Section B, and avoid appeal to technical issues in measure theory in\nwhat follows. \nThe essential elements in the discussion so far were the phase space\n\\(X\\), the time evolution \\(\\phi_t\\), and the measure \\(\\mu\\). And\nthese are also the ingredients for the definition of an abstract\ndynamical system. An abstract dynamical system is a triple \\([X, \\mu ,\nT_t]\\), where \\(\\{T_t \\mid t \\text{ is an instant of time}\\}\\) is a\nfamily of automorphisms, i.e., a family of transformations of \\(X\\)\nonto itself with the property that \\(T_{t_1 +t_2} = T_{t_1}(T_{t_2})\\)\nfor all \\(x \\in X\\) (Arnold and Avez 1968, 1); we say more about time\n below.[2]\n In the above example \\(X\\) is the phase space of the ball’s\nmotion, \\(\\mu\\) is the Lebesgue measure, and \\(T_t\\) is\n\\(\\phi_t\\). \nSo far we have described \\(T_t\\) as giving the time evolution of a\nsystem. Now let us look at this from a more mathematical point of\nview: the effect of \\(T_t\\) is that it assigns to every point in \\(X\\)\nanother point in \\(X\\) after \\(t\\) time units have elapsed. In the\nabove example \\(\\gamma_1\\) is mapped onto \\(\\gamma_2\\) under\n\\(\\phi_t\\) after \\(t = 5\\) seconds. Hence, from a mathematical point\nof view the time evolution of a system consists in a mapping of \\(X\\)\nonto itself, which is why the above definition takes \\(T_t\\) to be a\nfamily of mappings of \\(X\\) onto itself. Such a mapping is a\nprescription that tells you for every point \\(x\\) in \\(X\\) on which\nother point in \\(X\\) it is mapped (from now on we use \\(x\\) to denote\nany point in \\(X\\), and it no longer stands, as in the above example,\nfor the position of the ball).  \nThe systems studied in ergodic theory are forward deterministic. This\nmeans that if two identical copies of that system are in the same\nstate at one instant of time, then they must be in the same\nstate at all future instants of time. Intuitively speaking,\nthis means that for any given time there is only one way in which the\nsystem can evolve forward. For a discussion of determinism see Earman\n(1986). \nIt should be pointed out that no particular interpretation is intended\nin an abstract dynamical system. We have motivated the definition with\nan example from mechanics, but dynamical systems are not tied to that\ncontext. They are mathematical objects in their own right, and as such\nthey can be studied independently of particular applications. This\nmakes them a versatile tool in many different domains. In fact,\ndynamical systems are used, among others, in fields as diverse as\nphysics, biology, geology and economics.  \nThere are many different kinds of dynamical systems. The three most\nimportant distinctions are the following. \nDiscrete versus continuous time. We may consider discrete\ninstants of time or a continuum of instants of time. For ease of\npresentation, we shall say in the first case that time is discrete and\nin the second case that time is continuous. This is just a convenient\nterminology that has no implications for whether time is fundamentally\ndiscrete or continuous. In the above example with the ball time was\ncontinuous (it was taken to be a real number). But often it is\nconvenient to regard time as discrete. If time is continuous, then\n\\(t\\) is a real number and the family of automorphisms is \\(\\{T_t \\mid\nt \\in \\mathbb{R} \\}\\), where \\(\\mathbb{R}\\) is the set of real numbers. If time is\ndiscrete, then \\(t\\) is in the set \\(\\mathbb{Z} = \\{\\ldots -2, -1, 0,\n1, 2, \\ldots \\}\\), and the family of automorphisms is \\(\\{T_t \\mid t\n\\in \\mathbb{Z}\\}\\). In order to indicate that we are dealing with a\ndiscrete family rather than a continuous one we sometimes replace\n‘\\(T_t\\)’ with ‘\\(T_n\\)’; this is just a\nnotational convention of no conceptual\n importance.[3]\n In such systems the progression from one instant of time to the next\nis also referred to as a ‘step’. In population biology,\nfor instance, we often want to know how a population grows over a\ntypical breeding time (e.g. one year). In mathematical models of such\na population the points in \\(X\\) represent the size of a population\n(rather than the position and the momentum of a ball, as in the above\nexample), and the transformation \\(T_n\\) represents the growth of the\npopulation after \\(n\\) time units. A simple example would be \\(T_n = x\n+ n\\). \nDiscrete families of automorphisms have the interesting property that\nthey are generated by one mapping. As we have seen above, all\nautomorphisms satisfy \\(T_{t_1 +t_2} = T_{t_1}(T_{t_2})\\). From this\nit follows that \\(T_n (x) = T^{n}_1 (x)\\), that is \\(T_n\\) is the\n\\(n\\)-th iterate of \\(T_1\\). In this sense \\(T_1\\) generates \\(\\{T_t\n\\mid t \\in \\mathbb{Z}\\}\\); or, in other words, \\(\\{T_t \\mid t \\in\n\\mathbb{Z}\\}\\) can be ‘reduced’ to \\(T_1\\). For this\nreason one often drops the subscript ‘1’, simply calls the\nmap ‘\\(T\\)’, and writes the dynamical system as the triple\n\\([X, \\mu , T]\\), where it is understood that \\(T = T_1\\). \nFor ease of presentation we use discrete transformations from now on.\nThe definitions and theorems we formulate below carry over to\ncontinuous transformations without further ado, and where this is not\nthe case we explicitly say so and treat the two cases separately. \nMeasure preserving versus non-measure preserving\ntransformations. Roughly speaking, a transformation is measure\npreserving if the size of a set (like set \\(A\\) in the above example)\ndoes not change over the course of time: a set can change its form but\nit cannot shrink or grow (with respect to the measure). Formally,\n\\(T\\) is a measure-preserving transformation on \\(X\\) if and\nonly if (iff) for all sets \\(A\\) in \\(X: \\mu(A) = \\mu(T^{-1}(A))\\),\nwhere \\(T^{-1}(A)\\) is the set of points that gets mapped onto \\(A\\)\nunder \\(T\\); that is \\(T^{-1}(A) = \\{ x \\in X \\mid T(x) \\in A\n \\}\\).[4]\n From now on we also assume that the transformations we consider are\nmeasure\n preserving.[5] \nIn sum, from now on, unless stated otherwise, we consider discrete\nmeasure preserving transformations. \nIn order to introduce the concept of ergodicity we have to introduce\nthe phase and the time mean of a function \\(f\\) on \\(X\\).\nMathematically speaking, a function assigns each point in \\(X\\) a\nnumber. If the numbers are always real the function is a real-valued\nfunction; and if the numbers may be complex, then it is a\ncomplex-valued function. Intuitively we can think of these numbers as\nrepresenting the physical quantities of interest. Recalling the\nexample of the bouncing ball, \\(f\\) could for instance assign each\npoint in the phase space \\(X\\) the kinetic energy the system has at\nthat point; in this case we would have \\(f = p^2 / 2m\\), where \\(m\\)\nis the mass of the ball. For every function we can take two kinds of\naverages. The first is the infinite time average \\(f^*\\). The general\nidea of a time average is familiar from everyday contexts. You play\nthe lottery on three consecutive Saturdays. On the first you win $10;\non the second you win nothing; and on the third you win $50. Your\naverage gain is ($10 + $0 + $50)/3 = $20. Technically speaking this is\na time average. This simple idea can easily be put to use in a\ndynamical system: follow the system’s evolution over time (and\nremember that we are now talking about an average for discrete points\nof time), take the value of the relevant function at each step, add\nthe values, and then divide by the number of steps. This yields \nwhere \nis just an abbreviation for \nThis is the finite time average for \\(f\\) after \\(k\\) steps. If the\nsystem’s state continues to evolve infinitely and we keep\ntracking the system forever, then we get the infinite time\naverage: \nwhere the symbol ‘lim’ (from latin ‘limes’,\nmeaning border or limit) indicates that we are letting time tend\ntowards infinity (in mathematical symbols: \\(\\infty)\\). One point\ndeserves special attention, since it will become crucial later on: the\npresence of \\(x_0\\) in the above expression. Time averages depend on\nwhere the system starts; i.e., they depend on the initial condition.\nIf the process starts in a different state, the time average may well\nbe different. \nNext we have the space average \\(\\bar{f}\\). Let us again start with a\ncolloquial example: the average height of the students in a particular\nschool. This is easily calculated: just take each student’s\nheight, add up all the numbers, and divide the result by the number of\nstudents. Technically speaking this is a space average. In\nthe example the students in the school correspond to the points in\n\\(X\\); and the fact that we count each students once (we don’t,\nfor instance, take John’s height into account twice and omit\nJim’s) corresponds to the choice of a measure that gives equal\n‘weight’ to each point in \\(X\\). The transformation \\(T\\)\nhas no pendant in our example, and this is deliberate: space averages\nhave nothing to do with the dynamics of the system (that’s what\nsets them off from time averages). The general mathematical definition\nof the space average is as follows: \nwhere \\(\\int_X\\) is the integral over the phase space\n \\(X\\).[6]\n If the space consists of discrete elements, like the students of the\nschool (they are ‘discrete’ in that you can count them),\nthen the integral becomes equivalent to a sum like the one we have\nwhen we determine the average height of a population. If the \\(X\\) is\ncontinuous (as the phase space above) things are a bit more\ninvolved. \nWith these concepts in place, we can now define\n ergodicity.[7]\n A dynamical system \\([X, \\mu , T]\\) is ergodic iff \nfor all complex-valued Lebesgue integrable functions \\(f\\) almost\neverywhere, meaning for almost all initial conditions. The\nqualification ‘almost everywhere’ is non-trivial and is\nthe source of a famous problem in the foundations of statistical\nmechanics, the so-called ‘measure zero problem’ (to which\nwe turn in Section 3). So it is worth unpacking carefully what this\ncondition involves. Not all sets have a finite size. In fact, there\nare sets of measure zero. This may sound abstract but is very natural.\nTake a ruler and measure the length of certain objects. You will find,\nfor instance, that your pencil is 17cm long—in the language of\nmathematics this means that the one dimensional Lebegue measure of the\npencil is 17. Now measure a geometrical point and answer the question:\nhow long is the point? The answer is that such a point has no\nextension and so its length is zero. In mathematical parlance: a set\nconsisting of a geometrical point is a measure zero set. The same goes\nfor a set of two geometrical points: also two geometrical points\ntogether have no extension and hence have measure zero. Another\nexample is the following: you have device to measure the surface of\nobjects in a plane. You find out that an A4 sheet has a surface of\n623.7 square centimetres. Then you are asked what the surface of a\nline is. The answer is: zero. Lines don’t have surfaces. So with\nrespect to the two dimensional Lebesgue measure lines are measure zero\nsets. \nIn the context of ergodic theory, ‘almost everywhere’\nmeans, by definition, ‘everywhere in \\(X\\) except, perhaps, in a\nset of measure zero’. That is, whenever a claim is qualified as\n‘almost everywhere’ it means that it could be false for\nsome points in \\(X\\), but these taken together have measure zero. Now\nwe are in a position to explain what the phrase means in the\ndefinition of ergodicity. As we have seen above, the time average (but\nnot the space average!) depends on the initial condition. If we say\nthat \\(f^* = \\bar{f}\\) almost everywhere we mean that all those\ninitial conditions for which it turns out to be the case that \\(f^*\n\\ne \\bar{f}\\) taken together form a set of measure zero—they are\nlike a line in the plane. \nArmed with this understanding of the definition of ergodicity, we can\nnow discuss some important properties of ergodic systems. Consider a\nsubset \\(A\\) of \\(X\\). For instance, thinking again about the example\nof the oscillating ball, take the left half of the phase space. Then\ndefine the so-called characteristic function of \\(A, f_A\\), as\nfollows: \\(f_A (x) = 1\\) for all \\(x\\) in \\(A\\) and \\(f_A (x) = 0\\)\nfor all \\(x\\) not in \\(A\\). Plugging this function into the definition\nof ergodicity yields: \\(f^{*}_A = \\mu(A)\\). This means that the\nproportion of time that the system’s state spends in set \\(A\\)\nis proportional to the measure of that set. To make this even more\nintuitive, assume that the measure is normalised: \\(\\mu(X) = 1\\) (this\nis a very common and unproblematic assumption). If we then choose\n\\(A\\) so that \\(\\mu(A) = 1\\) ⁄ 2, then we know that the system\nspends half of the time in \\(A\\); if \\(\\mu(A) = 1\\) ⁄ 4, it\nspends a quarter of the time in \\(A\\); etc. As we will see below, this\nproperty of ergodic systems plays a crucial role in certain approaches\nto statistical mechanics. \nSince we are free to choose \\(A\\) as we wish, we immediately get\nanother important result: a system can be ergodic only if its\ntrajectory may access all parts of \\(X\\) of positive measure, i.e., if\nthe trajectory passes arbitrarily close to any point in \\(X\\)\ninfinitely many times as time tends towards infinity. And this implies\nthat the phase space of ergodic systems is called metrically\nindecomposable (or also ‘irreducible’ or\n‘inseparable’): every set invariant under \\(T\\) (i.e.,\nevery set that is mapped onto itself under \\(T)\\) has either measure 0\nor 1. As a consequence, \\(X\\) cannot be divided into two or more\nsubspaces (of non-zero measure) that are invariant under \\(T\\).\nConversely, a non-ergodic system is metrically decomposable. Hence,\nmetric indecomposability and ergodicity are equivalent. A metrically\ndecomposable system is schematically illustrated in Figure 3. \nFigure 3: Reducible system: no point in\nregion \\(P\\) evolves into region \\(Q\\) and vice versa. \nFinally, we would like to state a theorem that will become important\nin Section 4. One can prove that a system is ergodic iff \nholds for all subsets \\(A\\) and \\(B\\) of \\(X\\). Although this\ncondition does not have an immediate intuitive interpretation, we will\nsee below that it is crucial for the understanding of the kind of\nrandomness we find in ergodic systems. \nIt turns out that ergodicity is only the bottom level of an entire\nhierarchy of dynamical properties. This hierarchy is called the\nergodic hierarchy, and the study of this hierarchy is the\ncore task of a mathematical discipline called ergodic theory.\nThis choice of terminology is somewhat misleading, since ergodicity is\nonly the bottom level of this hierarchy and so EH contains much more\nthan ergodicity and the scope of ergodic theory stretches far beyond\nergodicty. Ergodic theory (thus understood) is part of dynamical\nsystems theory, which studies a wider class of dynamical systems\nthan ergodic theory.  \nEH is a nested classification of dynamical properties. The hierarchy\nis typically represented as consisting of the following five\nlevels: \nBernoulli \\(\\subset\\) Kolmogorov \\(\\subset\\) Strong Mixing \\(\\subset\\)\nWeak Mixing \\(\\subset\\) Ergodic  \nThe diagram is intended to indicate that all Bernoulli systems are\nKolmogorov systems, all Kolmogorov systems are strong mixing systems,\nand so on. Hence all systems in EH are ergodic. However, the converse\nrelations do not hold: not all ergodic systems are weak mixing, and so\non. In what follows a system that is ergodic but not weak mixing is\nreferred to as merely ergodic and similarly for the next\nthree\n levels.[8] \nFigure 4: Mixing \nMixing can be intuitively explained by the following example, first\nused by Gibbs in introducing the concept of mixing. Begin with a glass\nof water, then add a shot of scotch; this is illustrated in Fig. 4a.\nThe volume \\(C\\) of the cocktail (scotch + water) is \\(\\mu(C)\\) and\nthe volume of scotch that was added to the water is \\(\\mu(S)\\), so\nthat in \\(C\\)the concentration of scotch is \\(\\mu(S)/\\mu(C)\\). \nNow stir. Mathematically, stirring is represented by the time\nevolution \\(T\\), meaning that \\(T(S)\\) is the region occupied by the\nscotch after one unit of mixing time. Intuitively we say that the\ncocktail is thoroughly mixed, if the concentration of scotch equals\n\\(\\mu(S) / \\mu(C)\\) not only with respect to the whole volume\nof fluid, but with respect to any region \\(V\\) in that\nvolume. Hence, the drink is thoroughly mixed at time \\(n\\) if \nfor any volume \\(V\\) (of non-zero measure). Now assume that the volume\nof the cocktail is one unit: \\(\\mu(C) = 1\\) (which we can do without\nloss of generality since there is always a unit system in which the\nvolume of the glass is one). Then the cocktail is thoroughly mixed\niff \nfor any region \\(V\\) (of non-zero measure). But how large must \\(n\\)\nbe before the stirring ends with the cocktail well stirred? We now\ndon’t require that the drink must be thoroughly mixed at any finite\ntime, but only that it approaches a state of being thoroughly mixed as\ntime tends towards infinity: \nfor any region \\(V\\) (of non-zero measure). If we now associate the\nglass with the phase space \\(X\\) and replace the scotch \\(S\\) and the\nvolume \\(V\\) with two arbitrary subsets \\(A\\) and \\(B\\) of \\(X\\), then\nwe get the general definition of what is called strong mixing\n(often also referred to just as ‘mixing’): a system is\nstrong mixing iff \nfor all subsets \\(A\\) and \\(B\\) of \\(X.\\) This requirement for mixing\ncan be relaxed a bit by allowing for\n fluctuations.[9]\n That is, instead of requiring that the cocktail reach a uniform state\nof being mixed, we now only require that it be mixed on average. In\nother words, we allow that bubbles of either scotch or water may crop\nup every now and then, but they do so in a way that these fluctuations\naverage out as time tends towards infinity. This translates into\nmathematics in a straightforward way. The deviation from the ideally\nmixed state at some time \\(n\\) is \\(\\mu(T_n B \\cap A) -\n\\mu(B)\\mu(A)\\). The requirement that the average of these deviations\nvanishes inspires the notion of weak mixing. A system is weak\nmixing iff \nfor all subsets \\(A\\) and \\(B\\) of \\(X\\). The vertical strokes denote\nthe so-called absolute value; for instance: \\(\\lvert 5 \\rvert = \\lvert\n-5 \\rvert = 5\\). One can prove that there is a strict implication\nrelation between the three dynamical properties we have introduced so\nfar: strong mixing implies weak mixing, but not vice versa; and weak\nmixing implies ergodicity, but not vice versa. Hence, strong mixing is\nstronger condition than weak mixing, and weak mixing is stronger\ncondition than ergodicity. \nThe next higher level in EH are K-systems. Unlike in the cases of\nergodic and mixing systems, there is unfortunately no intuitive way of\nexplaining the standard definition of such systems, and the definition\nis such that one cannot read off from it the characteristics of\nK-systems (we state this definition in the\n Appendix,\n Section C). The least unintuitive way to present K-systems is via a\ntheorem due to Cornfeld et al (1982, 283), who prove that a\ndynamical system is a K-system iff it is K-mixing. A system\nis K-mixing iff for any subsets \\(A_0, A_1 , \\ldots ,A_r\\) of \\(X\\)\n(where \\(r\\) is a natural number of your choice) the following\ncondition holds: \nwhere \\(\\sigma(n, r)\\) is the minimal \\(\\sigma\\)-algebra generated by\nthe set \nIt is far from obvious what this so-called sigma algebra is and hence\nthe content of this condition is not immediately transparent. We will\ncome back to this issue in Section 5 where we provide an intuitive\nreading of this condition. What matters for the time being is its\nsimilarity to the mixing condition. Strong mixing is, trivially,\nequivalent to \nSo we see that K-mixing adds something to strong mixing. \nIn passing we would like to mention another important property of\nK-systems: one can prove that K-systems have positive\nKolmogorov-Sinai entropy (KS-entropy); for details see the\n Appendix,\nSection C. The KS-entropy itself does not have an intuitive\ninterpretation, but it relates to three other concepts of dynamical\nsystems theory in an interesting way, and these do have intuitive\ninterpretations. First, Lyapunov exponents are a measure for\nhow fast two originally nearby trajectories diverge on average, and\nthey are often used in chaos theory to characterise the chaotic nature\nof the dynamics of a system. Under certain circumstances (essentially,\nthe system has to be differentiable and ergodic) one can prove that a\ndynamical system has a positive KS-entropy if and only if it has\npositive Lyapounov exponents (Lichtenberg and Liebermann 1992, 304).\nIn such a system initially arbitrarily close trajectories diverge\nexponentially. This result is known as Pessin’s\ntheorem. Second, the algorithmic complexity of a\nsequence is the length of the shortest computer programme needed to\nreproduce the sequence. Some sequences are simple; e.g. a string of a\nmillion ‘1’ is simple: the programme needed to reproduce\nit basically is ‘write ‘1’ a million times’,\nwhich is very short. Others are complex: there is no pattern in the\nsequence 5%8£yu@*mS!}<74^F that one could exploit, and so a\nprogramme reproducing that sequence essentially reads ‘write\n5%8£yu@*mS!}<74^F’, which is similar in length to the\nsequence itself. In the discrete case a trajectory can be represented\nas a sequence of symbols which corresponds to the states of the system\nalong this trajectory.  It is then the case that if a system is a\nK-system, then its KS-entropy equals the algorithmic complexity of\nalmost all its trajectories (Brudno 1978). This is now known\nas Brudno’s theorem (Alekseev and Yakobson\n1981). Third, the Shannon entropy is a common measure for the\nuncertainty of a future outcome: the higher the entropy the more\nuncertain we are about what is going to happen. One can prove that,\ngiven certain plausible assumptions, the KS-entropy is equivalent to a\ngeneralised version of the Shannon entropy, and can hence be regarded\nas a measure for the uncertainty of future events given past events\n(Frigg 2004). \nBernoulli systems mark the highest level in EH. In order to define\nBernoulli systems we first have to introduce the notion of a partition\nof \\(X\\) (sometimes also called the ‘coarse graining of\n\\(X\\)’). A partition of X is a division of \\(X\\) into\ndifferent parts (the so-called ‘atoms of the partition’)\nso that these parts don’t overlap and jointly cover \\(X\\) (i.e.,\nthey are mutually exclusive and jointly exhaustive). For instance, in\nFigure 1 there is a partition of the phase space that has two atoms\n(the left and the right part). More formally, \\(\\alpha =\n\\{\\alpha_1,\\ldots , \\alpha_n\\}\\) is a partition of \\(X\\) (and the\n\\(\\alpha_i\\) its atoms) iff (i) the intersection of any two atoms of\nthe partition is the empty set, and (ii) the union of all atoms is\n\\(X\\) (up to measure zero). Furthermore it is important to notice that\na partition remains a partition under the dynamics of the system. That\nis, if \\(\\alpha\\) is a partition, then \\(T_n\\alpha = \\{T_n\\alpha_1\n,\\ldots ,T_n\\alpha_n\\}\\) is also a partition for all \\(n\\). \nThere are, of course, many different ways of partitioning a phase\nspace. In what follows we are going to study how different partitions\nrelate to each other. An important concept in this connection is\nindependence. Let \\(\\alpha\\) and \\(\\beta\\) be two partitions of \\(X\\).\nBy definition, these partitions are independent iff\n\\(\\mu(\\alpha_i \\cap \\beta_j) = \\mu(\\alpha_i)\\mu(\\beta_j)\\) for all\natoms \\(\\alpha_i\\) of \\(\\alpha\\) and all atoms \\(\\beta_j\\) of\n\\(\\beta\\). We will explain the intuitive meaning of this definition\n(and justify calling it ‘independence’) in Section 4; for\nthe time being we just use it as a formal definition. \nWith these notions in hand we can now define a Bernoulli\ntransformation: a transformation \\(T\\) is a Bernoulli\ntransformation iff there exists a partition \\(\\alpha\\) of \\(X\\) so\nthat the images of \\(\\alpha\\) under \\(T\\) at different instants of\ntime are independent; that is, the partitions \\(\\ldots ,T_{-1}\\alpha ,\nT_0 \\alpha , T_1 \\alpha ,\\ldots\\) are all\n independent.[10]\n In other words, \\(T\\) is a Bernoulli transformation iff \nfor all atoms \\(\\delta_i\\) of \\(T_k\\alpha\\) and all atoms \\(\\beta_j\\)\nof \\(T_l\\alpha\\) for all \\(k \\ne l\\). We then refer to \\(\\alpha\\) as\nthe Bernoulli partition, and we call a dynamical system \\([X,\n\\mu , T]\\) a Bernoulli system if \\(T\\) is a Bernoulli\nautomorphism, i.e., a Bernoulli transformation mapping \\(X\\) onto\nitself. \nLet us illustrate this with a well-known example, the\nbaker’s transformation (so named because of its\nsimilarity to the kneading of dough). This transformation maps the\nunit square onto itself. Using standard Cartesian coordinates the\ntransformation can be written as follows: \nIn words, for all points \\((x, y)\\) in the unit square that have an\n\\(x\\)-coordinate smaller than \\(1/2\\), the transformation \\(T\\)\ndoubles the value of \\(x\\) and halves the value of \\(y\\). For all the\npoints \\((x, y)\\) that have an \\(x\\)-coordinate greater or equal to\n\\(1/2\\), \\(T\\) transforms \\(x\\) in to \\(2x-1\\) and \\(y\\) into \\(y/2 +\n1/2\\). This is illustrated in Fig. 5a. \nFigure 5a: The Baker’s\ntransformation \nNow regard the two areas shown in the left-hand part of the above\nfigure as the two atoms of a partition \\(\\alpha = \\{\\alpha_1\n,\\alpha_2\\}\\). It is then easy to see that \\(\\alpha\\) and T\\(\\alpha\\)\nare independent: \\(\\mu(\\alpha_1 \\cap T\\alpha_2) =\n\\mu(\\alpha_1)\\mu(T\\alpha_2)\\), and similarly for all other atoms of\n\\(\\alpha\\) and \\(T\\alpha\\). This is illustrated in Figure 5b. \nFigure 5b: The independence of\n\\(\\alpha\\) and \\(T\\alpha\\). \nOne can prove that independence holds for all other iterates of\n\\(\\alpha\\) as well. So the baker’s transformation together with\nthe partition \\(\\alpha\\) is a Bernoulli transformation. \nIn the literature Bernoulli systems are often introduced using\nso-called shift maps (or Bernoulli shifts). We here\nbriefly indicate how shift maps are related to Bernoulli systems with\nthe example of the baker’s transformation; for a more general\ndiscussion see the\n Appendix,\n Section D. Choose a point in the unit square and write its \\(x\\) and\n\\(y\\) coordinates as binary numbers: \\(x = 0.a_1 a_2 a_3\\ldots\\) and\n\\(y = 0.b_1 b_2 b_3\\ldots\\), where all the \\(a_i\\) and \\(b_i\\) are\neither 0 or 1. Now put both strings together back to back with a dot\nin the middle to form one infinite string: \\(S= \\ldots b_3 b_2 b_1\n.a_1 a_2 a_3\\ldots\\), which may represent the state of the system just\nas a ‘standard’ two-dimensional vector does. Some\nstraightforward algebra then shows that \nFrom this we see that in our ‘one string’ representation\nof the point the operation of \\(T\\) amounts to shifting the dot one\nposition to the right: \\(TS= \\ldots b_3 b_2 b_1 a_1 .a_2 a_3\\ldots\\)\nHence, the baker’s transformation is equivalent to a shift on an\ninfinite string of zeros and\n ones.[11] \nThere are two further notions that are crucial to the theory of\nBernoulli systems, the property of being weak Bernoulli and\nvery weak Bernoulli. These properties play a crucial role in\nshowing that certain transformations are in fact Bernoulli. The\nbaker’s transformation is one of the few examples that have a\ngeometrically simple Bernoulli partition, and so one often cannot\nprove directly that a system is a Bernoulli system. One then shows\nthat a certain geometrically simple partition is weak Bernoulli and\nuses a theorem due to Ornstein to the effect that if a system is weak\nBernoulli then there exists a Bernoulli partition for that system. The\nmathematics of these notions and the associated proofs of equivalence\nare intricate and a presentation of them is beyond the scope of this\nentry. The interested reader is referred to Ornstein (1974) or Shields\n(1973). \nThe concepts of EH, and in particular ergodicity itself, play\nimportant roles in the foundation of statistical mechanics (SM). In\nthis section we review what these roles are. \nA discussion of SM faces an immediate problem.\nFoundational debates in many other fields of physics can take as their\npoint of departure a generally accepted formalism. Things are\ndifferent in SM. Unlike, say, relativity theory,\nSM has not yet found a generally accepted theoretical framework, let\nalone a canonical\n formulation.[12]\n What we find in SM is plethora of different approaches and schools,\neach with its own programme and mathematical\n apparatus.[13]\n However, all these schools use (slight variants) of either of two\ntheoretical frameworks, one of which can be associated with Boltzmann\n(1877) and the other with Gibbs (1902), and can thereby be classify\neither as ‘Boltzmannian’ or ‘Gibbsian’. For\nthis reason we divide our presentation of SM into a two parts, one for\neach of these families of approaches. \nBefore delving into a discussion of these theories, let us briefly\nreview the basic tenets of SM by dint of a common example. Consider a\ngas that is confined to the left half of a box. Now remove the barrier\nseparating the two halves of the box. As a result, the gas quickly\ndisperses, and it continues to do so until it uniformly fills the\nentire box. The gas has approached equilibrium. This raises two\nquestions. First, how is equilibrium characterised? That is, what does\nit take for a system to be in equilibrium? Second, how do we\ncharacterise the approach to equilibrium? That is, what are the\nsalient features of the approach to equilibrium and what features of a\nsystem make it behave in this way? These questions are addressed in\ntwo subdisciplines of SM: equilibrium SM and non-equilibrium SM. \nThere are two different ways of describing processes like the\nspreading of a gas. Thermodynamics describes the system using a few\nmacroscopic variables (in the case of the gas pressure, volume and\ntemperature), while disregarding the microscopic\nconstitution of the gas. As far as thermodynamics is concerned matter\ncould be a continuum rather than consisting of particles—it just\nwould not make any difference. For this reason thermodynamics is\ncalled a ‘macro theory’. \nThe cornerstone of thermodynamics is the so-called Second Law of\nthermodynamics. This law describes one of the salient features of the\nabove process: its unidirectionality. We see gases spread—i.e.,\nwe see them evolving towards equilibrium—but we never observe\ngases spontaneously reverting to the left half of a box—i.e., we\nnever see them move away from equilibrium when left alone. And this is\nnot a specific feature of gases. In fact, not only gases but also all\nother macroscopic systems behave in this way, irrespective of their\nspecific makeup. This fact is enshrined in the Second Law of\nthermodynamics, which, roughly, states that transitions from\nequilibrium to non-equilibrium states cannot occur in isolated\nsystems, which is the same as saying that entropy cannot decrease in\nisolated systems (where a system is isolated if it has no interaction\nwith its environment: there is no heat exchange, no one is compressing\nthe gas, etc.). \nBut there is an altogether different way of looking at that same gas;\nthat is, as consisting of a large number of molecules (a vessel on a\nlaboratory table contains something like \\(10^{23}\\) molecules). These\nmolecules bounce around under the influence of the forces exerted onto\nthem when they crash into the walls of the vessel and collide with\neach other. The motion of each molecule is governed by the laws of\nclassical mechanics in the same way as the motion of the bouncing\nball. So rather than attributing some macro variables to the gas and\nfocussing on them, we could try to understand the gas’ behaviour\nby studying the dynamics of its micro constituents. \nThis raises the question of how the two ways of looking at the gas fit\ntogether. Since neither the thermodynamic nor the mechanical approach\nis in any way privileged, both have to lead to the same conclusions.\nStatistical mechanics is the discipline that addresses this task. From\na more abstract point of view we can therefore also say that SM is the\nstudy of the connection between micro-physics and macro-physics: it\naims to account for a system’s macro behaviour in terms of the\ndynamical laws governing its microscopic constituents. The term\n‘statistical’ in its name is owed to the fact that, as we\nwill see, a mechanical explanation can only be given if we also\nintroduce probabilistic elements into the theory. \nWe first introduce the main elements of the Boltzmannian framework and\nthen turn to the use of ergodicity in it. Every system can posses\nvarious macrostates \\(M_1 ,\\ldots ,M_k\\). These macrostates are\ncharacterised by the values of macroscopic variables, in the case of a\ngas pressure, temperature, and\n volume.[14]\n In the introductory example one macro-state corresponds to the gas\nbeing confined to the left half, another one to it being spread out.\nIn fact, these two states have special status: the former is the\ngas’ initial state (also referred to as the ‘past\nstate’); the latter is the gas’ equilibrium state. We\nlabel the states \\(M_p\\) and \\(M_{eq}\\) respectively. \nIt is one of the fundamental posits of the Boltzmann approach that\nmacrostates supervene on microstates, meaning that a change in a\nsystem’s macrostate must be accompanied by a change in its\nmicrostate (for a discussion of supervenience see McLaughlin and\nBennett 2005, and references therein). For instance, it is not\npossible to change the pressure of a system and at the same time keep\nits micro-state constant. Hence, to every given microstate \\(x\\) there\ncorresponds exactly one macrostate. Let us refer to this\nmacrostate as \\(M(x)\\). This determination relation is not one-to-one;\nin fact many different \\(x\\) can correspond to the same macrostate. We\nnow group together all microstates \\(x\\) that correspond to the same\nmacro-state, which yields a partitioning of the phase space in\nnon-overlapping regions, each corresponding to a macro-state. For this\nreason we also use the same letters, \\(M_1 ,\\ldots ,M_k\\), to refer to\nmacro-states and the corresponding regions in phase space. This is\nillustrated in Figure 6a. \nFigure 6: The macrostate structure of\n\\(X\\). \nWe are now in a position to introduce the Boltzmann entropy. To this\nend recall that we have a measure \\(\\mu \\) on the phase space that\nassigns to every set a particular volume, hence a fortiori\nalso to macrostates. With this in mind, the Boltzmann entropy of a\nmacro-state \\(M_j\\) can be defined as \\(S_B = k_B\\log [\\mu(M_j)]\\),\nwhere \\(k_B\\) is the Boltzmann constant. The important feature of the\nlogarithm is that it is a monotonic function: the larger\n\\(M_j\\), the larger its logarithm. From this it follows that the\nlargest macro-state also has the highest entropy! \nOne can show that, at least in the case of dilute gases, the Boltzmann\nentropy coincides with the thermodynamic entropy (in the sense that\nboth have the same functional dependence on the basic state\nvariables), and so it is plausible to say that the equilibrium state\nis the macro-state for which the Boltzmann entropy is maximal (since\nthermodynamics posits that entropy be maximal for equilibrium states).\nBy assumption the system starts off in a low entropy state, the\ninitial state \\(M_p\\) (the gas being squeezed into the left half of\nthe box). The problem of explaining the approach to equilibrium then\namounts to answering the question: why does a system originally in\n\\(M_p\\) eventually move into \\(M_{eq}\\) and then stay there? (See\nFigure 6b.) \nIn the 1870s Boltzmann offered an important answer to this\n question.[15]\n At the heart of his answer lies the idea to assign probabilities to\nmacrostates according to their size. So Boltzmann adopted the\nfollowing postulate: \\(p(M_j) = c\\mu(M_j)\\) for all \\(j = 1,\\ldots\n,k\\), where \\(c\\) is a normalisation constant assuring that the\nprobabilities add up to one. Granted this postulate, it follows\nimmediately that the most likely state is the equilibrium state (since\nthe equilibrium state occupies the largest chunk of the phase space).\nFrom this point of view it seems natural to understand the approach to\nequilibrium as the evolution from an unlikely macrostate to a more\nlikely macrostate and finally to the most likely macro-state. This,\nBoltzmann argued, was a statistical justification of the Second Law of\nthermodynamics. \nBut Boltzmann knew that simply postulating \\(p(M_j) = c\\mu(M_j)\\)\nwould not solve the problem unless the postulate could be justified in\nterms of the dynamics of the system. This is where ergodicity enters\nthe scene. As we have seen above, ergodic systems have the property of\nspending a fraction of time in each part of the phase space that is\nproportional to its size (with respect to \\(\\mu)\\). As we have also\nseen, the equilibrium state is the largest macrostate. In fact, the\nequilibrium state is much larger than the other states. So if\nwe assume that the system is ergodic, then it is in equilibrium most\nof the time! It is then natural to interpret \\(p(M_j)\\) as a time\naverage: \\(p(M_j)\\) is the fraction of time that the system spends in\nstate \\(M_j\\) over the course of time. We now have the main elements\nof Boltzmann’s framework in front of us: (a) partition the phase\nspace of the system in macrostates and show that the equilibrium state\nis by far the largest state; (b) adopt a time average interpretation\nof probability; and (c) assume that the system in question is\nergodic. It then follows that the system is most likely to be found in\nequilibrium, which justifies (a probabilistic version of) the Second\nlaw. \nThree objections have been levelled against this line of thought.\nFirst, it is pointed that assuming ergodicity is too strong in two\nways. The first is that it turns out to be extremely difficult to\nprove that the systems of interest really are ergodic. Contrary to\nwhat is sometimes asserted, not even a system of \\(n\\) elastic hard\nballs moving in a cubic box with hard reflecting walls has been proven\nto be ergodic for arbitrary \\(n\\); it has been proven to be ergodic\nonly for \\(n \\le 4\\). To this charge one could reply that what looks\nlike defeat to some, appears to be a challenge to others. Progress in\nmathematics may eventually resolve the issue, and there is at least\none recent result that justifies optimism: Simanyi (2004) shows that a\nsystem of \\(n\\) hard balls on a torus of dimension 3 or greater is\nergodic, for an arbitrary natural number \\(n\\). \nThe second way in which ergodicity seems to be too strong is that even\nif eventually we can come by proofs of ergodicity for the relevant\nsystems, the assumption is too strong because there are systems that\nare known not to be ergodic and yet they behave in accordance with the\nSecond Law. Bricmont (2001) investigates the Kac Ring Model and a\nsystem of \\(n\\) uncoupled anharmonic oscillators of identical mass,\nand points out that both systems exhibit thermodynamic behaviour and\nyet they fail to be ergodic. Hence, ergodicity is not necessary for\nthermodynamic behaviour. Earman and Redei (1996, p. 70) and van Lith\n(2001, p. 585) argue that if ergodicity is not necessary for\nthermodynamic behaviour, then ergodicity cannot provide a satisfactory\nexplanation for this behaviour. Either there must be properties other\nthan ergodicity that explain thermodynamic behaviour in cases in which\nthe system is not ergodic, or there must be an altogether different\nexplanation for the approach to equilibrium even for systems which are\nergodic. \nIn response to this objection, Vranas (1998) and Frigg and Werndl\n(2011) argue that most systems that fail to be ergodic are\n‘almost ergodic’ in a specifiable way, and this is good\nenough. We discuss Vranas’ approach below when discussing\nGibbsian SM since that is the context in which he has put forward his\nsuggestion. Werndl and Frigg (2015a, 2015b) offer an alternative\ndefinition of Boltzmannian equilibrium and exploit the ergodic\ndecomposition theorem to show that even if a system is not ergodic it\nwill spend most of the time in equilibrium, as envisaged by Boltzmann\n(roughly the ergodic decomposition theorem says that the phase space\nof every measure preserving system can be partitioned into parts so\nthat the dynamics is ergodic on each part; for details see Petersen\n1983). Frigg (2009) suggested exploiting the fact that almost all\nHamiltonian systems are non-integrable, and that these systems have\nso-called Arnold webs, i.e., large regions of phase space on which the\nmotion of the system is ergodic. Lavis (2005) re-examined the Kac ring\nmodel and pointed out that even though the system is not ergodic, it\nhas an ergodic decomposition, which is sufficient to guarantee the\napproach to equilibrium. He also challenged the assumption, implicit\nin the above criticism, that providing an explanation for the approach\nto equilibrium amounts to identifying one (and only one!) property\nthat all systems have in common. In fact, it may be the case that\ndifferent properties are responsible for the approach to equilibrium\nin different systems, and there is no reason to rule out such\nexplanations. In sum, the tenor of all these responses is that even though\nergodicity simpliciter may not have the resources to explain\nthe approach to equilibrium, somewhat qualified properties do. \nThe second objection is that even if ergodicity obtains, this is not\nsufficient to give us what we need. As we have seen above, ergodicity\ncomes with the qualification ‘almost everywhere’. This\nqualification is usually understood as suggesting that sets of measure\nzero can be ignored without detriment. The idea is that points falling\nin a set of measure zero are ‘sparse’ and can therefore be\nneglected. The question of whether or not this move is legitimate is\nknown as the ‘measure zero problem’. \nSimply neglecting sets of measure zero seems to be problematic for\nvarious reasons. First, sets of measure zero can be rather\n‘big’; for instance, the rational numbers have measure\nzero within the real numbers. Moreover, a set of measure zero need not\nbe (or even appear) negligible if sets are compared with respect to\nproperties other than their measures. For instance, we can judge the\n‘size’ of a set by its cardinality or Baire category\nrather than by its measure, which leads us to different conclusions\nabout the set’s size (Sklar 1993, pp. 182–88). It is also\na mistake to assume that an event with measure zero cannot occur. In\nfact, having measure zero and being impossible are distinct notions.\nWhether or not the system at some point was in one of the special\ninitial conditions for which the space and time mean fail to be equal\nis a factual question that cannot be settled by appeal to measures;\npointing out that such points are scarce in the sense of measure\ntheory does resolve the problem because it does not imply that they\nare scarce in the world as well. \nIn response two things can said. First, discounting sets of measure\nzero is standard practice in physics and the problem is not specific\nto ergodic theory. So unless there is a good reason to suspect that\nspecific measure zero states are in fact important, one might argue\nthat the onus of proof is on those who think that discounting them in\nthis case is illegitimate. Second, the fact that SM works in so many\ncases suggests that they indeed are scarce. \nThe third criticism is rarely explicitly articulated, but it is\nclearly in the background of contemporary Boltzmannian approaches to\nSM such as Albert’s (2000), which reject Boltzmann’s\nstarting point, namely the postulate \\(p(M_j) = c\\mu(M_j)\\). Albert\nintroduces an alternative postulate, essentially providing transition\nprobabilities between two macrostates conditional on the so-called\nPast Hypothesis, the posit that the universe came into existence in a\nlow entropy state (the Big Bang). Albert then argues that in such an\naccount erogidicity becomes an idle wheel, and hence he rejects it as\ncompletely irrelevant to the foundations of SM. This, however, may\nwell be too hasty. Although it is true that ergodicity simpliciter\ncannot justify Albert’s probability postulate, another dynamical\nassumption is needed in order for this postulate to be true (Frigg\n2010). \nAt the basis of Gibbs’ approach stands a conceptual shift. The\nobject of study in the Boltzmannian framework is an individual system,\nconsisting of a large but finite number of micro constituents. By\ncontrast, within the Gibbs framework the object of study is a\nso-called ensemble: an imaginary collection of infinitely\nmany copies of the same system (they are the same in that they have\nthe same phase space, dynamics and measure), but who happen to be in\ndifferent states. An ensemble of gases, for instance, consists of\ninfinitely many copies of the same gas bit in different states: one is\nconcentrated in the left corner of the box, one is evenly distributed,\netc. It is important to emphasise that ensembles are fictions, or\n‘mental copies of the one system under consideration’\n(Schrödinger 1952, 3); or alternatively they can be thought of as\ncollections of possible states of the entire system. Hence, it is\nimportant not to confuse ensembles with collections of micro-objects\nsuch as the molecules of a gas! \nThe instantaneous state of one system of the ensemble is specified by\none point in its phase space. The state of the ensemble as a\nwhole is therefore specified by a density function \\(\\varrho\\) on\nthe system’s phase space. From a technical point of view\n\\(\\varrho\\) is a function just like \\(f\\) that we encountered in\nSection 1. We furthermore assume that \\(\\varrho\\) is a probability\ndensity, reflecting the probability density of finding the state of a\nsystem chosen at random from the entire ensemble in region \\(R\\), so\nthat the probability of the state being in \\(R\\) is \\(p(R) = \\int_R\n\\varrho d\\mu\\). To make this more intuitive consider the following\nanalogy. You play a special kind of darts: you fix a plank to the\nwall, which serves as your dart board. For some reason you know that\nthe probability of your dart landing at a particular place on the\nboard is given by the curve shown in Figure 7. You are then asked what\nthe probability is that your next dart lands in the left half of the\nboard. The answer is 1 ⁄ 2 since one half of the surface\nunderneath the curve is on the left side. The dart board then plays\nthe role of the system’s state space, a region of the board\n(here the left half) plays the role of \\(R\\), and throwing a dart\nplays the role of picking a system from the ensemble. \nFigure 7: Dart board \nThe importance of this is that it allows us to calculate expectation\nvalues. Assume that the game is such that you get one Pound if the\ndart hits the left half and three Pounds if it lands on the right\nhalf. What is your expected gain? The answer is 1 ⁄ \\(2 \\times\n1\\) Pound \\(+ 1\\) ⁄ \\(2 \\times 3\\) Pounds \\(= 2\\) Pounds. This\nis the expectation value. The same idea is at work in SM. Physical\nmagnitudes like, for instance, pressure are associated with functions\n\\(f\\) on the phase pace. We then calculate the expectation value of\nthese magnitudes, which, in general is given by \\(\\langle f \\rangle =\n\\int fd\\mu\\). In the context of Gibbsian SM these expectation values\nare also referred to as phase averages or ensemble\naverages. They are of central importance because these values are\nused as predictions for observed values. So if you want to use the\nformalism to predict what will be observed in an experiment, you first\nhave to figure out what the probability density \\(\\varrho\\) is, then\nfind the function \\(f\\) corresponding to the physical quantity you are\ninterested in, and then calculate the phase average. Neither of these\nsteps is easy in practice and working physicists spend most of their\ntime doing these calculations. However, these difficulties need not\noccupy us if we are interested in the conceptual issues underlying\nthis ‘recipe’. \nBy definition, a probability density \\(\\varrho\\) is stationary if it\ndoes not change over time. Given that observable quantities are\nassociated with phase averages and that equilibrium is defined in\nterms of the constancy of the macroscopic parameters characterising\nthe system, it is natural to regard the stationarity of the\ndistribution as a necessary condition for equilibrium because\nstationary distributions yield constant averages. For this reason\nGibbs refers to stationarity as the ‘condition of statistical\nequilibrium’. \nAmong all stationary distributions those satisfying a further\nrequirement, the Gibbsian maximum entropy principle, play a\nspecial role. The Gibbs entropy (sometimes called\n‘ensemble entropy’) is defined as \nThe Gibbsian maximum entropy principle then requires that \\(S_G\n(\\varrho)\\) be maximal, given the constraints that are imposed on the\n system.[16] \nThe last clause is essential because different constraints single out\ndifferent distributions. A common choice is to keep both the energy\nand the particle number in the system fixed. One can prove that under\nthese circumstances \\(S_G (\\varrho)\\) is maximal for the so-called\nmicrocanonical distribution (or microcanonical\nensemble). If we choose to hold the number of particles constant\nwhile allowing for energy fluctuations around a given mean value we\nobtain the so-called canonical distribution; if we also allow\nthe particle number to fluctuate around a given mean value we find the\nso-called grand-canonical\n distribution.[17] \nThis formalism is enormously successful in that correct predictions\ncan be derived for a vast class of systems. But the success of this\nformalism is rather puzzling. The first and most obvious question\nconcerns the relation of systems and ensembles. The probability\ndistribution in the Gibbs approach is defined over an ensemble, the\nformalism provides ensemble averages, and equilibrium is regarded as a\nproperty of an ensemble. But what we are really interested in is the\nbehaviour of a single system! What could the properties of an\nensemble—a fictional entity consisting of infinitely many mental\ncopies of the real system—tell us about the one real system on\nthe laboratory table? And more specifically, why do averages over an\nensemble coincide with the values found in measurements performed on\nan actual physical system in equilibrium? There is no obvious reason\nwhy this should be so, and it turns out that ergodicity plays a\ncentral role in answering these questions. \nCommon textbook wisdom justifies the use of phase averages as follows.\nAs we have seen, the Gibbs formalism associates physical quantities\nwith functions on the system’s phase space. Making an\nexperiment measuring one of these quantities takes time and it is\nassumed that what measurement devices register is not the\ninstantaneous value of the function in question, but rather its time\naverage over the duration of the measurement. Hence, time averages are\nwhat is empirically accessible. Then, so the argument continues,\nalthough measurements take an amount of time that is short by human\nstandards, it is long compared to microscopic time scales on which\ntypical molecular processes take place. For this reason it is assumed\nthat the measured finite time average is approximately equal\nto the infinite time average of the measured function. If we\nnow assume that the system is ergodic, then time averages equal phase\naverages. The latter can easily be obtained from the formalism. Hence\nwe have found the sought-after connection: the Gibbs formalism\nprovides phase averages which, due to ergodicity, are equal to\ninfinite time averages, and these are, to a good approximation, equal\nto the finite time averages obtained from measurements. \nThis argument is problematic for at least two reasons. First, from the\nfact that measurements take some time it does not follow that what is\nactually measured are time averages. For instance, it could be the\ncase that the value provided to us by the measurement device is simply\nthe value assumed by at the last moment of the measurement,\nirrespective of what the previous values of were (e.g.\nit’s simply the last pointer reading registered). So we would\nneed an argument for the conclusion that measurements indeed produce\ntime averages. Second, even if we take for granted that\nmeasurements do produce finite time averages, equating these\naverages with infinite time averages is problematic. Even if the\nduration of the measurement is long by experimental standards (which\nneed not be the case), finite and infinite averages may assume very\ndifferent values. That is not to say that they necessarily have to be\ndifferent; they could coincide. But whether or not they do is\nan empirical question, which depends on the specifics of the system\nunder investigation. So care is needed when replacing finite with\ninfinite time averages, and one cannot identify them without further\nargument.  \nMalament and Zabell (1980) respond to this challenge by suggesting a\nway of explaining the success of equilibrium theory that still invokes\nergodicity, but avoids appeal to time averages. This solves the above\nmentioned problems, but suffers from the difficulty that many systems\nthat are successfully dealt with by the formalism of SM are not\nergodic. To circumvent this difficulty Vranas (1998) suggested\nreplacing ergodicity with what he calls \\(\\varepsilon\\)-ergodicity.\nIntuitively a system is \\(\\varepsilon\\)-ergodic if it is ergodic not\non the entire phase space, but on a very large part of it (those parts\non which it is not ergodic having measure \\(\\varepsilon\\), where\n\\(\\varepsilon\\) is very small). The leading idea behind his approach\nis to challenge the commonly held belief that even if a system is just\na ‘little bit’ non-ergodic, then it behaves in a\ncompletely ‘un-ergodic’ way. Vranas points out that there\nis a middle ground and then argues that this middle ground actually\nprovides us with everything we need. This is a promising proposal, but\nit faces three challenges. First, it needs to be shown that all\nrelevant systems really are \\(\\varepsilon\\)-ergodicity. Second, the\nargument so far has only been developed for the microcanonical\nensemble, but one would like to know whether, and if so how, it works\nfor the canonical and the grandcanonical ensembles. Third, it is still\nbased on the assumption that equilibrium is characterised by a\nstationary distribution, which, as we will see below, is an obstacle\nwhen it comes to formulating a workable Gibbsian non-equilibrium\ntheory. \nThe second response begins with Khinchin’s work. Khinchin (1949)\npointed out that the problems of the ergodic programme are due to the\nfact that it focuses on too general a class of systems. Rather than\nstudying dynamical systems at a general level, we should focus on\nthose cases that are relevant in statistical mechanics. This involves\ntwo restrictions. First, we only have to consider systems with a large\nnumber of degrees of freedom; second, we only need to take into\naccount a special class of phase functions, the so-called ‘sum\nfunctions’. These functions are a sum of one-particle functions,\ni.e., functions that take into account only the position and momentum\nof one particle. Under these assumption Khinchin proved that as \\(n\\)\nbecomes larger, the measure of those regions on the energy\n hypersurface[18]\n where the time and the space means differ by more than a small amount\ntends towards zero. Roughly speaking, this result says that for large\n\\(n\\) the system behaves, for all practical purposes, as if it was\nergodic. \nThe problem with this result is that it is valid only for sum\nfunctions, and in particular only if the energy function of the system\nis itself a sum function, which is not the case when particles\ninteract. So the question is how this result can be generalised to\nmore realistic cases. This problem stands at the starting point of a\nresearch programme now known as the thermodynamic limit,\nchampioned, among others, by Lanford, Mazur, Ruelle, and van der\nLinden (see van Lith (2001) for a survey). Its leading question is\nwhether one can still prove ‘Khinchin-like’ results in the\ncase of energy function with interaction\n terms.[19]\n Results of this kind can be proven in the limit for \\(n \\rightarrow\n\\infty\\), if also the volume \\(V\\) of the system tends towards\ninfinity in such a way that the number-density \\(n/V\\) remains\nconstant. \nSo far we have only dealt with equilibrium, and things get worse once\nwe turn to non-equilibrium. The main problem is that it is a\nconsequence of the formalism that the Gibbs entropy is a constant!\nThis precludes a characterisation of the approach to equilibrium in\nterms of increasing Gibbs entropy, which is what one would expect if\nwe were to treat the Gibbs entropy as the SM counterpart of the\nthermodynamic entropy. The standard way around this problem is to\ncoarse-grain the phase space, and then define the so-called coarse\ngrained Gibbs entropy. Put simply, course-graining the phase space\namounts to putting a grid on the phase space and declare that all\npoints within one cell of the grid are indistinguishable. This\nprocedure turns a continuous phase space into a discrete collection of\ncells, and the state of the system is then specified by saying in\nwhich cell the system’s state is. If we define the Gibbs entropy on\nthis grid, it turns out (for purely mathematical reasons) that the\nentropy is no longer a constant and can actually increase or decrease.\nIf one then assumes that the system is mixing, it follows from the\nso-called convergence theorem of ergodic theory that the\ncoarse-grained Gibbs entropy approaches a maximum. However, this\nsolution is fraught with controversy, the two main bones of contention\nbeing the justification of coarse-graining and the assumption that the\nsystem is mixing. \nIn sum, ergodicity plays a central role in many attempts to justify\nthe posits of SM. And even where a simplistic use of ergodicity is\neventually unsuccessful, somewhat modified notions prove fruitful in\nan analysis of the problem and in the search for better solutions. \nEH is often presented as a hierarchy of increasing degrees of\nrandomness in deterministic systems: the higher up in this hierarchy a\nsystem is placed the more random its\n behaviour.[20]\n However, the definitions of different levels of EH do not make\nexplicit appeal to randomness; nor does the usual way of presenting EH\ninvolve a specification of the notion of randomness that is supposed\nto underlie the hierarchy. So there is a question about what notion of\nrandomness underlies EH and in what sense exactly EH is a hierarchy of\nrandom behaviour. \nBerkovitz, Frigg and Kronz (2006) discuss this problem and argue that\nEH is best understood as a hierarchy of random behaviour if randomness\nis explicated in terms of unpredictability, where unpredictability is\naccounted for in terms of probabilistic relevance. Different patterns\nof probabilistic relevance, in turn, are spelled out in terms of\ndifferent types of decay of correlation between a system’s\nstates at different times. Let us introduce these elements one at a\ntime. \nProperties of systems can be associated with different parts of the\nphase space. In the ball example, for instance, the property\nhaving positive momentum is associated with the right half of\nthe phase space; that is, it is associated with the set \\(\\{x \\in X\n\\mid p \\gt 0\\}\\). Generalising this idea we say that to every subset\n\\(A\\) of a system’s phase space there corresponds a property\n\\(P_A\\) so that the system possesses that property at time \\(t\\) iff\nthe system’s state \\(x\\) is in \\(A\\) at \\(t\\). The subset \\(A\\)\nmay be arbitrary and the property corresponding to \\(A\\) may not be\nintuitive, unlike, for example, the property of having positive\nmomentum. But nothing in the analysis to follow hangs on a\nproperty being ‘intuitive’. We then define the\nevent \\(A^t\\) as the obtaining of \\(P_A\\) at time \\(t\\). \nAt every time \\(t\\) there is a matter of fact whether \\(P_A\\) obtains,\nwhich is determined by the dynamics of the system. However, we may not\nknow whether or not this is the case. We therefore introduce epistemic\nprobabilities expressing our uncertainty about whether \\(P_A\\)\nobtains: \\(p(A^t)\\) reflects an agent’s degree of belief in\n\\(P_A\\)’s obtaining at time \\(t\\). In the same way we can\nintroduce conditional probabilities: \\(p(A^t \\mid B^{t_1})\\) is our\ndegree of belief in the system having \\(P_A\\) at \\(t\\) given that it\nhad \\(P_B\\) at an earlier time \\(t_1\\), where \\(B\\) is also a subset\nof the system’s phase space. By the usual rule of conditional\nprobability we have \\(p(A^t \\mid B^{t_1}) = p(A^t \\amp B^{t_1}) /\n(p(B^{t_1})\\). This can of course be generalised to more then one\nevent: \\(p(A^t \\mid B_{1}^{t_1} \\amp \\ldots \\amp B_{r}^{t_r})\\) is our\ndegree of belief in the system having \\(P_A\\) at \\(t\\) given that it\nhad \\(P_{B_1}\\) at \\(t_1, P_{B_2}\\) at \\(t_2,\\ldots\\), and \\(P_{B_{\nr}}\\) at \\(t_r\\), where \\(B_1 ,\\ldots ,B_r\\) are subsets of the\nsystem’s phase space (and \\(r\\) a natural number), and \\(t_1\n,\\ldots ,t_r\\) are successive instants of time (i.e., \\(t \\gt t_1 \\gt\n\\ldots \\gt t_r)\\).  \nIntuitively, an event in the past is relevant to our making\npredictions if taking the past event into account makes a difference\nto our predictions, or more specifically if it lowers or raises the\nprobability for a future event. In other words, \\(p(A^t \\mid\nB_{1}^{t_1}) - p(A^t)\\) is a measure of the relevance of \\(B^{t_1}\\)\nto predicting \\(A^t : B^{t_1}\\) is positively relevant if the \\(p(A^t\n\\mid B_{1}^{t_1}) - p(A^t) \\gt 0\\), negatively relevant if \\(p(A^t\n\\mid B_{1}^{t_1}) - p(A^t) \\lt 0\\), and irrelevant if \\(p(A^t \\mid\nB_{1}^{t_1}) - p(A^t) = 0\\). For technical reasons it turns out to be\neasier to work with a slightly different but equivalent notion of\nrelevance, which is obtained from the above by multiplying both sides\nof the equation by \\(p(B^{t_1})\\). Therefore we adopt the following\ndefinition. The relevance of \\(B^{t_1}\\) for \\(A^t\\) is \nThe generalisation of this definition to cases with more than one set\n\\(B\\) (as above) is straightforward. \nRelevance serves to explicate unpredictability. Intuitively, the less\nrelevant past events are for \\(A^t\\), the less predictable the system\nis. This basic idea can then be refined in various ways. First, the\ntype of unpredictability we obtain depends on the type of events to\nwhich (R) is applied. For instance, the degree of the unpredictability\nof \\(A^t\\) increases if its probability is independent not only of\n\\(B^{t_1}\\) or other ‘isolated’ past events, but rather\nthe entire past. Second, the unpredictability of an event \\(A^t\\)\nincreases if the probabilistic dependence of that event on past events\n\\(B^{t_1}\\) decreases rapidly with the increase of the temporal\ndistance between the events. Third, the probability of \\(A^t\\) may be\nindependent of past events simpliciter, or it may be\nindependent of such events only on average. These ideas underlie the\nanalysis of EH as a hierarchy of unpredictability. \nBefore we can provide such an analysis, two further steps are needed.\nFirst, if the probabilities are to be useful to understanding\nrandomness in a dynamical system, the probability assignment\nhas to reflect the properties of the system. So we have to connect the\nabove probabilities to features of the system. The natural choice is\nthe system’s measure\n \\(\\mu\\).[21]\n So we postulate that the probability of an event \\(A^t\\) is equal to\nthe measure of the set \\(A: p(A^{t}) = \\mu(A)\\) for all \\(t\\). This\ncan be generalised to joint probabilities as follows: \nfor all instants of time \\(t \\gt t_1\\) and all subsets \\(A\\) and \\(B\\)\nof the system’s phase space. \\(T_{t_1 \\rightarrow t}B\\) is the\nimage of the set \\(B\\) under the dynamics of the system from \\(t_1\\)\nto \\(t\\). We refer to this postulate as the Probability\nPostulate (P), which is illustrated in Figure 8. Again, this\ncondition is naturally generalised to cases of joint probabilities of\n\\(A^t\\) with multiple events \\(B^{t_i}\\). Granted (P) and its\ngeneralization, (R) reflects the dynamical properties of systems.  \nFigure 8: Condition (P). \nBefore briefly introducing the next element of the analysis let us\nmention that there is a question about whether the association of\nprobabilities with the measure of the system is reasonable. Prima\nfacie, a measure on a phase space can have a purely geometrical\ninterpretation and need not necessarily have anything to do with the\nquantification of uncertainty. For instance, we can use a measure to\ndetermine the length of a table, but this measure need not have\nanything to do with uncertainty. Whether or not such an association is\nlegitimate depends on the cases at hand and the interpretation of the\nmeasure. However, for systems of interest in statistical physics it is\nnatural and indeed standard to assume that the probability of the\nsystem’s state to be in a particular subset of the phase space\n\\(X\\) is proportional to the measure of \\(A\\). \nThe last element to be introduced is the notion of the correlation\nbetween two subsets \\(A\\) and \\(B\\) of the system’s phase space,\nwhich is defined as follows: \nIf the value of \\(C(A, B)\\) is positive (negative), there is positive\n(negative) correlation between \\(A\\) and \\(B\\); if it is zero, then\n\\(A\\) and \\(B\\) are uncorrelated. It then follows immediately from the\nabove that \n(RC) constitutes the basis for the interpretation of EH as a hierarchy\nof objective randomness. Granted this equation, the subjective\nprobabilistic relevance of the event \\(B^{t_1}\\) for the event \\(A^t\\)\nreflects objective dynamical properties of the system since for\ndifferent transformations \\(T\\ R(B^{t_1}, A^t)\\) will indicate\ndifferent kinds of probabilistic relevance of \\(B^{t_1}\\) for \\(A^t\\).\n \nTo put (RC) to use, it is important to notice that the equations\ndefining the various levels of EH above can be written in terms of\ncorrelations. Taking into account that we are dealing with discrete\nsystems (and hence we have \\(T_{t_1\\rightarrow t}B = T_k B\\) where\n\\(k\\) is the number of time steps it takes to get from \\(t_1\\) and\n\\(t)\\), these equations read: \nApplying (RC) to these expressions, we can explicate the nature of the\nunpredictability that each of the different levels of EH involves.\n \nLet us start at the top of EH. In Bernoulli systems the probabilities\nof the present state are totally independent of whatever happened in\nthe past, even if the past is only one time step back. So knowing the\npast of the system does not improve our predictive abilities in the\nleast; the past is simply irrelevant to predicting the future. This\nfact is often summarised in the slogan that Bernoulli systems are as\nrandom as a coin toss. We should emphasise, however, that this is true\nonly for events in the Bernoulli partition; the characterisation of a\nBernoulli system is silent about what random properties partitions\nother than the Bernoulli partition have. \nK-mixing is more difficult to analyse. We now have to tackle the\nquestion of how to understand \\(\\sigma(n, r)\\), the minimal\n\\(\\sigma\\)-algebra generated by the set \nthat we sidestepped earlier on. What matters for our analysis is that\nthe following types of sets are members of \nwhere the indices \\(j_i\\) range over \\(1, \\ldots ,r\\). Since we are\nfree to chose the sets A\\(_0\\), A\\(_1,\\ldots\\), A\\(_r\\) as we please,\nwe can always chose them so that they are the past history of the\nsystem: the system was in \\(A_{j_0}\\) \\(k\\) time steps back, in\n\\(A_{j_1}\\) \\( k+1\\) time steps back, etc. Call this the\n(coarse-grained) remote past of the\nsystem—‘remote’ because we only consider states that\nare more than \\(k\\) time steps back. The K-mixing condition then says\nthat the system’s entire remote past history becomes\nirrelevant to predicting what happens in the future as time tends\ntowards infinity. Typically Bernoulli systems are compared with\nK-systems by focussing on the events in the Bernoulli partition. With\nrespect to that partition K is weaker than Bernoulli.  The difference\nis both in the limit and the remote history. In a Bernoulli system the\nfuture is independent of the entire past (not only the remote\npast), and this is true without taking a limit (in the case of\nK-mixing independence only obtains in the limit).  However, this only\nholds for the Bernoulli partition; it may or may not hold for other\npartitions—the definition of a Bernoulli system says nothing\nabout that case.[22] \nThe interpretation of strong mixing is now straightforward. It says\nthat for any two sets \\(A\\) and \\(B\\), having been in \\(B\\) \\(k\\) time\nsteps back becomes irrelevant to the probability of being in \\(A\\)\nsome time in the future if time tends towards infinity (i.e. when n\ntends to infinity). In other words, past events \\(B\\) become\nincreasingly irrelevant for the probability of \\(A\\) as the temporal\ndistance between \\(A\\) and \\(B\\) becomes larger. This condition is\nweaker than K-mixing because it only states that the future is\nindependent of isolated events in the remote past, while K-mixing\nimplies independence of the entire remote past history. \nIn weakly mixing systems the past may be relevant to predicting the\nfuture, even in the remote past. The weak mixing condition only says\nthat this influence has to be weak enough for it to be the case that\nthe absolute value of the correlations between a future event and past\nevents vanishes on average; but this does not mean that all individual\ncorrelations vanish. So in weakly mixing systems events in the past\ncan remain relevant to the future. \nErgodicity, finally, implies no decay of correlation at all. The\nergodicity condition only says that the average of the correlations\n(and this time without an absolute value) of all past events with a \nfuture event is zero. But this is compatible with there being\nstrong correlations between every instant in the past and the future,\nprovided that positive and negative correlations average out. So in\nergodic systems the past does not become irrelevant. For this reason\nergodic system are not random at all (in the sense of random\nintroduced above). \nHow relevant are these insights to understanding the behaviour of\nactual systems? A frequently heard objection (which we have already\nencountered in Section 4) is that EH and more generally ergodic theory\nare irrelevant since most systems (including those that we are\nultimately interested in) are not ergodic at\n all.[23] \nThis charge is less acute than it appears at first glance. First, it\nis important to emphasise that it is not the sheer number of\napplications that make a physical concept important, but whether there\nare some important systems that are ergodic. And there are examples of\nsuch systems. For example, so-called ‘hard-ball systems’\n(and some more sophisticated variants of them) are effective\nidealizations of the dynamics of gas molecules, and these systems seem\nto be ergodic;for details, see Berkovitz, Frigg and Kronz 2006,\nSection 3.2, Vranas (1998) and Frigg and Werndl (2011). \nFurthermore, EH can be used to characterize randomness and chaos in\nboth ergodic and non-ergodic systems. Even if a system as a whole is\nnot ergodic (i.e., if it fails to be ergodic with respect to the\nentire phase space \\(X)\\), there can be (and usually there\nare) subsets of \\(X\\) on which the system is ergodic. This is what\nLichtenberg and Libermann (1992, p. 295) have in mind when they\nobserve that ‘[i]n a sense, ergodicity is universal, and the\ncentral question is to define the subspace over which it\nexists’. In fact, non-ergodic systems may have subsets that are\nnot only ergodic, but even Bernoulli! It then becomes interesting\nto ask what these subsets are, what their measures are, and what\ntopological features they have. These are questions studied in parts\nof dynamical systems theory, most notably KAM theory. Hence, KAM\ntheory does not demonstrate that ergodic theory is not useful in\nanalyzing the dynamical behavior of real physical systems (as is often\nclaimed). Indeed, KAM systems have regions in which the system\nmanifest either merely ergodic or Bernoulli behaviour, and accordingly\nEH is useful for charactering the dynamical properties of such systems\n(Berkovitz, Frigg and Kronz 2006, Section 4). Further, as we have\nmentioned in Section 4.1, almost all Hamiltonian systems are\nnon-integrable, and accordingly they have large regions of the phase\nspace in which their motion is ergodic-like. So EH is a useful tool in\nstudying the dynamical properties of systems even if the system fails\nto be ergodic tout court. \nAnother frequently heard objection is that EH is irrelevant in\npractice because most levels of EH (in fact, all except Bernoulli) are\ndefined in terms of infinite time limits and hence remain\nsilent about what happens in finite time. But all we ever\nobserve are finite times and so EH is irrelevant to physics as\npracticed by actual scientists. \nThis charge can be dispelled by a closer look at the definition of a\nlimit, which shows that infinite limits in fact have\nimportant implications for the dynamical behaviour of the system in\nfinite times. The definition of a limit is as follows (where \\(f\\) is\nan arbitrary function of time): lim\\(_{t\\rightarrow \\infty} f(t) = c\\)\niff for every \\(\\varepsilon \\gt 0\\) there exists an \\(t’ \\gt 0\\) so\nthat for all \\(t \\gt t’\\) we have \\(\\lvert f(t) - c\\rvert \\lt\n\\varepsilon\\). In words, for every number \\(\\varepsilon\\), no matter\nhow small, there is a finite time \\(t’\\) after which the\nvalues of \\(f\\) differ from \\(c\\) by less then \\(\\varepsilon\\). That\nis, once we are past \\(t’\\) the values of \\(f\\) never move more than\n\\(\\varepsilon\\) away from \\(c\\). With this in mind strong mixing, for\ninstance, says that for a given threshold \\(\\varepsilon\\) there exists\na finite time \\(t_n (n\\) units of time after the current\ntime) after which \\(C(T_n B, A)\\) is always smaller than\n\\(\\varepsilon\\). We are free to choose \\(\\varepsilon\\) to be an\nempirically relevant margin, and so we know that if a system is\nmixing, we should expect the correlations between the states of the\nsystem after t\\(_n\\) and its current state to be below\n\\(\\varepsilon\\). The upshot is that in strong mixing systems, being in\na state \\(B\\) at some past time becomes increasingly irrelevant for\nits probability of being in the state \\(A\\) now, as the temporal\ndistance between \\(A\\) and \\(B\\) becomes larger. Thus, the fact that\nsystem is strong mixing clearly has implications for its dynamical\nbehaviour in finite times. Furthermore, often (although not always)\nconvergence proofs provide effective bounds on rates of convergence\nand these bounds can be used to inform expectations about behaviour at\na given time. \nSince different levels of EH correspond to different degrees of\nrandomness, each explicated in terms of a different type of asymptotic\ndecay of correlations between states of systems at different times,\none might suspect that a similar pattern can be found in the rates of\ndecay. That is, one might be tempted to think that EH can equally be\ncharacterized as a hierarchy of increasing rates of decay of\ncorrelations: a K-system, for instance, which exhibits exponential\ndivergence of trajectories would be characterized by an exponential\nrate of decay of correlations, while a SM-system would exhibit a\npolynomial rate of decay. \nThis, unfortunately, does not work. Natural as it may seem, EH cannot\nbe interpreted as a hierarchy of increasing rates of decay of\ncorrelations. It is a mathematical fact that there is no particular\nrate of decay associated with each level of EH. For instance, one can\nconstruct K-systems in which the decay is as slow as one wishes it to\nbe. So the rate of decay is a feature of particular systems rather\nthan of a level of EH. \nThe question of how to characterise chaos has been\ncontroversially discussed ever since the inception of chaos theory;\nfor a survey see Smith (1998, Ch. 10). An important family of\napproaches defines chaos using EH. Belot and Earman (1997, 155) state\nthat being strong mixing is a necessary condition and being a K-system\nis a sufficient condition for a system to be chaotic. The view that\nbeing a K-system is the mark of chaos and that any lower degree of\nrandomness is not chaotic is frequently motivated by two ideas. The\nfirst is the idea that chaotic behaviour involves dynamical\ninstability in the form of exponential divergence of nearby\ntrajectories. Thus, since a system involves an exponential divergence\nof nearby trajectories only if it is a K-system, it is concluded that\n(merely) ergodic and mixing systems are not chaotic whereas K- and\nB-systems are. It is noteworthy, however, that SM is compatible with\nthere being polynomial divergence of nearby trajectories and that such\ndivergence sometimes exceeds exponential divergence in the short run.\nThus, if chaos is to be closely associated with the rate of divergence\nof nearby trajectories, there seems to be no good reason to deny that\nSM systems exhibit chaotic behaviour. \nThe second common motivation for the view that being a K-system is the\nmark of chaos is the idea that the shift from zero to positive\nKS-entropy marks the transition from a ‘regular’ to\n‘chaotic’ behaviour. This may suggest that having positive\nKS-entropy is both necessary and sufficient condition for chaotic\nbehaviour. Thus, since K-systems have positive KS-entropy while SM\nsystems don’t, it is concluded that K-systems are chaotic\nwhereas SM-systems are not. Why is KS-entropy a mark of chaos? There\nare three motivations, corresponding to three different\ninterpretations of KS-entropy. First, KS-entropy could be interpreted\nas entailing dynamical instability in the sense of having nearby\ndivergence of nearby trajectories (see Lichtenberg & Liebermann,\n1992, p. 304). Second, KS-entropy could be connected to algorithmic\ncomplexity (Brudno 1978). Yet, while such a complexity is sometimes\nmentioned as an indication of chaos, it is more difficult to connect\nit to physical intuitions about chaos. Third, KS-entropy could be\ninterpreted as a generalized version of Shannon’s information\ntheoretic entropy (see Frigg 2004). According to this approach,\npositive KS-entropy entails a certain degree of unpredictability,\nwhich is sufficiently high to deserve the title\n chaotic.[24] \nWerndl (2009b) argues that a careful review of all systems that one\ncommonly regards as chaotic shows that strong mixing is the crucial\ncriterion: a system is chaotic just in case it is strong mixing. As\nshe is careful to point out, this claim needs to be qualified: systems\nare rarely mixing on the entire phase space, but neither are they\nchaotic on the entire phase space. The crucial move is to restrict\nattention to those regions of phase space where the system is chaotic,\nand it then turns out that in these same regions the systems are also\nstrong mixing. Hence Werndl concludes that strong mixing is the\nhallmark of chaos. And surprisingly this is true also of dissipative\nsystems (i.e., systems that are not measure preserving). These systems\nhave attractors, and they are chaotic on their attractors rather than\non the entire phase space. The crucial point then is that one can\ndefine an invariant (preserved) measure on the attractor and\nshow that the system is strongly mixing with respect to that measure.\nSo strong mixing can define chaos in both conservative and dissipative\nsystems. \nThe search for necessary and sufficient conditions for chaos\npresupposes that there is a clear-cut divide between chaotic and\nnon-chaotic systems. EH may challenge this view, as every attempt to\ndraw a line somewhere to demarcate the chaotic from non-chaotic\nsystems is bound to be somewhat arbitrary. Ergodic systems are pretty\nregular, mixing systems are less regular and the higher positions in\nthe hierarchy exhibit still more haphazard behaviour. But is there one\nparticular point where the transition from ‘non-chaos’ to\nchaos takes place? Based on the argument that EH is a hierarchy of\nincreasing degrees of randomness and degrees of randomness correspond\nto different degrees of unpredictability (see Section 5), Berkovitz,\nFrigg and Kronz (2006, Section 5.3) suggest that chaos may well be\nviewed as a matter of degree rather than an all-or-nothing affair.\nBernoulli systems are very chaotic, K-systems are slightly less\nchaotic, SM-systems are still less chaotic, and ergodic systems are\nnon-chaotic. This suggestion connects well with the idea that chaos is\nclosely related to unpredictability. \nThe ergodic hierarchy has also been used to understand quantum chaos.\nCastagnino and Lombardi (2007) analyze the problem of quantum chaos as\na particular case of the classical limit of quantum mechanics and\nidentify mixing in the classical limit as the condition that a quantum\nsystem must satisfy to be nonintegrable. Gomez and Castagnino (2014,\n2015) generalize the entire ergodic hierarchy to the quantum context\nand argue that EH thus generalized is a helpful tool to understand\nquantum chaos; Fortin and Lombardi (2018) use EH to understand\ndecoherence; and Gomez (2018) discusses the KS entropy in quantum\nmixing systems. \nMixing, finally, has also been invoked in understanding the effects of\nstructural model error. Frigg, Bradley, Du and Smith (2014) argue that\nthe distinction between parameter error and structural model error is\ncrucial, and that the latter has significant and hitherto\nunappreciated impact on the predictive ability of a model. Wilson-Mayo\n(2015) points out that to put this observation on a solid foundation\nwe need a notion of structural chaos. He proposes such a notion by\nappealing to topological mixing.  \nEH is often regarded as relevant for explicating the nature of\nrandomness in deterministic dynamical systems. It is not clear,\nhowever, what notion of randomness this claim invokes. The formal\ndefinitions of EH do not make explicit appeal to randomness and the\nusual ways of presenting EH do not involve any specification of the\nnotion of randomness that is supposed to underlie EH. As suggested in\nSection 5, EH can be interpreted as a hierarchy of randomness if\ndegrees of randomness are explicated in terms of degrees of\nunpredictability, which in turn are explicated in terms of (coherent)\nconditional degrees of beliefs. In order for these degrees of belief\nto be indicative of the system’s dynamical properties, they have\nto be updated according to a system’s dynamical law. The idea is\nthen that the different levels of EH, except for merely ergodic\nsystems, correspond to different kinds of unpredictability, which\ncorrespond to different patterns of decay of correlations between\nsystems’ past states and their present states. Merely ergodic\nsystems seem to display no randomness, as the correlations between\ntheir past and present states need not decay at all. \nErgodic theory plays an important role in statistical physics, and EH,\nor some modification of it, constitutes an important measure of\nrandomness in both Hamiltonian and dissipative systems. It is\nsometimes argued that EH is by and large irrelevant for physics\nbecause real physical systems are not ergodic. But, this charge is\nunwarranted, and a closer look at non-ergodic systems reveals a rather\ndifferent picture, because EH can be fruitfully be used in the\nfoundations of statistical mechanics, analyses of randomness, and\nchaos theory. More recently it has also played a role in understanding\nlaws of nature (Filomeno 2019, List and Pivato 2019).","contact.mail":"jzberkovitz@yahoo.com","contact.domain":"yahoo.com"},{"date.published":"2011-04-13","date.changed":"2020-07-16","url":"https://plato.stanford.edu/entries/ergodic-hierarchy/","author1":"Roman Frigg","author2":"Joseph Berkovitz","author1.info":"http://www.lse.ac.uk/collections/philosophyLogicAndScientificMethod/WhosWho/staffhomepages/frigg.htm","entry":"ergodic-hierarchy","body.text":"\n\n\nThe Ergodic Hierarchy (EH) is a central part of ergodic theory. It is\na hierarchy of properties that dynamical systems can possess. Its five\nlevels are ergodicity, weak mixing, strong mixing, Kolmogorov, and\nBernoulli. Although EH is a mathematical theory, its concepts have\nbeen widely used in the foundations of statistical physics, accounts\nof randomness, and discussions about the nature of chaos, as well as\nin other sciences such economics. We introduce EH and discuss its\napplications.\n\nThe object of study in ergodic theory is a dynamical system. We first\nintroduce some basic concepts with a simple example, from which we\nabstract the general definition of a dynamical system. For a brief\nhistory of the modern notion of a dynamical system and the associated\nconcepts of EH see the\n Appendix,\n Section A. \nA lead ball is hanging from the ceiling on a spring. We then pull it\ndown a bit and let it go. The ball begins to oscillate. The mechanical\nstate of the ball is completely determined by a specification of the\nposition \\(x\\) and the momentum \\(p\\) of its center of mass; that is,\nif we know \\(x\\) and \\(p\\), then we know all that there is to know\nabout the mechanical state of the ball. If we now conjoin \\(x\\) and\n\\(p\\) in one vector space we obtain the so-called phase space\n\\(X\\) of the system (sometimes also referred to as ‘state\n space’).[1]\n This is illustrated in Figure 1 for a two-dimensional phase space of\nthe state of a ball moving up and down (i.e., the phase space has one\ndimension for the ball’s position and one for its momentum). \nFigure 1: The motion of a ball on a\nspring. \nEach point of \\(X\\) represents a state of the ball (because it gives\nthe ball’s position and momentum). Accordingly, the time\nevolution of the ball’s state is represented by a line in \\(X\\),\na so-called phase space trajectory (from now on\n‘trajectory’), showing where in phase space the system was\nat each instant of time. For instance, let us assume that at time \\(t\n= 0\\) the ball is located at point \\(x_1\\) and then moves to \\(x_2\\)\nwhere it arrives at time \\(t = 5\\). This motion is represented in\n\\(X\\) by the line segment connecting points \\(\\gamma_1\\) and\n\\(\\gamma_2\\). In other words, the motion of the ball is represented in\n\\(X\\) by the motion of a point representing the ball’s\n(instantaneous) state, and all the states that the ball is in over the\ncourse of a certain period of time jointly form a trajectory. The\nmotion of this point has a name: it is the phase flow\n\\(\\phi_t\\). The phase flow tells us where the ball is at some later\ntime \\(t\\) if we specify where it is at \\(t = 0\\); or, metaphorically\nspeaking, \\(\\phi_t\\) drags the ball’s state around in \\(X\\) so\nthat the movement of the state represents the motion of the real ball.\nIn other words, \\(\\phi_t\\) is a mathematical representation of the\nsystem’s time evolution. The state of the ball at time \\(t = 0\\)\nis commonly referred to as the initial condition. \\(\\phi_t\\)\nthen tells us, for every point in phase space, how this point evolves\nif it is chosen as an initial condition. In our concrete example, point\n\\(\\gamma_1\\) is the initial condition and we have \\(\\gamma_2 =\n\\phi_{t=5}(\\gamma_1)\\). More generally, let us call the ball’s\ninitial condition \\(\\gamma_0\\) and let \\(\\gamma(t)\\) be its state at\nsome later time \\(t\\). Then we have \\(\\gamma(t) = \\phi_t (\\gamma_0)\\).\nThis is illustrated in figure 2a. \nFigure 2: Evolution in Phase space. \nSince \\(\\phi_t\\) tells us for every point in \\(X\\) how it evolves in\ntime, it also tells us how sets of points move around. For instance,\nchoose an arbitrary set \\(A\\) in \\(X\\); then \\(\\phi_t (A)\\) is the\nimage of A after \\(t\\) time units under the dynamics of the\nsystem. This is illustrated in Figure 2b. Considering sets of points\nrather than single points is important when we think about physical\napplications of this mathematical formalism. We can never determine\nthe exact initial condition of a ball bouncing on a spring. No matter\nhow precisely we measure \\(\\gamma_0\\), there will always be some\nmeasurement error. So what we really want to know in practical\napplications is not how a precise mathematical point evolves, but\nrather how a set of points around the initial condition \\(\\gamma_0\\)\nevolves. In our example with the ball the evolution is\n‘tame’, in that the set keeps its original shape. As we\nwill see below, this is not always the case. \nAn important feature of \\(X\\) is that it is endowed with a so-called\nmeasure \\(\\mu\\). We are familiar with measures in many\ncontexts: from a mathematical point of view, the length that we\nattribute to a part of a line, the surface we attribute to a part of a\nplane, and the volume we attribute to a segment of space are measures.\nA measure is simply a device to attribute a ‘size’ to a\npart of a space. Although \\(X\\) is an abstract mathematical space, the\nleading idea of a measure remains the same: it is a tool to quantify\nthe size of a set. So we say that the set \\(A\\) has measure \\(\\mu(A)\\)\nin much the same way as we say that a certain collection of points of\nordinary space (for instance the ones that lie on the inside of a\nbottle) have a certain volume (for instance one litre). \nFrom a more formal point of view, a measure assigns numbers to certain\nsubsets of a set \\(X\\) (see Appendix B for a formal definition). This\ncan be done in different ways and hence there are different measures.\nConsider the example of a plane. There is a measure that simply\nassigns to each appropriate region of a plane the area of that region.\nBut now imagine that we pour a bucket of sugar on the plane. The sugar\nis not evenly distributed; there are little heaps in some places while\nthere is almost no sugar in other places. A measure different from the\narea measure is one that assigns to a region a number that is equal to\nthe amount of sugar on that region. One of these measures is\nparticularly important, namely the so-called Lebesgue\nmeasure. This measure has an intuitive interpretation: it is just\na precise formalisation of the measure we commonly use in geometry.\nThe interval [0, 2] has Lebesgue measure 2 and the interval [3, 4] has\nLebegues measure 1. In two dimensions, a square whose sides have\nLebesgue measure 2 has Lebesgue measure 4; etc. Although this sounds\nsimple, the mathematical theory of measures is rather involved. We\nstate the basics of measure theory in the\n Appendix,\n Section B, and avoid appeal to technical issues in measure theory in\nwhat follows. \nThe essential elements in the discussion so far were the phase space\n\\(X\\), the time evolution \\(\\phi_t\\), and the measure \\(\\mu\\). And\nthese are also the ingredients for the definition of an abstract\ndynamical system. An abstract dynamical system is a triple \\([X, \\mu ,\nT_t]\\), where \\(\\{T_t \\mid t \\text{ is an instant of time}\\}\\) is a\nfamily of automorphisms, i.e., a family of transformations of \\(X\\)\nonto itself with the property that \\(T_{t_1 +t_2} = T_{t_1}(T_{t_2})\\)\nfor all \\(x \\in X\\) (Arnold and Avez 1968, 1); we say more about time\n below.[2]\n In the above example \\(X\\) is the phase space of the ball’s\nmotion, \\(\\mu\\) is the Lebesgue measure, and \\(T_t\\) is\n\\(\\phi_t\\). \nSo far we have described \\(T_t\\) as giving the time evolution of a\nsystem. Now let us look at this from a more mathematical point of\nview: the effect of \\(T_t\\) is that it assigns to every point in \\(X\\)\nanother point in \\(X\\) after \\(t\\) time units have elapsed. In the\nabove example \\(\\gamma_1\\) is mapped onto \\(\\gamma_2\\) under\n\\(\\phi_t\\) after \\(t = 5\\) seconds. Hence, from a mathematical point\nof view the time evolution of a system consists in a mapping of \\(X\\)\nonto itself, which is why the above definition takes \\(T_t\\) to be a\nfamily of mappings of \\(X\\) onto itself. Such a mapping is a\nprescription that tells you for every point \\(x\\) in \\(X\\) on which\nother point in \\(X\\) it is mapped (from now on we use \\(x\\) to denote\nany point in \\(X\\), and it no longer stands, as in the above example,\nfor the position of the ball).  \nThe systems studied in ergodic theory are forward deterministic. This\nmeans that if two identical copies of that system are in the same\nstate at one instant of time, then they must be in the same\nstate at all future instants of time. Intuitively speaking,\nthis means that for any given time there is only one way in which the\nsystem can evolve forward. For a discussion of determinism see Earman\n(1986). \nIt should be pointed out that no particular interpretation is intended\nin an abstract dynamical system. We have motivated the definition with\nan example from mechanics, but dynamical systems are not tied to that\ncontext. They are mathematical objects in their own right, and as such\nthey can be studied independently of particular applications. This\nmakes them a versatile tool in many different domains. In fact,\ndynamical systems are used, among others, in fields as diverse as\nphysics, biology, geology and economics.  \nThere are many different kinds of dynamical systems. The three most\nimportant distinctions are the following. \nDiscrete versus continuous time. We may consider discrete\ninstants of time or a continuum of instants of time. For ease of\npresentation, we shall say in the first case that time is discrete and\nin the second case that time is continuous. This is just a convenient\nterminology that has no implications for whether time is fundamentally\ndiscrete or continuous. In the above example with the ball time was\ncontinuous (it was taken to be a real number). But often it is\nconvenient to regard time as discrete. If time is continuous, then\n\\(t\\) is a real number and the family of automorphisms is \\(\\{T_t \\mid\nt \\in \\mathbb{R} \\}\\), where \\(\\mathbb{R}\\) is the set of real numbers. If time is\ndiscrete, then \\(t\\) is in the set \\(\\mathbb{Z} = \\{\\ldots -2, -1, 0,\n1, 2, \\ldots \\}\\), and the family of automorphisms is \\(\\{T_t \\mid t\n\\in \\mathbb{Z}\\}\\). In order to indicate that we are dealing with a\ndiscrete family rather than a continuous one we sometimes replace\n‘\\(T_t\\)’ with ‘\\(T_n\\)’; this is just a\nnotational convention of no conceptual\n importance.[3]\n In such systems the progression from one instant of time to the next\nis also referred to as a ‘step’. In population biology,\nfor instance, we often want to know how a population grows over a\ntypical breeding time (e.g. one year). In mathematical models of such\na population the points in \\(X\\) represent the size of a population\n(rather than the position and the momentum of a ball, as in the above\nexample), and the transformation \\(T_n\\) represents the growth of the\npopulation after \\(n\\) time units. A simple example would be \\(T_n = x\n+ n\\). \nDiscrete families of automorphisms have the interesting property that\nthey are generated by one mapping. As we have seen above, all\nautomorphisms satisfy \\(T_{t_1 +t_2} = T_{t_1}(T_{t_2})\\). From this\nit follows that \\(T_n (x) = T^{n}_1 (x)\\), that is \\(T_n\\) is the\n\\(n\\)-th iterate of \\(T_1\\). In this sense \\(T_1\\) generates \\(\\{T_t\n\\mid t \\in \\mathbb{Z}\\}\\); or, in other words, \\(\\{T_t \\mid t \\in\n\\mathbb{Z}\\}\\) can be ‘reduced’ to \\(T_1\\). For this\nreason one often drops the subscript ‘1’, simply calls the\nmap ‘\\(T\\)’, and writes the dynamical system as the triple\n\\([X, \\mu , T]\\), where it is understood that \\(T = T_1\\). \nFor ease of presentation we use discrete transformations from now on.\nThe definitions and theorems we formulate below carry over to\ncontinuous transformations without further ado, and where this is not\nthe case we explicitly say so and treat the two cases separately. \nMeasure preserving versus non-measure preserving\ntransformations. Roughly speaking, a transformation is measure\npreserving if the size of a set (like set \\(A\\) in the above example)\ndoes not change over the course of time: a set can change its form but\nit cannot shrink or grow (with respect to the measure). Formally,\n\\(T\\) is a measure-preserving transformation on \\(X\\) if and\nonly if (iff) for all sets \\(A\\) in \\(X: \\mu(A) = \\mu(T^{-1}(A))\\),\nwhere \\(T^{-1}(A)\\) is the set of points that gets mapped onto \\(A\\)\nunder \\(T\\); that is \\(T^{-1}(A) = \\{ x \\in X \\mid T(x) \\in A\n \\}\\).[4]\n From now on we also assume that the transformations we consider are\nmeasure\n preserving.[5] \nIn sum, from now on, unless stated otherwise, we consider discrete\nmeasure preserving transformations. \nIn order to introduce the concept of ergodicity we have to introduce\nthe phase and the time mean of a function \\(f\\) on \\(X\\).\nMathematically speaking, a function assigns each point in \\(X\\) a\nnumber. If the numbers are always real the function is a real-valued\nfunction; and if the numbers may be complex, then it is a\ncomplex-valued function. Intuitively we can think of these numbers as\nrepresenting the physical quantities of interest. Recalling the\nexample of the bouncing ball, \\(f\\) could for instance assign each\npoint in the phase space \\(X\\) the kinetic energy the system has at\nthat point; in this case we would have \\(f = p^2 / 2m\\), where \\(m\\)\nis the mass of the ball. For every function we can take two kinds of\naverages. The first is the infinite time average \\(f^*\\). The general\nidea of a time average is familiar from everyday contexts. You play\nthe lottery on three consecutive Saturdays. On the first you win $10;\non the second you win nothing; and on the third you win $50. Your\naverage gain is ($10 + $0 + $50)/3 = $20. Technically speaking this is\na time average. This simple idea can easily be put to use in a\ndynamical system: follow the system’s evolution over time (and\nremember that we are now talking about an average for discrete points\nof time), take the value of the relevant function at each step, add\nthe values, and then divide by the number of steps. This yields \nwhere \nis just an abbreviation for \nThis is the finite time average for \\(f\\) after \\(k\\) steps. If the\nsystem’s state continues to evolve infinitely and we keep\ntracking the system forever, then we get the infinite time\naverage: \nwhere the symbol ‘lim’ (from latin ‘limes’,\nmeaning border or limit) indicates that we are letting time tend\ntowards infinity (in mathematical symbols: \\(\\infty)\\). One point\ndeserves special attention, since it will become crucial later on: the\npresence of \\(x_0\\) in the above expression. Time averages depend on\nwhere the system starts; i.e., they depend on the initial condition.\nIf the process starts in a different state, the time average may well\nbe different. \nNext we have the space average \\(\\bar{f}\\). Let us again start with a\ncolloquial example: the average height of the students in a particular\nschool. This is easily calculated: just take each student’s\nheight, add up all the numbers, and divide the result by the number of\nstudents. Technically speaking this is a space average. In\nthe example the students in the school correspond to the points in\n\\(X\\); and the fact that we count each students once (we don’t,\nfor instance, take John’s height into account twice and omit\nJim’s) corresponds to the choice of a measure that gives equal\n‘weight’ to each point in \\(X\\). The transformation \\(T\\)\nhas no pendant in our example, and this is deliberate: space averages\nhave nothing to do with the dynamics of the system (that’s what\nsets them off from time averages). The general mathematical definition\nof the space average is as follows: \nwhere \\(\\int_X\\) is the integral over the phase space\n \\(X\\).[6]\n If the space consists of discrete elements, like the students of the\nschool (they are ‘discrete’ in that you can count them),\nthen the integral becomes equivalent to a sum like the one we have\nwhen we determine the average height of a population. If the \\(X\\) is\ncontinuous (as the phase space above) things are a bit more\ninvolved. \nWith these concepts in place, we can now define\n ergodicity.[7]\n A dynamical system \\([X, \\mu , T]\\) is ergodic iff \nfor all complex-valued Lebesgue integrable functions \\(f\\) almost\neverywhere, meaning for almost all initial conditions. The\nqualification ‘almost everywhere’ is non-trivial and is\nthe source of a famous problem in the foundations of statistical\nmechanics, the so-called ‘measure zero problem’ (to which\nwe turn in Section 3). So it is worth unpacking carefully what this\ncondition involves. Not all sets have a finite size. In fact, there\nare sets of measure zero. This may sound abstract but is very natural.\nTake a ruler and measure the length of certain objects. You will find,\nfor instance, that your pencil is 17cm long—in the language of\nmathematics this means that the one dimensional Lebegue measure of the\npencil is 17. Now measure a geometrical point and answer the question:\nhow long is the point? The answer is that such a point has no\nextension and so its length is zero. In mathematical parlance: a set\nconsisting of a geometrical point is a measure zero set. The same goes\nfor a set of two geometrical points: also two geometrical points\ntogether have no extension and hence have measure zero. Another\nexample is the following: you have device to measure the surface of\nobjects in a plane. You find out that an A4 sheet has a surface of\n623.7 square centimetres. Then you are asked what the surface of a\nline is. The answer is: zero. Lines don’t have surfaces. So with\nrespect to the two dimensional Lebesgue measure lines are measure zero\nsets. \nIn the context of ergodic theory, ‘almost everywhere’\nmeans, by definition, ‘everywhere in \\(X\\) except, perhaps, in a\nset of measure zero’. That is, whenever a claim is qualified as\n‘almost everywhere’ it means that it could be false for\nsome points in \\(X\\), but these taken together have measure zero. Now\nwe are in a position to explain what the phrase means in the\ndefinition of ergodicity. As we have seen above, the time average (but\nnot the space average!) depends on the initial condition. If we say\nthat \\(f^* = \\bar{f}\\) almost everywhere we mean that all those\ninitial conditions for which it turns out to be the case that \\(f^*\n\\ne \\bar{f}\\) taken together form a set of measure zero—they are\nlike a line in the plane. \nArmed with this understanding of the definition of ergodicity, we can\nnow discuss some important properties of ergodic systems. Consider a\nsubset \\(A\\) of \\(X\\). For instance, thinking again about the example\nof the oscillating ball, take the left half of the phase space. Then\ndefine the so-called characteristic function of \\(A, f_A\\), as\nfollows: \\(f_A (x) = 1\\) for all \\(x\\) in \\(A\\) and \\(f_A (x) = 0\\)\nfor all \\(x\\) not in \\(A\\). Plugging this function into the definition\nof ergodicity yields: \\(f^{*}_A = \\mu(A)\\). This means that the\nproportion of time that the system’s state spends in set \\(A\\)\nis proportional to the measure of that set. To make this even more\nintuitive, assume that the measure is normalised: \\(\\mu(X) = 1\\) (this\nis a very common and unproblematic assumption). If we then choose\n\\(A\\) so that \\(\\mu(A) = 1\\) ⁄ 2, then we know that the system\nspends half of the time in \\(A\\); if \\(\\mu(A) = 1\\) ⁄ 4, it\nspends a quarter of the time in \\(A\\); etc. As we will see below, this\nproperty of ergodic systems plays a crucial role in certain approaches\nto statistical mechanics. \nSince we are free to choose \\(A\\) as we wish, we immediately get\nanother important result: a system can be ergodic only if its\ntrajectory may access all parts of \\(X\\) of positive measure, i.e., if\nthe trajectory passes arbitrarily close to any point in \\(X\\)\ninfinitely many times as time tends towards infinity. And this implies\nthat the phase space of ergodic systems is called metrically\nindecomposable (or also ‘irreducible’ or\n‘inseparable’): every set invariant under \\(T\\) (i.e.,\nevery set that is mapped onto itself under \\(T)\\) has either measure 0\nor 1. As a consequence, \\(X\\) cannot be divided into two or more\nsubspaces (of non-zero measure) that are invariant under \\(T\\).\nConversely, a non-ergodic system is metrically decomposable. Hence,\nmetric indecomposability and ergodicity are equivalent. A metrically\ndecomposable system is schematically illustrated in Figure 3. \nFigure 3: Reducible system: no point in\nregion \\(P\\) evolves into region \\(Q\\) and vice versa. \nFinally, we would like to state a theorem that will become important\nin Section 4. One can prove that a system is ergodic iff \nholds for all subsets \\(A\\) and \\(B\\) of \\(X\\). Although this\ncondition does not have an immediate intuitive interpretation, we will\nsee below that it is crucial for the understanding of the kind of\nrandomness we find in ergodic systems. \nIt turns out that ergodicity is only the bottom level of an entire\nhierarchy of dynamical properties. This hierarchy is called the\nergodic hierarchy, and the study of this hierarchy is the\ncore task of a mathematical discipline called ergodic theory.\nThis choice of terminology is somewhat misleading, since ergodicity is\nonly the bottom level of this hierarchy and so EH contains much more\nthan ergodicity and the scope of ergodic theory stretches far beyond\nergodicty. Ergodic theory (thus understood) is part of dynamical\nsystems theory, which studies a wider class of dynamical systems\nthan ergodic theory.  \nEH is a nested classification of dynamical properties. The hierarchy\nis typically represented as consisting of the following five\nlevels: \nBernoulli \\(\\subset\\) Kolmogorov \\(\\subset\\) Strong Mixing \\(\\subset\\)\nWeak Mixing \\(\\subset\\) Ergodic  \nThe diagram is intended to indicate that all Bernoulli systems are\nKolmogorov systems, all Kolmogorov systems are strong mixing systems,\nand so on. Hence all systems in EH are ergodic. However, the converse\nrelations do not hold: not all ergodic systems are weak mixing, and so\non. In what follows a system that is ergodic but not weak mixing is\nreferred to as merely ergodic and similarly for the next\nthree\n levels.[8] \nFigure 4: Mixing \nMixing can be intuitively explained by the following example, first\nused by Gibbs in introducing the concept of mixing. Begin with a glass\nof water, then add a shot of scotch; this is illustrated in Fig. 4a.\nThe volume \\(C\\) of the cocktail (scotch + water) is \\(\\mu(C)\\) and\nthe volume of scotch that was added to the water is \\(\\mu(S)\\), so\nthat in \\(C\\)the concentration of scotch is \\(\\mu(S)/\\mu(C)\\). \nNow stir. Mathematically, stirring is represented by the time\nevolution \\(T\\), meaning that \\(T(S)\\) is the region occupied by the\nscotch after one unit of mixing time. Intuitively we say that the\ncocktail is thoroughly mixed, if the concentration of scotch equals\n\\(\\mu(S) / \\mu(C)\\) not only with respect to the whole volume\nof fluid, but with respect to any region \\(V\\) in that\nvolume. Hence, the drink is thoroughly mixed at time \\(n\\) if \nfor any volume \\(V\\) (of non-zero measure). Now assume that the volume\nof the cocktail is one unit: \\(\\mu(C) = 1\\) (which we can do without\nloss of generality since there is always a unit system in which the\nvolume of the glass is one). Then the cocktail is thoroughly mixed\niff \nfor any region \\(V\\) (of non-zero measure). But how large must \\(n\\)\nbe before the stirring ends with the cocktail well stirred? We now\ndon’t require that the drink must be thoroughly mixed at any finite\ntime, but only that it approaches a state of being thoroughly mixed as\ntime tends towards infinity: \nfor any region \\(V\\) (of non-zero measure). If we now associate the\nglass with the phase space \\(X\\) and replace the scotch \\(S\\) and the\nvolume \\(V\\) with two arbitrary subsets \\(A\\) and \\(B\\) of \\(X\\), then\nwe get the general definition of what is called strong mixing\n(often also referred to just as ‘mixing’): a system is\nstrong mixing iff \nfor all subsets \\(A\\) and \\(B\\) of \\(X.\\) This requirement for mixing\ncan be relaxed a bit by allowing for\n fluctuations.[9]\n That is, instead of requiring that the cocktail reach a uniform state\nof being mixed, we now only require that it be mixed on average. In\nother words, we allow that bubbles of either scotch or water may crop\nup every now and then, but they do so in a way that these fluctuations\naverage out as time tends towards infinity. This translates into\nmathematics in a straightforward way. The deviation from the ideally\nmixed state at some time \\(n\\) is \\(\\mu(T_n B \\cap A) -\n\\mu(B)\\mu(A)\\). The requirement that the average of these deviations\nvanishes inspires the notion of weak mixing. A system is weak\nmixing iff \nfor all subsets \\(A\\) and \\(B\\) of \\(X\\). The vertical strokes denote\nthe so-called absolute value; for instance: \\(\\lvert 5 \\rvert = \\lvert\n-5 \\rvert = 5\\). One can prove that there is a strict implication\nrelation between the three dynamical properties we have introduced so\nfar: strong mixing implies weak mixing, but not vice versa; and weak\nmixing implies ergodicity, but not vice versa. Hence, strong mixing is\nstronger condition than weak mixing, and weak mixing is stronger\ncondition than ergodicity. \nThe next higher level in EH are K-systems. Unlike in the cases of\nergodic and mixing systems, there is unfortunately no intuitive way of\nexplaining the standard definition of such systems, and the definition\nis such that one cannot read off from it the characteristics of\nK-systems (we state this definition in the\n Appendix,\n Section C). The least unintuitive way to present K-systems is via a\ntheorem due to Cornfeld et al (1982, 283), who prove that a\ndynamical system is a K-system iff it is K-mixing. A system\nis K-mixing iff for any subsets \\(A_0, A_1 , \\ldots ,A_r\\) of \\(X\\)\n(where \\(r\\) is a natural number of your choice) the following\ncondition holds: \nwhere \\(\\sigma(n, r)\\) is the minimal \\(\\sigma\\)-algebra generated by\nthe set \nIt is far from obvious what this so-called sigma algebra is and hence\nthe content of this condition is not immediately transparent. We will\ncome back to this issue in Section 5 where we provide an intuitive\nreading of this condition. What matters for the time being is its\nsimilarity to the mixing condition. Strong mixing is, trivially,\nequivalent to \nSo we see that K-mixing adds something to strong mixing. \nIn passing we would like to mention another important property of\nK-systems: one can prove that K-systems have positive\nKolmogorov-Sinai entropy (KS-entropy); for details see the\n Appendix,\nSection C. The KS-entropy itself does not have an intuitive\ninterpretation, but it relates to three other concepts of dynamical\nsystems theory in an interesting way, and these do have intuitive\ninterpretations. First, Lyapunov exponents are a measure for\nhow fast two originally nearby trajectories diverge on average, and\nthey are often used in chaos theory to characterise the chaotic nature\nof the dynamics of a system. Under certain circumstances (essentially,\nthe system has to be differentiable and ergodic) one can prove that a\ndynamical system has a positive KS-entropy if and only if it has\npositive Lyapounov exponents (Lichtenberg and Liebermann 1992, 304).\nIn such a system initially arbitrarily close trajectories diverge\nexponentially. This result is known as Pessin’s\ntheorem. Second, the algorithmic complexity of a\nsequence is the length of the shortest computer programme needed to\nreproduce the sequence. Some sequences are simple; e.g. a string of a\nmillion ‘1’ is simple: the programme needed to reproduce\nit basically is ‘write ‘1’ a million times’,\nwhich is very short. Others are complex: there is no pattern in the\nsequence 5%8£yu@*mS!}<74^F that one could exploit, and so a\nprogramme reproducing that sequence essentially reads ‘write\n5%8£yu@*mS!}<74^F’, which is similar in length to the\nsequence itself. In the discrete case a trajectory can be represented\nas a sequence of symbols which corresponds to the states of the system\nalong this trajectory.  It is then the case that if a system is a\nK-system, then its KS-entropy equals the algorithmic complexity of\nalmost all its trajectories (Brudno 1978). This is now known\nas Brudno’s theorem (Alekseev and Yakobson\n1981). Third, the Shannon entropy is a common measure for the\nuncertainty of a future outcome: the higher the entropy the more\nuncertain we are about what is going to happen. One can prove that,\ngiven certain plausible assumptions, the KS-entropy is equivalent to a\ngeneralised version of the Shannon entropy, and can hence be regarded\nas a measure for the uncertainty of future events given past events\n(Frigg 2004). \nBernoulli systems mark the highest level in EH. In order to define\nBernoulli systems we first have to introduce the notion of a partition\nof \\(X\\) (sometimes also called the ‘coarse graining of\n\\(X\\)’). A partition of X is a division of \\(X\\) into\ndifferent parts (the so-called ‘atoms of the partition’)\nso that these parts don’t overlap and jointly cover \\(X\\) (i.e.,\nthey are mutually exclusive and jointly exhaustive). For instance, in\nFigure 1 there is a partition of the phase space that has two atoms\n(the left and the right part). More formally, \\(\\alpha =\n\\{\\alpha_1,\\ldots , \\alpha_n\\}\\) is a partition of \\(X\\) (and the\n\\(\\alpha_i\\) its atoms) iff (i) the intersection of any two atoms of\nthe partition is the empty set, and (ii) the union of all atoms is\n\\(X\\) (up to measure zero). Furthermore it is important to notice that\na partition remains a partition under the dynamics of the system. That\nis, if \\(\\alpha\\) is a partition, then \\(T_n\\alpha = \\{T_n\\alpha_1\n,\\ldots ,T_n\\alpha_n\\}\\) is also a partition for all \\(n\\). \nThere are, of course, many different ways of partitioning a phase\nspace. In what follows we are going to study how different partitions\nrelate to each other. An important concept in this connection is\nindependence. Let \\(\\alpha\\) and \\(\\beta\\) be two partitions of \\(X\\).\nBy definition, these partitions are independent iff\n\\(\\mu(\\alpha_i \\cap \\beta_j) = \\mu(\\alpha_i)\\mu(\\beta_j)\\) for all\natoms \\(\\alpha_i\\) of \\(\\alpha\\) and all atoms \\(\\beta_j\\) of\n\\(\\beta\\). We will explain the intuitive meaning of this definition\n(and justify calling it ‘independence’) in Section 4; for\nthe time being we just use it as a formal definition. \nWith these notions in hand we can now define a Bernoulli\ntransformation: a transformation \\(T\\) is a Bernoulli\ntransformation iff there exists a partition \\(\\alpha\\) of \\(X\\) so\nthat the images of \\(\\alpha\\) under \\(T\\) at different instants of\ntime are independent; that is, the partitions \\(\\ldots ,T_{-1}\\alpha ,\nT_0 \\alpha , T_1 \\alpha ,\\ldots\\) are all\n independent.[10]\n In other words, \\(T\\) is a Bernoulli transformation iff \nfor all atoms \\(\\delta_i\\) of \\(T_k\\alpha\\) and all atoms \\(\\beta_j\\)\nof \\(T_l\\alpha\\) for all \\(k \\ne l\\). We then refer to \\(\\alpha\\) as\nthe Bernoulli partition, and we call a dynamical system \\([X,\n\\mu , T]\\) a Bernoulli system if \\(T\\) is a Bernoulli\nautomorphism, i.e., a Bernoulli transformation mapping \\(X\\) onto\nitself. \nLet us illustrate this with a well-known example, the\nbaker’s transformation (so named because of its\nsimilarity to the kneading of dough). This transformation maps the\nunit square onto itself. Using standard Cartesian coordinates the\ntransformation can be written as follows: \nIn words, for all points \\((x, y)\\) in the unit square that have an\n\\(x\\)-coordinate smaller than \\(1/2\\), the transformation \\(T\\)\ndoubles the value of \\(x\\) and halves the value of \\(y\\). For all the\npoints \\((x, y)\\) that have an \\(x\\)-coordinate greater or equal to\n\\(1/2\\), \\(T\\) transforms \\(x\\) in to \\(2x-1\\) and \\(y\\) into \\(y/2 +\n1/2\\). This is illustrated in Fig. 5a. \nFigure 5a: The Baker’s\ntransformation \nNow regard the two areas shown in the left-hand part of the above\nfigure as the two atoms of a partition \\(\\alpha = \\{\\alpha_1\n,\\alpha_2\\}\\). It is then easy to see that \\(\\alpha\\) and T\\(\\alpha\\)\nare independent: \\(\\mu(\\alpha_1 \\cap T\\alpha_2) =\n\\mu(\\alpha_1)\\mu(T\\alpha_2)\\), and similarly for all other atoms of\n\\(\\alpha\\) and \\(T\\alpha\\). This is illustrated in Figure 5b. \nFigure 5b: The independence of\n\\(\\alpha\\) and \\(T\\alpha\\). \nOne can prove that independence holds for all other iterates of\n\\(\\alpha\\) as well. So the baker’s transformation together with\nthe partition \\(\\alpha\\) is a Bernoulli transformation. \nIn the literature Bernoulli systems are often introduced using\nso-called shift maps (or Bernoulli shifts). We here\nbriefly indicate how shift maps are related to Bernoulli systems with\nthe example of the baker’s transformation; for a more general\ndiscussion see the\n Appendix,\n Section D. Choose a point in the unit square and write its \\(x\\) and\n\\(y\\) coordinates as binary numbers: \\(x = 0.a_1 a_2 a_3\\ldots\\) and\n\\(y = 0.b_1 b_2 b_3\\ldots\\), where all the \\(a_i\\) and \\(b_i\\) are\neither 0 or 1. Now put both strings together back to back with a dot\nin the middle to form one infinite string: \\(S= \\ldots b_3 b_2 b_1\n.a_1 a_2 a_3\\ldots\\), which may represent the state of the system just\nas a ‘standard’ two-dimensional vector does. Some\nstraightforward algebra then shows that \nFrom this we see that in our ‘one string’ representation\nof the point the operation of \\(T\\) amounts to shifting the dot one\nposition to the right: \\(TS= \\ldots b_3 b_2 b_1 a_1 .a_2 a_3\\ldots\\)\nHence, the baker’s transformation is equivalent to a shift on an\ninfinite string of zeros and\n ones.[11] \nThere are two further notions that are crucial to the theory of\nBernoulli systems, the property of being weak Bernoulli and\nvery weak Bernoulli. These properties play a crucial role in\nshowing that certain transformations are in fact Bernoulli. The\nbaker’s transformation is one of the few examples that have a\ngeometrically simple Bernoulli partition, and so one often cannot\nprove directly that a system is a Bernoulli system. One then shows\nthat a certain geometrically simple partition is weak Bernoulli and\nuses a theorem due to Ornstein to the effect that if a system is weak\nBernoulli then there exists a Bernoulli partition for that system. The\nmathematics of these notions and the associated proofs of equivalence\nare intricate and a presentation of them is beyond the scope of this\nentry. The interested reader is referred to Ornstein (1974) or Shields\n(1973). \nThe concepts of EH, and in particular ergodicity itself, play\nimportant roles in the foundation of statistical mechanics (SM). In\nthis section we review what these roles are. \nA discussion of SM faces an immediate problem.\nFoundational debates in many other fields of physics can take as their\npoint of departure a generally accepted formalism. Things are\ndifferent in SM. Unlike, say, relativity theory,\nSM has not yet found a generally accepted theoretical framework, let\nalone a canonical\n formulation.[12]\n What we find in SM is plethora of different approaches and schools,\neach with its own programme and mathematical\n apparatus.[13]\n However, all these schools use (slight variants) of either of two\ntheoretical frameworks, one of which can be associated with Boltzmann\n(1877) and the other with Gibbs (1902), and can thereby be classify\neither as ‘Boltzmannian’ or ‘Gibbsian’. For\nthis reason we divide our presentation of SM into a two parts, one for\neach of these families of approaches. \nBefore delving into a discussion of these theories, let us briefly\nreview the basic tenets of SM by dint of a common example. Consider a\ngas that is confined to the left half of a box. Now remove the barrier\nseparating the two halves of the box. As a result, the gas quickly\ndisperses, and it continues to do so until it uniformly fills the\nentire box. The gas has approached equilibrium. This raises two\nquestions. First, how is equilibrium characterised? That is, what does\nit take for a system to be in equilibrium? Second, how do we\ncharacterise the approach to equilibrium? That is, what are the\nsalient features of the approach to equilibrium and what features of a\nsystem make it behave in this way? These questions are addressed in\ntwo subdisciplines of SM: equilibrium SM and non-equilibrium SM. \nThere are two different ways of describing processes like the\nspreading of a gas. Thermodynamics describes the system using a few\nmacroscopic variables (in the case of the gas pressure, volume and\ntemperature), while disregarding the microscopic\nconstitution of the gas. As far as thermodynamics is concerned matter\ncould be a continuum rather than consisting of particles—it just\nwould not make any difference. For this reason thermodynamics is\ncalled a ‘macro theory’. \nThe cornerstone of thermodynamics is the so-called Second Law of\nthermodynamics. This law describes one of the salient features of the\nabove process: its unidirectionality. We see gases spread—i.e.,\nwe see them evolving towards equilibrium—but we never observe\ngases spontaneously reverting to the left half of a box—i.e., we\nnever see them move away from equilibrium when left alone. And this is\nnot a specific feature of gases. In fact, not only gases but also all\nother macroscopic systems behave in this way, irrespective of their\nspecific makeup. This fact is enshrined in the Second Law of\nthermodynamics, which, roughly, states that transitions from\nequilibrium to non-equilibrium states cannot occur in isolated\nsystems, which is the same as saying that entropy cannot decrease in\nisolated systems (where a system is isolated if it has no interaction\nwith its environment: there is no heat exchange, no one is compressing\nthe gas, etc.). \nBut there is an altogether different way of looking at that same gas;\nthat is, as consisting of a large number of molecules (a vessel on a\nlaboratory table contains something like \\(10^{23}\\) molecules). These\nmolecules bounce around under the influence of the forces exerted onto\nthem when they crash into the walls of the vessel and collide with\neach other. The motion of each molecule is governed by the laws of\nclassical mechanics in the same way as the motion of the bouncing\nball. So rather than attributing some macro variables to the gas and\nfocussing on them, we could try to understand the gas’ behaviour\nby studying the dynamics of its micro constituents. \nThis raises the question of how the two ways of looking at the gas fit\ntogether. Since neither the thermodynamic nor the mechanical approach\nis in any way privileged, both have to lead to the same conclusions.\nStatistical mechanics is the discipline that addresses this task. From\na more abstract point of view we can therefore also say that SM is the\nstudy of the connection between micro-physics and macro-physics: it\naims to account for a system’s macro behaviour in terms of the\ndynamical laws governing its microscopic constituents. The term\n‘statistical’ in its name is owed to the fact that, as we\nwill see, a mechanical explanation can only be given if we also\nintroduce probabilistic elements into the theory. \nWe first introduce the main elements of the Boltzmannian framework and\nthen turn to the use of ergodicity in it. Every system can posses\nvarious macrostates \\(M_1 ,\\ldots ,M_k\\). These macrostates are\ncharacterised by the values of macroscopic variables, in the case of a\ngas pressure, temperature, and\n volume.[14]\n In the introductory example one macro-state corresponds to the gas\nbeing confined to the left half, another one to it being spread out.\nIn fact, these two states have special status: the former is the\ngas’ initial state (also referred to as the ‘past\nstate’); the latter is the gas’ equilibrium state. We\nlabel the states \\(M_p\\) and \\(M_{eq}\\) respectively. \nIt is one of the fundamental posits of the Boltzmann approach that\nmacrostates supervene on microstates, meaning that a change in a\nsystem’s macrostate must be accompanied by a change in its\nmicrostate (for a discussion of supervenience see McLaughlin and\nBennett 2005, and references therein). For instance, it is not\npossible to change the pressure of a system and at the same time keep\nits micro-state constant. Hence, to every given microstate \\(x\\) there\ncorresponds exactly one macrostate. Let us refer to this\nmacrostate as \\(M(x)\\). This determination relation is not one-to-one;\nin fact many different \\(x\\) can correspond to the same macrostate. We\nnow group together all microstates \\(x\\) that correspond to the same\nmacro-state, which yields a partitioning of the phase space in\nnon-overlapping regions, each corresponding to a macro-state. For this\nreason we also use the same letters, \\(M_1 ,\\ldots ,M_k\\), to refer to\nmacro-states and the corresponding regions in phase space. This is\nillustrated in Figure 6a. \nFigure 6: The macrostate structure of\n\\(X\\). \nWe are now in a position to introduce the Boltzmann entropy. To this\nend recall that we have a measure \\(\\mu \\) on the phase space that\nassigns to every set a particular volume, hence a fortiori\nalso to macrostates. With this in mind, the Boltzmann entropy of a\nmacro-state \\(M_j\\) can be defined as \\(S_B = k_B\\log [\\mu(M_j)]\\),\nwhere \\(k_B\\) is the Boltzmann constant. The important feature of the\nlogarithm is that it is a monotonic function: the larger\n\\(M_j\\), the larger its logarithm. From this it follows that the\nlargest macro-state also has the highest entropy! \nOne can show that, at least in the case of dilute gases, the Boltzmann\nentropy coincides with the thermodynamic entropy (in the sense that\nboth have the same functional dependence on the basic state\nvariables), and so it is plausible to say that the equilibrium state\nis the macro-state for which the Boltzmann entropy is maximal (since\nthermodynamics posits that entropy be maximal for equilibrium states).\nBy assumption the system starts off in a low entropy state, the\ninitial state \\(M_p\\) (the gas being squeezed into the left half of\nthe box). The problem of explaining the approach to equilibrium then\namounts to answering the question: why does a system originally in\n\\(M_p\\) eventually move into \\(M_{eq}\\) and then stay there? (See\nFigure 6b.) \nIn the 1870s Boltzmann offered an important answer to this\n question.[15]\n At the heart of his answer lies the idea to assign probabilities to\nmacrostates according to their size. So Boltzmann adopted the\nfollowing postulate: \\(p(M_j) = c\\mu(M_j)\\) for all \\(j = 1,\\ldots\n,k\\), where \\(c\\) is a normalisation constant assuring that the\nprobabilities add up to one. Granted this postulate, it follows\nimmediately that the most likely state is the equilibrium state (since\nthe equilibrium state occupies the largest chunk of the phase space).\nFrom this point of view it seems natural to understand the approach to\nequilibrium as the evolution from an unlikely macrostate to a more\nlikely macrostate and finally to the most likely macro-state. This,\nBoltzmann argued, was a statistical justification of the Second Law of\nthermodynamics. \nBut Boltzmann knew that simply postulating \\(p(M_j) = c\\mu(M_j)\\)\nwould not solve the problem unless the postulate could be justified in\nterms of the dynamics of the system. This is where ergodicity enters\nthe scene. As we have seen above, ergodic systems have the property of\nspending a fraction of time in each part of the phase space that is\nproportional to its size (with respect to \\(\\mu)\\). As we have also\nseen, the equilibrium state is the largest macrostate. In fact, the\nequilibrium state is much larger than the other states. So if\nwe assume that the system is ergodic, then it is in equilibrium most\nof the time! It is then natural to interpret \\(p(M_j)\\) as a time\naverage: \\(p(M_j)\\) is the fraction of time that the system spends in\nstate \\(M_j\\) over the course of time. We now have the main elements\nof Boltzmann’s framework in front of us: (a) partition the phase\nspace of the system in macrostates and show that the equilibrium state\nis by far the largest state; (b) adopt a time average interpretation\nof probability; and (c) assume that the system in question is\nergodic. It then follows that the system is most likely to be found in\nequilibrium, which justifies (a probabilistic version of) the Second\nlaw. \nThree objections have been levelled against this line of thought.\nFirst, it is pointed that assuming ergodicity is too strong in two\nways. The first is that it turns out to be extremely difficult to\nprove that the systems of interest really are ergodic. Contrary to\nwhat is sometimes asserted, not even a system of \\(n\\) elastic hard\nballs moving in a cubic box with hard reflecting walls has been proven\nto be ergodic for arbitrary \\(n\\); it has been proven to be ergodic\nonly for \\(n \\le 4\\). To this charge one could reply that what looks\nlike defeat to some, appears to be a challenge to others. Progress in\nmathematics may eventually resolve the issue, and there is at least\none recent result that justifies optimism: Simanyi (2004) shows that a\nsystem of \\(n\\) hard balls on a torus of dimension 3 or greater is\nergodic, for an arbitrary natural number \\(n\\). \nThe second way in which ergodicity seems to be too strong is that even\nif eventually we can come by proofs of ergodicity for the relevant\nsystems, the assumption is too strong because there are systems that\nare known not to be ergodic and yet they behave in accordance with the\nSecond Law. Bricmont (2001) investigates the Kac Ring Model and a\nsystem of \\(n\\) uncoupled anharmonic oscillators of identical mass,\nand points out that both systems exhibit thermodynamic behaviour and\nyet they fail to be ergodic. Hence, ergodicity is not necessary for\nthermodynamic behaviour. Earman and Redei (1996, p. 70) and van Lith\n(2001, p. 585) argue that if ergodicity is not necessary for\nthermodynamic behaviour, then ergodicity cannot provide a satisfactory\nexplanation for this behaviour. Either there must be properties other\nthan ergodicity that explain thermodynamic behaviour in cases in which\nthe system is not ergodic, or there must be an altogether different\nexplanation for the approach to equilibrium even for systems which are\nergodic. \nIn response to this objection, Vranas (1998) and Frigg and Werndl\n(2011) argue that most systems that fail to be ergodic are\n‘almost ergodic’ in a specifiable way, and this is good\nenough. We discuss Vranas’ approach below when discussing\nGibbsian SM since that is the context in which he has put forward his\nsuggestion. Werndl and Frigg (2015a, 2015b) offer an alternative\ndefinition of Boltzmannian equilibrium and exploit the ergodic\ndecomposition theorem to show that even if a system is not ergodic it\nwill spend most of the time in equilibrium, as envisaged by Boltzmann\n(roughly the ergodic decomposition theorem says that the phase space\nof every measure preserving system can be partitioned into parts so\nthat the dynamics is ergodic on each part; for details see Petersen\n1983). Frigg (2009) suggested exploiting the fact that almost all\nHamiltonian systems are non-integrable, and that these systems have\nso-called Arnold webs, i.e., large regions of phase space on which the\nmotion of the system is ergodic. Lavis (2005) re-examined the Kac ring\nmodel and pointed out that even though the system is not ergodic, it\nhas an ergodic decomposition, which is sufficient to guarantee the\napproach to equilibrium. He also challenged the assumption, implicit\nin the above criticism, that providing an explanation for the approach\nto equilibrium amounts to identifying one (and only one!) property\nthat all systems have in common. In fact, it may be the case that\ndifferent properties are responsible for the approach to equilibrium\nin different systems, and there is no reason to rule out such\nexplanations. In sum, the tenor of all these responses is that even though\nergodicity simpliciter may not have the resources to explain\nthe approach to equilibrium, somewhat qualified properties do. \nThe second objection is that even if ergodicity obtains, this is not\nsufficient to give us what we need. As we have seen above, ergodicity\ncomes with the qualification ‘almost everywhere’. This\nqualification is usually understood as suggesting that sets of measure\nzero can be ignored without detriment. The idea is that points falling\nin a set of measure zero are ‘sparse’ and can therefore be\nneglected. The question of whether or not this move is legitimate is\nknown as the ‘measure zero problem’. \nSimply neglecting sets of measure zero seems to be problematic for\nvarious reasons. First, sets of measure zero can be rather\n‘big’; for instance, the rational numbers have measure\nzero within the real numbers. Moreover, a set of measure zero need not\nbe (or even appear) negligible if sets are compared with respect to\nproperties other than their measures. For instance, we can judge the\n‘size’ of a set by its cardinality or Baire category\nrather than by its measure, which leads us to different conclusions\nabout the set’s size (Sklar 1993, pp. 182–88). It is also\na mistake to assume that an event with measure zero cannot occur. In\nfact, having measure zero and being impossible are distinct notions.\nWhether or not the system at some point was in one of the special\ninitial conditions for which the space and time mean fail to be equal\nis a factual question that cannot be settled by appeal to measures;\npointing out that such points are scarce in the sense of measure\ntheory does resolve the problem because it does not imply that they\nare scarce in the world as well. \nIn response two things can said. First, discounting sets of measure\nzero is standard practice in physics and the problem is not specific\nto ergodic theory. So unless there is a good reason to suspect that\nspecific measure zero states are in fact important, one might argue\nthat the onus of proof is on those who think that discounting them in\nthis case is illegitimate. Second, the fact that SM works in so many\ncases suggests that they indeed are scarce. \nThe third criticism is rarely explicitly articulated, but it is\nclearly in the background of contemporary Boltzmannian approaches to\nSM such as Albert’s (2000), which reject Boltzmann’s\nstarting point, namely the postulate \\(p(M_j) = c\\mu(M_j)\\). Albert\nintroduces an alternative postulate, essentially providing transition\nprobabilities between two macrostates conditional on the so-called\nPast Hypothesis, the posit that the universe came into existence in a\nlow entropy state (the Big Bang). Albert then argues that in such an\naccount erogidicity becomes an idle wheel, and hence he rejects it as\ncompletely irrelevant to the foundations of SM. This, however, may\nwell be too hasty. Although it is true that ergodicity simpliciter\ncannot justify Albert’s probability postulate, another dynamical\nassumption is needed in order for this postulate to be true (Frigg\n2010). \nAt the basis of Gibbs’ approach stands a conceptual shift. The\nobject of study in the Boltzmannian framework is an individual system,\nconsisting of a large but finite number of micro constituents. By\ncontrast, within the Gibbs framework the object of study is a\nso-called ensemble: an imaginary collection of infinitely\nmany copies of the same system (they are the same in that they have\nthe same phase space, dynamics and measure), but who happen to be in\ndifferent states. An ensemble of gases, for instance, consists of\ninfinitely many copies of the same gas bit in different states: one is\nconcentrated in the left corner of the box, one is evenly distributed,\netc. It is important to emphasise that ensembles are fictions, or\n‘mental copies of the one system under consideration’\n(Schrödinger 1952, 3); or alternatively they can be thought of as\ncollections of possible states of the entire system. Hence, it is\nimportant not to confuse ensembles with collections of micro-objects\nsuch as the molecules of a gas! \nThe instantaneous state of one system of the ensemble is specified by\none point in its phase space. The state of the ensemble as a\nwhole is therefore specified by a density function \\(\\varrho\\) on\nthe system’s phase space. From a technical point of view\n\\(\\varrho\\) is a function just like \\(f\\) that we encountered in\nSection 1. We furthermore assume that \\(\\varrho\\) is a probability\ndensity, reflecting the probability density of finding the state of a\nsystem chosen at random from the entire ensemble in region \\(R\\), so\nthat the probability of the state being in \\(R\\) is \\(p(R) = \\int_R\n\\varrho d\\mu\\). To make this more intuitive consider the following\nanalogy. You play a special kind of darts: you fix a plank to the\nwall, which serves as your dart board. For some reason you know that\nthe probability of your dart landing at a particular place on the\nboard is given by the curve shown in Figure 7. You are then asked what\nthe probability is that your next dart lands in the left half of the\nboard. The answer is 1 ⁄ 2 since one half of the surface\nunderneath the curve is on the left side. The dart board then plays\nthe role of the system’s state space, a region of the board\n(here the left half) plays the role of \\(R\\), and throwing a dart\nplays the role of picking a system from the ensemble. \nFigure 7: Dart board \nThe importance of this is that it allows us to calculate expectation\nvalues. Assume that the game is such that you get one Pound if the\ndart hits the left half and three Pounds if it lands on the right\nhalf. What is your expected gain? The answer is 1 ⁄ \\(2 \\times\n1\\) Pound \\(+ 1\\) ⁄ \\(2 \\times 3\\) Pounds \\(= 2\\) Pounds. This\nis the expectation value. The same idea is at work in SM. Physical\nmagnitudes like, for instance, pressure are associated with functions\n\\(f\\) on the phase pace. We then calculate the expectation value of\nthese magnitudes, which, in general is given by \\(\\langle f \\rangle =\n\\int fd\\mu\\). In the context of Gibbsian SM these expectation values\nare also referred to as phase averages or ensemble\naverages. They are of central importance because these values are\nused as predictions for observed values. So if you want to use the\nformalism to predict what will be observed in an experiment, you first\nhave to figure out what the probability density \\(\\varrho\\) is, then\nfind the function \\(f\\) corresponding to the physical quantity you are\ninterested in, and then calculate the phase average. Neither of these\nsteps is easy in practice and working physicists spend most of their\ntime doing these calculations. However, these difficulties need not\noccupy us if we are interested in the conceptual issues underlying\nthis ‘recipe’. \nBy definition, a probability density \\(\\varrho\\) is stationary if it\ndoes not change over time. Given that observable quantities are\nassociated with phase averages and that equilibrium is defined in\nterms of the constancy of the macroscopic parameters characterising\nthe system, it is natural to regard the stationarity of the\ndistribution as a necessary condition for equilibrium because\nstationary distributions yield constant averages. For this reason\nGibbs refers to stationarity as the ‘condition of statistical\nequilibrium’. \nAmong all stationary distributions those satisfying a further\nrequirement, the Gibbsian maximum entropy principle, play a\nspecial role. The Gibbs entropy (sometimes called\n‘ensemble entropy’) is defined as \nThe Gibbsian maximum entropy principle then requires that \\(S_G\n(\\varrho)\\) be maximal, given the constraints that are imposed on the\n system.[16] \nThe last clause is essential because different constraints single out\ndifferent distributions. A common choice is to keep both the energy\nand the particle number in the system fixed. One can prove that under\nthese circumstances \\(S_G (\\varrho)\\) is maximal for the so-called\nmicrocanonical distribution (or microcanonical\nensemble). If we choose to hold the number of particles constant\nwhile allowing for energy fluctuations around a given mean value we\nobtain the so-called canonical distribution; if we also allow\nthe particle number to fluctuate around a given mean value we find the\nso-called grand-canonical\n distribution.[17] \nThis formalism is enormously successful in that correct predictions\ncan be derived for a vast class of systems. But the success of this\nformalism is rather puzzling. The first and most obvious question\nconcerns the relation of systems and ensembles. The probability\ndistribution in the Gibbs approach is defined over an ensemble, the\nformalism provides ensemble averages, and equilibrium is regarded as a\nproperty of an ensemble. But what we are really interested in is the\nbehaviour of a single system! What could the properties of an\nensemble—a fictional entity consisting of infinitely many mental\ncopies of the real system—tell us about the one real system on\nthe laboratory table? And more specifically, why do averages over an\nensemble coincide with the values found in measurements performed on\nan actual physical system in equilibrium? There is no obvious reason\nwhy this should be so, and it turns out that ergodicity plays a\ncentral role in answering these questions. \nCommon textbook wisdom justifies the use of phase averages as follows.\nAs we have seen, the Gibbs formalism associates physical quantities\nwith functions on the system’s phase space. Making an\nexperiment measuring one of these quantities takes time and it is\nassumed that what measurement devices register is not the\ninstantaneous value of the function in question, but rather its time\naverage over the duration of the measurement. Hence, time averages are\nwhat is empirically accessible. Then, so the argument continues,\nalthough measurements take an amount of time that is short by human\nstandards, it is long compared to microscopic time scales on which\ntypical molecular processes take place. For this reason it is assumed\nthat the measured finite time average is approximately equal\nto the infinite time average of the measured function. If we\nnow assume that the system is ergodic, then time averages equal phase\naverages. The latter can easily be obtained from the formalism. Hence\nwe have found the sought-after connection: the Gibbs formalism\nprovides phase averages which, due to ergodicity, are equal to\ninfinite time averages, and these are, to a good approximation, equal\nto the finite time averages obtained from measurements. \nThis argument is problematic for at least two reasons. First, from the\nfact that measurements take some time it does not follow that what is\nactually measured are time averages. For instance, it could be the\ncase that the value provided to us by the measurement device is simply\nthe value assumed by at the last moment of the measurement,\nirrespective of what the previous values of were (e.g.\nit’s simply the last pointer reading registered). So we would\nneed an argument for the conclusion that measurements indeed produce\ntime averages. Second, even if we take for granted that\nmeasurements do produce finite time averages, equating these\naverages with infinite time averages is problematic. Even if the\nduration of the measurement is long by experimental standards (which\nneed not be the case), finite and infinite averages may assume very\ndifferent values. That is not to say that they necessarily have to be\ndifferent; they could coincide. But whether or not they do is\nan empirical question, which depends on the specifics of the system\nunder investigation. So care is needed when replacing finite with\ninfinite time averages, and one cannot identify them without further\nargument.  \nMalament and Zabell (1980) respond to this challenge by suggesting a\nway of explaining the success of equilibrium theory that still invokes\nergodicity, but avoids appeal to time averages. This solves the above\nmentioned problems, but suffers from the difficulty that many systems\nthat are successfully dealt with by the formalism of SM are not\nergodic. To circumvent this difficulty Vranas (1998) suggested\nreplacing ergodicity with what he calls \\(\\varepsilon\\)-ergodicity.\nIntuitively a system is \\(\\varepsilon\\)-ergodic if it is ergodic not\non the entire phase space, but on a very large part of it (those parts\non which it is not ergodic having measure \\(\\varepsilon\\), where\n\\(\\varepsilon\\) is very small). The leading idea behind his approach\nis to challenge the commonly held belief that even if a system is just\na ‘little bit’ non-ergodic, then it behaves in a\ncompletely ‘un-ergodic’ way. Vranas points out that there\nis a middle ground and then argues that this middle ground actually\nprovides us with everything we need. This is a promising proposal, but\nit faces three challenges. First, it needs to be shown that all\nrelevant systems really are \\(\\varepsilon\\)-ergodicity. Second, the\nargument so far has only been developed for the microcanonical\nensemble, but one would like to know whether, and if so how, it works\nfor the canonical and the grandcanonical ensembles. Third, it is still\nbased on the assumption that equilibrium is characterised by a\nstationary distribution, which, as we will see below, is an obstacle\nwhen it comes to formulating a workable Gibbsian non-equilibrium\ntheory. \nThe second response begins with Khinchin’s work. Khinchin (1949)\npointed out that the problems of the ergodic programme are due to the\nfact that it focuses on too general a class of systems. Rather than\nstudying dynamical systems at a general level, we should focus on\nthose cases that are relevant in statistical mechanics. This involves\ntwo restrictions. First, we only have to consider systems with a large\nnumber of degrees of freedom; second, we only need to take into\naccount a special class of phase functions, the so-called ‘sum\nfunctions’. These functions are a sum of one-particle functions,\ni.e., functions that take into account only the position and momentum\nof one particle. Under these assumption Khinchin proved that as \\(n\\)\nbecomes larger, the measure of those regions on the energy\n hypersurface[18]\n where the time and the space means differ by more than a small amount\ntends towards zero. Roughly speaking, this result says that for large\n\\(n\\) the system behaves, for all practical purposes, as if it was\nergodic. \nThe problem with this result is that it is valid only for sum\nfunctions, and in particular only if the energy function of the system\nis itself a sum function, which is not the case when particles\ninteract. So the question is how this result can be generalised to\nmore realistic cases. This problem stands at the starting point of a\nresearch programme now known as the thermodynamic limit,\nchampioned, among others, by Lanford, Mazur, Ruelle, and van der\nLinden (see van Lith (2001) for a survey). Its leading question is\nwhether one can still prove ‘Khinchin-like’ results in the\ncase of energy function with interaction\n terms.[19]\n Results of this kind can be proven in the limit for \\(n \\rightarrow\n\\infty\\), if also the volume \\(V\\) of the system tends towards\ninfinity in such a way that the number-density \\(n/V\\) remains\nconstant. \nSo far we have only dealt with equilibrium, and things get worse once\nwe turn to non-equilibrium. The main problem is that it is a\nconsequence of the formalism that the Gibbs entropy is a constant!\nThis precludes a characterisation of the approach to equilibrium in\nterms of increasing Gibbs entropy, which is what one would expect if\nwe were to treat the Gibbs entropy as the SM counterpart of the\nthermodynamic entropy. The standard way around this problem is to\ncoarse-grain the phase space, and then define the so-called coarse\ngrained Gibbs entropy. Put simply, course-graining the phase space\namounts to putting a grid on the phase space and declare that all\npoints within one cell of the grid are indistinguishable. This\nprocedure turns a continuous phase space into a discrete collection of\ncells, and the state of the system is then specified by saying in\nwhich cell the system’s state is. If we define the Gibbs entropy on\nthis grid, it turns out (for purely mathematical reasons) that the\nentropy is no longer a constant and can actually increase or decrease.\nIf one then assumes that the system is mixing, it follows from the\nso-called convergence theorem of ergodic theory that the\ncoarse-grained Gibbs entropy approaches a maximum. However, this\nsolution is fraught with controversy, the two main bones of contention\nbeing the justification of coarse-graining and the assumption that the\nsystem is mixing. \nIn sum, ergodicity plays a central role in many attempts to justify\nthe posits of SM. And even where a simplistic use of ergodicity is\neventually unsuccessful, somewhat modified notions prove fruitful in\nan analysis of the problem and in the search for better solutions. \nEH is often presented as a hierarchy of increasing degrees of\nrandomness in deterministic systems: the higher up in this hierarchy a\nsystem is placed the more random its\n behaviour.[20]\n However, the definitions of different levels of EH do not make\nexplicit appeal to randomness; nor does the usual way of presenting EH\ninvolve a specification of the notion of randomness that is supposed\nto underlie the hierarchy. So there is a question about what notion of\nrandomness underlies EH and in what sense exactly EH is a hierarchy of\nrandom behaviour. \nBerkovitz, Frigg and Kronz (2006) discuss this problem and argue that\nEH is best understood as a hierarchy of random behaviour if randomness\nis explicated in terms of unpredictability, where unpredictability is\naccounted for in terms of probabilistic relevance. Different patterns\nof probabilistic relevance, in turn, are spelled out in terms of\ndifferent types of decay of correlation between a system’s\nstates at different times. Let us introduce these elements one at a\ntime. \nProperties of systems can be associated with different parts of the\nphase space. In the ball example, for instance, the property\nhaving positive momentum is associated with the right half of\nthe phase space; that is, it is associated with the set \\(\\{x \\in X\n\\mid p \\gt 0\\}\\). Generalising this idea we say that to every subset\n\\(A\\) of a system’s phase space there corresponds a property\n\\(P_A\\) so that the system possesses that property at time \\(t\\) iff\nthe system’s state \\(x\\) is in \\(A\\) at \\(t\\). The subset \\(A\\)\nmay be arbitrary and the property corresponding to \\(A\\) may not be\nintuitive, unlike, for example, the property of having positive\nmomentum. But nothing in the analysis to follow hangs on a\nproperty being ‘intuitive’. We then define the\nevent \\(A^t\\) as the obtaining of \\(P_A\\) at time \\(t\\). \nAt every time \\(t\\) there is a matter of fact whether \\(P_A\\) obtains,\nwhich is determined by the dynamics of the system. However, we may not\nknow whether or not this is the case. We therefore introduce epistemic\nprobabilities expressing our uncertainty about whether \\(P_A\\)\nobtains: \\(p(A^t)\\) reflects an agent’s degree of belief in\n\\(P_A\\)’s obtaining at time \\(t\\). In the same way we can\nintroduce conditional probabilities: \\(p(A^t \\mid B^{t_1})\\) is our\ndegree of belief in the system having \\(P_A\\) at \\(t\\) given that it\nhad \\(P_B\\) at an earlier time \\(t_1\\), where \\(B\\) is also a subset\nof the system’s phase space. By the usual rule of conditional\nprobability we have \\(p(A^t \\mid B^{t_1}) = p(A^t \\amp B^{t_1}) /\n(p(B^{t_1})\\). This can of course be generalised to more then one\nevent: \\(p(A^t \\mid B_{1}^{t_1} \\amp \\ldots \\amp B_{r}^{t_r})\\) is our\ndegree of belief in the system having \\(P_A\\) at \\(t\\) given that it\nhad \\(P_{B_1}\\) at \\(t_1, P_{B_2}\\) at \\(t_2,\\ldots\\), and \\(P_{B_{\nr}}\\) at \\(t_r\\), where \\(B_1 ,\\ldots ,B_r\\) are subsets of the\nsystem’s phase space (and \\(r\\) a natural number), and \\(t_1\n,\\ldots ,t_r\\) are successive instants of time (i.e., \\(t \\gt t_1 \\gt\n\\ldots \\gt t_r)\\).  \nIntuitively, an event in the past is relevant to our making\npredictions if taking the past event into account makes a difference\nto our predictions, or more specifically if it lowers or raises the\nprobability for a future event. In other words, \\(p(A^t \\mid\nB_{1}^{t_1}) - p(A^t)\\) is a measure of the relevance of \\(B^{t_1}\\)\nto predicting \\(A^t : B^{t_1}\\) is positively relevant if the \\(p(A^t\n\\mid B_{1}^{t_1}) - p(A^t) \\gt 0\\), negatively relevant if \\(p(A^t\n\\mid B_{1}^{t_1}) - p(A^t) \\lt 0\\), and irrelevant if \\(p(A^t \\mid\nB_{1}^{t_1}) - p(A^t) = 0\\). For technical reasons it turns out to be\neasier to work with a slightly different but equivalent notion of\nrelevance, which is obtained from the above by multiplying both sides\nof the equation by \\(p(B^{t_1})\\). Therefore we adopt the following\ndefinition. The relevance of \\(B^{t_1}\\) for \\(A^t\\) is \nThe generalisation of this definition to cases with more than one set\n\\(B\\) (as above) is straightforward. \nRelevance serves to explicate unpredictability. Intuitively, the less\nrelevant past events are for \\(A^t\\), the less predictable the system\nis. This basic idea can then be refined in various ways. First, the\ntype of unpredictability we obtain depends on the type of events to\nwhich (R) is applied. For instance, the degree of the unpredictability\nof \\(A^t\\) increases if its probability is independent not only of\n\\(B^{t_1}\\) or other ‘isolated’ past events, but rather\nthe entire past. Second, the unpredictability of an event \\(A^t\\)\nincreases if the probabilistic dependence of that event on past events\n\\(B^{t_1}\\) decreases rapidly with the increase of the temporal\ndistance between the events. Third, the probability of \\(A^t\\) may be\nindependent of past events simpliciter, or it may be\nindependent of such events only on average. These ideas underlie the\nanalysis of EH as a hierarchy of unpredictability. \nBefore we can provide such an analysis, two further steps are needed.\nFirst, if the probabilities are to be useful to understanding\nrandomness in a dynamical system, the probability assignment\nhas to reflect the properties of the system. So we have to connect the\nabove probabilities to features of the system. The natural choice is\nthe system’s measure\n \\(\\mu\\).[21]\n So we postulate that the probability of an event \\(A^t\\) is equal to\nthe measure of the set \\(A: p(A^{t}) = \\mu(A)\\) for all \\(t\\). This\ncan be generalised to joint probabilities as follows: \nfor all instants of time \\(t \\gt t_1\\) and all subsets \\(A\\) and \\(B\\)\nof the system’s phase space. \\(T_{t_1 \\rightarrow t}B\\) is the\nimage of the set \\(B\\) under the dynamics of the system from \\(t_1\\)\nto \\(t\\). We refer to this postulate as the Probability\nPostulate (P), which is illustrated in Figure 8. Again, this\ncondition is naturally generalised to cases of joint probabilities of\n\\(A^t\\) with multiple events \\(B^{t_i}\\). Granted (P) and its\ngeneralization, (R) reflects the dynamical properties of systems.  \nFigure 8: Condition (P). \nBefore briefly introducing the next element of the analysis let us\nmention that there is a question about whether the association of\nprobabilities with the measure of the system is reasonable. Prima\nfacie, a measure on a phase space can have a purely geometrical\ninterpretation and need not necessarily have anything to do with the\nquantification of uncertainty. For instance, we can use a measure to\ndetermine the length of a table, but this measure need not have\nanything to do with uncertainty. Whether or not such an association is\nlegitimate depends on the cases at hand and the interpretation of the\nmeasure. However, for systems of interest in statistical physics it is\nnatural and indeed standard to assume that the probability of the\nsystem’s state to be in a particular subset of the phase space\n\\(X\\) is proportional to the measure of \\(A\\). \nThe last element to be introduced is the notion of the correlation\nbetween two subsets \\(A\\) and \\(B\\) of the system’s phase space,\nwhich is defined as follows: \nIf the value of \\(C(A, B)\\) is positive (negative), there is positive\n(negative) correlation between \\(A\\) and \\(B\\); if it is zero, then\n\\(A\\) and \\(B\\) are uncorrelated. It then follows immediately from the\nabove that \n(RC) constitutes the basis for the interpretation of EH as a hierarchy\nof objective randomness. Granted this equation, the subjective\nprobabilistic relevance of the event \\(B^{t_1}\\) for the event \\(A^t\\)\nreflects objective dynamical properties of the system since for\ndifferent transformations \\(T\\ R(B^{t_1}, A^t)\\) will indicate\ndifferent kinds of probabilistic relevance of \\(B^{t_1}\\) for \\(A^t\\).\n \nTo put (RC) to use, it is important to notice that the equations\ndefining the various levels of EH above can be written in terms of\ncorrelations. Taking into account that we are dealing with discrete\nsystems (and hence we have \\(T_{t_1\\rightarrow t}B = T_k B\\) where\n\\(k\\) is the number of time steps it takes to get from \\(t_1\\) and\n\\(t)\\), these equations read: \nApplying (RC) to these expressions, we can explicate the nature of the\nunpredictability that each of the different levels of EH involves.\n \nLet us start at the top of EH. In Bernoulli systems the probabilities\nof the present state are totally independent of whatever happened in\nthe past, even if the past is only one time step back. So knowing the\npast of the system does not improve our predictive abilities in the\nleast; the past is simply irrelevant to predicting the future. This\nfact is often summarised in the slogan that Bernoulli systems are as\nrandom as a coin toss. We should emphasise, however, that this is true\nonly for events in the Bernoulli partition; the characterisation of a\nBernoulli system is silent about what random properties partitions\nother than the Bernoulli partition have. \nK-mixing is more difficult to analyse. We now have to tackle the\nquestion of how to understand \\(\\sigma(n, r)\\), the minimal\n\\(\\sigma\\)-algebra generated by the set \nthat we sidestepped earlier on. What matters for our analysis is that\nthe following types of sets are members of \nwhere the indices \\(j_i\\) range over \\(1, \\ldots ,r\\). Since we are\nfree to chose the sets A\\(_0\\), A\\(_1,\\ldots\\), A\\(_r\\) as we please,\nwe can always chose them so that they are the past history of the\nsystem: the system was in \\(A_{j_0}\\) \\(k\\) time steps back, in\n\\(A_{j_1}\\) \\( k+1\\) time steps back, etc. Call this the\n(coarse-grained) remote past of the\nsystem—‘remote’ because we only consider states that\nare more than \\(k\\) time steps back. The K-mixing condition then says\nthat the system’s entire remote past history becomes\nirrelevant to predicting what happens in the future as time tends\ntowards infinity. Typically Bernoulli systems are compared with\nK-systems by focussing on the events in the Bernoulli partition. With\nrespect to that partition K is weaker than Bernoulli.  The difference\nis both in the limit and the remote history. In a Bernoulli system the\nfuture is independent of the entire past (not only the remote\npast), and this is true without taking a limit (in the case of\nK-mixing independence only obtains in the limit).  However, this only\nholds for the Bernoulli partition; it may or may not hold for other\npartitions—the definition of a Bernoulli system says nothing\nabout that case.[22] \nThe interpretation of strong mixing is now straightforward. It says\nthat for any two sets \\(A\\) and \\(B\\), having been in \\(B\\) \\(k\\) time\nsteps back becomes irrelevant to the probability of being in \\(A\\)\nsome time in the future if time tends towards infinity (i.e. when n\ntends to infinity). In other words, past events \\(B\\) become\nincreasingly irrelevant for the probability of \\(A\\) as the temporal\ndistance between \\(A\\) and \\(B\\) becomes larger. This condition is\nweaker than K-mixing because it only states that the future is\nindependent of isolated events in the remote past, while K-mixing\nimplies independence of the entire remote past history. \nIn weakly mixing systems the past may be relevant to predicting the\nfuture, even in the remote past. The weak mixing condition only says\nthat this influence has to be weak enough for it to be the case that\nthe absolute value of the correlations between a future event and past\nevents vanishes on average; but this does not mean that all individual\ncorrelations vanish. So in weakly mixing systems events in the past\ncan remain relevant to the future. \nErgodicity, finally, implies no decay of correlation at all. The\nergodicity condition only says that the average of the correlations\n(and this time without an absolute value) of all past events with a \nfuture event is zero. But this is compatible with there being\nstrong correlations between every instant in the past and the future,\nprovided that positive and negative correlations average out. So in\nergodic systems the past does not become irrelevant. For this reason\nergodic system are not random at all (in the sense of random\nintroduced above). \nHow relevant are these insights to understanding the behaviour of\nactual systems? A frequently heard objection (which we have already\nencountered in Section 4) is that EH and more generally ergodic theory\nare irrelevant since most systems (including those that we are\nultimately interested in) are not ergodic at\n all.[23] \nThis charge is less acute than it appears at first glance. First, it\nis important to emphasise that it is not the sheer number of\napplications that make a physical concept important, but whether there\nare some important systems that are ergodic. And there are examples of\nsuch systems. For example, so-called ‘hard-ball systems’\n(and some more sophisticated variants of them) are effective\nidealizations of the dynamics of gas molecules, and these systems seem\nto be ergodic;for details, see Berkovitz, Frigg and Kronz 2006,\nSection 3.2, Vranas (1998) and Frigg and Werndl (2011). \nFurthermore, EH can be used to characterize randomness and chaos in\nboth ergodic and non-ergodic systems. Even if a system as a whole is\nnot ergodic (i.e., if it fails to be ergodic with respect to the\nentire phase space \\(X)\\), there can be (and usually there\nare) subsets of \\(X\\) on which the system is ergodic. This is what\nLichtenberg and Libermann (1992, p. 295) have in mind when they\nobserve that ‘[i]n a sense, ergodicity is universal, and the\ncentral question is to define the subspace over which it\nexists’. In fact, non-ergodic systems may have subsets that are\nnot only ergodic, but even Bernoulli! It then becomes interesting\nto ask what these subsets are, what their measures are, and what\ntopological features they have. These are questions studied in parts\nof dynamical systems theory, most notably KAM theory. Hence, KAM\ntheory does not demonstrate that ergodic theory is not useful in\nanalyzing the dynamical behavior of real physical systems (as is often\nclaimed). Indeed, KAM systems have regions in which the system\nmanifest either merely ergodic or Bernoulli behaviour, and accordingly\nEH is useful for charactering the dynamical properties of such systems\n(Berkovitz, Frigg and Kronz 2006, Section 4). Further, as we have\nmentioned in Section 4.1, almost all Hamiltonian systems are\nnon-integrable, and accordingly they have large regions of the phase\nspace in which their motion is ergodic-like. So EH is a useful tool in\nstudying the dynamical properties of systems even if the system fails\nto be ergodic tout court. \nAnother frequently heard objection is that EH is irrelevant in\npractice because most levels of EH (in fact, all except Bernoulli) are\ndefined in terms of infinite time limits and hence remain\nsilent about what happens in finite time. But all we ever\nobserve are finite times and so EH is irrelevant to physics as\npracticed by actual scientists. \nThis charge can be dispelled by a closer look at the definition of a\nlimit, which shows that infinite limits in fact have\nimportant implications for the dynamical behaviour of the system in\nfinite times. The definition of a limit is as follows (where \\(f\\) is\nan arbitrary function of time): lim\\(_{t\\rightarrow \\infty} f(t) = c\\)\niff for every \\(\\varepsilon \\gt 0\\) there exists an \\(t’ \\gt 0\\) so\nthat for all \\(t \\gt t’\\) we have \\(\\lvert f(t) - c\\rvert \\lt\n\\varepsilon\\). In words, for every number \\(\\varepsilon\\), no matter\nhow small, there is a finite time \\(t’\\) after which the\nvalues of \\(f\\) differ from \\(c\\) by less then \\(\\varepsilon\\). That\nis, once we are past \\(t’\\) the values of \\(f\\) never move more than\n\\(\\varepsilon\\) away from \\(c\\). With this in mind strong mixing, for\ninstance, says that for a given threshold \\(\\varepsilon\\) there exists\na finite time \\(t_n (n\\) units of time after the current\ntime) after which \\(C(T_n B, A)\\) is always smaller than\n\\(\\varepsilon\\). We are free to choose \\(\\varepsilon\\) to be an\nempirically relevant margin, and so we know that if a system is\nmixing, we should expect the correlations between the states of the\nsystem after t\\(_n\\) and its current state to be below\n\\(\\varepsilon\\). The upshot is that in strong mixing systems, being in\na state \\(B\\) at some past time becomes increasingly irrelevant for\nits probability of being in the state \\(A\\) now, as the temporal\ndistance between \\(A\\) and \\(B\\) becomes larger. Thus, the fact that\nsystem is strong mixing clearly has implications for its dynamical\nbehaviour in finite times. Furthermore, often (although not always)\nconvergence proofs provide effective bounds on rates of convergence\nand these bounds can be used to inform expectations about behaviour at\na given time. \nSince different levels of EH correspond to different degrees of\nrandomness, each explicated in terms of a different type of asymptotic\ndecay of correlations between states of systems at different times,\none might suspect that a similar pattern can be found in the rates of\ndecay. That is, one might be tempted to think that EH can equally be\ncharacterized as a hierarchy of increasing rates of decay of\ncorrelations: a K-system, for instance, which exhibits exponential\ndivergence of trajectories would be characterized by an exponential\nrate of decay of correlations, while a SM-system would exhibit a\npolynomial rate of decay. \nThis, unfortunately, does not work. Natural as it may seem, EH cannot\nbe interpreted as a hierarchy of increasing rates of decay of\ncorrelations. It is a mathematical fact that there is no particular\nrate of decay associated with each level of EH. For instance, one can\nconstruct K-systems in which the decay is as slow as one wishes it to\nbe. So the rate of decay is a feature of particular systems rather\nthan of a level of EH. \nThe question of how to characterise chaos has been\ncontroversially discussed ever since the inception of chaos theory;\nfor a survey see Smith (1998, Ch. 10). An important family of\napproaches defines chaos using EH. Belot and Earman (1997, 155) state\nthat being strong mixing is a necessary condition and being a K-system\nis a sufficient condition for a system to be chaotic. The view that\nbeing a K-system is the mark of chaos and that any lower degree of\nrandomness is not chaotic is frequently motivated by two ideas. The\nfirst is the idea that chaotic behaviour involves dynamical\ninstability in the form of exponential divergence of nearby\ntrajectories. Thus, since a system involves an exponential divergence\nof nearby trajectories only if it is a K-system, it is concluded that\n(merely) ergodic and mixing systems are not chaotic whereas K- and\nB-systems are. It is noteworthy, however, that SM is compatible with\nthere being polynomial divergence of nearby trajectories and that such\ndivergence sometimes exceeds exponential divergence in the short run.\nThus, if chaos is to be closely associated with the rate of divergence\nof nearby trajectories, there seems to be no good reason to deny that\nSM systems exhibit chaotic behaviour. \nThe second common motivation for the view that being a K-system is the\nmark of chaos is the idea that the shift from zero to positive\nKS-entropy marks the transition from a ‘regular’ to\n‘chaotic’ behaviour. This may suggest that having positive\nKS-entropy is both necessary and sufficient condition for chaotic\nbehaviour. Thus, since K-systems have positive KS-entropy while SM\nsystems don’t, it is concluded that K-systems are chaotic\nwhereas SM-systems are not. Why is KS-entropy a mark of chaos? There\nare three motivations, corresponding to three different\ninterpretations of KS-entropy. First, KS-entropy could be interpreted\nas entailing dynamical instability in the sense of having nearby\ndivergence of nearby trajectories (see Lichtenberg & Liebermann,\n1992, p. 304). Second, KS-entropy could be connected to algorithmic\ncomplexity (Brudno 1978). Yet, while such a complexity is sometimes\nmentioned as an indication of chaos, it is more difficult to connect\nit to physical intuitions about chaos. Third, KS-entropy could be\ninterpreted as a generalized version of Shannon’s information\ntheoretic entropy (see Frigg 2004). According to this approach,\npositive KS-entropy entails a certain degree of unpredictability,\nwhich is sufficiently high to deserve the title\n chaotic.[24] \nWerndl (2009b) argues that a careful review of all systems that one\ncommonly regards as chaotic shows that strong mixing is the crucial\ncriterion: a system is chaotic just in case it is strong mixing. As\nshe is careful to point out, this claim needs to be qualified: systems\nare rarely mixing on the entire phase space, but neither are they\nchaotic on the entire phase space. The crucial move is to restrict\nattention to those regions of phase space where the system is chaotic,\nand it then turns out that in these same regions the systems are also\nstrong mixing. Hence Werndl concludes that strong mixing is the\nhallmark of chaos. And surprisingly this is true also of dissipative\nsystems (i.e., systems that are not measure preserving). These systems\nhave attractors, and they are chaotic on their attractors rather than\non the entire phase space. The crucial point then is that one can\ndefine an invariant (preserved) measure on the attractor and\nshow that the system is strongly mixing with respect to that measure.\nSo strong mixing can define chaos in both conservative and dissipative\nsystems. \nThe search for necessary and sufficient conditions for chaos\npresupposes that there is a clear-cut divide between chaotic and\nnon-chaotic systems. EH may challenge this view, as every attempt to\ndraw a line somewhere to demarcate the chaotic from non-chaotic\nsystems is bound to be somewhat arbitrary. Ergodic systems are pretty\nregular, mixing systems are less regular and the higher positions in\nthe hierarchy exhibit still more haphazard behaviour. But is there one\nparticular point where the transition from ‘non-chaos’ to\nchaos takes place? Based on the argument that EH is a hierarchy of\nincreasing degrees of randomness and degrees of randomness correspond\nto different degrees of unpredictability (see Section 5), Berkovitz,\nFrigg and Kronz (2006, Section 5.3) suggest that chaos may well be\nviewed as a matter of degree rather than an all-or-nothing affair.\nBernoulli systems are very chaotic, K-systems are slightly less\nchaotic, SM-systems are still less chaotic, and ergodic systems are\nnon-chaotic. This suggestion connects well with the idea that chaos is\nclosely related to unpredictability. \nThe ergodic hierarchy has also been used to understand quantum chaos.\nCastagnino and Lombardi (2007) analyze the problem of quantum chaos as\na particular case of the classical limit of quantum mechanics and\nidentify mixing in the classical limit as the condition that a quantum\nsystem must satisfy to be nonintegrable. Gomez and Castagnino (2014,\n2015) generalize the entire ergodic hierarchy to the quantum context\nand argue that EH thus generalized is a helpful tool to understand\nquantum chaos; Fortin and Lombardi (2018) use EH to understand\ndecoherence; and Gomez (2018) discusses the KS entropy in quantum\nmixing systems. \nMixing, finally, has also been invoked in understanding the effects of\nstructural model error. Frigg, Bradley, Du and Smith (2014) argue that\nthe distinction between parameter error and structural model error is\ncrucial, and that the latter has significant and hitherto\nunappreciated impact on the predictive ability of a model. Wilson-Mayo\n(2015) points out that to put this observation on a solid foundation\nwe need a notion of structural chaos. He proposes such a notion by\nappealing to topological mixing.  \nEH is often regarded as relevant for explicating the nature of\nrandomness in deterministic dynamical systems. It is not clear,\nhowever, what notion of randomness this claim invokes. The formal\ndefinitions of EH do not make explicit appeal to randomness and the\nusual ways of presenting EH do not involve any specification of the\nnotion of randomness that is supposed to underlie EH. As suggested in\nSection 5, EH can be interpreted as a hierarchy of randomness if\ndegrees of randomness are explicated in terms of degrees of\nunpredictability, which in turn are explicated in terms of (coherent)\nconditional degrees of beliefs. In order for these degrees of belief\nto be indicative of the system’s dynamical properties, they have\nto be updated according to a system’s dynamical law. The idea is\nthen that the different levels of EH, except for merely ergodic\nsystems, correspond to different kinds of unpredictability, which\ncorrespond to different patterns of decay of correlations between\nsystems’ past states and their present states. Merely ergodic\nsystems seem to display no randomness, as the correlations between\ntheir past and present states need not decay at all. \nErgodic theory plays an important role in statistical physics, and EH,\nor some modification of it, constitutes an important measure of\nrandomness in both Hamiltonian and dissipative systems. It is\nsometimes argued that EH is by and large irrelevant for physics\nbecause real physical systems are not ergodic. But, this charge is\nunwarranted, and a closer look at non-ergodic systems reveals a rather\ndifferent picture, because EH can be fruitfully be used in the\nfoundations of statistical mechanics, analyses of randomness, and\nchaos theory. More recently it has also played a role in understanding\nlaws of nature (Filomeno 2019, List and Pivato 2019).","contact.mail":"fkronz@nsf.gov","contact.domain":"nsf.gov"}]
