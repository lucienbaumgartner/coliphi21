[{"date.published":"2019-06-03","url":"https://plato.stanford.edu/entries/qm-retrocausality/","author1":"Simon Friederich","author1.info":"http://simonfriederich.eu","author2.info":"https://hapi.uq.edu.au/profile/450/peter-evans","entry":"qm-retrocausality","body.text":"\n\n\nQuantum theory provides a framework for modern theoretical physics\nthat enjoys enormous predictive and explanatory success. Yet, in view\nof the so-called “measurement problem”, there is no\nconsensus on how physical reality can possibly be such that this\nframework has this success. The theory is thus an extremely\nwell-functioning algorithm to predict and explain the results of\nobservations, but no consensus on which kind of objective reality\nmight plausibly underlie these observations.\n\n\nAmongst the many attempts to provide an “interpretation”\nof quantum theory to account for this predictive and explanatory\nsuccess, one class of interpretations hypothesizes backward-in-time\ncausal influences—retrocausality—as the basis for\nconstructing a convincing foundational account of quantum theory. This\nentry presents an overview of retrocausal approaches to the\ninterpretation of quantum theory, the main motivations for adopting\nthis approach, a selection of concrete suggested retrocausal models,\nand a review of the objections brought forward against such\napproaches.\n\nFrom the birth of the theory of quantum mechanics in 1925/6 to the\noutbreak of war in Europe, a clear orthodoxy emerged in the conceptual\nand ontological framework for understanding quantum theory. Now known\nas the Copenhagen interpretation, this framework embodied the\npositivistic tendencies of Heisenberg and Bohr, and was set opposed to\nthe more realist tendencies of de Broglie, Einstein, and\nSchrödinger. It was not until Bell’s theorem in the 1960s,\nand its experimental tests in the 1970s and 1980s, that new energy was\nbreathed into this interpretational debate. However, beginning in the\nmid-1940s, the first suggestions of retrocausality as part of the\nconceptual and ontological framework in quantum theory had already\nmaterialized. \nThere are two key ideas that punctuate the historical development of\nthe notion of retrocausality in quantum mechanics. The first proposal\nof retroactive influence in quantum mechanics comes from a suggestion\nmade by Wheeler and Feynman (1945, 1949). They were led to this idea\nwhile considering the potentially classical origins of some of the\ndifficulties of quantum theory. Consider the following problem from\nclassical electrodynamics: an accelerating electron emits\nelectromagnetic radiation and, through this process, the acceleration\nof the electron is damped. Various attempts to account for this\nphenomenon in terms of the classical theory of electrodynamics lacked\neither empirical adequacy or a coherent physical interpretation.\nWheeler and Feynman attempted to remedy this situation by\nreinterpreting Dirac’s (1938) theory of radiating electrons. \nThe core of Wheeler and Feynman’s proposed “absorber\ntheory of radiation” is a suggestion that the process of\nelectromagnetic radiation emission and absorption should be thought of\nas an interaction between a source and an absorber rather than as an\nindependent elementary process. (This idea has its roots as far back\nas Tetrode 1922 and G. Lewis 1926.) Wheeler and Feynman imagine an\naccelerated point charge located within an absorbing system and\nconsider the nature of the electromagnetic field associated with the\nacceleration. An electromagnetic disturbance can be imagined\n“initially” to travel outwards from the source to perturb\neach particle of the absorber. The particles of the absorber then\ngenerate together a subsequent field. According to the Wheeler-Feynman\nview, this new field is comprised of half the sum of the retarded\n(forward-in-time) and advanced (backward-in-time) solutions to\nMaxwell’s equations. The sum of the advanced effects of all the\nparticles of the absorber then yields an advanced incoming field that\nis present at the source simultaneous with the moment of emission\n(although see\n §5\n for more on how one should understand this “process”).\nThe claim is that this advanced field exerts a finite force on the\nsource which has exactly the required magnitude and direction to\naccount for the observed energy transferred from source to absorber;\nthis is Dirac’s radiative damping field. In addition, when this\nadvanced field is combined with the equivalent half-retarded,\nhalf-advanced field of the source, the total observed disturbance is\nthe full retarded field known empirically to be emitted by accelerated\npoint charges. \nThe crucial point to note about the Wheeler-Feynman schema is that due\nto the advanced field of the absorber, the radiative damping\nfield is present at the source at exactly the time of the initial\nacceleration. This schema of advanced and retarded waves now forms the\nbasis for the most fully-formed retrocausal model of quantum\nmechanics, the transactional interpretation (see\n §5). \nThe second key idea in the historical development of retrocausality in\nquantum mechanics occurs around the same time as Wheeler and\nFeynman’s absorber theory. French physicist Costa de Beauregard,\na student of de Broglie, noticed a potential objection to the\nreasoning found in Einstein, Podolsky, and Rosen’s famous paper\n(1935) on the completeness of quantum mechanics (see the entry on\n the Einstein-Podolsky-Rosen argument in quantum theory).\n Now widely known as the EPR argument, They argue that quantum\nmechanics must be incomplete on the basis of the following assumption:\nno reasonable definition of reality could be expected to permit the\nreality of some system being dependent upon the process of measurement\ncarried out on some other distant system which does not in any way\ndisturb the first system. The “reasonable definition of\nreality” to which Einstein et al. refer is implicitly an\nassumption of relativistic “locality” (made explicit in\nEinstein 1948), which combines causal asymmetry with the Lorentz\ninvariance of special relativity (more on this in a moment). Costa de\nBeauregard, however, was alert to a particular kind of unorthodox\ninterpretation of this assumption which undermined its role in the EPR\nargument. His proposal was that two distant systems could\n“remain correlated by means of a successively advanced and\nretarded wave” (Costa de Beauregard 1953: 1634); that is, one\nsystem could influence, via an advanced wave, the state of the\ncombined systems in their common past, which then, via a retarded\nwave, could influence the state of the distant system in a kind of\n“zigzag” through spacetime. This way, there could be a\ndependence between the two distant systems without any violation of\nLorentz invariance. Thus, as Costa de Beauregard (1987b: 252) puts it,\n \nEinstein of course is right in seeing an incompatibility between his\nspecial relativity theory and the distant quantal correlations, but\nonly under the assumption that advanced actions are\nexcluded. \nWhen Costa de Beauregard in 1947 suggested this response to the EPR\nargument to his then supervisor de Broglie, de Broglie was “far\nfrom willing to accept” the proposal (1987b: 252) and forbade\nCosta de Beauregard to publish his unorthodox idea (Price &\nWharton 2015). However, in 1948 Feynman had developed his eponymous\ndiagrams in which antiparticles were to be interpreted as particles\nmoving backward-in-time along the particle trajectories, and so by\n1953 de Broglie had endorsed the publication of Costa de\nBeauregard’s response. On the seeming craziness of the proposal,\nCosta de Beauregard claims, “[t]oday, as the phenomenon of the\nEPR correlations is very well validated experimentally, and is in\nitself a ‘crazy phenomenon’, any explanation of it must be\n‘crazy’” (1987b: 252; see also Costa de Beauregard\n1976, 1977b, 1987a). \nBefore addressing two of the main motivations for adopting the\nhypothesis of retrocausality in the foundations of quantum theory, it\nwill be worthwhile to provide a few comments on two key notions that\nplay a significant role in the following discussion: causality and\nlocality. This will help to pin down what exactly is meant, and not\nmeant, by retrocausality. \nThere is a tradition that stretches back at least as far as Russell\n(1913) that denies that there is any place for causal notions in the\nfundamental sciences, including physics: the notion serves no purpose,\nand simply does not appear, in the fundamental sciences. The argument\ngoes that, since at least the nineteenth century, the laws that govern\nphysical behavior in fundamental sciences such as physics are almost\nalways differential equations. Such equations are notable for\nspecifying, given some initial conditions, exact properties of systems\nfor all time. And thus if everything is specified for all time, there\nis no place left for causality. Thus Russell advocates that\n“causality” should be eliminated from the philosophers\nlexicon, because it is certainly not a part of the scientific\nlexicon. \nIn contrast to Russell’s position, Cartwright (1979: 420) claims\nthat we do have a need and use for a causal vocabulary in science:\n“causal laws cannot be done away with, for they are needed to\nground the distinction between effective strategies and ineffective\nones”. One of the main contemporary accounts of causation, the\ninterventionist account of causation (Woodward 2003; see also the\nentry on\n causation and manipulability),\n is an embodiment of Cartwright’s dictum. In a nutshell, the\ninterventionist account claims that A is a cause of B if\nand only if manipulating A is an effective means of\n(indirectly) manipulating B. Causality in the present entry,\nunless specified otherwise, should be understood along broadly\ninterventionist lines. According to accounts of quantum theory that\nhypothesize retrocausality, manipulating the setting of a measurement\napparatus can be an effective means of manipulating aspects of the\npast. A broadly interventionist view of causality indeed underlies\nmost contemporary attempts to harness the tool kit of causal modeling\n(see the entry on\n causal models;\n Spirtes, Glymour, & Scheines 2000; Pearl 2009) in the foundations\nof quantum theory (Leifer & Spekkens 2013; Cavalcanti & Lal\n2014; Costa & Shrapnel 2016; Allen et al. 2017). \nUsing the notion of causality along broadly interventionist lines in\nthe foundations of quantum theory does not commit one to realism (or\nanti-realism) about the causal relations at issue. Woodward combines\ninterventionism with realism about causality while acknowledging  \nimportant differences between, on the one hand, the way in which\ncausal notions figure in common sense and the special sciences and the\nempirical assumptions that underlie their application and, on the\nother hand, the ways in which these notions figure in physics.\n(Woodward 2007: 67; although see Frisch 2014: chs. 4 and 5 for a\nresponse)  \nAnother suggested strategy to take into account Russell’s worry\nwhile continuing to apply causal notions in physics in a consistent\nmanner is to understand interventionism in “perspectival”\nterms (Price 2007; Price & Corry 2007; Price & Weslake 2010;\nIsmael 2016). Perspectivalism is usually staged, as seems natural in\nthe setting of modern physics (although more will be said on this\nbelow), in the framework of a block universe view where the past,\npresent, and future are equally real. In this framework, causality\ncannot have anything to do with changing the future or the\npast because both are—from an “external”\nperspective—completely “fixed”. But one can\nunderstand causation in the block universe from an\n“internal” perspective, according to which causal\ncorrelations are precisely those that are stable under interventions\non those variables that we refer to as the “causes”. \nThe important difference between the two viewpoints—internal and\nexternal to the block—is that there is a discrepancy between the\nparts of the spacetime block that are epistemically accessible from\neach perspective. The spatiotemporally constrained perspective by\nwhich we are bound permits us only limited epistemic accessibility to\nother spatiotemporal regions. This is the perspective in which,\naccording to causal perspectivalism, causal notions are perfectly\nserviceable. Once, on the other hand, we imagine ourselves to be\nomniscient beings that have epistemic access to the whole\nspatiotemporal block, it should not come as a surprise that our causal\nintuitions get confused when we attempt to consider how a\nspatiotemporally bound agent can deliberate about whether or not to\naffect a particular event that is already determined from our imagined\nomniscient perspective. It is because we do not know which\nevents are determined to occur and are ignorant about many others that\nwe can be deliberative agents at all. Again, these considerations are\nrelevant just as much to ordinary forward-in-time causation as they\nare to backward-in-time causation. \nMany of the retrocausal approaches to quantum theory considered in\n §6\n are best understood with some type of perspectival interventionist\naccount of causality in mind. A notable exception is the transactional\ninterpretation\n (§5),\n in which causality might be best understood in terms of processes\nunderscored by conserved quantities. The possibilist extension of the\ntransactional interpretation, defended by Kastner (2006, 2013),\nmoreover eschews the block universe picture. \nAccording to Bell’s theorem (Bell 1964; Clauser et al. 1969; see\nalso the entry on\n Bell’s theorem)\n and its descendants (e.g., Greenberger, Horne, & Zeilinger 1989;\nsee also Goldstein et al. 2011; Brunner et al. 2014 for an overview),\nany theory that reproduces all the correlations of measurement\noutcomes predicted by quantum theory must violate a principle that\nBell calls local causality (Bell 1976, 1990; see also Norsen\n2011; Wiseman & Cavalcanti 2017). In a locally causal theory,\nprobabilities of spatiotemporally localized events occurring in some\nregion 1 are independent of what occurs in a region 2 that is\nspacelike separated from region 1, given a complete specification of\nwhat occurs in a spacetime region 3 in region 1’s backward light\ncone that completely shields off region 1 from the backward light cone\nof region 2. (See, for instance, Figs. 4 and 6 in Bell 1990 or Fig. 2\nin Goldstein et al. 2011.) \nIn a relativistic setting, then, the notion of locality involves\nprohibiting conditional dependences between spacelike separated\nevents, provided that the region upon which these spacelike separated\nevents are conditioned constitutes their common causal (Minkowski)\npast. This characterization of locality implicitly assumes causal\nasymmetry. Thus locality is the idea that there are no causal\nrelations between spacelike separated events. \nThere is another sense of “local” that is sometimes used\nthat will be worth avoiding for the purposes of clarity. This is the\nidea that causal influences are constrained along timelike\ntrajectories. Thus, given Costa de Beauregard’s suggestion of\n“zigzag” causal influences, it is perfectly possible for a\nretrocausal model of quantum phenomena to be nonlocal in the sense\nthat causal relations exist between spacelike separated events, but\n“local” in the sense that these causal influences are\nmediated by timelike trajectories. To avoid ambiguity, it will be\nuseful to refer to this latter sense as\n“action-by-contact” (set apart from\naction-at-a-distance). \nThe first of two main motivating considerations for invoking\nretrocausality in the foundations of quantum mechanics derives from\nthe exploitation of what is essentially the same loophole in a range\nof theorems collectively known as “no-go theorems”.\nAccording to these theorems, any theory or model that is able to\naccount for the empirically confirmed consequences of quantum theory\nmust be unavoidably nonlocal, contextual, and\n\\(\\psi\\)-ontic (i.e., ascribe reality to the quantum states\n\\(\\psi\\)). \nOne way to understand the role that retrocausality plays in\ncircumventing the results of the no-go theorems is to consider each\ntheorem to be underpinned by what is known as the ontological models\nframework (Harrigan & Spekkens 2010; Leifer 2014; Ringbauer 2017).\nThe ontological models framework formalizes and captures the central\nnotion of realism in quantum theory (and so subsumes local hidden\nvariable approaches to quantum mechanics). The framework consists of\nan operational description of a general quantum process, which\ndescribes observed statistics for outcomes of measurements given both\npreparations and transformations, along with an ontological model (or\n“ontic extension”) accounting for the observed statistics.\nFor every preparation procedure, which is usually said to result in a\nquantum state \\(\\psi\\), the quantum system is in fact prepared in an\n“ontic” state \\(\\lambda\\), chosen from a set of states\n\\(\\Lambda\\), which completely specifies the system’s properties.\nThe framework leaves open (and is ultimately used to define) whether\nthe quantum state \\(\\psi\\) is itself an ontic or epistemic state (if\n\\(\\psi\\) is ontic, \\(\\lambda\\) either includes additional ontic\ndegrees of freedom, or is in one-to-one correspondence with \\(\\psi\\);\nsee\n §3.3).\n Each preparation is assumed to result in some \\(\\lambda\\) via a\nclassical probability density over \\(\\Lambda\\), and a set of\nmeasurement procedures that determine conditional probabilities for\noutcomes dependent upon \\(\\lambda\\) (which thus screens off the\npreparation procedure \\(\\psi\\)); explicitly, \\(\\lambda\\) does not\ncausally depend on any future measurement setting \\(\\alpha\\). Finally,\nthe operational statistics must reproduce the quantum statistics. \nImportant for our purposes here is the qualification that \\(\\lambda\\)\ndoes not causally depend on the measurement setting \\(\\alpha\\). This\nassumption is referred to as “measurement independence”\n(\\(\\lambda\\) is “conditionally independent” of\n\\(\\alpha\\)). This assumption is explicitly violated by allowing\nretrocausal influences to be at play in the system. Thus, in so far as\nthe no-go theorems are underpinned by the ontological models\nframework, the no-go theorems are no longer applicable to models that\nallow retrocausality. And in so far as there is motivation to avoid\nthe consequences of the no-go theorems for quantum theory,\nretrocausality is well placed to provide such a model (or so the\nargument goes). Notably, it has been argued that admitting\nretrocausality (i) makes it possible to account for the correlations\nentailed by quantum theory using action-by-contact causal influences\n(and so ensures Lorentz invariance); (ii) undermines as implausible\nthe assumption of (certain types of) noncontextuality from the outset;\nand (iii) may enable an independently attractive \\(\\psi\\)-epistemic\ninterpretation of the wavefunction underpinned by local hidden\nvariables. These arguments are addressed in turn. \nThe principle of local causality, according to Bell, is meant to spell\nout the idea that  \n[t]he direct causes (and effects) of events are near by, and even the\nindirect causes (and effects) are no further away than permitted by\nthe velocity of light. (1990: 105)  \nViolation of this principle, according to some researchers in the\nfoundations of quantum theory, indicates a fundamental incompatibility\nbetween quantum theory and the spirit, perhaps even the letter, of\nrelativity theory (Maudlin 2011). That the correlations entailed by\nquantum theory which violate local causality actually occur in nature\nhas been experimentally documented many times (for example, by\nFreedman & Clauser 1972; Aspect, Dalibard, & Roger 1982; and\nAspect, Grangier, & Roger 1982). \nBell’s result crucially depends not only on the assumption of\nlocal causality, but also on the assumption that whatever variables\n\\(\\lambda\\) describes in some spacetime region, which Bell calls\n“local beables”, do not depend probabilistically on which\nmeasurement setting \\(\\alpha\\) some experimenters choose in the future\nof that region:  \nThis is the aforementioned assumption of measurement independence. It\nis also sometimes referred to as “no superdeterminism”\nbecause it is incompatible with a particularly strong form of\ndeterminism (“superdeterminism”) according to which the\njoint past of the measurement setting \\(\\alpha\\) and the measured\nsystem state \\(\\lambda\\) determines them both completely and induces a\ncorrelation between them. But, as pointed out in the first instance by\nCosta de Beauregard (1977a) and then by Price (1994, 1996),\nsuperdeterminism is not the only way in which Eq.\n(\\ref{eq:independence}) can be violated: if there is retrocausality\n(understood along interventionist lines as outlined in\n §2.1),\n the choice of measurement setting \\(\\alpha\\) may causally influence\nthe physical state \\(\\lambda\\) at an earlier time and thereby also\nrender Eq. (\\ref{eq:independence}) incorrect. Consequently, by\npostulating Eq. (\\ref{eq:independence}) one assumes retrocausality to\nbe absent. \nWithout Eq. (\\ref{eq:independence}), in turn, Bell’s theorem can\nno longer be derived. Thus, admitting the possibility of\nretrocausality in principle reopens the possibility of giving a causal\naccount of the nonlocal correlations entailed by quantum theory as\nmediated by purely action-by-contact, spatiotemporally contiguous,\nLorentz invariant causal influences (of the type envisaged by Costa de\nBeauregard) acting between systems described by local beables.\nConcrete steps towards a model that might fulfill this promise will be\nreviewed in\n §6. \nA wealth of theoretical results establish that models and theories\nwhich reproduce the predictions of quantum theory must be in some\nsense “contextual”. A model or theory is\nnoncontextual when the properties attributed to the system\n(e.g., the value of some dynamical variable that the system is found\nto have) are independent of the manner in which those\nproperties are measured or observed (the context), beyond the actual\nobservation itself (for instance, the other properties that are\nmeasured in conjunction with the measurement). In classical mechanics,\nthe properties that systems are found to have and that are attributed\nto them as a consequence of measurement do not in principle depend on\nthe measurement context, so classical mechanics is noncontextual. The\nfirst contextuality theorems for quantum theory go back to Bell (1966)\nand Kochen and Specker (1967). They demonstrate that if one were to\nconsider quantum measurements as deterministically uncovering the\nvalues of arbitrary pre-existing dynamical variables of quantum\nsystems, then such a model would have to be contextual (that is,\nmeasured values would have to depend upon the context of measurement).\nThus, no noncontextual deterministic local hidden variable model can\nreproduce the observed statistics of quantum theory. \nA new way of understanding contextuality operationally was pioneered\nby Spekkens (2005). Employing the ontological models framework, an\nontological model of an operational theory is noncontextual\nwhen operationally equivalent experimental procedures have equivalent\nrepresentations in the ontological model. This understanding of\nnoncontextuality is then a principle of parsimony akin to\nLeibniz’ law of Identity of Indiscernibles: no ontological\ndifference without operational difference. Using this operational\nunderstanding, Spekkens expands the class of ontological models to\nwhich contextuality applies beyond deterministic local hidden variable\nmodels. But the story is essentially the same: hypothesizing\nunderlying ontic properties for quantum systems that are distinct only\nif there are corresponding operational differences cannot account for\nthe correlations entailed by quantum theory. Thus, no noncontextual\nontological model can reproduce the observed statistics of quantum\ntheory. (For an extended criticism of Spekkens’ operational\nunderstanding of contextuality see Hermens 2011.) \nHypothesizing retrocausal influences alleviates both of these worries.\nRetrocausality renders Kochen-Specker-type contextuality potentially\nexplainable as a form of “causal contextuality” (see the entry\non\n the Kochen-Specker theorem,\n §5.3). If there is a backward-directed influence of the chosen\nmeasurement setting (and context) on the pre-measurement ontic state,\nit is no longer to be expected that the measurement process is simply\nuncovering an independently existing definite value for some property\nof the system, rather the measurement process can play a causal role\nin bringing about such values (the measurement process is\nretrocausal rather than retrodictive). Indeed, one\nmight argue contextuality of measured values is just what one might\nexpect when admitting retrocausal influences. As Wharton (2014: 203)\nputs it, “Kochen-Specker contextuality is the failure\nof the Independence Assumption”, i.e., the failure of\nmeasurement independence. \nWith respect to Spekkens’ more general understanding of\ncontextuality, recall that it is an explicit assumption of the\nontological models framework that the ontic state is independent of\nthe measurement procedure. Thus, hypothesizing retrocausality is an\nunequivocal rejection of the ontological models framework. But one\nmight wonder whether Spekkens’ principle of parsimony might be\nrecast to apply more generally to retrocausal models.\n §7.3\n considers a result due to Shrapnel and Costa (2018) that indicates\nthat retrocausal accounts must indeed accept Spekkens-type\ncontextuality. \nIn some models staged in the ontological models framework, an ontic\nstate \\(\\lambda\\) is only compatible with preparation procedures\nassociated with one specific quantum state \\(\\psi\\). In that case,\n\\(\\psi\\) can be seen as part of \\(\\lambda\\) and, therefore, as an\nelement of reality itself. Models of this type are referred to as\n“\\(\\psi\\)-ontic”. In other models, an ontic state\n\\(\\lambda\\) can be the result of preparation procedures associated\nwith different quantum states \\(\\psi\\). In these models, \\(\\psi\\) is\nnot an aspect of reality and is often more naturally seen as\nreflecting an agent’s incomplete knowledge about the underlying\nontic state \\(\\lambda\\). Models of this type are referred to as\n“\\(\\psi\\)-epistemic”. Spekkens (2007) motivates the search\nfor attractive \\(\\psi\\)-epistemic models by pointing out various\nparallels between quantum mechanics and a toy model in which the\nanalogue of the quantum state is epistemic in that it reflects\nincomplete information about an underlying ontic state. \nHowever, so-called “\\(\\psi\\)-ontology theorems” (Pusey,\nBarrett, & Rudolph 2012; Hardy 2013; Colbeck & Renner 2012;\nsee Leifer 2014 for an overview), establish that, given certain\nplausible assumptions, only \\(\\psi\\)-ontic models can reproduce the\nempirical consequences of quantum theory. The specific assumptions\nused to derive this conclusion differ for the various theorems. All of\nthem, however, rely on the ontological models framework which, as\nnoted a number of times already, explicitly incorporates an assumption\nof measurement independence (Eq. (\\ref{eq:independence})). Since, as\nshown above, measurement independence implicitly rules out\nretrocausality, these theorems apply only inasmuch as retrocausality\nis assumed to be absent. \nMoreover, the theorem due to Pusey, Barrett, and Rudolph\n(2012)—which according to Leifer (2014) uses the least\ncontestable assumptions of all—relies on an additional\nindependence assumption according to which the ontic states of two or\nmore distinct, randomly prepared, systems are uncorrelated before\nundergoing joint measurement. Wharton (2014: 203) points out that in a\nframework that is open to retrocausality this is a problematic\nassumption because one would then expect “past correlations [to]\narise […] for exactly the same reason that future correlations\narise in entanglement experiments”. Exploring the prospects for\nretrocausal accounts is therefore one—perhaps\nthe—important open route for developing an attractive\n\\(\\psi\\)-epistemic model that may be able to explain the quantum\ncorrelations. \nIt is worth pausing at this point to consider the metaphysical\nmotivation for taking a retrocausal approach to quantum theory,\nespecially in light of circumventing these no-go theorems. The\northodox reading of the no-go theorems is that, whatever is said about\nthe ultimate conceptual and ontological framework for understanding\nquantum theory, it cannot be completely classical: it must be nonlocal\nand/or contextual and/or ascribe reality to indeterminate states. In\nshort, quantum theory cannot be about local hidden variables. Part of\nthe appeal of hypothesizing retrocausality in the face of these no-go\ntheorems is to regain (either partial or complete) classicality in\nthese senses (albeit, with—perhaps nonclassical—symmetric\ncausal influences). That is, retrocausality holds the potential to\nallow a metaphysical framework for quantum mechanics that contains\ncausally action-by-contact, noncontextual (or where any contextuality\nis underpinned by noncontextual epistemic constraints),\ncounterfactually definite, determinate (although possibly\nindeterministic), spatiotemporally located properties for physical\nsystems—in other words, a classical ontology. \nA nice to way to consider this is in terms of Quine’s (1951)\ndistinction between the “ontology” and the\n“ideology” of a scientific theory, where the ideology of a\ntheory consists of the ideas that can be expressed in that theory.\nIdeological economy is then a measure of the economy of primitive\nundefined statements employed to reproduce this ideology. The claim\nhere would then be that the ideology of symmetric causal influences is\nmore economical than rejecting a classical ontology. Thus in so far as\nquantum mechanics is telling us something profound about the\nfundamental nature of reality, the hypothesis of retrocausality shows\nthat the lesson of quantum mechanics is not necessarily that the\nquantum ontology can no longer be classical. Some might see this as a\nvirtue of retrocausal approaches. (Although the Shrapnel-Costa no-go\ntheorem reviewed in\n §7.3\n significantly jeopardizes this view.) \nThe laws of nature at the most fundamental level at which they are\ncurrently known are combined in the Standard Model of elementary\nparticle physics. These laws are CPT-invariant, i.e., they remain the\nsame under the combined operations of charge-reversal C\n(replacing all particles by their anti-particles), parity P\n(flipping the signs of all spatial coordinates), and time-reversal\nT. The asymmetries in time which are pervasive in our everyday\nlives are a consequence not of any temporal asymmetry in these laws\nbut, instead, of the boundary conditions of the universe, notably in\nits very early stages. It seems natural to assume that the\ntime-symmetry of the laws (modulo the combined operation of C\nand P) extends to causal dependences at the fundamental\n“ontic” level that underlies the empirical success of\nquantum theory. If so, there may be backward-in-time no less than\nforward-in-time causal influences at that ontic level. \nPrice (2012) turns these sketchy considerations into a rigorous\nargument. He shows that, when combined with two assumptions concerning\nquantum ontology, time-symmetry implies retrocausality (understood\nalong broadly interventionist lines). The ontological assumptions are\n(i) that at least some aspects of the quantum state \\(\\psi\\) are real\n(notably, in Price’s example, there is a “beable”\nencoding photon polarization angle), and (ii) that inputs and outputs\nof quantum experiments are discrete emission and detection\nevents. Moreover, it is important to Price’s argument that\ndynamical time-symmetry (that the dynamical laws of the theory are\ntime-symmetric) be understood as implying that operational\ntime-symmetry (that the set of all possible combinations of\npreparation and measurement procedures in a theory, with associated\nprobabilities for outputs given inputs, is closed under interchange of\npreparation and measurement) translates into ontic time-symmetry\n(operational time-symmetry plus a suitable map between the ontic state\nspaces of the symmetric combinations). Given these conditions, any\nfoundational account that reproduces the empirical verdicts of quantum\ntheory must be retrocausal. \nLeifer and Pusey (2017) (see also Leifer 2017 in\n Other Internet Resources)\n strengthen Price’s argument by showing that his assumption\nabout the reality of (aspects of) the quantum state \\(\\psi\\) can be\nrelaxed. As they demonstrate, if measurement outcomes depend only on a\nsystem’s ontic state \\(\\lambda\\), i.e., if that state completely\nmediates any correlations between preparation procedures and\nmeasurement outcomes (“\\(\\lambda\\)-mediation”), this\nsuffices for operational time-symmetry to entail the existence of\nretrocausality. Foundational accounts which like Bohmian mechanics\n(Bohm 1952a,b) or GRW-theory (Ghirardi, Rimini, & Weber 1986)\navoid postulating retrocausality do so by violating time-symmetry in\nsome way. The GRW-theory does so by introducing explicitly\ntime-asymmetric dynamics. In Bohmian mechanics the dynamics is\ntime-symmetric, but the theory is applied in a time-asymmetric manner\nwhen assessing which quantum states are actually realized. Notably,\none assumes that the quantum states of a measured system and a\nmeasurement device connected to it are uncorrelated prior to\nmeasurement, whereas they are in general correlated after measurement\n(Leifer & Pusey 2017: §X.). \nThe more advanced relativistic quantum theories that underlie modern\nparticle physics are typically formulated in the Lagrangian, path\nintegral-based, formalism, where information about the symmetries and\ninteractions of the theory are encoded in the action, S.\nWharton, Miller, and Price (2011) use this formalism as a foundation\nto suggest that action symmetries must be reflected as ontological\nsymmetries. In particular, Wharton et al. claim that for any two\nexperimental arrangements related by a spacetime transformation (say,\nreflection in time) that leaves the action unchanged, the ontologies\nmust also remain unchanged across the same spacetime transformation.\nThis principle, referred to by Wharton et al. as action\nduality, imposes non-trivial constraints on realist accounts. \nNotably, Evans, Price, & Wharton (2013) argue that there is such\nan action symmetry between a pair of entangled photons passing through\nseparate polarizers and a single photon passing sequentially through\ntwo polarizers. Thus, if action duality is well-founded, the action\nsymmetry should be reflected in identical ontologies for the\naction-dual experiments. Short of rejecting action duality, this\nimplies one of two things: either the usual causal explanation for the\nbehavior of the single photon must be reflected in a causal\nexplanation for the typically-quantum behavior of the pair of\nentangled photons, and so there must be retrocausality at the level of\nthe ontic state \\(\\lambda\\), or the nonlocality ascribed to the\ntypically-quantum behavior of the pair of entangled photons must be\nsimilarly ascribed to the behavior of the single photon, and so\nnonlocality is even more dramatically widespread than usually assumed,\nand indeed is not unique to entangled bipartite quantum systems. In so\nfar as there is a strong case for the behavior of a single photon\npassing sequentially through two polarizers having a perfectly good\ncausal explanation based on spatiotemporally localized properties, the\nargument of Evans et al. is that this amounts to an equally strong\ncase for retrocausality in quantum theory. \nThis entry so far has considered the two most significant motivating\narguments in favor of adopting retrocausality as a hypothesis for\ndealing with the interpretational challenges of quantum theory. But\nthese motivations do not by themselves amount to an interpretation or\nmodel of quantum theory.\n §6\n consists of a survey of a range of retrocausal models, but this\nsection first considers perhaps the most prominent retrocausal model,\nthe transactional interpretation. Developed by Cramer in the 1980s\n(Cramer 1980, 1986, 1988), the transactional interpretation is heavily\ninfluenced by the framework of the Wheeler-Feynman absorber approach\nto electrodynamics (see\n §1);\n the Wheeler-Feynman schema can be adopted to describe the microscopic\nexchange of a single quantum of energy, momentum, etc.,\nbetween and within quantum systems. \nAt the heart of the transactional interpretation is the\n“transaction”: real physical events are identified with\nso-called “handshakes” between forward-evolving quantum\nstates \\(\\psi\\) and backward-evolving complex-conjugates \\(\\psi^*\\).\nWhen a quantum emitter (such as a vibrating electron or atom in an\nexcited state) is to emit a single quantum (a photon, in these cases),\nthe source produces a radiative field—the “offer”\nwave. Analogously to the Wheeler-Feynman description, this field\npropagates outwards both forward and backward in time (as well as\nacross space). When this field encounters an absorber, a new field is\ngenerated—the “confirmation” wave—that\nlikewise propagates both forward and backward in time, and so is\npresent as an advanced incident wave at the emitter at the instant of\nemission. Both the retarded field produced by the absorber and the\nadvanced field produced by the emitter exactly cancel with the\nretarded field produced by the emitter and advanced field produced by\nthe absorber for all times before the emission and after the\nabsorption of the photon; only between the emitter and the absorber is\nthere a radiative field. Thus the transaction is completed with this\n“handshake”: a cycle of offer and confirmation waves  \nrepeats until the response of the emitter and absorber is sufficient\nto satisfy all of the quantum boundary conditions…at which point the\ntransaction is completed. (Crammer 1986: 662)  \nMany confirmation waves from potential absorbers may converge on the\nemitter at the time of emission but the quantum boundary conditions\ncan usually only permit a single transaction to form. Any observer who\nwitnesses this process would perceive only the completed transaction,\nwhich would be interpreted as the passage of a particle (e.g., a\nphoton) between emitter and absorber. \nThe transactional interpretation takes the wave function to be a real\nphysical wave with spatial extent. The wave function of the quantum\nmechanical formalism is identical with the initial offer wave of the\ntransaction mechanism and the collapsed wave function is identical\nwith the completed transaction. Quantum particles are thus not to be\nthought of as represented by the wave function but rather by the\ncompleted transaction, of which the wave function is only the initial\nphase. As Cramer explains: \nThe transaction may involve a single emitter and absorber or multiple\nemitters and absorbers, but it is only complete when appropriate\nboundary conditions are satisfied at all loci of emission and\nabsorption. Particles transferred have no separate identity\nindependent from the satisfaction of these boundary conditions. (1986:\n666) \nThe amplitude of the confirmation wave which is produced by the\nabsorber is proportional to the local amplitude of the incident wave\nthat stimulated it and this, in turn, is dependent on the attenuation\nit received as it propagated from the source. Thus, the total\namplitude of the confirmation wave is just the absolute square of the\ninitial offer wave (evaluated at the absorber), which yields the Born\nrule. Since the Born rule arises as a product of the transaction\nmechanism, there is no special significance attached to the role of\nthe observer in the act of measurement. The “collapse of the\nwave function” is interpreted as the completion of the\ntransaction. \nThe transactional interpretation explicitly interprets the quantum\nstate \\(\\psi\\) as real, and so does not constitute an attempt to\nexploit the retrocausality loopholes to the theorems that rule out\n\\(\\psi\\)-epistemic accounts. Additionally, the transactional\ninterpretation subverts the dilemma at the core of the EPR argument\n(Einstein, et al. 1935) by permitting incompatible observables to take\non definite values simultaneously: the wavefunction, according to the\ntransactional interpretation, \nbrings to each potential absorber the full range of possible outcomes,\nand all have “simultaneous reality” in the EPR sense. The\nabsorber interacts so as to cause one of these outcomes to emerge in\nthe transaction, so that the collapsed [wavefunction] manifests only\none of these outcomes. (Crammer 1986: 668). \nMost importantly, however, the transactional interpretation employs\nboth retarded and advanced waves, and in doing so admits the\npossibility of providing a “zigzag” explanation of the\nnonlocality associated with entangled quantum systems. \nBefore turning to one of the more significant objections to the\ntransactional interpretation, and to retrocausality in general, it is\ninstructive to tease apart here two complementary descriptions of this\ntransaction process. On the one hand there is a description of the\nreal physical process, consisting of the passage of a particle between\nemitter and absorber, that a temporally bound experimenter would\nobserve; and on the other hand there is a description of a dynamical\nprocess of offer and confirmation waves that is instrumental in\nestablishing the transaction. This latter process simply cannot occur\nin an ordinary time sequence, not least because any temporally bound\nobserver by construction cannot detect any offer or confirmation\nwaves. Cramer suggests that the “dynamical process” be\nunderstood as occurring in a “pseudotime” sequence: \nThe account of an emitter-absorber transaction presented here employs\nthe semantic device of describing a process extending across a\nlightlike or a timelike interval of space-time as if it occurred in a\ntime sequence external to the process. The reader is reminded that\nthis is only a pedagogical convention for the purposes of description.\nThe process is atemporal and the only observables come from the\nsuperposition of all “steps” to form the final\ntransaction. (Crammer 1986: 661, fn.14) \nThese steps are of course the cyclically repeated exchange of offer\nand confirmation waves which continue “until the net exchange of\nenergy and other conserved quantities satisfies the quantum boundary\nconditions of the system” (1986: 662). There is a strong sense\nhere that any process described as occurring in pseudotime is not\na process at all but, as Cramer reminds, merely a\n“pedagogical convention for the purposes of description”.\nWhether it is best to understand causality according to the\ntransactional interpretation in terms of processes underscored by\nconserved quantities is closely tied to how one should best understand\nthis pseudotemporal process. \nMaudlin (2011) outlines a selection of problems that arise in\nCramer’s theory as a result of the pseudotemporal account of the\ntransaction mechanism: processes important to the completion of a\ntransaction take place in pseudotime only (rather than in real time)\nand thus cannot be said to have taken place at all. Since a temporally\nbound observer can only ever perceive a completed transaction, i.e., a\ncollapsed wavefunction, the uncollapsed wavefunction never actually\nexists. Since the initial offer wave is identical to the wavefunction\nof the quantum formalism, any ensuing exchange of advanced and\nretarded waves required to provide the quantum mechanical\nprobabilities, according to Maudlin, also do not exist. Moreover,\nCramer’s exposition of the transaction mechanism seems to\nsuggest that the stimulation of sequential offer and confirmation\nwaves occurs deterministically, leaving a gaping hole in any\nexplanation the transactional interpretation might provide of the\nstochastic nature of quantum mechanics. Although these problems are\nsignificant, Maudlin admits that they may indeed be peculiar to\nCramer’s theory. Maudlin also sets out a more general objection\nto retrocausal models of quantum mechanics which he claims to pose a\nproblem for “any theory in which both backwards and forwards\ninfluences conspire to shape events” (2011: 184). \nMaudlin’s main objection to the transactional interpretation\nhinges upon the fact that the transaction process depends crucially on\nthe fixity of the absorbers “just sitting out there in the\nfuture, waiting to absorb” (2011: 182); one cannot presume that\npresent events are unable to influence the future disposition of the\nabsorbers. Maudlin offers a thought experiment to illustrate this\nobjection. A radioactive source is constrained to emit a\n\\(\\beta\\)-particle either to the left or to the right. To the right\nsits absorber A at a distance of 1 unit. Absorber B is\nalso located to the right but at a distance of 2 units and is built on\npivots so that it can be swung around to the left on command. A\n\\(\\beta\\)-particle emitted at time \\(t_{0}\\) to the right will be\nabsorbed by absorber A at time \\(t_{1}\\). If after time\n\\(t_{1}\\) the \\(\\beta\\)-particle is not detected at absorber A,\nabsorber B is quickly swung around to the left to detect the\n\\(\\beta\\)-particle after time \\(2t_{1}\\). \nAccording to the transactional interpretation, since there are two\npossible outcomes (detection at absorber A or detection at\nabsorber B), there will be two confirmation waves sent back\nfrom the future, one for each absorber. Furthermore, since it is\nequally probable that the \\(\\beta\\)-particle be detected at either\nabsorber, the amplitudes of these confirmation waves should be equal.\nHowever, a confirmation wave from absorber B can only be sent\nback to the emitter if absorber B is located on the left. For\nthis to be the case, absorber A must not have detected the\n\\(\\beta\\)-particle and thus the outcome of the experiment must already\nhave been decided. The incidence of a confirmation wave from absorber\nB at the emitter ensures that the \\(\\beta\\)-particle\nis to be sent to the left, even though the amplitude of this wave\nimplies a probability of a half of this being the case. As Maudlin\n(2011: 184) states so succinctly, “Cramer’s theory\ncollapses”. \nThe key challenge from Maudlin is that any retrocausal mechanism must\nensure that the future behavior of the system transpires consistently\nwith the spatiotemporal structure dictated by any potential future\ncauses: “stochastic outcomes at a particular point in time may\ninfluence the future, but that future itself is supposed to play a\nrole in producing the outcomes” (2011: 181). In the\ntransactional interpretation the existence of the confirmation wave\nitself presupposes some determined future state of the system with\nretrocausal influence. However, with standard (i.e., forward-in-time)\nstochastic causal influences affecting the future from the present, a\ndetermined future may not necessarily be guaranteed in every\nsuch case, as shown by Maudlin’s experiment. \nMaudlin’s challenge to the transactional interpretation has been\nmet with a range of responses (see P. Lewis 2013 and the entry on\n action at a distance in quantum mechanics\n for more discussion of possible responses). The responses generally\nfall into two types (P. Lewis 2013). The first type of response\nattempts to accommodate Maudlin’s example within the\ntransactional interpretation. Berkovitz (2002) defends the\ntransactional interpretation by showing that causal loops of the type\nfound in Maudlin’s experiment need not obey the assumptions\nabout probabilities that are common in linear causal situations.\nMarchildon (2006) proposes considering the absorption properties of\nthe long distance boundary conditions: if the universe is a perfect\nabsorber of all radiation then a confirmation wave from the left will\nalways be received by the radioactive source at the time of emission\nand it will encode the correct probabilistic information. Kastner\n(2006) proposes differentiating between competing initial states of\nthe radioactive source, corresponding to the two emission\npossibilities, that together characterize an unstable bifurcation\npoint between distinct worlds, where the seemingly problematic\nprobabilities reflect a probabilistic structure across both possible\nworlds. \nThe second type of response is to modify the transactional\ninterpretation. For instance, Cramer (2016) introduces the idea of a\nhierarchy of advanced-wave echoes dependent upon the magnitude of the\nspatiotemporal interval from which they originate. Kastner (2013)\nsurmises that the source of the problem that Maudlin has exposed,\nhowever, is the idea that quantum processes take place in the\n“block world”, and rejects this conception of processes in\nher own development of the transactional interpretation. According to\nher “possibilist” transactional interpretation, all the\npotential transactions exist in a real space of possibilities, which\namounts at once to a kind of modal realism and an indeterminacy\nregarding future states of the system (hence Kastner’s rejection\nof the block universe view). The possibilist transactional\ninterpretation arguably handles multi-particle scenarios more\nnaturally, and presents the most modern sustained development of the\ntransactional interpretation (although see P. Lewis 2013 for\ncriticisms of the possibilist transactional interpretation specific to\nMaudlin’s challenge). \nThe transactional interpretation might be seen as the most\nprominent—and historically significant—retrocausal model\non the market, but it is not the only one. While retrocausality in\nquantum mechanics has been the subject of considerable analysis and\ncritique over the years (Rietdijk 1978; Sutherland 1983, 1985, 1989;\nPrice 1984, 2001; Hokkyo 1988, 2008; Miller 1996, 1997; Atkinson 2000;\nArgaman 2010; Evans 2015), the focus of this section is a review of\nsome of the more concrete proposals for retrocausal models. \nWhat is now known as the two-state vector formalism was first proposed\nby Watanabe (1955), and then rediscovered by Aharonov, Bergmann, and\nLebowitz (1964). The proposal is that the usual forward evolving\nquantum state contains insufficient information to completely specify\na quantum system; rather the forward evolving state must be\nsupplemented by a backward evolving state to provide a complete\nspecification. Thus, according to the two-state vector formalism, the\ncomplete quantum description contains a state vector that evolves\nforward in time from the initial boundary condition towards the\nfuture, and a state vector that evolves backward in time from the\nfuture boundary condition towards the past. It is only a combination\nof complete measurements at both initial and final boundaries that can\nprovide a complete specification of a quantum system. The two-state\nvector formalism is empirically equivalent to standard quantum\nmechanics (Aharonov & Vaidman 1991, 2008). \nThe emphasis of the two-state vector formalism is on the operational\nelements of the theory, and there are very few ontological\nprescriptions, including how best to understand causality. It is in\nprinciple compatible with a variety of supplemented retrocausal\nontologies, e.g., the causally symmetric Bohm model\n (§6.3). \nToy models have been employed to illustrate how retrocausality could\nbe possibly realized. Price (2008) suggests a simple toy model\nfeaturing linked nodes that can assume different values, which he dubs\nthe “Helsinki model”. If one interprets the nodes as\npartially ordered in time and the values of the exogenous boundary\nnodes as controllable, the dynamics specified by Price entails causal\nbi-directionality, i.e., both ordinary forward causality and\nretrocausality, understood in an interventionist sense. Although the\nmodel does indeed display particular behavior that can be naturally\ninterpreted as retrocausal, a full-blown analogy with standard quantum\nmechanics is limited. \nThe Helsinki model consists of three primitive endogenous nodes, each\nnode comprising a meeting point of three edges, with two\n“internal” edges linking the three nodes and five\n“external” edges. Each edge has one of three\n“flavors”, A, B, or C. The system is\ntemporally oriented with three exogenous nodes, each joined to a\nsingle edge, representing system “inputs”\n(preparations/interventions), and two system “outputs”\n(measurement outcomes/observations) joined to the remaining two edges.\nThe two internal edges represent hidden flavors that cannot be\ndirectly controlled or observed. There are two basic rules that govern\nthe dynamics of this toy system: (i) each node must be strictly\ninhomogeneous—i.e., comprising three edges of different\nflavors—or strictly homogeneous—i.e., three edges of the\nsame flavor, and (ii) successive homogeneous nodes are prohibited. \nThe retrocausal behavior of the model arises as a result of the heavy\nconstraints that the two basic rules place on the possible flavors of\nthe hidden edges, which establishes correlations between the input\nstates and the hidden states. Thus interventions on the left or right\nexogenous variables influence the complete set of possible hidden\nstates of the system. Assuming that the hidden states are in the past\nof these variable choices, the hidden state depends\n“retrocausally” on the left and right input settings.\nMoreover, under specific node variables, some interventions on the\nleft or right exogenous variables amount to interventions on the\ndistant output variable, displaying a kind of nonlocality (but\nviolating no-signaling). \nMore ambitiously, Corry (2015) proposes three toy models, also using\nnodes and links, which exhibit Bell-type correlations and account for\nthem in a purely local manner in that the constraints governing nodes\nmake reference only to information available at the respective nodes.\nThe models do not in general specify values for unobserved nodes,\nhowever. As a way out, Corry suggests, one may either postulate such\nvalues at the price of accepting “retrofinkish\ndispositions” (dispositions which, if they were triggered, would\nnot have been there in the first place) or simply deny the existence\nof such values outright. \nA model that hypothesizes retrocausality and reproduces the\nconsequences of standard quantum theory without violating Lorentz\ninvariance is Sutherland’s (2008, 2015, 2017) causally symmetric\nversion of Bohmian mechanics. This model adds to the standard quantum\nstate \\(\\psi_i\\) of ordinary Bohmian mechanics, which is fixed by an\ninitial boundary condition, an additional quantum state \\(\\psi_f\\),\nwhich is fixed by some final boundary condition. An analogue to the\n“guiding equation” for particle motion in ordinary Bohmian\nmechanics is derived by Sutherland that is symmetric with respect to\nthese states. The zero-component of the probability current, which is\ndirectly related to the probability density, is computed as  \nwhere  \nFinally, for consistency, the model also requires that the conditional\nprobability of the final state being \\(\\psi_f\\), given the initial\nstate \\(\\psi_i\\), is  \nBerkovitz (2008) criticizes the use of the additional assumption Eq.\n(\\ref{eq:addass}) and the need for it in Sutherland’s model,\narguing that it leads to an undesirable form of causal loops and has\nan ad hoc character because there is no independent reason to\nthink that \\(\\psi_i\\) and \\(\\psi_f\\) should be correlated in this\nway. \nSutherland’s model is explicitly retrocausal in that particle\ndynamics are influenced by \\(\\psi_f\\), which in turn depends on the\nfuture measurement setting. It also contains action-by-contact causal\ninfluences in that Bell-type correlations are accounted for in the\n“zigzag” manner envisaged by Costa de Beauregard, as\noutlined in\n §1. \nSilberstein, Stuckey, and McDevitt (2018), based on earlier work together with Cifone\n(Stuckey, Silberstein, & Cifone 2008; Silberstein, Cifone, &\nStuckey. 2008), suggest a realist interpretation that they call the\n“relational blockworld view”. The ontology of this\ninterpretation consists of so-called “spacetimesource\nelements”, which are characterized as “amalgams of space,\ntime, and sources” (Silberstein, Stuckey, & McDevitt 2018:\n153). \nTechnically, the relational blockworld approach is set up as a\nmodification of lattice gauge theory, with the Feynman path integral\nfunctioning as an “adynamical global constraint”. While\nadynamical and acausal rather than retrocausal in spirit, on the\nauthors’ view this interpretation exploits the retrocausality\nloopholes of the no-go theorems. They conceive of the relational\nblockworld view as \\(\\psi\\)-epistemic. In more recent work (2018: Ch.\n6), Silberstein, Stuckey and McDevitt have developed this approach\ninto an ambitious overarching programme in fundamental physics, aiming\nat field-theoretic unification as well as a novel account of quantum\ngravity. \nSchulman (1997, 2012) proposes a solution to the quantum measurement\nproblem based on the possibility of future conditioning, which\noriginates from his analysis of the thermodynamic arrow of time.\nBeginning with Loschmidt’s challenge to Boltzmann—that it\nshould not be possible to derive irreversible dynamical behavior from\ntime-symmetric dynamics (see the entry on\n thermodynamic asymmetry in time)—Schulman\n notes that successful “retrodiction” of past macroscopic\nstates of some thermodynamic system is indeed asymmetric to successful\n“prediction” of future macroscopic states. For successful\nretrodiction, rather than evolving each of the microscopic states of\nsome macroscopic state according to the dynamical laws (which is\nidentical to the process of prediction, and is the basis of\nLoschmidt’s challenge), one instead hypothesizes a prior\nmacroscopic state to which the prediction process can be applied such\nthat the current macroscopic state of the system obtains with high\nlikelihood given the dynamical evolution of the prior microscopic\nstates. With respect to the set of possible microscopic states for the\ncurrent macroscopic state, since the evolution of the vast majority\n(on Liouville measure) of these states according to the dynamical laws\nresult in trajectories that conflict with the retrodiction hypothesis,\nthese states are effectively rejected in the process of retrodiction\nin favor of those special few microscopic states that correspond to\nthe dynamical evolution of acceptable hypothetical initial\nconditions. \nSchulman’s proposal in response to this asymmetry is that, just\nas the set of possible final microscopic states must be restricted for\nsuccessful retrodiction, so should the permissible initial microscopic\nstates be restricted for the purposes of prediction. Thus, since final\nmicroscopic states are subject to conditioning from the past state of\nthe system, the initial microscopic states will be subject to\nconditioning from the future state of the system—a conditioning\nthat is hidden in thermodynamic processes where the microscopic states\nof the system are indistinguishable macroscopically. This future\nconditioning is a central feature of Schulman’s two-time\nboundary condition proposal and the hidden nature of this conditioning\nSchulman calls “cryptic constraints”. \nRegarding quantum theory, Schulman claims that quantum superpositions\nof macroscopically distinct states generated by quantum evolution,\nwhich are at the heart of many of the nonclassical elements of quantum\ntheory, are what he calls “grotesque” states, and proposes\nthat these states are avoided in quantum systems due to future\nconditioning and cryptic constraints. Just like there are special\nmicroscopic states of thermodynamical systems that evolve\n“against” the second law of thermodynamics (which are\nidentified in the process of retrodiction), so too must there be\n“special” microscopic states of a quantum system which do\nnot lead to grotesque states when evolved according to the quantum\ndynamical laws, rather these states will lead to one particular\ndefinite state of the superposition. Schulman’s solution to the\nquantum measurement problem is that, in every performed experiment,\nthe initial conditions of the quantum system are among these special\nstates which, through pure unitary quantum evolution, yield a single\noutcome to the experiment (Schulman 1997: 214). Grotesque states, the\nproblematic states of the quantum measurement problem, are thus\navoided. \nSchulman’s proposal implicitly assumes that, in the preparation\nof an experiment, it is only the macroscopic state of the system which\nis under the control of the experimenter; it is impossible to control\nthe precise microscopic state. It is this initial microscopic state\nthat Schulman suggests is always a special state. Schulman envisages\nthe special-state constraint as correlated with future conditions and,\nsince these constraints are not apparent to the experimenter until the\nfuture conditions are “measured” at the end of the\nexperiment, these constraints are cryptic. As a result of the future\nconditioning of initial states, Schulman’s proposal is a kind of\nretrocausal mechanism, understood in an interventionist sense. (For an\nextension of Schulman’s model into the Lagrangian schema\n (§6.6)\n see Almada et al. 2016 and Wharton 2014, 2018.) \nWharton (2010a; see also Wharton 2007, 2010b, 2013, 2016, 2018;\nWharton, Miller, & Price 2011; Evans, Price, & Wharton 2013)\nproposes a “novel interpretation of the Klein-Gordon\nequation” for a neutral, spinless relativistic particle. The\naccount is a retrocausal picture based on Hamilton’s principle\nand the symmetric constraint of both initial and final boundary\nconditions to construct equations of motion from a Lagrangian, and is\na natural setting for a perspectival interventionist account of\ncausality. Wharton treats external measurements as physical\nconstraints imposed on a system in the same way that boundary\nconstraints are imposed on the action integral of Hamilton’s\nprinciple; the final measurement does not simply reveal preexisting\nvalues of the parameters, but constrains those values (just as the\ninitial boundary condition would). Wharton’s model has been\ndescribed as an “all-at-once” approach, since the dynamics\nof physical systems between an initial and final boundary emerges\nen bloc as the solution to a two-time boundary value\nproblem. \nOn this interpretation, one considers reality exclusively between two\ntemporal boundaries as being described by a classical field \\(\\phi\\)\nthat is a solution to the Klein-Gordon equation: specification of\nfield values at both an initial and final boundary (as opposed to\nfield values and their rate of change at only the initial boundary)\nconstrains the field solutions between the boundaries. Wharton argues\nthat constraining such fields at both an initial and a final boundary\n(or a closed hypersurface in spacetime) generates two strikingly\nquantum features: quantization of certain field properties and\ncontextuality of the unknown parameters characterizing the field\nbetween the boundaries. (That there are unknown parameters before the\nimposition of the final condition is ensured due to the\nunderdetermination of the classical field by specifying only the field\nvalues, and not their rate of change, in the initial data.) \nFrom within Wharton’s picture, an invariant joint probability\ndistribution associated with each possible pair of initial and final\nconditions can be constructed, and the usual conditional probabilities\ncan be formed by conditioning on any chosen portion of the boundary\n(Wharton 2010a,b). Probability is then a manifestation of our\nignorance: if one knew only the initial boundary, one would only be\nable to describe the subsequent field probabilistically (due to the\nfuture constraint); given the final boundary as well, one would then\nbe able to retrodict the field values between the two boundaries. \nIn more recent work (Wharton 2016), Wharton explores the prospects for\na realist retrocausal interpretation of quantum theory based on the\nFeynman path integral formalism (Feynman 1942). In that formalism as\napplied to a single particle undergoing position measurements at\nspacetime points \\((\\mathbf{x_0},t_0)\\) and \\((\\mathbf{x_1},t_1)\\),\nthe joint probability distribution for all pairs of points and given\ntimes \\(t_0\\) and \\(t_1\\) is given by  \nThe sum in this formula is to be understood as the infinitesimal limit\nof a discretized set of spacetime trajectories connecting\n\\((\\mathbf{x_0},t_0)\\) and \\((\\mathbf{x_1},t_1)\\). S is the\nclassical action of the particle along the respective trajectory. \nA straightforward but naive interpretation of this equation according\nto which the probability reflects ignorance concerning the path taken\ndoes not work because the right hand side of Eq. (\\ref{eq:feynman}) is\nnot a sum of positive numbers (interpretable as probabilities of\ntrajectories) but rather a positive number obtained by taking the\nmodulus squared of a sum of complex numbers. Wharton explores the\nprospects for giving an ignorance interpretation of the path integral\nalong less straightforward lines, noting that Eq. (\\ref{eq:feynman})\ncan be brought into the form  \nwhere the \\(\\left|c_i\\right|\\) are distinct groups of\ntrajectories A. The form Eq. (\\ref{eq:wharton}), according\nto Wharton, invites an interpretation of the probability in terms of\nignorance concerning the actual group of trajectories \\(c_i\\), and he\ntentatively proposes a field interpretation of the \\(c_i\\). The\ninterpretation is retrocausal because the probabilities over groups of\ntrajectories are influenced by the future measurement time and\nsetting. Open questions for this approach concern the grouping of\ntrajectories, which is so far inherently ambiguous, the details of the\nsuggested field interpretation, as well as generalizations to\nmany-particle and other more general settings. \nThis final section reviews some of the most common, and then two of\nthe most significant, objections against the proposal of\nretrocausality in quantum mechanics. \nThere is a tradition in philosophy for regarding the very idea of\nretrocausality as incoherent. The most prominent worry, forcefully\nmade by Black (1956), is the so-called “bilking argument”\n(see the entry on\n time travel).\n Imagine a pair of events, a cause, C, and an effect, E,\nwhich we believe to be retrocausally connected (E occurs\nearlier in time than C). It seems possible to devise an\nexperiment which could confirm whether our belief in the retrocausal\nconnection is correct or not. Namely, once we had observed that\nE had occurred, we could then set about ensuring that C\ndoes not occur, thereby breaking any retrocausal connection that could\nhave existed between them. If we were successful in doing this, then\nthe effect would have been “bilked” of its cause. \nThe bilking argument drives one towards the claim that any belief an\nagent might hold in the positive retrocausal correlation between event\nC and event E is simply false. However, Dummett (1964)\ndisputes that giving up this belief is the only solution to the\nbilking argument. Rather, according to Dummett, what the bilking\nargument actually shows is that a set of three conditions concerning\nthe two events, and the agent’s relationship to them, is\nincoherent: \nIt is interesting to note that these conditions do not specify in\nwhich order events C and E occur. On simple reflection,\nthere is a perfectly natural reason why it is not possible to bilk\nfuture effects of their causes, since condition (iii) fails\nto hold for future events: we simply have no access to which future\nevents occur independently of the role we play as causal agents to\nbring the events about. When we lack that epistemic access to past\nevents, the same route out of the bilking argument becomes\navailable. \nDummett’s defense against the bilking argument is especially\nrelevant to quantum mechanics. In fact, once a suitable specification\nis made of how condition (iii) can be violated, we find that there\nexists a strong parallel between the conditions which need to hold to\njustify a belief in bringing about the past and the structure of\nquantum mechanics. Price (1996: 174) points out that bilking is\nimpossible in the following circumstances: rather than suppose that a\nviolation of condition (iii) entails that the relevant agent has\nno epistemic access to the relevant past events independently\nof any intention to bring them about, suppose that the means by which\nknowledge of these past events is gathered breaks the claimed\ncorrelation between the agent’s action and those past events.\nSuch a condition can be stated as follows: \nThe significance of this weakened violation of condition (iii) is that\nit is just the sort of condition one would expect to hold if the\nsystem in question were a quantum system. The very nature of quantum\nmechanics ensures that any claimed positive correlation between the\nfuture measurement settings and the hidden variables characterizing a\nquantum system cannot possibly be bilked of their causes because\ncondition (iv) is perennially violated. Moreover, so long as we\nsubscribe to the epistemic interpretation of the wavefunction, we lack\nepistemic access to the “hidden” variables of the system\nand we lack this access in principle as a result of the\nstructure of quantum theory. \nAnother prominent challenge against the very idea of retrocausality is\nthat it inevitably would give rise to vicious causal loops (Mellor\n1998). (See Faye 1994 for a response and the entry on\n backward causation\n for a more detailed review of the objections raised against the idea\nof retrocausality.) \nCausal modeling (Spirtes, Glymour, & Scheines 2000; Pearl 2009) is\na practice that has arisen from the field of machine learning that\nconsists in the development of algorithms that can automate the\ndiscovery of causes from correlations in large data sets. The causal\ndiscovery algorithms permit an inference from given statistical\ndependences and independences between distinct measurable elements of\nsome system to a causal model for that system. As part of the\nalgorithms, a series of constraints must be placed on the resulting\nmodels that capture general features that we take to be characteristic\nof causality. Two of the more significant assumptions are (i) the\ncausal Markov condition, which ensures that every statistical\ndependence in the data results in a causal dependence in the\nmodel—essentially a formalization of Reichenbach’s common\ncause principle—and (ii) faithfulness, which ensures that every\nstatistical independence implies a causal independence, or no causal\nindependence is the result of a fine-tuning of the model. \nIt has long been recognized (Butterfield 1992; Hausman 1999; Hausman\n& Woodward 1999) that quantum correlations force one to give up at\nleast one of the assumptions usually made in the causal modeling\nframework. Wood and Spekkens (2015) argue that any causal model\npurporting to causally explain the observed quantum\ncorrelations must be fine-tuned (i.e., must violate the faithfulness\nassumption). More precisely, according to them, since the observed\nstatistical independences in an entangled bipartite quantum system\nimply no signaling between the parties, when it is then assumed that\nevery statistical independence implies a causal independence (which is\nwhat faithfulness dictates), it must be inferred that there can be no\n(direct or mediated) causal link between the parties. Since there is\nan observed statistical dependence between the outcomes of\nmeasurements on the bipartite system, we can no longer account for\nthis dependence with a causal link unless this link is fine tuned to\nensure that the no-signaling independences still hold. There is thus a\nfundamental tension between the observed quantum correlations and the\nno-signaling requirement, the faithfulness assumption and the\npossibility of a causal explanation. \nFormally, Wood and Spekkens argue that the following three assumptions\nform an inconsistent set: (i) the predictions of quantum theory\nconcerning the observed statistical dependences and independences are\ncorrect; (ii) the observed statistical dependences and independences\ncan be given a causal explanation; (iii) the faithfulness assumption\nholds. Wood and Spekkens conclude that, since the faithfulness\nassumption is an indispensable element of causal discovery, the second\nassumption must yield. The contrapositive of this is that any\npurported causal explanation of the observed correlations in an\nentangled bipartite quantum system falls afoul of the tension between\nthe no-signaling constraint and no fine tuning and, thus, must violate\nthe assumption of faithfulness. Such causal explanations, so the\nargument goes, including retrocausal explanations, should therefore be\nruled out as viable explanations. \nAs a brief aside, this fine-tuning worry for retrocausality in the\nquantum context arises in a more straightforward way. There is no good\nevidence to suggest that signaling towards the past is possible; that\nis, there is no retrocausality at the operational level. (Pegg 2006,\n2008 argues that this can be explained formally as a result of the\ncompleteness condition on the measurement operators, introducing an\nasymmetry in normalization conditions for preparation and\nmeasurement.) Yet, despite there being no signaling towards the past,\nretrocausal accounts assume causal influences towards past. That these\ncausal influences do not show up as statistical dependences\nexploitable for signaling purposes raises exactly the same fine-tuning\nworry as Wood and Spekkens raise. \nAn obvious response to the challenge set by Wood and Spekkens is to\nsimply reject the assumption of faithfulness. But this should not be\ntaken lightly; the intuition behind the faithfulness assumption is\nbasic and compelling. When no statistical correlation exists between\nthe occurrences of a pair of events, there is no reason for supposing\nthere to be a causal connection between them. Conversely, if we were\nto allow the possibility of a causal connection between statistically\nuncorrelated events, we would have a particularly hard task\ndetermining which of these uncorrelated sets could be harboring a\nconspiratorial causal connection that hides the correlation. The\nfaithfulness assumption is thus a principle of parsimony—the\nsimplest explanation for a pair of statistically uncorrelated events\nis that they are causally independent—much the same way that\nSpekkens’ (2005) definition of contextuality is, too (see\n §3.2);\n indeed, Cavalcanti (2018) argues that contextuality can be construed\nas a form of fine-tuning. \nThere are, however, well-known examples of systems that potentially\nshow a misapplication of the faithfulness assumption. One such\nexample, originating in Hesslow (1976), involves a contraceptive pill\nthat can cause thrombosis while simultaneously lowering the chance of\npregnancy, which can also cause thrombosis. As Cartwright (2001: 246)\npoints out, given the right weight for these process, it is\nconceivable that the net effect of the pills on the frequency of\nthrombosis be zero. This is a case of “cancel ling paths”,\nwhere the effect of two or more causal routes between a pair of\nvariables cancels to achieve statistical independence. In a case such\nas this, since we can have independent knowledge of the separate\ncausal mechanisms involved here, there are grounds for arguing that\nthere really is a causal connection between the variables despite\ntheir statistical independence. Thus, it is certainly possible to\nimagine a scenario in which the faithfulness assumption could lead us\nastray. However, in defense of the general principle, an example such\nas this clearly contains what Wood and Spekkens refer to as fine\ntuning; the specific weights for these processes would need to match\nprecisely to erase the statistical dependence, and such a balance\nwould generally be thought as unstable (any change in background\nconditions, etc. would reveal the causal connection in the form of a\nstatistical dependence). \nNäger (2016) raises the possibility that unfaithfulness can occur\nwithout conspiratorial fine tuning if the unfaithfulness arises in a\nstable way. In the quantum context, Näger suggests that the\nfine-tuning mechanism is what he calls “internal cancel ling\npaths”. This mechanism is analogous to the usual cancel ling\npaths scenario, but the path-cancel ling mechanism does not manifest\nat the level of variables, but at the level of values. On this view,\nsuch fine tuning would occur as a result of the particular causal\nand/or nomological process that governs the system, and it is in this\nsense that the cancel ling paths mechanism is internal, and it is the\nfact that the mechanism is internal that renders the associated fine\ntuning stable to external disturbances. Thus  \nif the laws of nature are such that disturbances always alter the\ndifferent paths in a balanced way, then it is physically impossible to\nunbalance the paths. (Näger 2016: 26)  \nThe possibility raised by Näger would circumvent the problem that\nviolations of faithfulness ultimately undermine our ability to make\nsuitable inferences of causal independence based on statistical\nindependence by allowing only a specific kind of\nunfaithfulness—a principled or law-based unfaithfulness that is\n“internal” and is thus stable to background\nconditions—which is much less conspiratorial, as the fine-tuning\nis a function of the specific process involved. Evans (2018) argues\nthat a basic retrocausal model of the sort envisaged by Costa de\nBeauregard (see\n §1)\n employs just such an internal cancel ling paths explanation to\naccount for the unfaithful (no signaling) causal channels. See also\nAlmada et al. (2016) for an argument that fine tuning in the quantum\ncontext is robust and arises as a result of symmetry\nconsiderations. \nRecall\n (§3.2)\n that Spekkens’ (2005) claim that no noncontextual ontological\nmodel can reproduce the observed statistics of quantum theory based on\nhis principle of parsimony (that there can be no ontological\ndifference without operational difference) was sidestepped by\nretrocausal approaches due to the explicit assumption of the\nontological models framework that the ontic state is independent of\nthe measurement procedure (i.e., that there is no retrocausality). It\nwas noted there the possibility that Spekkens’ principle of\nparsimony might be recast to apply more generally to retrocausal\nmodels. Shrapnel and Costa (2018) achieve just this in a no-go theorem\nthat applies to any exotic causal structure used to sidestep the\nontological models framework, including retrocausal accounts,\nrendering such models contextual after all. \nShrapnel and Costa’s result is based on a generalization of the\nontological models framework which replaces the operational\npreparation, transformation, and measurement procedures with the\ntemporally and causally neutral notions of local controllables and\nenvironmental processes that mediate correlations between different\nlocal systems, and generate the joint statistics for a set of events.\n“These include any global properties, initial states, connecting\nmechanisms, causal influence, or global dynamics” (2018: 5).\nFurthermore, they replace the ontic state \\(\\lambda\\) with the ontic\n“process” \\(\\omega\\): \nour ontic process captures the physical properties of the world that\nremain invariant under our local operations. That is, although we\nallow local properties to change under specific operations,\nwe wish our ontic process to capture those aspects of reality\nthat are independent of this probing. (2018: 8) \nAs a result, the notion of \\(\\lambda\\)-mediation (encountered in\n §4.1)\n is replaced by the notion of \\(\\omega\\)-mediation, in which the ontic\nprocess \\(\\omega\\) completely specifies the properties of the\nenvironment that mediate correlations between regions, and screens off\noutcomes produced by local controllables from the rest of the\nenvironment. Shrapnel and Costa (2018: 9) define the notion of\n“instrument noncontextuality” as a law of parsimony (along\nthe lines of Spekkens’ own definition of noncontextuality):\n“Operationally indistinguishable pairs of outcomes and local\ncontrollables should remain indistinguishable at the ontological\nlevel”. They then show that no instrument noncontextual model\ncan reproduce the quantum statistical predictions. \nCrucially, what is contextual is not just the traditional notion of\n“state”, but any supposedly objective feature of the\ntheory, such as a dynamical law or boundary condition. (2018: 2) \nSince preparations, transformations, and measurements have been\nreplaced by local controllables, there is no extra assumption in\nShrapnel and Costa’s framework that \\(\\omega\\) is correlated\nwith some controllables but independent of others. Thus the usual\nroute out of the ontological models framework, and so the no-go\ntheorems of\n §3,\n open to retrocausal approaches—that the framework assumes no\nretrocausality—is closed off in the Shrapnel-Costa theorem,\nrendering retrocausal approaches contextual along with the rest of the\nmodels captured by the ontological models framework. \nThis presents a significant worry for retrocausal approaches to\nquantum theory. If the main motivation for pursing the hypothesis of\nretrocausality is to recapture in some sense a classical ontology for\nquantum theory (see\n §3.4),\n then the Shrapnel-Costa theorem has made this task either impossible,\nor beholden to the possibility of some further story explaining how\nthe contextual features of the model arise from some noncontextual\nfooting. On this latter point, it is difficult to see how this story\nmight be told without significantly reducing the ideological economy\nof the conceptual framework of retrocausality, again jeopardizing a\npotential virtue of retrocausality. \nAs mentioned above\n (§7.2),\n contextuality can be construed as a form of fine tuning (Cavalcanti\n2018), especially when the demand for noncontextuality is understood\nas a requirement of parsimony, as above. The worries raised in this\nsection and the last underline the fact that the challenge to account\nfor various types of fine tuning is the most serious principled\nobstacle that retrocausal accounts continue to face.","contact.mail":"email@simonfriederich.eu","contact.domain":"simonfriederich.eu"},{"date.published":"2019-06-03","url":"https://plato.stanford.edu/entries/qm-retrocausality/","author1":"Simon Friederich","author1.info":"http://simonfriederich.eu","author2.info":"https://hapi.uq.edu.au/profile/450/peter-evans","entry":"qm-retrocausality","body.text":"\n\n\nQuantum theory provides a framework for modern theoretical physics\nthat enjoys enormous predictive and explanatory success. Yet, in view\nof the so-called “measurement problem”, there is no\nconsensus on how physical reality can possibly be such that this\nframework has this success. The theory is thus an extremely\nwell-functioning algorithm to predict and explain the results of\nobservations, but no consensus on which kind of objective reality\nmight plausibly underlie these observations.\n\n\nAmongst the many attempts to provide an “interpretation”\nof quantum theory to account for this predictive and explanatory\nsuccess, one class of interpretations hypothesizes backward-in-time\ncausal influences—retrocausality—as the basis for\nconstructing a convincing foundational account of quantum theory. This\nentry presents an overview of retrocausal approaches to the\ninterpretation of quantum theory, the main motivations for adopting\nthis approach, a selection of concrete suggested retrocausal models,\nand a review of the objections brought forward against such\napproaches.\n\nFrom the birth of the theory of quantum mechanics in 1925/6 to the\noutbreak of war in Europe, a clear orthodoxy emerged in the conceptual\nand ontological framework for understanding quantum theory. Now known\nas the Copenhagen interpretation, this framework embodied the\npositivistic tendencies of Heisenberg and Bohr, and was set opposed to\nthe more realist tendencies of de Broglie, Einstein, and\nSchrödinger. It was not until Bell’s theorem in the 1960s,\nand its experimental tests in the 1970s and 1980s, that new energy was\nbreathed into this interpretational debate. However, beginning in the\nmid-1940s, the first suggestions of retrocausality as part of the\nconceptual and ontological framework in quantum theory had already\nmaterialized. \nThere are two key ideas that punctuate the historical development of\nthe notion of retrocausality in quantum mechanics. The first proposal\nof retroactive influence in quantum mechanics comes from a suggestion\nmade by Wheeler and Feynman (1945, 1949). They were led to this idea\nwhile considering the potentially classical origins of some of the\ndifficulties of quantum theory. Consider the following problem from\nclassical electrodynamics: an accelerating electron emits\nelectromagnetic radiation and, through this process, the acceleration\nof the electron is damped. Various attempts to account for this\nphenomenon in terms of the classical theory of electrodynamics lacked\neither empirical adequacy or a coherent physical interpretation.\nWheeler and Feynman attempted to remedy this situation by\nreinterpreting Dirac’s (1938) theory of radiating electrons. \nThe core of Wheeler and Feynman’s proposed “absorber\ntheory of radiation” is a suggestion that the process of\nelectromagnetic radiation emission and absorption should be thought of\nas an interaction between a source and an absorber rather than as an\nindependent elementary process. (This idea has its roots as far back\nas Tetrode 1922 and G. Lewis 1926.) Wheeler and Feynman imagine an\naccelerated point charge located within an absorbing system and\nconsider the nature of the electromagnetic field associated with the\nacceleration. An electromagnetic disturbance can be imagined\n“initially” to travel outwards from the source to perturb\neach particle of the absorber. The particles of the absorber then\ngenerate together a subsequent field. According to the Wheeler-Feynman\nview, this new field is comprised of half the sum of the retarded\n(forward-in-time) and advanced (backward-in-time) solutions to\nMaxwell’s equations. The sum of the advanced effects of all the\nparticles of the absorber then yields an advanced incoming field that\nis present at the source simultaneous with the moment of emission\n(although see\n §5\n for more on how one should understand this “process”).\nThe claim is that this advanced field exerts a finite force on the\nsource which has exactly the required magnitude and direction to\naccount for the observed energy transferred from source to absorber;\nthis is Dirac’s radiative damping field. In addition, when this\nadvanced field is combined with the equivalent half-retarded,\nhalf-advanced field of the source, the total observed disturbance is\nthe full retarded field known empirically to be emitted by accelerated\npoint charges. \nThe crucial point to note about the Wheeler-Feynman schema is that due\nto the advanced field of the absorber, the radiative damping\nfield is present at the source at exactly the time of the initial\nacceleration. This schema of advanced and retarded waves now forms the\nbasis for the most fully-formed retrocausal model of quantum\nmechanics, the transactional interpretation (see\n §5). \nThe second key idea in the historical development of retrocausality in\nquantum mechanics occurs around the same time as Wheeler and\nFeynman’s absorber theory. French physicist Costa de Beauregard,\na student of de Broglie, noticed a potential objection to the\nreasoning found in Einstein, Podolsky, and Rosen’s famous paper\n(1935) on the completeness of quantum mechanics (see the entry on\n the Einstein-Podolsky-Rosen argument in quantum theory).\n Now widely known as the EPR argument, They argue that quantum\nmechanics must be incomplete on the basis of the following assumption:\nno reasonable definition of reality could be expected to permit the\nreality of some system being dependent upon the process of measurement\ncarried out on some other distant system which does not in any way\ndisturb the first system. The “reasonable definition of\nreality” to which Einstein et al. refer is implicitly an\nassumption of relativistic “locality” (made explicit in\nEinstein 1948), which combines causal asymmetry with the Lorentz\ninvariance of special relativity (more on this in a moment). Costa de\nBeauregard, however, was alert to a particular kind of unorthodox\ninterpretation of this assumption which undermined its role in the EPR\nargument. His proposal was that two distant systems could\n“remain correlated by means of a successively advanced and\nretarded wave” (Costa de Beauregard 1953: 1634); that is, one\nsystem could influence, via an advanced wave, the state of the\ncombined systems in their common past, which then, via a retarded\nwave, could influence the state of the distant system in a kind of\n“zigzag” through spacetime. This way, there could be a\ndependence between the two distant systems without any violation of\nLorentz invariance. Thus, as Costa de Beauregard (1987b: 252) puts it,\n \nEinstein of course is right in seeing an incompatibility between his\nspecial relativity theory and the distant quantal correlations, but\nonly under the assumption that advanced actions are\nexcluded. \nWhen Costa de Beauregard in 1947 suggested this response to the EPR\nargument to his then supervisor de Broglie, de Broglie was “far\nfrom willing to accept” the proposal (1987b: 252) and forbade\nCosta de Beauregard to publish his unorthodox idea (Price &\nWharton 2015). However, in 1948 Feynman had developed his eponymous\ndiagrams in which antiparticles were to be interpreted as particles\nmoving backward-in-time along the particle trajectories, and so by\n1953 de Broglie had endorsed the publication of Costa de\nBeauregard’s response. On the seeming craziness of the proposal,\nCosta de Beauregard claims, “[t]oday, as the phenomenon of the\nEPR correlations is very well validated experimentally, and is in\nitself a ‘crazy phenomenon’, any explanation of it must be\n‘crazy’” (1987b: 252; see also Costa de Beauregard\n1976, 1977b, 1987a). \nBefore addressing two of the main motivations for adopting the\nhypothesis of retrocausality in the foundations of quantum theory, it\nwill be worthwhile to provide a few comments on two key notions that\nplay a significant role in the following discussion: causality and\nlocality. This will help to pin down what exactly is meant, and not\nmeant, by retrocausality. \nThere is a tradition that stretches back at least as far as Russell\n(1913) that denies that there is any place for causal notions in the\nfundamental sciences, including physics: the notion serves no purpose,\nand simply does not appear, in the fundamental sciences. The argument\ngoes that, since at least the nineteenth century, the laws that govern\nphysical behavior in fundamental sciences such as physics are almost\nalways differential equations. Such equations are notable for\nspecifying, given some initial conditions, exact properties of systems\nfor all time. And thus if everything is specified for all time, there\nis no place left for causality. Thus Russell advocates that\n“causality” should be eliminated from the philosophers\nlexicon, because it is certainly not a part of the scientific\nlexicon. \nIn contrast to Russell’s position, Cartwright (1979: 420) claims\nthat we do have a need and use for a causal vocabulary in science:\n“causal laws cannot be done away with, for they are needed to\nground the distinction between effective strategies and ineffective\nones”. One of the main contemporary accounts of causation, the\ninterventionist account of causation (Woodward 2003; see also the\nentry on\n causation and manipulability),\n is an embodiment of Cartwright’s dictum. In a nutshell, the\ninterventionist account claims that A is a cause of B if\nand only if manipulating A is an effective means of\n(indirectly) manipulating B. Causality in the present entry,\nunless specified otherwise, should be understood along broadly\ninterventionist lines. According to accounts of quantum theory that\nhypothesize retrocausality, manipulating the setting of a measurement\napparatus can be an effective means of manipulating aspects of the\npast. A broadly interventionist view of causality indeed underlies\nmost contemporary attempts to harness the tool kit of causal modeling\n(see the entry on\n causal models;\n Spirtes, Glymour, & Scheines 2000; Pearl 2009) in the foundations\nof quantum theory (Leifer & Spekkens 2013; Cavalcanti & Lal\n2014; Costa & Shrapnel 2016; Allen et al. 2017). \nUsing the notion of causality along broadly interventionist lines in\nthe foundations of quantum theory does not commit one to realism (or\nanti-realism) about the causal relations at issue. Woodward combines\ninterventionism with realism about causality while acknowledging  \nimportant differences between, on the one hand, the way in which\ncausal notions figure in common sense and the special sciences and the\nempirical assumptions that underlie their application and, on the\nother hand, the ways in which these notions figure in physics.\n(Woodward 2007: 67; although see Frisch 2014: chs. 4 and 5 for a\nresponse)  \nAnother suggested strategy to take into account Russell’s worry\nwhile continuing to apply causal notions in physics in a consistent\nmanner is to understand interventionism in “perspectival”\nterms (Price 2007; Price & Corry 2007; Price & Weslake 2010;\nIsmael 2016). Perspectivalism is usually staged, as seems natural in\nthe setting of modern physics (although more will be said on this\nbelow), in the framework of a block universe view where the past,\npresent, and future are equally real. In this framework, causality\ncannot have anything to do with changing the future or the\npast because both are—from an “external”\nperspective—completely “fixed”. But one can\nunderstand causation in the block universe from an\n“internal” perspective, according to which causal\ncorrelations are precisely those that are stable under interventions\non those variables that we refer to as the “causes”. \nThe important difference between the two viewpoints—internal and\nexternal to the block—is that there is a discrepancy between the\nparts of the spacetime block that are epistemically accessible from\neach perspective. The spatiotemporally constrained perspective by\nwhich we are bound permits us only limited epistemic accessibility to\nother spatiotemporal regions. This is the perspective in which,\naccording to causal perspectivalism, causal notions are perfectly\nserviceable. Once, on the other hand, we imagine ourselves to be\nomniscient beings that have epistemic access to the whole\nspatiotemporal block, it should not come as a surprise that our causal\nintuitions get confused when we attempt to consider how a\nspatiotemporally bound agent can deliberate about whether or not to\naffect a particular event that is already determined from our imagined\nomniscient perspective. It is because we do not know which\nevents are determined to occur and are ignorant about many others that\nwe can be deliberative agents at all. Again, these considerations are\nrelevant just as much to ordinary forward-in-time causation as they\nare to backward-in-time causation. \nMany of the retrocausal approaches to quantum theory considered in\n §6\n are best understood with some type of perspectival interventionist\naccount of causality in mind. A notable exception is the transactional\ninterpretation\n (§5),\n in which causality might be best understood in terms of processes\nunderscored by conserved quantities. The possibilist extension of the\ntransactional interpretation, defended by Kastner (2006, 2013),\nmoreover eschews the block universe picture. \nAccording to Bell’s theorem (Bell 1964; Clauser et al. 1969; see\nalso the entry on\n Bell’s theorem)\n and its descendants (e.g., Greenberger, Horne, & Zeilinger 1989;\nsee also Goldstein et al. 2011; Brunner et al. 2014 for an overview),\nany theory that reproduces all the correlations of measurement\noutcomes predicted by quantum theory must violate a principle that\nBell calls local causality (Bell 1976, 1990; see also Norsen\n2011; Wiseman & Cavalcanti 2017). In a locally causal theory,\nprobabilities of spatiotemporally localized events occurring in some\nregion 1 are independent of what occurs in a region 2 that is\nspacelike separated from region 1, given a complete specification of\nwhat occurs in a spacetime region 3 in region 1’s backward light\ncone that completely shields off region 1 from the backward light cone\nof region 2. (See, for instance, Figs. 4 and 6 in Bell 1990 or Fig. 2\nin Goldstein et al. 2011.) \nIn a relativistic setting, then, the notion of locality involves\nprohibiting conditional dependences between spacelike separated\nevents, provided that the region upon which these spacelike separated\nevents are conditioned constitutes their common causal (Minkowski)\npast. This characterization of locality implicitly assumes causal\nasymmetry. Thus locality is the idea that there are no causal\nrelations between spacelike separated events. \nThere is another sense of “local” that is sometimes used\nthat will be worth avoiding for the purposes of clarity. This is the\nidea that causal influences are constrained along timelike\ntrajectories. Thus, given Costa de Beauregard’s suggestion of\n“zigzag” causal influences, it is perfectly possible for a\nretrocausal model of quantum phenomena to be nonlocal in the sense\nthat causal relations exist between spacelike separated events, but\n“local” in the sense that these causal influences are\nmediated by timelike trajectories. To avoid ambiguity, it will be\nuseful to refer to this latter sense as\n“action-by-contact” (set apart from\naction-at-a-distance). \nThe first of two main motivating considerations for invoking\nretrocausality in the foundations of quantum mechanics derives from\nthe exploitation of what is essentially the same loophole in a range\nof theorems collectively known as “no-go theorems”.\nAccording to these theorems, any theory or model that is able to\naccount for the empirically confirmed consequences of quantum theory\nmust be unavoidably nonlocal, contextual, and\n\\(\\psi\\)-ontic (i.e., ascribe reality to the quantum states\n\\(\\psi\\)). \nOne way to understand the role that retrocausality plays in\ncircumventing the results of the no-go theorems is to consider each\ntheorem to be underpinned by what is known as the ontological models\nframework (Harrigan & Spekkens 2010; Leifer 2014; Ringbauer 2017).\nThe ontological models framework formalizes and captures the central\nnotion of realism in quantum theory (and so subsumes local hidden\nvariable approaches to quantum mechanics). The framework consists of\nan operational description of a general quantum process, which\ndescribes observed statistics for outcomes of measurements given both\npreparations and transformations, along with an ontological model (or\n“ontic extension”) accounting for the observed statistics.\nFor every preparation procedure, which is usually said to result in a\nquantum state \\(\\psi\\), the quantum system is in fact prepared in an\n“ontic” state \\(\\lambda\\), chosen from a set of states\n\\(\\Lambda\\), which completely specifies the system’s properties.\nThe framework leaves open (and is ultimately used to define) whether\nthe quantum state \\(\\psi\\) is itself an ontic or epistemic state (if\n\\(\\psi\\) is ontic, \\(\\lambda\\) either includes additional ontic\ndegrees of freedom, or is in one-to-one correspondence with \\(\\psi\\);\nsee\n §3.3).\n Each preparation is assumed to result in some \\(\\lambda\\) via a\nclassical probability density over \\(\\Lambda\\), and a set of\nmeasurement procedures that determine conditional probabilities for\noutcomes dependent upon \\(\\lambda\\) (which thus screens off the\npreparation procedure \\(\\psi\\)); explicitly, \\(\\lambda\\) does not\ncausally depend on any future measurement setting \\(\\alpha\\). Finally,\nthe operational statistics must reproduce the quantum statistics. \nImportant for our purposes here is the qualification that \\(\\lambda\\)\ndoes not causally depend on the measurement setting \\(\\alpha\\). This\nassumption is referred to as “measurement independence”\n(\\(\\lambda\\) is “conditionally independent” of\n\\(\\alpha\\)). This assumption is explicitly violated by allowing\nretrocausal influences to be at play in the system. Thus, in so far as\nthe no-go theorems are underpinned by the ontological models\nframework, the no-go theorems are no longer applicable to models that\nallow retrocausality. And in so far as there is motivation to avoid\nthe consequences of the no-go theorems for quantum theory,\nretrocausality is well placed to provide such a model (or so the\nargument goes). Notably, it has been argued that admitting\nretrocausality (i) makes it possible to account for the correlations\nentailed by quantum theory using action-by-contact causal influences\n(and so ensures Lorentz invariance); (ii) undermines as implausible\nthe assumption of (certain types of) noncontextuality from the outset;\nand (iii) may enable an independently attractive \\(\\psi\\)-epistemic\ninterpretation of the wavefunction underpinned by local hidden\nvariables. These arguments are addressed in turn. \nThe principle of local causality, according to Bell, is meant to spell\nout the idea that  \n[t]he direct causes (and effects) of events are near by, and even the\nindirect causes (and effects) are no further away than permitted by\nthe velocity of light. (1990: 105)  \nViolation of this principle, according to some researchers in the\nfoundations of quantum theory, indicates a fundamental incompatibility\nbetween quantum theory and the spirit, perhaps even the letter, of\nrelativity theory (Maudlin 2011). That the correlations entailed by\nquantum theory which violate local causality actually occur in nature\nhas been experimentally documented many times (for example, by\nFreedman & Clauser 1972; Aspect, Dalibard, & Roger 1982; and\nAspect, Grangier, & Roger 1982). \nBell’s result crucially depends not only on the assumption of\nlocal causality, but also on the assumption that whatever variables\n\\(\\lambda\\) describes in some spacetime region, which Bell calls\n“local beables”, do not depend probabilistically on which\nmeasurement setting \\(\\alpha\\) some experimenters choose in the future\nof that region:  \nThis is the aforementioned assumption of measurement independence. It\nis also sometimes referred to as “no superdeterminism”\nbecause it is incompatible with a particularly strong form of\ndeterminism (“superdeterminism”) according to which the\njoint past of the measurement setting \\(\\alpha\\) and the measured\nsystem state \\(\\lambda\\) determines them both completely and induces a\ncorrelation between them. But, as pointed out in the first instance by\nCosta de Beauregard (1977a) and then by Price (1994, 1996),\nsuperdeterminism is not the only way in which Eq.\n(\\ref{eq:independence}) can be violated: if there is retrocausality\n(understood along interventionist lines as outlined in\n §2.1),\n the choice of measurement setting \\(\\alpha\\) may causally influence\nthe physical state \\(\\lambda\\) at an earlier time and thereby also\nrender Eq. (\\ref{eq:independence}) incorrect. Consequently, by\npostulating Eq. (\\ref{eq:independence}) one assumes retrocausality to\nbe absent. \nWithout Eq. (\\ref{eq:independence}), in turn, Bell’s theorem can\nno longer be derived. Thus, admitting the possibility of\nretrocausality in principle reopens the possibility of giving a causal\naccount of the nonlocal correlations entailed by quantum theory as\nmediated by purely action-by-contact, spatiotemporally contiguous,\nLorentz invariant causal influences (of the type envisaged by Costa de\nBeauregard) acting between systems described by local beables.\nConcrete steps towards a model that might fulfill this promise will be\nreviewed in\n §6. \nA wealth of theoretical results establish that models and theories\nwhich reproduce the predictions of quantum theory must be in some\nsense “contextual”. A model or theory is\nnoncontextual when the properties attributed to the system\n(e.g., the value of some dynamical variable that the system is found\nto have) are independent of the manner in which those\nproperties are measured or observed (the context), beyond the actual\nobservation itself (for instance, the other properties that are\nmeasured in conjunction with the measurement). In classical mechanics,\nthe properties that systems are found to have and that are attributed\nto them as a consequence of measurement do not in principle depend on\nthe measurement context, so classical mechanics is noncontextual. The\nfirst contextuality theorems for quantum theory go back to Bell (1966)\nand Kochen and Specker (1967). They demonstrate that if one were to\nconsider quantum measurements as deterministically uncovering the\nvalues of arbitrary pre-existing dynamical variables of quantum\nsystems, then such a model would have to be contextual (that is,\nmeasured values would have to depend upon the context of measurement).\nThus, no noncontextual deterministic local hidden variable model can\nreproduce the observed statistics of quantum theory. \nA new way of understanding contextuality operationally was pioneered\nby Spekkens (2005). Employing the ontological models framework, an\nontological model of an operational theory is noncontextual\nwhen operationally equivalent experimental procedures have equivalent\nrepresentations in the ontological model. This understanding of\nnoncontextuality is then a principle of parsimony akin to\nLeibniz’ law of Identity of Indiscernibles: no ontological\ndifference without operational difference. Using this operational\nunderstanding, Spekkens expands the class of ontological models to\nwhich contextuality applies beyond deterministic local hidden variable\nmodels. But the story is essentially the same: hypothesizing\nunderlying ontic properties for quantum systems that are distinct only\nif there are corresponding operational differences cannot account for\nthe correlations entailed by quantum theory. Thus, no noncontextual\nontological model can reproduce the observed statistics of quantum\ntheory. (For an extended criticism of Spekkens’ operational\nunderstanding of contextuality see Hermens 2011.) \nHypothesizing retrocausal influences alleviates both of these worries.\nRetrocausality renders Kochen-Specker-type contextuality potentially\nexplainable as a form of “causal contextuality” (see the entry\non\n the Kochen-Specker theorem,\n §5.3). If there is a backward-directed influence of the chosen\nmeasurement setting (and context) on the pre-measurement ontic state,\nit is no longer to be expected that the measurement process is simply\nuncovering an independently existing definite value for some property\nof the system, rather the measurement process can play a causal role\nin bringing about such values (the measurement process is\nretrocausal rather than retrodictive). Indeed, one\nmight argue contextuality of measured values is just what one might\nexpect when admitting retrocausal influences. As Wharton (2014: 203)\nputs it, “Kochen-Specker contextuality is the failure\nof the Independence Assumption”, i.e., the failure of\nmeasurement independence. \nWith respect to Spekkens’ more general understanding of\ncontextuality, recall that it is an explicit assumption of the\nontological models framework that the ontic state is independent of\nthe measurement procedure. Thus, hypothesizing retrocausality is an\nunequivocal rejection of the ontological models framework. But one\nmight wonder whether Spekkens’ principle of parsimony might be\nrecast to apply more generally to retrocausal models.\n §7.3\n considers a result due to Shrapnel and Costa (2018) that indicates\nthat retrocausal accounts must indeed accept Spekkens-type\ncontextuality. \nIn some models staged in the ontological models framework, an ontic\nstate \\(\\lambda\\) is only compatible with preparation procedures\nassociated with one specific quantum state \\(\\psi\\). In that case,\n\\(\\psi\\) can be seen as part of \\(\\lambda\\) and, therefore, as an\nelement of reality itself. Models of this type are referred to as\n“\\(\\psi\\)-ontic”. In other models, an ontic state\n\\(\\lambda\\) can be the result of preparation procedures associated\nwith different quantum states \\(\\psi\\). In these models, \\(\\psi\\) is\nnot an aspect of reality and is often more naturally seen as\nreflecting an agent’s incomplete knowledge about the underlying\nontic state \\(\\lambda\\). Models of this type are referred to as\n“\\(\\psi\\)-epistemic”. Spekkens (2007) motivates the search\nfor attractive \\(\\psi\\)-epistemic models by pointing out various\nparallels between quantum mechanics and a toy model in which the\nanalogue of the quantum state is epistemic in that it reflects\nincomplete information about an underlying ontic state. \nHowever, so-called “\\(\\psi\\)-ontology theorems” (Pusey,\nBarrett, & Rudolph 2012; Hardy 2013; Colbeck & Renner 2012;\nsee Leifer 2014 for an overview), establish that, given certain\nplausible assumptions, only \\(\\psi\\)-ontic models can reproduce the\nempirical consequences of quantum theory. The specific assumptions\nused to derive this conclusion differ for the various theorems. All of\nthem, however, rely on the ontological models framework which, as\nnoted a number of times already, explicitly incorporates an assumption\nof measurement independence (Eq. (\\ref{eq:independence})). Since, as\nshown above, measurement independence implicitly rules out\nretrocausality, these theorems apply only inasmuch as retrocausality\nis assumed to be absent. \nMoreover, the theorem due to Pusey, Barrett, and Rudolph\n(2012)—which according to Leifer (2014) uses the least\ncontestable assumptions of all—relies on an additional\nindependence assumption according to which the ontic states of two or\nmore distinct, randomly prepared, systems are uncorrelated before\nundergoing joint measurement. Wharton (2014: 203) points out that in a\nframework that is open to retrocausality this is a problematic\nassumption because one would then expect “past correlations [to]\narise […] for exactly the same reason that future correlations\narise in entanglement experiments”. Exploring the prospects for\nretrocausal accounts is therefore one—perhaps\nthe—important open route for developing an attractive\n\\(\\psi\\)-epistemic model that may be able to explain the quantum\ncorrelations. \nIt is worth pausing at this point to consider the metaphysical\nmotivation for taking a retrocausal approach to quantum theory,\nespecially in light of circumventing these no-go theorems. The\northodox reading of the no-go theorems is that, whatever is said about\nthe ultimate conceptual and ontological framework for understanding\nquantum theory, it cannot be completely classical: it must be nonlocal\nand/or contextual and/or ascribe reality to indeterminate states. In\nshort, quantum theory cannot be about local hidden variables. Part of\nthe appeal of hypothesizing retrocausality in the face of these no-go\ntheorems is to regain (either partial or complete) classicality in\nthese senses (albeit, with—perhaps nonclassical—symmetric\ncausal influences). That is, retrocausality holds the potential to\nallow a metaphysical framework for quantum mechanics that contains\ncausally action-by-contact, noncontextual (or where any contextuality\nis underpinned by noncontextual epistemic constraints),\ncounterfactually definite, determinate (although possibly\nindeterministic), spatiotemporally located properties for physical\nsystems—in other words, a classical ontology. \nA nice to way to consider this is in terms of Quine’s (1951)\ndistinction between the “ontology” and the\n“ideology” of a scientific theory, where the ideology of a\ntheory consists of the ideas that can be expressed in that theory.\nIdeological economy is then a measure of the economy of primitive\nundefined statements employed to reproduce this ideology. The claim\nhere would then be that the ideology of symmetric causal influences is\nmore economical than rejecting a classical ontology. Thus in so far as\nquantum mechanics is telling us something profound about the\nfundamental nature of reality, the hypothesis of retrocausality shows\nthat the lesson of quantum mechanics is not necessarily that the\nquantum ontology can no longer be classical. Some might see this as a\nvirtue of retrocausal approaches. (Although the Shrapnel-Costa no-go\ntheorem reviewed in\n §7.3\n significantly jeopardizes this view.) \nThe laws of nature at the most fundamental level at which they are\ncurrently known are combined in the Standard Model of elementary\nparticle physics. These laws are CPT-invariant, i.e., they remain the\nsame under the combined operations of charge-reversal C\n(replacing all particles by their anti-particles), parity P\n(flipping the signs of all spatial coordinates), and time-reversal\nT. The asymmetries in time which are pervasive in our everyday\nlives are a consequence not of any temporal asymmetry in these laws\nbut, instead, of the boundary conditions of the universe, notably in\nits very early stages. It seems natural to assume that the\ntime-symmetry of the laws (modulo the combined operation of C\nand P) extends to causal dependences at the fundamental\n“ontic” level that underlies the empirical success of\nquantum theory. If so, there may be backward-in-time no less than\nforward-in-time causal influences at that ontic level. \nPrice (2012) turns these sketchy considerations into a rigorous\nargument. He shows that, when combined with two assumptions concerning\nquantum ontology, time-symmetry implies retrocausality (understood\nalong broadly interventionist lines). The ontological assumptions are\n(i) that at least some aspects of the quantum state \\(\\psi\\) are real\n(notably, in Price’s example, there is a “beable”\nencoding photon polarization angle), and (ii) that inputs and outputs\nof quantum experiments are discrete emission and detection\nevents. Moreover, it is important to Price’s argument that\ndynamical time-symmetry (that the dynamical laws of the theory are\ntime-symmetric) be understood as implying that operational\ntime-symmetry (that the set of all possible combinations of\npreparation and measurement procedures in a theory, with associated\nprobabilities for outputs given inputs, is closed under interchange of\npreparation and measurement) translates into ontic time-symmetry\n(operational time-symmetry plus a suitable map between the ontic state\nspaces of the symmetric combinations). Given these conditions, any\nfoundational account that reproduces the empirical verdicts of quantum\ntheory must be retrocausal. \nLeifer and Pusey (2017) (see also Leifer 2017 in\n Other Internet Resources)\n strengthen Price’s argument by showing that his assumption\nabout the reality of (aspects of) the quantum state \\(\\psi\\) can be\nrelaxed. As they demonstrate, if measurement outcomes depend only on a\nsystem’s ontic state \\(\\lambda\\), i.e., if that state completely\nmediates any correlations between preparation procedures and\nmeasurement outcomes (“\\(\\lambda\\)-mediation”), this\nsuffices for operational time-symmetry to entail the existence of\nretrocausality. Foundational accounts which like Bohmian mechanics\n(Bohm 1952a,b) or GRW-theory (Ghirardi, Rimini, & Weber 1986)\navoid postulating retrocausality do so by violating time-symmetry in\nsome way. The GRW-theory does so by introducing explicitly\ntime-asymmetric dynamics. In Bohmian mechanics the dynamics is\ntime-symmetric, but the theory is applied in a time-asymmetric manner\nwhen assessing which quantum states are actually realized. Notably,\none assumes that the quantum states of a measured system and a\nmeasurement device connected to it are uncorrelated prior to\nmeasurement, whereas they are in general correlated after measurement\n(Leifer & Pusey 2017: §X.). \nThe more advanced relativistic quantum theories that underlie modern\nparticle physics are typically formulated in the Lagrangian, path\nintegral-based, formalism, where information about the symmetries and\ninteractions of the theory are encoded in the action, S.\nWharton, Miller, and Price (2011) use this formalism as a foundation\nto suggest that action symmetries must be reflected as ontological\nsymmetries. In particular, Wharton et al. claim that for any two\nexperimental arrangements related by a spacetime transformation (say,\nreflection in time) that leaves the action unchanged, the ontologies\nmust also remain unchanged across the same spacetime transformation.\nThis principle, referred to by Wharton et al. as action\nduality, imposes non-trivial constraints on realist accounts. \nNotably, Evans, Price, & Wharton (2013) argue that there is such\nan action symmetry between a pair of entangled photons passing through\nseparate polarizers and a single photon passing sequentially through\ntwo polarizers. Thus, if action duality is well-founded, the action\nsymmetry should be reflected in identical ontologies for the\naction-dual experiments. Short of rejecting action duality, this\nimplies one of two things: either the usual causal explanation for the\nbehavior of the single photon must be reflected in a causal\nexplanation for the typically-quantum behavior of the pair of\nentangled photons, and so there must be retrocausality at the level of\nthe ontic state \\(\\lambda\\), or the nonlocality ascribed to the\ntypically-quantum behavior of the pair of entangled photons must be\nsimilarly ascribed to the behavior of the single photon, and so\nnonlocality is even more dramatically widespread than usually assumed,\nand indeed is not unique to entangled bipartite quantum systems. In so\nfar as there is a strong case for the behavior of a single photon\npassing sequentially through two polarizers having a perfectly good\ncausal explanation based on spatiotemporally localized properties, the\nargument of Evans et al. is that this amounts to an equally strong\ncase for retrocausality in quantum theory. \nThis entry so far has considered the two most significant motivating\narguments in favor of adopting retrocausality as a hypothesis for\ndealing with the interpretational challenges of quantum theory. But\nthese motivations do not by themselves amount to an interpretation or\nmodel of quantum theory.\n §6\n consists of a survey of a range of retrocausal models, but this\nsection first considers perhaps the most prominent retrocausal model,\nthe transactional interpretation. Developed by Cramer in the 1980s\n(Cramer 1980, 1986, 1988), the transactional interpretation is heavily\ninfluenced by the framework of the Wheeler-Feynman absorber approach\nto electrodynamics (see\n §1);\n the Wheeler-Feynman schema can be adopted to describe the microscopic\nexchange of a single quantum of energy, momentum, etc.,\nbetween and within quantum systems. \nAt the heart of the transactional interpretation is the\n“transaction”: real physical events are identified with\nso-called “handshakes” between forward-evolving quantum\nstates \\(\\psi\\) and backward-evolving complex-conjugates \\(\\psi^*\\).\nWhen a quantum emitter (such as a vibrating electron or atom in an\nexcited state) is to emit a single quantum (a photon, in these cases),\nthe source produces a radiative field—the “offer”\nwave. Analogously to the Wheeler-Feynman description, this field\npropagates outwards both forward and backward in time (as well as\nacross space). When this field encounters an absorber, a new field is\ngenerated—the “confirmation” wave—that\nlikewise propagates both forward and backward in time, and so is\npresent as an advanced incident wave at the emitter at the instant of\nemission. Both the retarded field produced by the absorber and the\nadvanced field produced by the emitter exactly cancel with the\nretarded field produced by the emitter and advanced field produced by\nthe absorber for all times before the emission and after the\nabsorption of the photon; only between the emitter and the absorber is\nthere a radiative field. Thus the transaction is completed with this\n“handshake”: a cycle of offer and confirmation waves  \nrepeats until the response of the emitter and absorber is sufficient\nto satisfy all of the quantum boundary conditions…at which point the\ntransaction is completed. (Crammer 1986: 662)  \nMany confirmation waves from potential absorbers may converge on the\nemitter at the time of emission but the quantum boundary conditions\ncan usually only permit a single transaction to form. Any observer who\nwitnesses this process would perceive only the completed transaction,\nwhich would be interpreted as the passage of a particle (e.g., a\nphoton) between emitter and absorber. \nThe transactional interpretation takes the wave function to be a real\nphysical wave with spatial extent. The wave function of the quantum\nmechanical formalism is identical with the initial offer wave of the\ntransaction mechanism and the collapsed wave function is identical\nwith the completed transaction. Quantum particles are thus not to be\nthought of as represented by the wave function but rather by the\ncompleted transaction, of which the wave function is only the initial\nphase. As Cramer explains: \nThe transaction may involve a single emitter and absorber or multiple\nemitters and absorbers, but it is only complete when appropriate\nboundary conditions are satisfied at all loci of emission and\nabsorption. Particles transferred have no separate identity\nindependent from the satisfaction of these boundary conditions. (1986:\n666) \nThe amplitude of the confirmation wave which is produced by the\nabsorber is proportional to the local amplitude of the incident wave\nthat stimulated it and this, in turn, is dependent on the attenuation\nit received as it propagated from the source. Thus, the total\namplitude of the confirmation wave is just the absolute square of the\ninitial offer wave (evaluated at the absorber), which yields the Born\nrule. Since the Born rule arises as a product of the transaction\nmechanism, there is no special significance attached to the role of\nthe observer in the act of measurement. The “collapse of the\nwave function” is interpreted as the completion of the\ntransaction. \nThe transactional interpretation explicitly interprets the quantum\nstate \\(\\psi\\) as real, and so does not constitute an attempt to\nexploit the retrocausality loopholes to the theorems that rule out\n\\(\\psi\\)-epistemic accounts. Additionally, the transactional\ninterpretation subverts the dilemma at the core of the EPR argument\n(Einstein, et al. 1935) by permitting incompatible observables to take\non definite values simultaneously: the wavefunction, according to the\ntransactional interpretation, \nbrings to each potential absorber the full range of possible outcomes,\nand all have “simultaneous reality” in the EPR sense. The\nabsorber interacts so as to cause one of these outcomes to emerge in\nthe transaction, so that the collapsed [wavefunction] manifests only\none of these outcomes. (Crammer 1986: 668). \nMost importantly, however, the transactional interpretation employs\nboth retarded and advanced waves, and in doing so admits the\npossibility of providing a “zigzag” explanation of the\nnonlocality associated with entangled quantum systems. \nBefore turning to one of the more significant objections to the\ntransactional interpretation, and to retrocausality in general, it is\ninstructive to tease apart here two complementary descriptions of this\ntransaction process. On the one hand there is a description of the\nreal physical process, consisting of the passage of a particle between\nemitter and absorber, that a temporally bound experimenter would\nobserve; and on the other hand there is a description of a dynamical\nprocess of offer and confirmation waves that is instrumental in\nestablishing the transaction. This latter process simply cannot occur\nin an ordinary time sequence, not least because any temporally bound\nobserver by construction cannot detect any offer or confirmation\nwaves. Cramer suggests that the “dynamical process” be\nunderstood as occurring in a “pseudotime” sequence: \nThe account of an emitter-absorber transaction presented here employs\nthe semantic device of describing a process extending across a\nlightlike or a timelike interval of space-time as if it occurred in a\ntime sequence external to the process. The reader is reminded that\nthis is only a pedagogical convention for the purposes of description.\nThe process is atemporal and the only observables come from the\nsuperposition of all “steps” to form the final\ntransaction. (Crammer 1986: 661, fn.14) \nThese steps are of course the cyclically repeated exchange of offer\nand confirmation waves which continue “until the net exchange of\nenergy and other conserved quantities satisfies the quantum boundary\nconditions of the system” (1986: 662). There is a strong sense\nhere that any process described as occurring in pseudotime is not\na process at all but, as Cramer reminds, merely a\n“pedagogical convention for the purposes of description”.\nWhether it is best to understand causality according to the\ntransactional interpretation in terms of processes underscored by\nconserved quantities is closely tied to how one should best understand\nthis pseudotemporal process. \nMaudlin (2011) outlines a selection of problems that arise in\nCramer’s theory as a result of the pseudotemporal account of the\ntransaction mechanism: processes important to the completion of a\ntransaction take place in pseudotime only (rather than in real time)\nand thus cannot be said to have taken place at all. Since a temporally\nbound observer can only ever perceive a completed transaction, i.e., a\ncollapsed wavefunction, the uncollapsed wavefunction never actually\nexists. Since the initial offer wave is identical to the wavefunction\nof the quantum formalism, any ensuing exchange of advanced and\nretarded waves required to provide the quantum mechanical\nprobabilities, according to Maudlin, also do not exist. Moreover,\nCramer’s exposition of the transaction mechanism seems to\nsuggest that the stimulation of sequential offer and confirmation\nwaves occurs deterministically, leaving a gaping hole in any\nexplanation the transactional interpretation might provide of the\nstochastic nature of quantum mechanics. Although these problems are\nsignificant, Maudlin admits that they may indeed be peculiar to\nCramer’s theory. Maudlin also sets out a more general objection\nto retrocausal models of quantum mechanics which he claims to pose a\nproblem for “any theory in which both backwards and forwards\ninfluences conspire to shape events” (2011: 184). \nMaudlin’s main objection to the transactional interpretation\nhinges upon the fact that the transaction process depends crucially on\nthe fixity of the absorbers “just sitting out there in the\nfuture, waiting to absorb” (2011: 182); one cannot presume that\npresent events are unable to influence the future disposition of the\nabsorbers. Maudlin offers a thought experiment to illustrate this\nobjection. A radioactive source is constrained to emit a\n\\(\\beta\\)-particle either to the left or to the right. To the right\nsits absorber A at a distance of 1 unit. Absorber B is\nalso located to the right but at a distance of 2 units and is built on\npivots so that it can be swung around to the left on command. A\n\\(\\beta\\)-particle emitted at time \\(t_{0}\\) to the right will be\nabsorbed by absorber A at time \\(t_{1}\\). If after time\n\\(t_{1}\\) the \\(\\beta\\)-particle is not detected at absorber A,\nabsorber B is quickly swung around to the left to detect the\n\\(\\beta\\)-particle after time \\(2t_{1}\\). \nAccording to the transactional interpretation, since there are two\npossible outcomes (detection at absorber A or detection at\nabsorber B), there will be two confirmation waves sent back\nfrom the future, one for each absorber. Furthermore, since it is\nequally probable that the \\(\\beta\\)-particle be detected at either\nabsorber, the amplitudes of these confirmation waves should be equal.\nHowever, a confirmation wave from absorber B can only be sent\nback to the emitter if absorber B is located on the left. For\nthis to be the case, absorber A must not have detected the\n\\(\\beta\\)-particle and thus the outcome of the experiment must already\nhave been decided. The incidence of a confirmation wave from absorber\nB at the emitter ensures that the \\(\\beta\\)-particle\nis to be sent to the left, even though the amplitude of this wave\nimplies a probability of a half of this being the case. As Maudlin\n(2011: 184) states so succinctly, “Cramer’s theory\ncollapses”. \nThe key challenge from Maudlin is that any retrocausal mechanism must\nensure that the future behavior of the system transpires consistently\nwith the spatiotemporal structure dictated by any potential future\ncauses: “stochastic outcomes at a particular point in time may\ninfluence the future, but that future itself is supposed to play a\nrole in producing the outcomes” (2011: 181). In the\ntransactional interpretation the existence of the confirmation wave\nitself presupposes some determined future state of the system with\nretrocausal influence. However, with standard (i.e., forward-in-time)\nstochastic causal influences affecting the future from the present, a\ndetermined future may not necessarily be guaranteed in every\nsuch case, as shown by Maudlin’s experiment. \nMaudlin’s challenge to the transactional interpretation has been\nmet with a range of responses (see P. Lewis 2013 and the entry on\n action at a distance in quantum mechanics\n for more discussion of possible responses). The responses generally\nfall into two types (P. Lewis 2013). The first type of response\nattempts to accommodate Maudlin’s example within the\ntransactional interpretation. Berkovitz (2002) defends the\ntransactional interpretation by showing that causal loops of the type\nfound in Maudlin’s experiment need not obey the assumptions\nabout probabilities that are common in linear causal situations.\nMarchildon (2006) proposes considering the absorption properties of\nthe long distance boundary conditions: if the universe is a perfect\nabsorber of all radiation then a confirmation wave from the left will\nalways be received by the radioactive source at the time of emission\nand it will encode the correct probabilistic information. Kastner\n(2006) proposes differentiating between competing initial states of\nthe radioactive source, corresponding to the two emission\npossibilities, that together characterize an unstable bifurcation\npoint between distinct worlds, where the seemingly problematic\nprobabilities reflect a probabilistic structure across both possible\nworlds. \nThe second type of response is to modify the transactional\ninterpretation. For instance, Cramer (2016) introduces the idea of a\nhierarchy of advanced-wave echoes dependent upon the magnitude of the\nspatiotemporal interval from which they originate. Kastner (2013)\nsurmises that the source of the problem that Maudlin has exposed,\nhowever, is the idea that quantum processes take place in the\n“block world”, and rejects this conception of processes in\nher own development of the transactional interpretation. According to\nher “possibilist” transactional interpretation, all the\npotential transactions exist in a real space of possibilities, which\namounts at once to a kind of modal realism and an indeterminacy\nregarding future states of the system (hence Kastner’s rejection\nof the block universe view). The possibilist transactional\ninterpretation arguably handles multi-particle scenarios more\nnaturally, and presents the most modern sustained development of the\ntransactional interpretation (although see P. Lewis 2013 for\ncriticisms of the possibilist transactional interpretation specific to\nMaudlin’s challenge). \nThe transactional interpretation might be seen as the most\nprominent—and historically significant—retrocausal model\non the market, but it is not the only one. While retrocausality in\nquantum mechanics has been the subject of considerable analysis and\ncritique over the years (Rietdijk 1978; Sutherland 1983, 1985, 1989;\nPrice 1984, 2001; Hokkyo 1988, 2008; Miller 1996, 1997; Atkinson 2000;\nArgaman 2010; Evans 2015), the focus of this section is a review of\nsome of the more concrete proposals for retrocausal models. \nWhat is now known as the two-state vector formalism was first proposed\nby Watanabe (1955), and then rediscovered by Aharonov, Bergmann, and\nLebowitz (1964). The proposal is that the usual forward evolving\nquantum state contains insufficient information to completely specify\na quantum system; rather the forward evolving state must be\nsupplemented by a backward evolving state to provide a complete\nspecification. Thus, according to the two-state vector formalism, the\ncomplete quantum description contains a state vector that evolves\nforward in time from the initial boundary condition towards the\nfuture, and a state vector that evolves backward in time from the\nfuture boundary condition towards the past. It is only a combination\nof complete measurements at both initial and final boundaries that can\nprovide a complete specification of a quantum system. The two-state\nvector formalism is empirically equivalent to standard quantum\nmechanics (Aharonov & Vaidman 1991, 2008). \nThe emphasis of the two-state vector formalism is on the operational\nelements of the theory, and there are very few ontological\nprescriptions, including how best to understand causality. It is in\nprinciple compatible with a variety of supplemented retrocausal\nontologies, e.g., the causally symmetric Bohm model\n (§6.3). \nToy models have been employed to illustrate how retrocausality could\nbe possibly realized. Price (2008) suggests a simple toy model\nfeaturing linked nodes that can assume different values, which he dubs\nthe “Helsinki model”. If one interprets the nodes as\npartially ordered in time and the values of the exogenous boundary\nnodes as controllable, the dynamics specified by Price entails causal\nbi-directionality, i.e., both ordinary forward causality and\nretrocausality, understood in an interventionist sense. Although the\nmodel does indeed display particular behavior that can be naturally\ninterpreted as retrocausal, a full-blown analogy with standard quantum\nmechanics is limited. \nThe Helsinki model consists of three primitive endogenous nodes, each\nnode comprising a meeting point of three edges, with two\n“internal” edges linking the three nodes and five\n“external” edges. Each edge has one of three\n“flavors”, A, B, or C. The system is\ntemporally oriented with three exogenous nodes, each joined to a\nsingle edge, representing system “inputs”\n(preparations/interventions), and two system “outputs”\n(measurement outcomes/observations) joined to the remaining two edges.\nThe two internal edges represent hidden flavors that cannot be\ndirectly controlled or observed. There are two basic rules that govern\nthe dynamics of this toy system: (i) each node must be strictly\ninhomogeneous—i.e., comprising three edges of different\nflavors—or strictly homogeneous—i.e., three edges of the\nsame flavor, and (ii) successive homogeneous nodes are prohibited. \nThe retrocausal behavior of the model arises as a result of the heavy\nconstraints that the two basic rules place on the possible flavors of\nthe hidden edges, which establishes correlations between the input\nstates and the hidden states. Thus interventions on the left or right\nexogenous variables influence the complete set of possible hidden\nstates of the system. Assuming that the hidden states are in the past\nof these variable choices, the hidden state depends\n“retrocausally” on the left and right input settings.\nMoreover, under specific node variables, some interventions on the\nleft or right exogenous variables amount to interventions on the\ndistant output variable, displaying a kind of nonlocality (but\nviolating no-signaling). \nMore ambitiously, Corry (2015) proposes three toy models, also using\nnodes and links, which exhibit Bell-type correlations and account for\nthem in a purely local manner in that the constraints governing nodes\nmake reference only to information available at the respective nodes.\nThe models do not in general specify values for unobserved nodes,\nhowever. As a way out, Corry suggests, one may either postulate such\nvalues at the price of accepting “retrofinkish\ndispositions” (dispositions which, if they were triggered, would\nnot have been there in the first place) or simply deny the existence\nof such values outright. \nA model that hypothesizes retrocausality and reproduces the\nconsequences of standard quantum theory without violating Lorentz\ninvariance is Sutherland’s (2008, 2015, 2017) causally symmetric\nversion of Bohmian mechanics. This model adds to the standard quantum\nstate \\(\\psi_i\\) of ordinary Bohmian mechanics, which is fixed by an\ninitial boundary condition, an additional quantum state \\(\\psi_f\\),\nwhich is fixed by some final boundary condition. An analogue to the\n“guiding equation” for particle motion in ordinary Bohmian\nmechanics is derived by Sutherland that is symmetric with respect to\nthese states. The zero-component of the probability current, which is\ndirectly related to the probability density, is computed as  \nwhere  \nFinally, for consistency, the model also requires that the conditional\nprobability of the final state being \\(\\psi_f\\), given the initial\nstate \\(\\psi_i\\), is  \nBerkovitz (2008) criticizes the use of the additional assumption Eq.\n(\\ref{eq:addass}) and the need for it in Sutherland’s model,\narguing that it leads to an undesirable form of causal loops and has\nan ad hoc character because there is no independent reason to\nthink that \\(\\psi_i\\) and \\(\\psi_f\\) should be correlated in this\nway. \nSutherland’s model is explicitly retrocausal in that particle\ndynamics are influenced by \\(\\psi_f\\), which in turn depends on the\nfuture measurement setting. It also contains action-by-contact causal\ninfluences in that Bell-type correlations are accounted for in the\n“zigzag” manner envisaged by Costa de Beauregard, as\noutlined in\n §1. \nSilberstein, Stuckey, and McDevitt (2018), based on earlier work together with Cifone\n(Stuckey, Silberstein, & Cifone 2008; Silberstein, Cifone, &\nStuckey. 2008), suggest a realist interpretation that they call the\n“relational blockworld view”. The ontology of this\ninterpretation consists of so-called “spacetimesource\nelements”, which are characterized as “amalgams of space,\ntime, and sources” (Silberstein, Stuckey, & McDevitt 2018:\n153). \nTechnically, the relational blockworld approach is set up as a\nmodification of lattice gauge theory, with the Feynman path integral\nfunctioning as an “adynamical global constraint”. While\nadynamical and acausal rather than retrocausal in spirit, on the\nauthors’ view this interpretation exploits the retrocausality\nloopholes of the no-go theorems. They conceive of the relational\nblockworld view as \\(\\psi\\)-epistemic. In more recent work (2018: Ch.\n6), Silberstein, Stuckey and McDevitt have developed this approach\ninto an ambitious overarching programme in fundamental physics, aiming\nat field-theoretic unification as well as a novel account of quantum\ngravity. \nSchulman (1997, 2012) proposes a solution to the quantum measurement\nproblem based on the possibility of future conditioning, which\noriginates from his analysis of the thermodynamic arrow of time.\nBeginning with Loschmidt’s challenge to Boltzmann—that it\nshould not be possible to derive irreversible dynamical behavior from\ntime-symmetric dynamics (see the entry on\n thermodynamic asymmetry in time)—Schulman\n notes that successful “retrodiction” of past macroscopic\nstates of some thermodynamic system is indeed asymmetric to successful\n“prediction” of future macroscopic states. For successful\nretrodiction, rather than evolving each of the microscopic states of\nsome macroscopic state according to the dynamical laws (which is\nidentical to the process of prediction, and is the basis of\nLoschmidt’s challenge), one instead hypothesizes a prior\nmacroscopic state to which the prediction process can be applied such\nthat the current macroscopic state of the system obtains with high\nlikelihood given the dynamical evolution of the prior microscopic\nstates. With respect to the set of possible microscopic states for the\ncurrent macroscopic state, since the evolution of the vast majority\n(on Liouville measure) of these states according to the dynamical laws\nresult in trajectories that conflict with the retrodiction hypothesis,\nthese states are effectively rejected in the process of retrodiction\nin favor of those special few microscopic states that correspond to\nthe dynamical evolution of acceptable hypothetical initial\nconditions. \nSchulman’s proposal in response to this asymmetry is that, just\nas the set of possible final microscopic states must be restricted for\nsuccessful retrodiction, so should the permissible initial microscopic\nstates be restricted for the purposes of prediction. Thus, since final\nmicroscopic states are subject to conditioning from the past state of\nthe system, the initial microscopic states will be subject to\nconditioning from the future state of the system—a conditioning\nthat is hidden in thermodynamic processes where the microscopic states\nof the system are indistinguishable macroscopically. This future\nconditioning is a central feature of Schulman’s two-time\nboundary condition proposal and the hidden nature of this conditioning\nSchulman calls “cryptic constraints”. \nRegarding quantum theory, Schulman claims that quantum superpositions\nof macroscopically distinct states generated by quantum evolution,\nwhich are at the heart of many of the nonclassical elements of quantum\ntheory, are what he calls “grotesque” states, and proposes\nthat these states are avoided in quantum systems due to future\nconditioning and cryptic constraints. Just like there are special\nmicroscopic states of thermodynamical systems that evolve\n“against” the second law of thermodynamics (which are\nidentified in the process of retrodiction), so too must there be\n“special” microscopic states of a quantum system which do\nnot lead to grotesque states when evolved according to the quantum\ndynamical laws, rather these states will lead to one particular\ndefinite state of the superposition. Schulman’s solution to the\nquantum measurement problem is that, in every performed experiment,\nthe initial conditions of the quantum system are among these special\nstates which, through pure unitary quantum evolution, yield a single\noutcome to the experiment (Schulman 1997: 214). Grotesque states, the\nproblematic states of the quantum measurement problem, are thus\navoided. \nSchulman’s proposal implicitly assumes that, in the preparation\nof an experiment, it is only the macroscopic state of the system which\nis under the control of the experimenter; it is impossible to control\nthe precise microscopic state. It is this initial microscopic state\nthat Schulman suggests is always a special state. Schulman envisages\nthe special-state constraint as correlated with future conditions and,\nsince these constraints are not apparent to the experimenter until the\nfuture conditions are “measured” at the end of the\nexperiment, these constraints are cryptic. As a result of the future\nconditioning of initial states, Schulman’s proposal is a kind of\nretrocausal mechanism, understood in an interventionist sense. (For an\nextension of Schulman’s model into the Lagrangian schema\n (§6.6)\n see Almada et al. 2016 and Wharton 2014, 2018.) \nWharton (2010a; see also Wharton 2007, 2010b, 2013, 2016, 2018;\nWharton, Miller, & Price 2011; Evans, Price, & Wharton 2013)\nproposes a “novel interpretation of the Klein-Gordon\nequation” for a neutral, spinless relativistic particle. The\naccount is a retrocausal picture based on Hamilton’s principle\nand the symmetric constraint of both initial and final boundary\nconditions to construct equations of motion from a Lagrangian, and is\na natural setting for a perspectival interventionist account of\ncausality. Wharton treats external measurements as physical\nconstraints imposed on a system in the same way that boundary\nconstraints are imposed on the action integral of Hamilton’s\nprinciple; the final measurement does not simply reveal preexisting\nvalues of the parameters, but constrains those values (just as the\ninitial boundary condition would). Wharton’s model has been\ndescribed as an “all-at-once” approach, since the dynamics\nof physical systems between an initial and final boundary emerges\nen bloc as the solution to a two-time boundary value\nproblem. \nOn this interpretation, one considers reality exclusively between two\ntemporal boundaries as being described by a classical field \\(\\phi\\)\nthat is a solution to the Klein-Gordon equation: specification of\nfield values at both an initial and final boundary (as opposed to\nfield values and their rate of change at only the initial boundary)\nconstrains the field solutions between the boundaries. Wharton argues\nthat constraining such fields at both an initial and a final boundary\n(or a closed hypersurface in spacetime) generates two strikingly\nquantum features: quantization of certain field properties and\ncontextuality of the unknown parameters characterizing the field\nbetween the boundaries. (That there are unknown parameters before the\nimposition of the final condition is ensured due to the\nunderdetermination of the classical field by specifying only the field\nvalues, and not their rate of change, in the initial data.) \nFrom within Wharton’s picture, an invariant joint probability\ndistribution associated with each possible pair of initial and final\nconditions can be constructed, and the usual conditional probabilities\ncan be formed by conditioning on any chosen portion of the boundary\n(Wharton 2010a,b). Probability is then a manifestation of our\nignorance: if one knew only the initial boundary, one would only be\nable to describe the subsequent field probabilistically (due to the\nfuture constraint); given the final boundary as well, one would then\nbe able to retrodict the field values between the two boundaries. \nIn more recent work (Wharton 2016), Wharton explores the prospects for\na realist retrocausal interpretation of quantum theory based on the\nFeynman path integral formalism (Feynman 1942). In that formalism as\napplied to a single particle undergoing position measurements at\nspacetime points \\((\\mathbf{x_0},t_0)\\) and \\((\\mathbf{x_1},t_1)\\),\nthe joint probability distribution for all pairs of points and given\ntimes \\(t_0\\) and \\(t_1\\) is given by  \nThe sum in this formula is to be understood as the infinitesimal limit\nof a discretized set of spacetime trajectories connecting\n\\((\\mathbf{x_0},t_0)\\) and \\((\\mathbf{x_1},t_1)\\). S is the\nclassical action of the particle along the respective trajectory. \nA straightforward but naive interpretation of this equation according\nto which the probability reflects ignorance concerning the path taken\ndoes not work because the right hand side of Eq. (\\ref{eq:feynman}) is\nnot a sum of positive numbers (interpretable as probabilities of\ntrajectories) but rather a positive number obtained by taking the\nmodulus squared of a sum of complex numbers. Wharton explores the\nprospects for giving an ignorance interpretation of the path integral\nalong less straightforward lines, noting that Eq. (\\ref{eq:feynman})\ncan be brought into the form  \nwhere the \\(\\left|c_i\\right|\\) are distinct groups of\ntrajectories A. The form Eq. (\\ref{eq:wharton}), according\nto Wharton, invites an interpretation of the probability in terms of\nignorance concerning the actual group of trajectories \\(c_i\\), and he\ntentatively proposes a field interpretation of the \\(c_i\\). The\ninterpretation is retrocausal because the probabilities over groups of\ntrajectories are influenced by the future measurement time and\nsetting. Open questions for this approach concern the grouping of\ntrajectories, which is so far inherently ambiguous, the details of the\nsuggested field interpretation, as well as generalizations to\nmany-particle and other more general settings. \nThis final section reviews some of the most common, and then two of\nthe most significant, objections against the proposal of\nretrocausality in quantum mechanics. \nThere is a tradition in philosophy for regarding the very idea of\nretrocausality as incoherent. The most prominent worry, forcefully\nmade by Black (1956), is the so-called “bilking argument”\n(see the entry on\n time travel).\n Imagine a pair of events, a cause, C, and an effect, E,\nwhich we believe to be retrocausally connected (E occurs\nearlier in time than C). It seems possible to devise an\nexperiment which could confirm whether our belief in the retrocausal\nconnection is correct or not. Namely, once we had observed that\nE had occurred, we could then set about ensuring that C\ndoes not occur, thereby breaking any retrocausal connection that could\nhave existed between them. If we were successful in doing this, then\nthe effect would have been “bilked” of its cause. \nThe bilking argument drives one towards the claim that any belief an\nagent might hold in the positive retrocausal correlation between event\nC and event E is simply false. However, Dummett (1964)\ndisputes that giving up this belief is the only solution to the\nbilking argument. Rather, according to Dummett, what the bilking\nargument actually shows is that a set of three conditions concerning\nthe two events, and the agent’s relationship to them, is\nincoherent: \nIt is interesting to note that these conditions do not specify in\nwhich order events C and E occur. On simple reflection,\nthere is a perfectly natural reason why it is not possible to bilk\nfuture effects of their causes, since condition (iii) fails\nto hold for future events: we simply have no access to which future\nevents occur independently of the role we play as causal agents to\nbring the events about. When we lack that epistemic access to past\nevents, the same route out of the bilking argument becomes\navailable. \nDummett’s defense against the bilking argument is especially\nrelevant to quantum mechanics. In fact, once a suitable specification\nis made of how condition (iii) can be violated, we find that there\nexists a strong parallel between the conditions which need to hold to\njustify a belief in bringing about the past and the structure of\nquantum mechanics. Price (1996: 174) points out that bilking is\nimpossible in the following circumstances: rather than suppose that a\nviolation of condition (iii) entails that the relevant agent has\nno epistemic access to the relevant past events independently\nof any intention to bring them about, suppose that the means by which\nknowledge of these past events is gathered breaks the claimed\ncorrelation between the agent’s action and those past events.\nSuch a condition can be stated as follows: \nThe significance of this weakened violation of condition (iii) is that\nit is just the sort of condition one would expect to hold if the\nsystem in question were a quantum system. The very nature of quantum\nmechanics ensures that any claimed positive correlation between the\nfuture measurement settings and the hidden variables characterizing a\nquantum system cannot possibly be bilked of their causes because\ncondition (iv) is perennially violated. Moreover, so long as we\nsubscribe to the epistemic interpretation of the wavefunction, we lack\nepistemic access to the “hidden” variables of the system\nand we lack this access in principle as a result of the\nstructure of quantum theory. \nAnother prominent challenge against the very idea of retrocausality is\nthat it inevitably would give rise to vicious causal loops (Mellor\n1998). (See Faye 1994 for a response and the entry on\n backward causation\n for a more detailed review of the objections raised against the idea\nof retrocausality.) \nCausal modeling (Spirtes, Glymour, & Scheines 2000; Pearl 2009) is\na practice that has arisen from the field of machine learning that\nconsists in the development of algorithms that can automate the\ndiscovery of causes from correlations in large data sets. The causal\ndiscovery algorithms permit an inference from given statistical\ndependences and independences between distinct measurable elements of\nsome system to a causal model for that system. As part of the\nalgorithms, a series of constraints must be placed on the resulting\nmodels that capture general features that we take to be characteristic\nof causality. Two of the more significant assumptions are (i) the\ncausal Markov condition, which ensures that every statistical\ndependence in the data results in a causal dependence in the\nmodel—essentially a formalization of Reichenbach’s common\ncause principle—and (ii) faithfulness, which ensures that every\nstatistical independence implies a causal independence, or no causal\nindependence is the result of a fine-tuning of the model. \nIt has long been recognized (Butterfield 1992; Hausman 1999; Hausman\n& Woodward 1999) that quantum correlations force one to give up at\nleast one of the assumptions usually made in the causal modeling\nframework. Wood and Spekkens (2015) argue that any causal model\npurporting to causally explain the observed quantum\ncorrelations must be fine-tuned (i.e., must violate the faithfulness\nassumption). More precisely, according to them, since the observed\nstatistical independences in an entangled bipartite quantum system\nimply no signaling between the parties, when it is then assumed that\nevery statistical independence implies a causal independence (which is\nwhat faithfulness dictates), it must be inferred that there can be no\n(direct or mediated) causal link between the parties. Since there is\nan observed statistical dependence between the outcomes of\nmeasurements on the bipartite system, we can no longer account for\nthis dependence with a causal link unless this link is fine tuned to\nensure that the no-signaling independences still hold. There is thus a\nfundamental tension between the observed quantum correlations and the\nno-signaling requirement, the faithfulness assumption and the\npossibility of a causal explanation. \nFormally, Wood and Spekkens argue that the following three assumptions\nform an inconsistent set: (i) the predictions of quantum theory\nconcerning the observed statistical dependences and independences are\ncorrect; (ii) the observed statistical dependences and independences\ncan be given a causal explanation; (iii) the faithfulness assumption\nholds. Wood and Spekkens conclude that, since the faithfulness\nassumption is an indispensable element of causal discovery, the second\nassumption must yield. The contrapositive of this is that any\npurported causal explanation of the observed correlations in an\nentangled bipartite quantum system falls afoul of the tension between\nthe no-signaling constraint and no fine tuning and, thus, must violate\nthe assumption of faithfulness. Such causal explanations, so the\nargument goes, including retrocausal explanations, should therefore be\nruled out as viable explanations. \nAs a brief aside, this fine-tuning worry for retrocausality in the\nquantum context arises in a more straightforward way. There is no good\nevidence to suggest that signaling towards the past is possible; that\nis, there is no retrocausality at the operational level. (Pegg 2006,\n2008 argues that this can be explained formally as a result of the\ncompleteness condition on the measurement operators, introducing an\nasymmetry in normalization conditions for preparation and\nmeasurement.) Yet, despite there being no signaling towards the past,\nretrocausal accounts assume causal influences towards past. That these\ncausal influences do not show up as statistical dependences\nexploitable for signaling purposes raises exactly the same fine-tuning\nworry as Wood and Spekkens raise. \nAn obvious response to the challenge set by Wood and Spekkens is to\nsimply reject the assumption of faithfulness. But this should not be\ntaken lightly; the intuition behind the faithfulness assumption is\nbasic and compelling. When no statistical correlation exists between\nthe occurrences of a pair of events, there is no reason for supposing\nthere to be a causal connection between them. Conversely, if we were\nto allow the possibility of a causal connection between statistically\nuncorrelated events, we would have a particularly hard task\ndetermining which of these uncorrelated sets could be harboring a\nconspiratorial causal connection that hides the correlation. The\nfaithfulness assumption is thus a principle of parsimony—the\nsimplest explanation for a pair of statistically uncorrelated events\nis that they are causally independent—much the same way that\nSpekkens’ (2005) definition of contextuality is, too (see\n §3.2);\n indeed, Cavalcanti (2018) argues that contextuality can be construed\nas a form of fine-tuning. \nThere are, however, well-known examples of systems that potentially\nshow a misapplication of the faithfulness assumption. One such\nexample, originating in Hesslow (1976), involves a contraceptive pill\nthat can cause thrombosis while simultaneously lowering the chance of\npregnancy, which can also cause thrombosis. As Cartwright (2001: 246)\npoints out, given the right weight for these process, it is\nconceivable that the net effect of the pills on the frequency of\nthrombosis be zero. This is a case of “cancel ling paths”,\nwhere the effect of two or more causal routes between a pair of\nvariables cancels to achieve statistical independence. In a case such\nas this, since we can have independent knowledge of the separate\ncausal mechanisms involved here, there are grounds for arguing that\nthere really is a causal connection between the variables despite\ntheir statistical independence. Thus, it is certainly possible to\nimagine a scenario in which the faithfulness assumption could lead us\nastray. However, in defense of the general principle, an example such\nas this clearly contains what Wood and Spekkens refer to as fine\ntuning; the specific weights for these processes would need to match\nprecisely to erase the statistical dependence, and such a balance\nwould generally be thought as unstable (any change in background\nconditions, etc. would reveal the causal connection in the form of a\nstatistical dependence). \nNäger (2016) raises the possibility that unfaithfulness can occur\nwithout conspiratorial fine tuning if the unfaithfulness arises in a\nstable way. In the quantum context, Näger suggests that the\nfine-tuning mechanism is what he calls “internal cancel ling\npaths”. This mechanism is analogous to the usual cancel ling\npaths scenario, but the path-cancel ling mechanism does not manifest\nat the level of variables, but at the level of values. On this view,\nsuch fine tuning would occur as a result of the particular causal\nand/or nomological process that governs the system, and it is in this\nsense that the cancel ling paths mechanism is internal, and it is the\nfact that the mechanism is internal that renders the associated fine\ntuning stable to external disturbances. Thus  \nif the laws of nature are such that disturbances always alter the\ndifferent paths in a balanced way, then it is physically impossible to\nunbalance the paths. (Näger 2016: 26)  \nThe possibility raised by Näger would circumvent the problem that\nviolations of faithfulness ultimately undermine our ability to make\nsuitable inferences of causal independence based on statistical\nindependence by allowing only a specific kind of\nunfaithfulness—a principled or law-based unfaithfulness that is\n“internal” and is thus stable to background\nconditions—which is much less conspiratorial, as the fine-tuning\nis a function of the specific process involved. Evans (2018) argues\nthat a basic retrocausal model of the sort envisaged by Costa de\nBeauregard (see\n §1)\n employs just such an internal cancel ling paths explanation to\naccount for the unfaithful (no signaling) causal channels. See also\nAlmada et al. (2016) for an argument that fine tuning in the quantum\ncontext is robust and arises as a result of symmetry\nconsiderations. \nRecall\n (§3.2)\n that Spekkens’ (2005) claim that no noncontextual ontological\nmodel can reproduce the observed statistics of quantum theory based on\nhis principle of parsimony (that there can be no ontological\ndifference without operational difference) was sidestepped by\nretrocausal approaches due to the explicit assumption of the\nontological models framework that the ontic state is independent of\nthe measurement procedure (i.e., that there is no retrocausality). It\nwas noted there the possibility that Spekkens’ principle of\nparsimony might be recast to apply more generally to retrocausal\nmodels. Shrapnel and Costa (2018) achieve just this in a no-go theorem\nthat applies to any exotic causal structure used to sidestep the\nontological models framework, including retrocausal accounts,\nrendering such models contextual after all. \nShrapnel and Costa’s result is based on a generalization of the\nontological models framework which replaces the operational\npreparation, transformation, and measurement procedures with the\ntemporally and causally neutral notions of local controllables and\nenvironmental processes that mediate correlations between different\nlocal systems, and generate the joint statistics for a set of events.\n“These include any global properties, initial states, connecting\nmechanisms, causal influence, or global dynamics” (2018: 5).\nFurthermore, they replace the ontic state \\(\\lambda\\) with the ontic\n“process” \\(\\omega\\): \nour ontic process captures the physical properties of the world that\nremain invariant under our local operations. That is, although we\nallow local properties to change under specific operations,\nwe wish our ontic process to capture those aspects of reality\nthat are independent of this probing. (2018: 8) \nAs a result, the notion of \\(\\lambda\\)-mediation (encountered in\n §4.1)\n is replaced by the notion of \\(\\omega\\)-mediation, in which the ontic\nprocess \\(\\omega\\) completely specifies the properties of the\nenvironment that mediate correlations between regions, and screens off\noutcomes produced by local controllables from the rest of the\nenvironment. Shrapnel and Costa (2018: 9) define the notion of\n“instrument noncontextuality” as a law of parsimony (along\nthe lines of Spekkens’ own definition of noncontextuality):\n“Operationally indistinguishable pairs of outcomes and local\ncontrollables should remain indistinguishable at the ontological\nlevel”. They then show that no instrument noncontextual model\ncan reproduce the quantum statistical predictions. \nCrucially, what is contextual is not just the traditional notion of\n“state”, but any supposedly objective feature of the\ntheory, such as a dynamical law or boundary condition. (2018: 2) \nSince preparations, transformations, and measurements have been\nreplaced by local controllables, there is no extra assumption in\nShrapnel and Costa’s framework that \\(\\omega\\) is correlated\nwith some controllables but independent of others. Thus the usual\nroute out of the ontological models framework, and so the no-go\ntheorems of\n §3,\n open to retrocausal approaches—that the framework assumes no\nretrocausality—is closed off in the Shrapnel-Costa theorem,\nrendering retrocausal approaches contextual along with the rest of the\nmodels captured by the ontological models framework. \nThis presents a significant worry for retrocausal approaches to\nquantum theory. If the main motivation for pursing the hypothesis of\nretrocausality is to recapture in some sense a classical ontology for\nquantum theory (see\n §3.4),\n then the Shrapnel-Costa theorem has made this task either impossible,\nor beholden to the possibility of some further story explaining how\nthe contextual features of the model arise from some noncontextual\nfooting. On this latter point, it is difficult to see how this story\nmight be told without significantly reducing the ideological economy\nof the conceptual framework of retrocausality, again jeopardizing a\npotential virtue of retrocausality. \nAs mentioned above\n (§7.2),\n contextuality can be construed as a form of fine tuning (Cavalcanti\n2018), especially when the demand for noncontextuality is understood\nas a requirement of parsimony, as above. The worries raised in this\nsection and the last underline the fact that the challenge to account\nfor various types of fine tuning is the most serious principled\nobstacle that retrocausal accounts continue to face.","contact.mail":"p.evans@uq.edu.au","contact.domain":"uq.edu.au"}]
