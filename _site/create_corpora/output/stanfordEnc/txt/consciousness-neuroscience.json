[{"date.published":"2018-10-09","url":"https://plato.stanford.edu/entries/consciousness-neuroscience/","author1":"Wayne Wu","entry":"consciousness-neuroscience","body.text":"\n\n\nConscious experience in humans depends on brain activity, so\nneuroscience will contribute to explaining consciousness. What would\nit be for neuroscience to explain consciousness? How much progress has\nneuroscience made in doing so? What challenges does it face? How can\nit meet those challenges? What is the philosophical significance of\nits findings? This entry addresses these and related questions.\n\n\nTo bridge the gulf between brain and consciousness, we need neural\ndata, computational and psychological models, and philosophical\nanalysis to identify principles to connect brain activity to conscious\nexperience in an illuminating way. This entry will focus on\nidentifying such principles without shying away from the neural\ndetails. The notion of neuroscientific explanation here conceives\nof it as providing informative answers to concrete questions that can\nbe addressed by neuroscientific approaches. Accordingly, the theories\nand data to be considered will be organized around constructing\nanswers to two questions (see\n section 1.4\n for more precise formulations):\n\nGeneric Consciousness: How might neural properties explain when a\nstate is conscious rather than not?\n\nSpecific Consciousness: How might neural properties explain what\nthe content of a conscious state is?\n\nA challenge for an objective science of consciousness is to dissect an\nessentially subjective phenomenon. As investigators cannot experience\nanother subject’s conscious states, they rely on the\nsubject’s observable behavior to track consciousness. Priority\nis given to a subject’s introspective reports as these express\nthe subject’s take on her experience. Introspection thus\nprovides a fundamental way, perhaps the fundamental way, to\ntrack consciousness. That said, consciousness pervasively influences\nhuman behavior, so other forms of behavior beyond introspective\nreports provide a window on consciousness. How to leverage disparate\nbehavioral evidence is a central issue. \n\n\nThe term “neuroscience” covers those scientific fields\nwhose explanations advert to the properties of neurons, populations of\nneurons, or larger parts of the nervous\n system.[1]\n This includes, but is not limited to, psychologists’ use of\nvarious neuroimaging methods to monitor the activity of tens of\nmillions of neurons, computational theorists’ modelling of\nbiological and artificial neural networks, neuroscientists’ use\nof electrodes inserted into brain tissue to record neural activity\nfrom individual or populations of neurons, and clinicians’ study\nof patients with altered conscious experiences in light of damage to\nbrain areas.\n\n\nGiven the breadth of neuroscience so conceived, an overview of\nsufficient depth must restrict breadth. On the neuroscience side, this\nreview focuses on the central nervous system and the electrical\nproperties of neurons, particularly in the cerebral cortex. On the\nside of consciousness, it focuses on perceptual consciousness, with\nemphasis on vision. This is not because visual consciousness is more\nimportant than other forms of consciousness. Rather, the level of\ndetail in empirical work on vision often speaks more comprehensively\nto the issues that we shall confront.\n\n\nThat said, there are many forms of consciousness that we will not\ndiscuss. Some are covered in other entries\nsuch as split brain phenomena (see the entry on\n the unity of consciousness,\n section 4.1.1), animal consciousness (see the entry on\n animal consciousness),\nand neural correlates of the will and agency (see the\nentry on \n agency,\n section 5). In addition, this entry will not discuss the neuroscience\nof consciousness in audition, olfaction or gustation; disturbed\nconsciousness in mental disorders such as schizophrenia; conscious\naspects of pleasure, pain and the emotions; the phenomenology of\nthought; the neural basis of dreams; and modulations of consciousness\nduring sleep and anesthesia among other issues. These are important\ntopics, and the principles and approaches highlighted in this\ndiscussion will apply to many of these domains.\n\nIt will be helpful to grasp the basic anatomy of the brain. A central\ndistinction concerns the difference between the cerebral\ncortex and the subcortex. The cortex is divided into\ntwo hemispheres, left and right, each of which can be divided into\nfour lobes: frontal, parietal, temporal and occipital. \nFigure 1: The Cerebral Cortex and Salient\nAreas\n\nFigure Legend: The four lobes of the primate brain, shown for the left\nhemisphere. Some areas of interest are highlighted. Abbreviations:\nPFC: prefrontal cortex; IT: inferotemporal cortex; S1: primary\nsomatosensory cortex; IPL and SPL: Inferior and Superior Parietal\nLobule; MST: medial superior temporal visual area; MT: middle temporal\nvisual area (also called V5 in humans); V1: primary visual cortex;\nV2-V6 consist of additional visual areas. \nThe discussion that follows will highlight specific areas of cortex\nincluding the prefrontal cortex that will figure in\ndiscussions of confidence\n (section 2.2),\n the global neuronal workspace\n (section 3.1)\n and higher order theories\n (section 3.3);\n the dorsal visual stream that projects into parietal cortex\nand the ventral visual stream that projects into temporal\ncortex including visual areas specialized for processing places,\nfaces, and word forms (see sections\n 2.6\n on places,\n 4.1\n on visual agnosia and\n 5.3.3\n on seeing words); primary somatosensory cortex S1 (see\nsection\n 5.3.2\n on tactile sensation); and early visual areas in the occipital cortex\nincluding the primary visual area, V1 (see sections\n 4.2\n on blindsight and\n 5.2\n on binocular rivalry) and a motion sensitive area V5, also\nknown as the middle temporal area (MT;\n section 5.3.1\n on seeing motion). Beneath the cortex is the subcortex, divided into\nthe forebrain, midbrain, and hindbrain, which covers many regions\nalthough our discussion will largely touch on the superior\ncolliculus and the thalamus, two areas that play an\nimportant role in visual processing. \nA neuroscientific explanation of consciousness adduces properties of\nthe brain, typically the brain’s electrical properties. A\nsalient phenomenon is neural signaling through action\npotentials or spikes. A spike is a large change in\nelectrical potential across a neuron’s cellular membrane which\ncan be transmitted between neurons that form a neural circuit. For a\nsensory neuron, the spikes it generates are tied to its receptive\nfield. For example, in a visual neuron, its receptive field is\nunderstood in spatial terms and corresponds to that area of external\nspace where an appropriate stimulus triggers the neuron to spike.\nGiven this correlation between stimulus and spikes, the latter carries\ninformation about the former. Information processing in sensory\nsystems involves processing of information regarding stimuli within\nreceptive fields. \nWhich electrical property provides the most fruitful explanatory basis\nfor understanding consciousness remains an open question. For example,\nwhen looking at a single neuron, neuroscientists are not interested in\nspikes per se but the spike rate generated by a neuron per unit time.\nYet spike rate is one among many potentially relevant neural\nproperties. Consider the blood oxygen level dependent signal (BOLD)\nmeasure in functional magnetic resonance imaging (fMRI). The BOLD\nsignal is a measure of changes in blood flow in the brain when neural\ntissue is active and is postulated to be a function of electrical\nproperties at a different part of a neuron than that part tied to\nspikes. Specifically, given a synapse which is the connection\nbetween two neurons to form a basic circuit motif, spikes are tied to\nthe presynaptic side while the BOLD signal is thought to be a\nfunction of electrical changes on the postsynaptic side\n(signal flow is from pre to post). Furthermore, neuroscientists are\ntypically not interested in the response of a single neuron but rather\nthat of a population of neurons, of whole brain regions, and/or their\ninteractions. Higher order properties of brain regions include the\nlocal field potential generated by populations of neurons and\ncorrelated activity such as synchrony between activity in\ndifferent areas of the brain (neural oscillations were postulated to\nbe central to consciousness by Crick & Koch 1990). \nThe number of neural properties potentially relevant to explaining\nmental phenomena is dizzying. This review focuses on the facts that\nneural sensory systems carry information about the subject’s\nenvironment and that neural information processing can be tied to a\nnotion of neural representation. How precisely to understand\nneural representation is itself a vexed question (Cao 2012, 2014; Shea\n2014), but we will deploy a simple assumption with respect to spikes\nwhich can be reconfigured for other properties: where a sensory neuron\ngenerates spikes when a stimulus is placed in its receptive field, the\nspikes carry information about the stimulus (strictly speaking, about\na random variable). Information as used in neuroscience is\ntypically not a semantic notion, but bearing in mind that caveat, it\nwill simplify matters to speak of a sensory neuron’s activity as\nrepresenting the relevant aspect of the stimulus that drives\nthe neuron’s response (e.g., direction of motion or intensity of\na\n sound).[2]\n This way of speaking is imprecise, so we shall return to neural\nrepresentation in the final section when discussing how neural\nrepresentations might explain conscious contents. \nAn important distinction separates access consciousness from\nphenomenal consciousness (Block 1995). “Phenomenal\nconsciousness” refers to those properties of experience that\ncorrespond to what it is like for a subject to have those\nexperiences (Nagel 1974 and the entry on\n qualia).\n These features are apparent to the subject from the inside, so\ntracking them arguably depends on one’s having the relevant\nexperience. For example, one understands what it is like to see red\nonly if one has visual experiences of the relevant type (Jackson\n1982). \nAs noted earlier, introspection is the first source of evidence about\nconsciousness. Introspective reports bridge the subjective and\nobjective. They serve as a behavioral measure that expresses the\nsubject’s own take on what it is like for her in having an\nexperience. While there have been recent concerns about the\nreliability or empirical usefulness of introspection (Schwitzgebel\n2011; Irvine 2012a), there are plausibly many contexts where\nintrospection is reliable (Spener 2015; see Irvine 2012b for an\nextended discussion of introspection in consciousness science; for\nphilosophical theories, see Smithies & Stoljar 2012). \nIntrospective reports demonstrate that the subject can access the\ntargeted conscious state. That is, the state is\naccess-conscious: it is accessible for use in reasoning,\nreport, and the control of action. Talk of access-consciousness must\nkeep track of the distinction between actual access versus\naccessibility. When one reports on one’s conscious\nstate, one accesses the state. Thus, access consciousness provides\nmuch of the evidence for empirical theories of consciousness. Still,\nit seems plausible that a state can be conscious even if one does not\naccess it in report so long as that state is accessible. One\ncan report it. Access-consciousness is usually defined in\nterms of this dispositional notion of accessibility.  \nWe must also consider the type of access/accessibility. Block’s\noriginal characterization of access-consciousness emphasized\naccessibility in terms of the rational control of behavior,\nso we can summarize his account as follows: \nA representation is access-conscious if it is poised for free use in\nreasoning and for direct “rational” control of action and\nspeech. \nRational access contrasts with a broader conception of\nintentional access that takes a mental state to be\naccess-conscious if it can inform goal-directed or intentional\nbehavior including behavior that is not rational or done for a reason.\nThis broader notion allows for additional measurable behaviors as\nrelevant in assessing phenomenal consciousness especially in\nnon-linguistic animals. So, if access provides us with evidence for\nphenomenal consciousness, this can be (a) through introspective\nreports; (b) through rational behavior, (c) through intentional\nbehavior including nonrational behavior. Indeed, in certain contexts,\nreflexive behavior provides measures of consciousness\n (section 2.3). \nExplanations answer specific questions. Two questions regarding\nphenomenal consciousness frame this entry: Generic and Specific. The\nfirst focuses on a mental state’s being conscious in\ngeneral as opposed to not being conscious. Call this property\ngeneric consciousness, a property shared by specific\nconscious states such as seeing a red rose, feeling a touch, or being\nangry. Thus: \nGeneric Consciousness: What conditions/states N of\nnervous systems are necessary and (or) sufficient for a mental state,\nM, to be conscious as opposed to not? \nIf there is such an N, then the presence of N entails\nthat an associated mental state M is conscious and (or) its\nabsence entails that M is unconscious. \nA second focus will be on the content of consciousness, say that\nassociated with a perceptual experience’s being of some\nperceptible X. This yields a question about specific contents\nof consciousness such as experiencing the motion of an object (see\n section 5.3.1)\n or a vibration on one’s finger (see\n section 5.3.2): \nSpecific Consciousness: What neural states or properties are\nnecessary and/or sufficient for a conscious perceptual state to have\ncontent X rather than Y? \nExpanding a bit, perceptual states have intentional content and\nspecifying that content is one way of describing what that state is\nlike. In introspectively accessing her conscious states, a subject\nreports what her experience is like by reporting what she experiences.\nThus, the subject can report seeing an object moving, changing color,\nor being of a certain kind (e.g., a mug) and thus specify the content\nof the perceptual state. Discussion of specific consciousness will\nfocus on perceptual states described as consciously perceiving\nX where X can be a particular such as a face, a property\nsuch as the frequency of a vibration or a proposition, say seeing that\nan object moves in a certain direction. \nMany philosophers take perceiving X to be perceptually\nrepresenting X. Intentional content on this reading is a\nsemantic notion, and this suggests a linking principle tying conscious\ncontent to the brain: Perceptually representing X is based on\nneural representations of X. The “based on”\nlocution hedges on the precise relation between neural contents and\nconscious contents, but a simple relation is identity: neural content\nis perceptual\n content.[3]\n This principle expresses a type of first-order\nrepresentationalism about phenomenal content, a topic we return\nto in\n section 5;\n see also the entry on\n representational theories of consciousness.\n The principle explains specific consciousness by appeal to neural\nrepresentational content. \nPosing a clear question involves grasping its possible answers and in\nscience, this is informed by identifying experiments that can provide\nevidence for such answers. The emphasis on necessary and sufficient\nconditions in our two questions indicates how to empirically test\nspecific proposals. To test sufficiency, one would aim to produce or\nmodulate a certain neural state and then demonstrate that\nconsciousness of a certain form arises. To test necessity, one would\neliminate a certain neural state and demonstrate that consciousness is\nabolished. Notice that such tests go beyond mere correlation between\nneural states and conscious states (see\n section 1.6\n on neural correlates and sections\n 2.2,\n 4 and\n 5\n for tests of necessity and sufficiency). \nIn many experimental contexts, the underlying idea is causal\nnecessity and sufficiency. However, if \\(A=B\\), then A’s\npresence is also necessary and sufficient for B’s\npresence since they are identical. Thus, a brain lesion that\neliminates N and thereby eliminates conscious state S\nmight do so either because N is causally necessary for S\nor because \\(N=S\\). An intermediate relation is that N\nconstitutes or grounds S which does not imply\nthat \\(N=S\\) (see the entry on\n metaphysical grounding).\n Whichever option holds for S, the first step is to find\nN, a neural correlate of consciousness\n (section 1.6). \nIn what follows, to explain generic consciousness, various global\nproperties of neural systems will be considered\n (section 3)\n as well as specific anatomical regions that are tied to conscious\nversus unconscious vision as a case study\n (section 4).\n For specific consciousness, fine-grained manipulations of neural\nrepresentations will be examined that plausibly shift and modulate the\ncontents of perceptual experience\n (section 5). \nDavid Chalmers presents the hard problem as follows: \nIt is undeniable that some organisms are subjects of experience. But\nthe question of how it is that these systems are subjects of\nexperience is perplexing. Why is it that when our cognitive systems\nengage in visual and auditory information-processing, we have visual\nor auditory experience: the quality of deep blue, the sensation of\nmiddle C? How can we explain why there is something it is like to\nentertain a mental image, or to experience an emotion? It is widely\nagreed that experience arises from a physical basis, but we have no\ngood explanation of why and how it so arises. Why should physical\nprocessing give rise to a rich inner life at all? It seems objectively\nunreasonable that it should, and yet it does. If any problem qualifies\nas the problem of consciousness, it is this one. (Chalmers\n1995: 212) \nThe Hard Problem can be specified in terms of generic and specific\nconsciousness (Chalmers 1996). In both cases, Chalmers argues that\nthere is an inherent limitation to empirical explanations of\nphenomenal consciousness in that empirical explanations will be\nfundamentally either structural or functional, yet phenomenal\nconsciousness is not reducible to either. This means that there will\nbe something that is left out in empirical explanations of\nconsciousness, a missing ingredient (see also the explanatory\ngap [Levine 1983]). \nThere are different responses to the hard problem. One response is to\nsharpen the explanatory targets of neuroscience by focusing on what\nChalmers calls structural features of phenomenal\nconsciousness, such as the spatial structure of visual experience, or\non the contents of phenomenal consciousness. When we assess\nexplanations of specific contents of consciousness, these focus on the\nneural representations that fix conscious contents. These explanations\nleave open exactly what the secret ingredient is that shifts a state\nwith that content from unconsciousness to consciousness. On\ningredients explaining generic consciousness, a variety of options\nhave been proposed (see\n section 3),\n but it is unclear whether these answer the Hard Problem, especially\nif any answer to that the Problem has a necessary condition that the\nexplanation must conceptually close off certain possibilities, say the\npossibility that the ingredient could be added yet consciousness not\nignite as in a zombie, a creature without phenomenal consciousness\n(see the entry on\n zombies).\n Indeed, some philosophers deny the hard problem (see Dennett 2018 for\na recent statement). Patricia Churchland urges: “Learn the\nscience, do the science, and see what happens” (Churchland 1996:\n408). \nPerhaps the most common attitude for neuroscientists is to set the\nhard problem aside. Instead of explaining the existence of\nconsciousness in the biological world, they set themselves to\nexplaining generic consciousness by identifying neural properties that\ncan turn consciousness on and off and explaining specific\nconsciousness by identifying the neural representational basis of\nconscious contents. \nModern neuroscience of consciousness has attempted to explain\nconsciousness by focusing on neural correlates of\nconsciousness or NCCs (Crick & Koch 1998, 2003). Identifying\ncorrelates is an important first step in understanding consciousness,\nbut it is an early step. After all, correlates are not necessarily\nexplanatory in the sense of answering specific questions posed by\nneuroscience. That one does not want a mere correlate was recognized\nby Chalmers who defined an NCC as follows: \nAn NCC is a minimal neural system N such that there is a\nmapping from states of N to states of consciousness, where a\ngiven state of N is sufficient under conditions C, for\nthe corresponding state of consciousness. (Chalmers 2000: 31) \nSimilarly, Christof Koch and others speak of “the minimal neural\nmechanisms jointly sufficient for any one specific conscious\nexperience” (Koch et al. 2016: 307). One wants a minimal neural\nsystem since, crudely put, the brain is sufficient for consciousness\nbut to point this out is hardly to explain consciousness even if it\nprovides an answer to questions about sufficiency. There is, of\ncourse, much more to be said that is informative even if one does not\ndrill down to a “minimal” neural system which is tricky to\ndefine or operationalize (see Chalmers 2000 for discussion; for\ncriticisms of the NCC approach, see Noë & Thompson 2004; for\ncriticisms of Chalmers’ definition, see Fink 2016). \nThe emphasis on sufficiency goes beyond mere correlation, as\nneuroscientists aim to answer more than the question: What is a neural\ncorrelate for conscious phenomenon C? For example,\nChalmers’ and Koch’s emphases on sufficiency indicate that\nthey aim to answer the question: What neural phenomenon is sufficient\nfor consciousness? Perhaps more specifically: What neural phenomenon\nis causally sufficient for consciousness? Accordingly, talk of\n“correlate” is unfortunate since sufficiency implies\ncorrelation but not vice versa. For example, correlation does not\nimply causal sufficiency, so not every correlate will be explanatory\nin the sense of answering Chalmers’ and Koch’s question.\nAfter all, assume that the NCC is type identical to a conscious state.\nThen many neural states will correlate with the conscious state: (1)\nthe NCC’s typical effects, (2) its typical causes, and (3)\nstates that are necessary for the NCCs obtaining (e.g., the presence\nof sufficient oxygen). Thus, some correlated effects will not be\nexplanatory. For example, citing the effects of consciousness will not\nprovide causally sufficient conditions for consciousness. \nWhile many theorists are focused on explanatory correlates, it is not\nclear that the field has always grasped this, something recent\ntheorists have been at pains to emphasize (Graaf, Hsieh, & Sack\n2012; Aru et al. 2012; Koch et al. 2016). In other contexts,\nneuroscientists speak of the neural basis of a phenomenon\nwhere the basis does not simply correlate with the phenomenon but also\nexplains and possibly grounds it. However, talk of correlates is\nentrenched in the neuroscience of consciousness, so one must remember\nthat the goal is to find the subset of neural correlates that are\nexplanatory, in answering concrete questions. Reference to neural\ncorrelates in this entry will always mean neural explanatory correlate\nof consciousness (on occasion, I will speak of these as the neural\nbasis of consciousness). That is, our two questions about\nspecific and generic consciousness focus the discussion on\nneuroscientific theories and data that contribute to explaining them.\nThis project allows that there are limits to neural explanations of\nconsciousness, precisely because of the explanatory gap (Levine\n1983). \nSince studying consciousness requires that scientists track its\npresence, it will be important to examine various methods used in\nneuroscience to isolate and probe conscious states. \nScientists primarily study phenomenal consciousness through subjective\nreports. We can treat reports in neuroscience as conceptual in that\nthey express how the subject recognizes things to be, whether\nregarding what they perceive (perceptual or observational reports, as\nin psychophysics) or regarding what mental states they are in\n(introspective reports). A report’s conceptual content can be\nconveyed in words or through another overt behavior whose significance\nis fixed within an experimental context (e.g., pressing a button to\nindicate that a stimulus is present or that one sees it). Subjective\nreports of conscious states draw on distinctively first-personal\naccess to that state. The subject introspects. \nIntrospection raises questions that science has only recently begun to\naddress systematically in large part because of longstanding suspicion\nregarding introspective methods. Early modern psychology relied on\nintrospection to parse mental processes but ultimately abandoned it\ndue to worries about introspection’s reliability (Feest 2012;\nSpener forthcoming). Introspection was judged to be an unreliable\nmethod for addressing questions about mental processing. To address\nthese worries, we must understand how introspection works, but unlike\nmany other psychological capacities, we lack detailed models of\nintrospection of consciousness (Feest 2014; for theories of\nintrospecting propositional attitudes, see Nichols & Stich 2003;\nGoldman 2006; Heal 1996; Carruthers 2011). This makes it difficult to\naddress long-standing worries about introspective reliability\nregarding consciousness. \nIn science, questions raised about the reliability of a method are\nanswered by calibrating and testing the method. This calibration has\nnot been done with respect to the type of introspection commonly\npracticed by philosophers. Such introspection has revealed many\nphenomenal features that are targets of active investigation such as\nthe phenomenology of mineness (Ehrsson 2009), sense of agency (Bayne\n2011; Vignemont & Fourneret 2004; Marcel 2003; Horgan, Tienson,\n& Graham 2003); transparency (Harman 1990; Tye 1992),\nself-consciousness (Kriegel 2003: 122), cognitive phenomenology (Bayne\n& Montague 2011); phenomenal unity (Bayne & Chalmers 2003)\namong others. A scientist might worry that philosophical introspection\nmerely recycles rejected methods of a century ago, indeed without the\nstringent controls or training imposed by earlier psychologists. How\ncan we ascertain and ensure the reliability of introspection in the\nempirical study of consciousness? \nOne way to address the issue is to connect introspection to attention.\nPhilosophical conceptions of introspective attention construe it as\ncapable of directly focusing on phenomenal properties and experiences.\nAs this idea is fleshed out, however, it is clearly not a form of\nattention studied by cognitive science, for the posited direct\nintrospective attention is neither perceptual attention nor what\npsychologists call internal attention (e.g., the retrieval of thought\ncontents as in memory recollection; (Chun, Golomb, & Turk-Browne\n2011)). Calibrating introspection as it is used in the science of\nconsciousness would benefit from concrete models of introspection,\nmodels we lack (see Spener 2015, for a general form of\ncalibration). \nOne philosophical tradition links introspection to perceptual\nattention, and this allows construction of concrete models informed by\nscience. The intuitive idea is expressed in Harman’s\nobservation: \nLook at a tree and try to turn your attention to intrinsic features of\nyour visual experience. I predict you will find that the only features\nthere to turn your attention to will be features of the presented\ntree, including relational features of the tree “from\nhere”. (Harman 1990: 39)  \nThis is related to a proposal inspired by Gareth Evans (1982): in\nintrospecting perceptual states, say judging that one sees an object,\none draws on the same perceptual capacities used to answer the\nquestion whether the object is present. In introspection, one then\nappends a further concept of “seeing” to one’s\nperceptual\n report.[4]\n Thus, instead of simply reporting that a red stimulus is present, one\nreports that one sees the red stimulus. Paradoxically, introspection\nrelies on externally directed perceptual attention, but as noted\nearlier, identifying what one perceives is a way of characterizing\nwhat one’s perception is like, so this “outward”\nperspective can provide information about the inner. Further, the\nadvantage of this proposal is that questions of reliability come down\nto questions of the reliability of psychological capacities that can\nbe empirically assessed, say perceptual, attentional and conceptual\nreliability. For example, Peters and Lau (2015) showed that accuracy\nin judgments about the visibility of a stimulus, the introspective\nmeasure, coincided with accuracy in judgments about stimulus\nproperties, the objective measure (see also Rausch & Zehetleitner\n2016). \nIntrospection can be reliable. Successful clinical practice relies on\naccurate introspection as when dealing with pain or correcting blurry\nvision in optometry. The success of medical interventions suggests\nthat patient reports of these phenomenal states are reliable. Further,\nin many of the examples to be discussed, the perceptual\nattention-based account provides a plausible cognitive model of\nintrospection. Subjects report on what they perceptually experience by\nattending to the object of their experience, and where perception and\nattention are reliable, a plausible hypothesis is that their\nintrospective judgments will be reliable as well. Accordingly, I\nassume the reliability of introspection in the empirical studies to be\ndiscussed. Still, given that no scientist should assert the\nreliability of a method without calibration, introspection must be\nsubject to the same standards. There is more work to be done. \nIntrospection illustrates a type of cognitive access, for a state that\nis introspected is access conscious. This raises a question that has\nepistemic implications: is access consciousness necessary for\nphenomenal consciousness? If it is not, then there can be phenomenal\nstates that are not access conscious, so are in principle not\nreportable. That is, phenomenal consciousness can overflow\naccess consciousness (Block 2007). \nAccess is tied to attention. For example, the Global Workspace theory\nof consciousness understands consciousness in terms of access\n (section 3.1)\n where the accessibility of a perceived object requires attention to\nthe object (attention puts an object, namely a representation of it,\nin the global workspace). So, the necessity of attention for\nphenomenal consciousness is entailed by the necessity of access for\nphenomenal\n consciousness.[5]\n In contrast, recurrent processing theory holds that there can be\nphenomenal states that are not accessible\n (section 3.2). \nMany scientists of consciousness take there to be evidence for no\nphenomenal consciousness without access and little if no evidence of\nphenomenal consciousness outside of access. An important set of\nstudies focuses on the thesis that attention is a necessary gate for\nphenomenal consciousness, where attention is tied to access. Call this\nthe gatekeeping thesis. To assess that evidence, we must ask:\nwhat is attention? An uncontroversial conception of attention is that\nit is subject selection of a target to inform task performance (Wu\n2014b). The experimental studies thought to support the necessity of\nattention for consciousness draw on this\n conception.[6]\n For example, in inattentional blindness paradigms (Mack\n& Rock 1998), attention is directed by asking subjects to perform\na task on target T while a surprising stimulus S is\npresented. This approach tests necessity by ensuring through task\nperformance that the subject is not attending to S. One then\nmeasures whether the subject is aware of S by observing whether\nthe subject reports it. If the subject does not report S, then\nthe hypothesis is that failure of attention to S explains the\nfailure of conscious awareness of S and hence the failure of\nreport. \nA well-known experiment asks subjects to attend to the number of\npasses of a ball thrown by players in white shirts while ignoring a\nsecond ball passed by players in black shirts (Simons & Chabris\n1999). During the task, a person in a gorilla costume walks across the\nscene. Half of the subjects fail to notice and report the gorilla,\nthis being construed as evidence for the absence of visual awareness\nof the gorilla. Hence, failure to attend to the gorilla is said to\nrender subjects phenomenally blind to it. Similar claims are made in\nchange blindness where subjects fail to detect the difference\nbetween two similar pictures (Simons & Ambinder 2005), the\nattentional blink where subjects fail to detect a stimulus\npresented immediately after detecting a prior stimulus (Dux &\nMarois 2009; Martens & Wyble 2010), and hemispatial\nneglect where patients fail to report objects in a part of their\nvisual field that they cannot attend to due to brain lesions. \nThe gatekeeping thesis holds that attention is necessary for\nconsciousness, so that removing it from a target eliminates\nconsciousness of it. Yet there is a flaw in the methodology. To report\na stimulus, one must attend to it, i.e., select it for report. The\nexperimental logic requires eliminating attention to a stimulus\nS to test if attention is a necessary condition for\nconsciousness (e.g., eliminating attention to the gorilla by\ndistracting the subject with the ball). Yet even if the subject were\nconscious of S, when attention to S is eliminated, one\ncan predict that the subject will fail to act (report) on\nS since attention is necessary for report. The observed results\nare actually consistent with the subject being conscious of S\nwithout attending to it, and thus are neutral between overflow and\ngatekeeping. Instead, the experiments concern parameters for the\ncapture of attention and not consciousness. \nWhile those antagonistic to overflow have argued that it is not\nempirically testable (M.A. Cohen & Dennett 2011), gatekeeping\nmight be equally untestable. After all, to test the necessity of\nattention for consciousness, we must eliminate attention to a target\nwhile gathering evidence for the absence of consciousness. Yet if\ngathering evidence for consciousness requires attention, then in\nfulfilling the conditions for testing the necessity of attention, we\nundercut the access needed to substantiate the absence of\nconsciousness (Wu 2017; for a monograph length discussion of attention\nand consciousness, see Montemayor & Haladjian 2015). How then can\nwe gather the required evidence to assess competing theories? \nOne response to is to draw on no-report paradigms which\nmeasure reflexive behaviors correlated with conscious states to\nprovide a window on the phenomenal that is independent of access\n(Lumer & Rees 1999; Tse et al. 2005). For example, Frässle et\nal. (2014) demonstrate that certain occular reflexes are correlated\nwith perceptual experience in binocular rivalry (Naber, Frässle,\n& Einhäuser 2011). They presented subjects either with\nstimuli moving in opposite directions or stimuli of different\nluminance values, one stimulus in each pair presented separately to\neach eye. This induces binocular rivalry, an alternation in which of\nthe two stimuli is visually experienced (see\n section 5.2).\n Where the stimuli involved motion, subjects demonstrated\noptokinetic nystagmus where the eye slowly moves in the\ndirection of the stimulus and then makes a fast, corrective saccade\n(ballistic eye movement) in the opposite direction. Frässle et\nal. observed that optokinetic nystagmus tracked the perceived\ndirection of the stimulus as reported by the subject. Similarly, for\nstimuli of different luminance, the pupils would dilate, being wider\nfor dimmer stimuli, and narrower for brighter stimuli, again\ncorrelating with subjective reports of the intensity of the\nstimulus. \nNo-report paradigms use reflexive responses to track the\nsubject’s perceptual experience in the absence of explicit\n(conceptualized) report. They seem to provide a way to track\nphenomenal consciousness even when access is eliminated. This would\nbroaden the evidential basis for consciousness beyond introspection\nand indeed, beyond intentional behavior (our “broad”\nconception of access). Yet no-report paradigms do not circumvent\nintrospection (Overgaard & Fazekas 2016). For example, optokinetic\nnystagmus’s usefulness depends on validating its correlation\nwith alternating experience given subjective reports. Once it\nis validated, monitoring this reflex can provide a way to substitute\nfor subjective reports within that paradigm. One cannot, however,\nsimply extend the use of no-report paradigms outside the behavioral\ncontexts within which the method is validated. With each new\nexperimental context, we must revalidate the measure with\nintrospective report. \nCan we use no report paradigms to address whether access is necessary\nfor phenomenal consciousness? A likely experiment would be one that\nvalidates no-report correlates for some conscious phenomenon P\nin a concrete experimental context C. With this validation in\nhand, one then eliminates accessibility and attention with respect to\nP in C. If the no-report correlate remains, would this\nclearly support overflow? Perhaps, though gatekeeping theorists likely\nwill respond that the result does not rule out the possibility that\nphenomenal consciousness disappears with access consciousness\ndespite the no-report correlate remaining. For example, the\nreflexive response and phenomenal consciousness might have a common\ncause that remains even if phenomenal consciousness is selectively\neliminated by removing access. \nGiven worries about calibrating introspection, researchers have asked\nsubjects to provide a different metacognitive assessment of conscious\nstates via reports about confidence (Grimaldi, Lau, &\nBasso 2015; Pouget, Drugowitsch, & Kepecs 2016). A standard\napproach is to have subjects perform a task, say perceptual\ndiscrimination of a stimulus, and then indicate how confident they are\nthat their perceptual judgment was accurate. This judgment about\nperception can be assessed for accuracy by comparing the metacognitive\njudgment with perceptual performance (for discussion of formal methods\nsuch as metacognitive signal detection theory, see Maniscalco &\nLau 2012). Related paradigms include post-decision wagering where\nsubjects place wagers on specific responses as a way of estimating\ntheir confidence (Persaud, McLeod, & Cowey 2007; but see Dienes\n& Seth 2010). \nHow is metacognitive assessment of performance tied to consciousness?\nThe metacognitive judgment reflects introspective assessment of the\nquality of perceptual states and can provide information about the\npresence of consciousness. For example, Peters and Lau (2015) tested\nwhether normal subjects have unconscious vision under conditions of\nvisual masking, a method that seems to eliminate conscious vision to a\nstimulus but allows for accurate visually guided behavior to it\n(Breitmeyer & Ogmen 2006). They presented stimuli in two temporal\n“windows” under masking but where the stimulus was present\nin only one window and a “blank” present in the other. If\nsubjects accurately respond to the stimulus but showed no difference\nin metacognitive confidence in respect of the quality of perception of\nthe target versus of the blank, this would provide evidence of the\nabsence of consciousness in vision (effectively, blindsight in normal\nsubjects;\n section 4.2).\n Interestingly, Peters and Lau found no evidence for unconscious\nvision in their specific paradigm. \nOne concern with metacognitive approaches is that they also rely on\nintrospection (Rosenthal 2018; see also Sandberg et al. 2010; Dienes\n& Seth 2010). If metacognition relies on introspection, does it\nnot accrue all the disadvantages of the latter? One advantage of\nmetacognition is that it allows for psychophysical analysis. There has\nalso been work done on metacognition and its neural basis. Studies\nwith non-human primates and rodents have begun to shed light on neural\nprocessing for metacognition (for a review, see Grimaldi, Lau, &\nBasso 2015; Pouget, Drugowitsch, & Kepecs 2016). From animal\nstudies, one theory is that metacognitive information regarding\nperception is already present in perceptual areas that guide\nobservational judgments, and these studies implicate parietal cortex\n(Kiani & Shadlen 2009; Fetsch et al. 2014) or the superior\ncolliculus (Kim & Basso 2008; but see Odegaard et al. 2018).\nAlternatively, information about confidence might be read out by other\nstructures, say prefrontal cortex (see\n section 3.3\n on Higher-Order Theory; also the entry on\n higher order theories of consciousness). \nMetacognitive and introspective judgments result from intentional\naction, so why not look at intentional action, broadly construed, for\nevidence of consciousness? Often, when subjects perform perception\nguided actions, we infer that they are relevantly conscious. It would\nbe odd if a person cooks dinner and then denies having seen any of the\ningredients. That they did something intentionally provides evidence\nthat they were consciously aware of what they acted on. An emphasis on\nintentional action embraces a broader evidential basis for\nconsciousness. Consider the Intentional Action Inference to\nphenomenal consciousness: \nIf some subject acts intentionally, where her action is guided by a\nperceptual state, then the perceptual state is phenomenally\nconscious. \nAn epistemic version takes the action to provide good evidence that\nthe state is conscious. Notice that introspection is typically an\nintentional action so it is covered by the inference. In this way, the\nInference effectively levels the evidential playing field:\nintrospective reports are simply one form among many types of\nintentional actions that provide evidence for consciousness. Those\nreports are not privileged. \nThe intentional action inference and no-report paradigms highlight the\nfact the science of consciousness has largely restricted its\nbehavioral data to one type of intentional action, introspection. What\nis the basis of privileging one intentional action over others?\nConsider the calibration issue. For many types of intentional action\ndeployed in experiments, scientists can calibrate performance by\nobjective measures such as accuracy. This has not been done for\nintrospection of consciousness, so scientists have privileged an\nuncalibrated measure over a calibrated one. This seems empirically\nill-advised. On the flip side, one worry about the intentional action\ninference is that it ignores guidance by unconscious perceptual states\n(see sections\n 4\n and\n 5.3.1). \nThe Intentional Action Inference is operative when subjective reports\nare not available. For example, it is deployed in arguing that some\npatients diagnosed as being in the vegetative state are conscious\n(Shea & Bayne 2010; see also Drayson 2014). \nA patient in the vegetative state appears at times to be wakeful, with\ncycles of eye closure and eye opening resembling those of sleep and\nwaking. However, close observation reveals no sign of awareness or of\na ‘functioning mind’: specifically, there is no evidence\nthat the patient can perceive the environment or his/her own body,\ncommunicate with others, or form intentions. As a rule, the patient\ncan breathe spontaneously and has a stable circulation. The state may\nbe a transient stage in the recovery from coma or it may persist until\ndeath. (Working Party RCP 2003: 249) \nVegetative state patients are not clinically comatose but fall short\nof being in a “minimally conscious state”. Unlike\nvegetative state patients, minimally conscious state patients\nseemingly perform intentional actions. \nRecent work suggests that some patients diagnosed as in the vegetative\nstate are conscious. Owen et al. (2006) used fMRI to demonstrate\ncorrelated activity in such patients in response to commands to deploy\nimagination. In an early study, a young female patient was scanned by\nfMRI while presented with three auditory commands: “imagine\nplaying tennis”, “imagine visiting the rooms in your\nhome”, “now just relax”. The commands were presented\nat the beginning of a thirty-second period, alternating between\nimagination and relax commands. The patient demonstrated similar\nactivity when matched to control subjects performing the same task:\nsustained activation of the supplementary motor area (SMA) was\nobserved during the motor imagery task while sustained activation of\nthe parahippocampal gyrus including the parahippocampal place area\n(PPA) was observed during the spatial imagery task. Later work\nreproduced this result in other patients and in one patient, the tasks\nwere used as a proxy for “yes”/ “no” responses\nto questions (Monti et al. 2010; for a review, see\nFernández-Espejo & Owen 2013). Note that these tasks probe\nspecific contents of consciousness by monitoring neural\ncorrelates of conscious imagery. \nSeveral authors (Greenberg 2007; Nachev & Husain 2007) have\ncountered that the observed activity was an automatic, non-intentional\nresponse to the command sentences, specifically to the words\n“tennis” and “house”. In normal subjects,\nreading action words is known to activate sensorimotor areas\n(Pulvermüller 2005). Owen et al. (2007), responded that the\nsustained activity over thirty-seconds made an automatic response less\nlikely than an intentional response. One way to rule out automaticity\nis to provide the patient with different sentences such as “do\nnot imagine playing tennis” or “Sharlene\nwas playing tennis”. Owen et al. (2007) demonstrated that\npresenting “Sharlene was playing tennis” to a normal\nsubject did not induce the same activity as when the subject obeyed\nthe command “imagine playing tennis”, but the result was\nnot duplicated in patients.  \nOwen et al. draw on a neural correlate of imagination, a mental\naction. Arguing that the neural correlate provides evidence of the\npatient’s executing an intentional action, they invoke a version\nof the Intentional Action Inference to argue that performance provides\nevidence for specific consciousness tied to the information carried in\nthe brain areas activated. Of note, experiments stimulating the\nparahippocampal place area induces seeming hallucinations of places\n(Mégevand et al.\n 2014).[7] \nRecall that the Generic Consciousness question asks: \nWhat conditions/states N of nervous systems are necessary and/or\nsufficient for a mental state, M, to be conscious as opposed to\nnot? \nVictor Lamme notes: \nDeciding whether there is phenomenality in a mental representation\nimplies putting a boundary—drawing a line—between\ndifferent types of representations…We have to start from the\nintuition that consciousness (in the phenomenal sense) exists, and is\na mental function in its own right. That intuition immediately implies\nthat there is also unconscious information processing. (Lamme\n2010: 208) \nIt is uncontroversial that there is unconscious information\nprocessing, say processing occurring in a computer. What Lamme means\nis that there are conscious and unconscious mental states\n(representations). For example, there might be visual states of seeing\nX that are conscious or not\n (section 4). \nIn what follows, the theories discussed provide higher level neural\nproperties that are necessary and/or sufficient for generic\nconsciousness of a given state. To provide a gloss on the hypotheses:\nFor the Global Neuronal Workspace, entry into the neural workspace is\nnecessary and sufficient for a state or content to be consciousness.\nFor Recurrent Processing Theory, a type of recurrent processing in\nsensory areas is necessary and sufficient for perceptual\nconsciousness, so entry into the Workspace is not necessary. For\nHigher-Order Theories, the presence of a higher-order state tied to\nprefrontal areas is necessary and sufficient for phenomenal\nexperience, so recurrent processing in sensory areas is not necessary\nnor is entry into the workspace. For Information Integration Theories,\na type of integration of information is necessary and sufficient for a\nstate to be conscious. \nOne explanation of generic consciousness invokes the global\nneuronal workspace. Bernard Baars first proposed the global\nworkspace theory as a cognitive/computational model (Baars 1988), but\nwe will focus on the neural version of Stanislas Dehaene and\ncolleagues: a state is conscious when and only when it (or its\ncontent) is present in the global neuronal workspace making the state\n(content) globally accessible to multiple systems including\nlong-term memory, motor, evaluational, attentional and perceptual\nsystems (Dehaene, Kerszberg, & Changeux 1998; Dehaene &\nNaccache 2001; Dehaene et al. 2006). Notice that the previous\ncharacterization does not commit to whether it is phenomenal or access\nconsciousness that is being defined.  \nAccess should be understood as a relational notion: \nA system X accesses content from system Y iff X\nuses that content in its computations/processing. \nThe accessibility of information is then defined as its\npotential access by other systems. Dehaene (Dehaene et al. 2006)\nintroduces a threefold distinction: (1) neural states that carry\ninformation that is not accessible (subliminal information);\n(2) states that carry information that is accessible but not accessed\n(not in the workspace; preconscious information); and (3)\nstates whose information is accessed by the workspace\n(conscious information) and is globally accessible to other\nsystems. So, a necessary and sufficient condition for a state’s\nbeing conscious rather than not is the access of a state or content by\nthe workspace, making that state or content accessible to other\nsystems. Hence, only states in (3) are conscious. \nFigure 2. The Global Neuronal\nWorkspace \nFigure Legend: The top figure provides a neural architecture for the\nworkspace, indicating the systems that can be involved. The lower\nfigure sets the architecture within the six layers of the cortex\nspanning frontal and sensory areas, with emphasis on neurons in layers\n2 and 3. Figure reproduced from Dehaene, Kerszberg, and Changeux 1998. Copyright\n(1998) National Academy of Sciences. \nThe global neuronal workspace theory ties access to brain\narchitecture. It postulates a cortical structure that involves\nworkspace neurons with long-range connections linking systems:\nperceptual, mnemonic, attentional, evaluational and motoric. \nWhat is the global workspace in neural terms? Long-range workspace\nneurons within different systems can constitute the workspace, but\nthey should not necessarily be identified with the workspace.\nA subset of workspace neurons becomes the workspace when they\nexemplify certain neural properties. What determines which workspace\nneurons constitute the workspace at a given time is the activity of\nthose neurons given the subject’s current state. The workspace\nthen is not a rigid neural structure but a rapidly changing neural\nnetwork, typically only a proper subset of all workspace neurons. \nConsider then a neural population that carries content p and is\nconstituted by workspace neurons. In virtue of being workspace\nneurons, the content p is accessible to other systems, but it\ndoes not yet follow that the neurons then constitute the global\nworkspace. A further requirement is that workspace neurons are (1) put\ninto an active state that must be sustained so that (2) the activation\ngenerates a recurrent activity between workspace systems.\nOnly when these systems are recurrently activated are they, along with\nthe units that access the information they carry, constituents of the\nworkspace. This activity accounts for the idea of global\nbroadcast in that workspace contents are accessible to\nfurther systems. Broadcasting explains the idea of consciousness as\nfor the subject: globally broadcasted content is accessible\nfor the subject’s use in informing behavior. \nThe global neuronal workspace theory provides an account of\naccess consciousness but what of phenomenal consciousness?\nThe theory predicts widespread activation of a cortical workspace\nnetwork as correlated with phenomenal conscious experience, and\nproponents often appeal to imaging results that reveal widespread\nactivation when consciousness is reported (Dehaene & Changeux\n2011). There is, however, a potential confound. We track phenomenal\nconsciousness by access in introspective report, so widespread\nactivity during reports of conscious experience correlates with both\naccess and phenomenal consciousness. Correlation cannot tell us\nwhether the observed activity is the basis of phenomenal consciousness\nor of access consciousness in report (Block 2007). This remains a live\nquestion for as discussed in\n section 2.2,\n we do not have empirical evidence that overflow is false. \nTo eliminate the confound, experimenters ensure that performance does\nnot differ between conditions where consciousness is present and where\nit is not. Where this was controlled, widespread activation was not\nclearly observed (Lau & Passingham 2006). Still, the absence of\nobserved activity by an imaging technique does not imply the absence\nof actual activity for the activity might be beyond the limits of\ndetection of that technique. Further, there is a general concern about\nthe significance of null results given that neuroscience studies\nfocused on prefrontal cortex are typically underpowered (for\ndiscussion, see Odegaard, Knight, & Lau 2017). \nA different explanation ties perceptual consciousness to processing\nindependent of the workspace, with focus on recurrent\nactivity in sensory areas. This approach emphasizes properties of\nfirst-order neural representation as explaining consciousness. Victor\nLamme (2006, 2010) argues that recurrent processing is necessary and\nsufficient for consciousness. Recurrent processing occurs where\nsensory systems are highly interconnected and involve feedforward and\nfeedback connections. For example, forward connections from primary\nvisual area V1, the first cortical visual area, carry information to\nhigher-level processing areas, and the initial registration of visual\ninformation involves a forward sweep of processing. At the same time,\nthere are many feedback connections linking visual areas (Felleman\n& Van Essen 1991), and later in processing, these connections are\nactivated yielding dynamic activity within the visual system. \nLamme identifies four stages of normal visual processing: \nLamme holds that recurrent processing in Stage 3 is necessary and\nsufficient for consciousness. Thus, what it is for a visual state to\nbe conscious is for a certain recurrent processing state to hold of\nthe relevant visual circuitry. This identifies the crucial difference\nbetween the global neuronal workspace and recurrent processing theory:\nthe former holds that recurrent processing at Stage 4 is necessary for\nconsciousness while the latter holds that recurrent processing at\nStage 3 is sufficient. Thus, recurrent processing theory affirms\nphenomenal consciousness without access by the global neuronal\nworkspace. In that sense, it is an overflow theory (see\n section 2.2). \nWhy think that Stage 3 processing is sufficient for consciousness?\nGiven that Stage 3 processing is not accessible to introspective\nreport, we lack introspective evidence for sufficiency. Lamme appeals\nto experiments with brief presentation of stimuli such as letters\nwhere subjects are said to report seeing more than they can identify\nin report (Lamme 2010). For example, in George Sperling’s\npartial report paradigm (Sperling 1960), subjects are briefly\npresented with an array of 12 letters (e.g., in 300 ms presentations)\nbut are typically able to report only three to four letters even as\nthey claim to see more letters (but see Phillips 2011). It is not\nclear that this is strong motivation for recurrent processing, since\nthe very fact that subjects can report seeing more letters shows that\nthey have some access to them, just not access to letter identity. \nLamme also presents what he calls neuroscience arguments.\nThis strategy compares two neural networks, one taken to be sufficient\nfor consciousness, say the processing at Stage 4 as per Global\nWorkspace theories, and one where sufficiency is in dispute, say\nrecurrent activity in Stage 3. Lamme argues that certain features\nfound in Stage 4 are also found in Stage 3 and given this similarity,\nit is reasonable to hold that Stage 3 processing suffices for\nconsciousness. For example, both stages exhibit recurrent processing.\nGlobal neuronal workspace theorists can allow that recurrent\nprocessing in stage 3 is correlated, even necessary, but deny that\nthis activity is explanatory in the relevant sense of identifying\nsufficient conditions for consciousness. \nIt is worth reemphasizing the empirical challenge in testing whether\naccess is necessary for phenomenal consciousness (sections\n 2.1–3).\n The two theories return different answers, one requiring access, the\nother denying it. As we saw, the methodological challenge in testing\nfor the presence of phenomenal consciousness independently of access\nremains a hurdle for both theories. \nA long-standing approach to conscious states holds that one is in a\nconscious state if and only if one relevantly represents oneself as\nbeing in such a state. For example, one is in a conscious visual state\nof seeing a moving object if and only if one suitably represents oneself\nbeing in that visual state. This higher-order state, in representing\nthe first-order state that represents the world, results in the first\norder state’s being conscious as opposed to not. The intuitive\nrationale for such theories is that if one were in a visual state but\nin no way aware of that state, then the visual state would not be\nconscious. Thus, to be in a conscious state, one must be aware of it,\ni.e., represent it (see the entry on\n higher order theories of consciousness;\n Rosenthal 2002). Higher-order theories merge with empirical work by\ntying high-order representations with activity in prefrontal cortex\nwhich is taken to be the neural substrate of the required higher-order\nrepresentations. On certain higher-order theories, one can be in a\nconscious visual state even if there is no visual system activity, so\nlong as one represents oneself as being in that state. \nThe focus on prefrontal cortex allows for empirical tests of the\nhigher-order theory as against other accounts (Lau & Rosenthal\n2011). For example, on the higher-order theory, lesions to prefrontal\ncortex should affect consciousness (Kozuch 2014), testing the\nnecessity of prefrontal cortex for consciousness. Against higher-order\ntheories, some reports claim that patients with prefrontal cortex\nsurgically removed maintain preserved perceptual consciousness (Boly\net al. 2017). This would lend support to recurrent processing theories\nthat hold that prefrontal cortical activity is not necessary for\nconsciousness. It is not clear, however, that the interventions\nsucceeded in removing all of prefrontal cortex, leaving\nperhaps sufficient frontal areas needed to sustain consciousness\n(Odegaard, Knight, & Lau 2017). Bilateral suppression of\nprefrontal activity using transcranial magnetic stimulation also seems\nto selectively impair visibility as evidenced by metacognitive report\n(Rounis et al. 2010). Furthermore, certain syndromes and experimental\nmanipulations suggest consciousness in the absence of appropriate\nsensory processing as predicted by some higher-order accounts (Lau\n& Brown forthcoming), a claim that coheres with the theory’s sufficiency\nclaims. \nInformation Integration Theory of Consciousness (IIT) draws\non the notion of integrated information, symbolized by Φ, as a way to explain generic consciousness (Tononi 2004,\n2008). IIT defines integrated information in terms of the effective\ninformation carried by the parts of the system in light of its causal\nprofile. For example, we can focus on a part of the whole circuit, say\ntwo connected nodes, and compute the effective information that can be\ncarried by this microcircuit. The system carries integrated\ninformation if the effective informational content of the whole\nis greater than the sum of the informational content of the parts. If\nthere is no partitioning where the summed informational content of the\nparts equals the whole, then the system as a whole carries integrated\ninformation and it has a positive value for Φ. Intuitively, the\ninteraction of the parts adds more to the system than the parts do\nalone. \nIIT holds that a non-zero value for Φ implies that a neural\nsystem is conscious, with more consciousness going with greater values\nfor Φ. For example, Tononi has argued that the human cerebellum\nhas a low value for Φ despite there being four to five times\nthe number of neurons in the cerebellum versus in human cortex. On\nIIT, what matters is the presence of appropriate connections and not\nthe number of neurons. \nA potential problem for IIT is that it treats many things to be\nconscious which are prima facie not (in\n Other Internet Resources,\n see Aaronson 2014a; for striking counterexamples and Aaronson 2014b\nwith a response from\n Tononi).[8]\n That said, the idea of integrated information might be useful for\nneuroscience, but we must show that invoking Φ can explain\ngeneric\n consciousness.[9] \nIn recent years, one way to frame the debate between theories of\ngeneric consciousness is whether the “front” or the\n“back” of the brain is crucial. Using this rough\ndistinction allows us to draw the following contrasts: Recurrent\nprocessing theories focus on sensory areas (in vision, the\n“back” of the brain) such that where processing achieves a\ncertain recurrent state, the relevant contents are conscious even if\nno higher-order thought is formed or no content enters the global\nworkspace. Similarly, proponents of IIT have recently emphasized a\n“posterior hot zone” covering parietal and occipital areas\n(Boly et al. 2017) as a neural correlate for consciousness, as they\nspeculate that this zone may have the highest value for Φ. For\ncertain higher-order thought theories, having a higher-order state,\nsupported by prefrontal cortex, without corresponding sensory states\ncan suffice for conscious states. In this case, the front of the brain\nwould be sufficient for consciousness. Finally, the global neuronal\nworkspace, drawing on workspace neurons that are present across brain\nareas to form the workspace, might be taken to straddle the\ndifference, depending on the type of conscious state involved. They\nrequire entry into the global workspace such that neither sensory\nactivity nor a higher order thought on its own is sufficient, i.e.,\nneither just the front nor the back of the brain. \nThe point of talking coarsely of brain anatomy in this way is to\nhighlight the neural focus of each theory and thus, of targets of\nmanipulation as we aim for explanatory neural correlates in terms of\nwhat is necessary and/or sufficient for generic consciousness. What is\nclear is that once theories make concrete predictions of brain areas\ninvolved in generic consciousness, neuroscience can test them. \nSince generic consciousness is a matter of a state’s being\nconscious or not, we can examine work on specific types of mental\nstate that shift between being conscious or not and isolate neural\nsubstrates. Work on unconscious vision provides an informative\nexample. In the past decades, scientists have argued for unconscious\nseeing and investigated its brain basis especially in\nneuropsychology, the study of subjects with brain damage.\nInterestingly, if there is unconscious seeing, then the intentional\naction inference must be restricted in scope since some intentional\nbehaviors might be guided by unconscious perception\n (section 2.5).\n That is, the existence of unconscious perception blocks a direct\ninference from perceptually guided intentional behavior to perceptual\nconsciousness. The case study of unconscious vision promises to\nilluminate more specific studies of generic consciousness along with\nhaving repercussions for how we attribute conscious states. \nSince the groundbreaking work of Leslie Ungerleider and Mortimer\nMishkin (1982), scientists divide primate cortical vision into two\nstreams: dorsal and ventral (for further dissection, see Kravitz et\nal. 2011). The dorsal stream projects into the parietal lobe while the\nventral stream projects into the temporal lobe (see\n Figure 1).\n Controversy surrounds the functions of the streams. Ungerleider and\nMishkin originally argued that the streams were functionally divided\nin terms of what and where: the ventral stream for\ncategorical perception and the dorsal stream for spatial perception.\nDavid Milner and Melvyn Goodale (1995) have argued that the dorsal\nstream is for action and the ventral stream for\n“perception”, namely for guiding thought, memory\nand complex action planning (see Goodale & Milner 2004 for an\nengaging overview). There continues to be debate surrounding the\nMilner and Goodale account (Schenk and McIntosh 2010) but it has\nstrongly influenced philosophers of mind. \nSubstantial motivation for Milner and Goodale’s division draws\non lesion studies in humans. Lesions to the dorsal stream do not seem\nto affect conscious vision in that subjects are able to provide\naccurate reports of what they see (but see Wu 2014a). Rather, dorsal\nlesions can affect visual-guidance of action with optic\nataxia being a common result. Optic ataxic subjects perform\ninaccurate motor actions. For example, they grope for objects, yet\nthey can accurately report the object’s features (for reviews,\nsee Andersen et al. 2014; Pisella et al. 2009; Rossetti, Pisella,\n& Vighetto 2003). Lesions in the ventral stream disrupt normal\nconscious vision, yielding visual agnosia, an inability to see visual\nform or to visually categorize objects (Farah 2004). \nDorsal stream processing is said to be unconscious. If the dorsal\nstream is critical in the visual guidance of many motor actions such\nas reaching and grasping, then those actions would be guided by\nunconscious visual states. The visual agnosic patient DF provides\ncritical support for this\n claim.[10]\n Due to carbon monoxide poisoning, DF suffered focal lesions largely\nin the ventral stream spanning the lateral occipital complex\nthat is associated with processing of visual form (high resolution\nimaging also reveals small lesions in the parietal lobe, [James et al.\n2003]). Like other visual agnosics with similar lesions, DF is at\nchance in reporting aspects of form, say the orientation of a line or\nthe shape of objects. Nevertheless, she retains color and texture\nvision. Strikingly, DF can generate accurate visually guided action,\nsay the manipulation of objects along specific parameters: putting an\nobject through a slot or reaching for and grasping round stones in a\nway sensitive to their center of mass. Simultaneously, DF denies\nseeing the relevant features and, if asked to verbally report them,\nshe is at chance. In this dissociation, DF’s verbal reports give\nevidence that she does not visually experience the features to which\nher motor actions remain sensitive. \nWhat is uncontroversial is that there is a division in explanatory\nneural correlates of visually guided behavior with the dorsal stream\nweighted towards the visual guidance of motor movements and the\nventral stream weighted towards the visual guidance of conceptual\nbehavior such as report and reasoning (see\n section 5.3.3\n on manipulation of seeing words via ventral stream stimulation). A\nsubstantial further inference is that consciousness is segregated away\nfrom the dorsal stream to the ventral stream. How strong is this\ninference? \nRecall the intentional action inference. In performing the slot task,\nDF is doing something intentionally and in a visually guided way. For\ncontrol subjects performing the task, we conclude that this visually\nguided behavior is guided by conscious vision. Indeed, a\nfolk-psychological assumption might be that consciousness informs\nmundane action (Clark 2001; for a different perspective see Wallhagen\n2007). Since DF shows similar performance on the same task, why not\nconclude that she is also visually conscious? Presumably, one\nhesitates because DF’s introspective reports clash with the\nintentional action inference. DF denies seeing features she is\nvisually sensitive to in action. Should introspection then trump\nintentional action in attributing consciousness? \nTwo issues are worth considering. The first is that introspective\nreports involve a specific type of intentional action guided by the\nexperience at issue. One type of intentional behavior is being\nprioritized over another in adjudicating whether a subject is\nconscious. What is the empirical justification for this\nprioritization? The second issue is that DF is possibly unique among\nvisual agnosics. It is a substantial inference to move from DF to a\ngeneral claim about the dorsal stream being unconscious in\nneurotypical individuals (see Mole 2009 for arguments that\nconsciousness does not divide between the streams and Wu 2013 for an\nargument for unconscious visually guided action in normal subjects).\nWhat this shows is that the methodological decisions that we make\nregarding how we track consciousness are substantial in theorizing\nabout the neural bases of conscious and unconscious vision. \nA second neuropsychological phenomenon also highlighting putative\nunconscious vision is blindsight which results from lesions\nin primary visual cortex (V1) typically leading to blindness over the\npart of visual space contralateral to the sight of the lesion\n(Weiskrantz 1986). For example, left hemisphere V1 deals with right\nvisual space, so lesions in left V1 lead to deficits in seeing the\nright side of space. Subjects then report that they cannot see a\nvisual stimulus in the affected visual space. Strikingly, these\nclinically blind subjects can draw on information from the\n“unseen” stimulus to visually inform behavior regarding\nit, often in striking ways. For example, a blindsight patient with\nbilateral damage to V1 (i.e., in both hemispheres) who is blind across\nthe visual field can walk down a hallway around obstacles he reports\nbeing unable to see (Gelder et al. 2008). Blindsight patients see in\nthe sense of visually discriminating the stimulus to act on it yet\ndeny that they see it. The contrast between behavior and report leads\nto the paradoxical term, “blindsight”. Like DF,\nblindsighters show a dissociation between certain actions and report,\nbut unlike DF, they do not spontaneously respond to relevant features\nbut must be encouraged to generate behaviors towards\n them.[11] \nThe neuroanatomical basis of blindsight capacities remains unclear.\nCertainly, the loss of V1 deprives later cortical visual areas of a\nnormal source of visual information. Still, there are other ways that\ninformation from the eye bypasses V1 to provide inputs to later visual\nareas. Alternative pathways include the superior colliculus (SC), the\nlateral geniculate nucleus (LGN) in the thalamus, and the pulvinar as\nlikely sources. \nFigure 3: Subcortical Pathways and their\nConnection to Cortical Vision (from Urbanski, Coubard, &\nBourlon 2014) \nFigure Legend: The front of the head is to the left, the back of the\nhead is to the right. One should imagine that the blue-linked regions\nare above the orange-linked regions, cortex above subcortex. V4 is\nassigned to the base of the ventral stream; V5, called area MT in\nnonhuman primates, is assigned to the base of the dorsal stream. \nThe latter two have direct extrastriate projections\n(projections to visual areas in the occipital lobe outside of V1)\nwhile the superior colliculus synapses onto neurons in the LGN and\npulvinar which then connect to extrastriate areas (Figure 3). Which of\nthese provide for the basis for blindsight remains an open question\nthough all pathways might play some role (Cowey 2010; Leopold 2012).\nIf blindsight involves nonphenomenal, unconscious vision, then these\npathways would be a substrate for it, and a functioning V1 might be\nnecessary for normal conscious vision. \nCampion et al. (1983) raised an important alternative explanation:\nblindsight subjects in fact have severely degraded conscious vision\nbut merely report on them with low confidence. In their reports,\nblindsight subjects feel like they are guessing about stimuli they can\nobjectively discriminate. Campion et al. drew on signal detection\ntheory, which emphasizes two determinants of detection behavior:\nperceptual sensitivity and response criterion. A\nsubject’s ability to visually detect a signal will depend partly\non how well her visual system extracts the signal from noise (sensitivity) but also\non the criterion that is set as a threshold for response. Consider\ntrying to detect something moving in the brush at twilight versus at\nnoon. In the latter, the signal will be greatly separated from noise\n(the object will be easier to detect) while in the former, the signal\nwill not be (the object will be harder to detect). Yet in either case,\none might operate with a conservative response criterion, say because\none is afraid to be wrong. Thus, even if the signal is detectable, one\nmight still opt not to report on it given a conservative\nbias (criterion), say if one is in the twilight scenario and would be ridiculed\nfor “false alarms”, i.e., claiming the object to be\npresent when it is not. \nCampion et al. hypothesized that blindsight patients are conscious in\nthat they are aware of visual signal where discriminability is low\n(cf. the twilight condition). Further, blindsight patients are more\nconservative in their response so will be apt to report the absence of\na signal by saying that they do not see the relevant stimulus even\nthough the signal is there, and they can detect it, as verified by\ntheir above chance visually guided behavior. This possibility was\nexplicitly tested by Azzopardi and Cowey (1997) with the well-studied\nblindsight patient, GY. They compared blindsight performance with\nnormal subjects at threshold vision using signal detection measures\nand found that with respect to motion stimuli, the difference between\ndiscrimination and detection used to argue for blindsight can be\nexplained by changes in response criterion, as Campion et al.\nhypothesized. That is, GYs claim that he does not see the stimulus is\ndue to a conservative criterion and not to a detection incapacity.\nInterestingly, for static stimuli, his response criterion did not\nchange but his sensitivity did, as if he was tapping into two\ndifferent visual processing mechanisms in each task (for an\nalternative explanation based on shifting response criterion, see Ko\n& Lau 2012). \nIn introspecting, what concepts are available to subjects will\ndetermine their sensitivity in report. In many studies with\nblindsight, subjects are given a binary option: do you see the\nstimulus or do you not see it? The concern is that the do not\nsee option would cover cases of degraded consciousness that\nsubjects might be unwilling to classify as seeing due to a conservative\nresponse criterion. So, what if subjects are given more options for\nreport? Ramsøy and Overgaard (2004; see also Overgaard et al.\n2006) provided subjects with four categories for introspective report:\nno experience; brief glimpse; almost clear experience; clear\nexperience. Using this perceptual awareness scale, they\nfound that subjects’ objective performance tracked their\nintrospective reports where performance was at chance when subjects\nreported no visual experience. As visibility increased, so did\nperformance. When the scale was used with a blindsight patient\n(Overgaard et al. 2008), no above chance performance was detected when\nthe subject reported no visual experience (see also Mazzi, Bagattini,\n& Savazzi 2016 for further evidence). A live alternative\nhypothesis is that blindsight does not present a case of unconscious\nvision, but of degraded conscious vision with a conservative response\nbias that affects introspection. At the very least, the issue depends\non how introspection is deployed, a topic that deserves further\nattention (see Phillips 2016 for further discussion of\nblindsight). \nBlindsight and DF show that damage to specific regions of the brain\ndisrupts normal visual processing, yet subjects can access visual\ninformation in preserved visual circuits to inform behavior despite\nfailing to report on the relevant visual contents. The received view\nis that these subjects demonstrate unconscious vision. One implication\nis that the normal processing in the ventral stream, tied to normal V1\nactivity, plays a necessary role in normal conscious vision. Another\nis that dorsal stream processing or visual stream processing that\nbypasses V1 via subcortical processing yields only unconscious visual\nstates. This points to a set of networks that begin to provide an\nanswer to what makes visual states conscious or not. An important\nfurther step will be to integrate these results with the general\ntheories noted earlier\n (section 3). \nStill, the complexities of the empirical data bring us back to\nmethodological issues about tracking consciousness and the following\nquestion: What behavioral data should form the basis of attributions\nof phenomenal consciousness? The intentional action inference is used\nin a variety of cases to attribute conscious states, yet the results\nof the previous sections counsel us to be wary of applying that\ninference widely. After all, some intentional behavior might be\nunconsciously guided. \nIn the case of DF, we noted that unlike many other visual agnosics,\nshe can direct motor actions towards stimuli that she cannot\nexplicitly report and which she denies seeing. In her case, we\nprioritize introspective reports over intentional action as evidence\nfor unconscious vision. Yet, one might take a broader view that vision\nfor action is always conscious and that what DF vividly illustrates is\nthat some visual contents (dorsal stream) are tied directly to\nperformance of intentional motor behavior and are not directly\navailable to conceptual capacities deployed in report. In contrast, other\naspects of conscious vision, supported by the ventral stream, are directly available to\nguide reports. This functional divergence is explained by the\nanatomical division in cortical visual processing. \nFor some time now, these striking cases have been taken as clear cases\nof unconscious vision and if this hypothesis is correct, the work has\nbegun to identify visual areas critical for creating seeing, sometimes\nconscious and sometimes not. The neuroanatomy demonstrates that\nvisually-guided behavior has a complex neural basis involving cortical\nand subcortical structures that demonstrate a substantial level of\nspecialization. Understanding consciousness and unconsciousness in\nvision will need to be sensitive to the complexities of the underlying\nneural substrate. \nWe turn to experimental work on specific consciousness: \nSpecific Consciousness: What neural states or properties are\nnecessary and/or sufficient for a conscious perceptual state to have\ncontent X rather than Y? \nIn this section, we examine attempts to address claims about\nnecessity and sufficiency by manipulation of the contents of\nconsciousness through direct modulation of neural representational\ncontent. \nIn thinking about neural explanations of specific consciousness,\nnamely the contents of consciousness, we will provisionally assume a\ntype of first-order representationalism about phenomenal content,\nnamely that such content supervenes on neural content (see the entry\non\n representational theories of consciousness).\n One strong position would be that phenomenal content is\nidentical to appropriate neural content. A weaker correlation\nclaim affirms only supervenience: no change in phenomenal content\nwithout a change in neural content. This neural representationalism\nallows us to link phenomenal properties to the brain via linking\nneural contents to perceptual contents. \nA common approach, the contrast strategy, enjoins\nexperimentalists to identify relevant correlates for some phenomenon\nP by contrasting cases where P is present from cases\nwhere P is not. Work on binocular rivalry illustrates this\nstrategy (among many reviews, see Tong, Meng, & Blake 2006; Blake,\nBrascamp, & Heeger 2014). When each eye receives a different image\nsimultaneously, the subject does not see both, say one stimulus\noverlapping the other. Rather, visual experience alternates between\nthem. Call this phenomenal alternation. An initial\nrestatement of our question about specific consciousness in respect of\nbinocular rivalry is: \nSpecific Rivalry: What neural property is necessary and/or\nsufficient for phenomenal alternation in binocular rivalry in\ncondition C? \nThat is, empirical theories aim to explain how visual content\nalternates in binocular\n rivalry.[12]\n Notice that this is a question about specific rather than generic\nconsciousness, as the contrast is not between a state’s being\nconscious versus not but about the contrast between two conscious\nstates with different contents. \nNeural explanations of binocular rivalry concern competition at some\nlevel of visual processing: (a) “interocular” competition\nbetween monocular neurons early in the visual system, namely\nvisual neurons that receive input from only one eye or (b) competition\nbetween binocular neurons later in the visual system, namely\nneurons that receive input from both eyes. The winner of competition\nfixes which stimulus the subject experiences at a given time. Some of\nthe earliest electrophysiological studies (Leopold & Logothetis\n1996; Logothetis, Leopold, & Sheinberg 1996) on awake behaving\nmonkeys supported later binocular processing as the neural basis of\nbinocular rivalry. Processing in later (inferotemporal cortex, IT; see\n figure 1)\n rather than earlier visual areas (V1 or V2) were observed to be best\ncorrelated to the monkey’s reported perception based on the\nmonkey’s stimuli-specific response. In contrast, imaging studies\nin humans suggested that neural activity in V1 did correlate with\nalternation. For example, Polonsky et al. used fMRI to demonstrate\nthat V1 activity to competing stimuli tracked perception (Polonsky et\nal. 2000; but see Maier et al. 2008). \nRecent accounts have taken binocular rivalry as resulting from\nprocesses at multiple levels (Wilson 2003; Freeman 2005; Tong, Meng,\n& Blake 2006). For example, when the two competing stimuli have\nparts that can be fused into a coherent stimulus, as when half of a\npicture is presented to each eye, the subject can perceive the fusion,\nintegrating content from each eye (Kovács et al. 1996; Ngo et\nal. 2000). This suggests that binocular rivalry can be sensitive to\nglobal properties of the stimulus (see Baker & Graf 2009). What\nunifies the mechanisms, perhaps, is the function of resolving a\nconflict generated by the stimuli. \nAssume that some neural process R resolves interocular\ncompetition: when R resolves competition between stimuli\nX and Y in favor of X, then the subject is\nphenomenally conscious of X rather than Y and vice\nversa. Notice that R has the same “gating” function\nfor any stimuli X and Y that are subject to binocular\nrivalry. So, while the presence of R can explain why the\nsubject is having one conscious visual experience rather than another,\nR is not tied to a specific content. This suggests that in\nanswering the question about rivalry, we will at best be identifying a\nnecessary but not sufficient condition for a conscious visual state\nhaving a content X. R is a general gate for\nconsciousness (cf. attention in global workspace theory). \nA narrower explanation of specific consciousness would identify the\nspecific neural representations that explain a conscious state’s\nhaving the specific content X (rather than Y). By the\nrepresentationalism assumption, this will involve identifying neural\nrepresentations with the same content, X. Focusing on a gate in\nexplaining alternation in rivalry stops short of identifying those\nrepresentations. Still, binocular rivalry can provide a useful method\nfor isolating neural populations that carry relevant content. In\nprinciple, for any stimulus type of interest, X (e.g., faces,\nwords, etc.), so long as X is subject to binocular rivalry, we\ncan use rivalry paradigms to isolate brain areas that carry the\nrelevant information that correlate with the subject’s\nperceiving X. That would allow us to identify potential\ncandidates for the neural basis of conscious content. \nThere are limited opportunities to manipulate human brain activity in\na targeted way. Recent use of transcranial magnetic stimulation to\nactivate or suppress neural activity has provided illumination, but\nsuch interventions are coarse-grained. Ultimately, to locate an\nexplanatory correlate for specific conscious contents, we will need\nmore fine-grained interventions in brain tissue. In humans, such\nopportunities are generally confined to manipulation before surgical\ninterventions, say for brain tumors or epilepsy (see\n section 5.3.3\n for work with epilepsy patients). \nIn the middle of the last century, neurosurgeon Wilder Penfield and\ncolleagues performed a set of direct electrical microstimulations\nduring preoperative procedures (Penfield & Perot 1963), and in\ncertain cases induced hallucinations by stimulating primary sensory\ncortices such as V1 or S1 (see\n figure 1).\n This provided evidence that endogenous activity could be causally\nsufficient for phenomenal experiences. Penfield’s interventions,\nhowever, were not based on fine-grained targeting of specific neural\nrepresentations. As Cohen and Newsome note,  \nPenfield’s approach failed to generate substantial new insights\ninto the neural basis of perception and cognition…because the\ngross electrical activation elicited by surface electrodes could not\nbe related mechanistically to the information being processed within\nthe excited neural tissue. (Cohen & Newsome 2004: 1) \nA different approach begins with a more detailed understanding of\nunderlying neural representations tied to different brain regions. For\nexample, the fusiform face (FFA) area appears to be necessary for\nnormal human face experience in that lesions in FFA lead to\nprosopagnosia, the inability to see faces even if one can see\ntheir parts. FFA is part of a larger network that is important in\nvisual processing of faces (Behrmann & Plaut 2013). Recently,\nmicrostimulation of FFA in an awake human epilepsy patient induced\nvisual distortions of actual faces as opposed to other objects\n(Parvizi et al. 2012). Alterations of visual experience were also\nreported during microstimulating the parahippocampal place (PPA) area\nin an awake pre-operative epileptic patient that induced visual\nhallucinations of scenes (Mégevand et al. 2014). PPA is the\nsame area that showed activation in vegetative state patients when\nthey putatively imagined walking around their home\n (section 2.5). \nFigure 4. Ventral Stream Areas  \nA view from the bottom of cortex with location of areas FFA, PPA and\nLO identified. Occipital cortex is on the bottom. LO is lesioned in the visual agnosic patient, DF (see\n section 4.1).\n This figure is modified from figure 1 of Behrmann and Plaut 2013, kindly provided by Marlene\nBehrmann and used with her permission. \nIt is worth noting that many neuroscientists of vision take themselves\nto be investigating seeing in the ordinary sense, one that implies\nconsciousness, but very few of them would characterize their work as\nabout consciousness. That said, their work is of direct relevance to\nour understanding of specific consciousness even if it is not always\ncharacterized as such. \nAn important approach in visual neuroscience was articulated by A.J.\nParker and William Newsome in “Sense and the Single\nNeuron” (1998) via “principles” to connect\nelectrophysiological data about information processing to perception\n(for a recent discussion, see Ruff & Cohen 2014). To probe the\nneural basis of perception, neuroscientists need to explanatorily link\nneural data to the subject’s perception that guides behavior.\nThe experimenter must ensure that recorded neural content correlates\nwith perceptual content and not just response. Further, manipulation\nof the neurons carrying information should affect perception: inducing\nappropriate neural activity should shift perceptual response while\nabolishing or reducing that activity should eliminate or reduce\nperceptual response as measured in behavior. These proposals address\nconcerns about necessity and sufficiency. \nThe intentional action inference is applicable (or at least its\nevidential version): \nIf some subject acts intentionally, where her action is guided by a\nperceptual state, then that state is phenomenally conscious. \nWe will consider the strength of this inference in three cases. The\nfirst case, visual motion perception, introduces the principles that\nguide the manipulation of neural content while the second case\nconcerns tactile experience of vibration. These cases involve\nexperiments with non-human primates, so we lack introspective reports.\nThe final case concerns direct manipulation of the human brain along\nwith introspective reports. \nThese experiments involve microstimulation of small populations of\nneurons that are targeted precisely because of their informational\ncontent. Microstimulation involves injecting a small current from the\ntip of an electrode inserted into brain tissue that directly\nstimulates nearby neurons or, through synaptic connections to other\nneurons, indirectly activates more distant neurons (see Histed, Ni,\n& Maunsell 2013 for a review). It is assumed that neurons\ntuned in similar ways, that is neurons that respond to\nsimilar stimuli, tend to be interconnected, so microstimulation is\ntaken to largely drive similarly tuned neurons. \nWe begin with visual motion perception in primates. Since the\nprinciples introduced here are central to much perceptual neuroscience\nand provide the basis for probing the link between neural\nrepresentations and perceptual content, I examine it carefully. The\nsalient question will be whether conscious experience is changed by\nthe manipulations. \nThe work we shall discuss was done in awake behaving macaque monkeys.\nVisual area MT in the monkey brain (called V5 in humans) plays an\nimportant role in the visual experience of motion. MT is taken to lie\nin the dorsal visual stream\n (figure 1).\n Lesions that disrupt MT are known to cause akinetopsia, the\ninability to see motion. One patient with an MT (V5) lesion reported\nthe following phenomenology: “people were suddenly here or there\nbut I have not seen them moving” (Zihl, Von Cramon, & Mai\n1983: 315). MT processing looks to be necessary for normal visual\nmotion experience. Furthermore, MT neurons represent (carry\ninformation regarding) the direction of motion of visible stimuli: MT\nneurons are tuned for motion in specific directions with the\nhighest firing rate for a specific direction of motion (for other\nfunctions and responses of MT, see Born & Bradley 2005). By\nplacing motion stimuli in a neuron’s receptive field, scientists\ncan map its tuning: \nFigure 5. MT Neuron Tuning Curve \nFigure Legend: Tuning of a neuron in MT showing a peak response in\nspiking rate at 0 degrees of motion. The dashed curve is generated when the animal is\nattending to the motion stimulus while it is in the receptive field\n(we shall not discuss the neural basis of attention, but see Wu 2014b,\nchap. 2, for a summary of the neuroscience of attention). The solid curve shows MT response when the animal is not attending to the motion stimulus in the receptive field. Figure from\nLee & Maunsell 2009. \nWhat is plotted is the activity of an MT neuron, in spikes per second,\nto a specific type of motion stimulus placed within its receptive\nfield. How to relate a tuning curve to a determinate content is\ncomplicated. Since the neural response is not simply to one stimulus\nvalue, it is not obvious that the neuron should be taken to represent\n0 degrees of motion, namely the value at its peak response. Indeed,\ntheorists have noted that the tuning curve looks like a probability\ndensity function, and many now take neurons to have\nprobabilistic content\n (section 5.4). \nExperimenters have trained macaque monkeys to perform discrimination\ntasks reporting direction of motion. Typically, the monkey maintains\nfixation while the moving stimulus is placed within the receptive\nfield of the recorded neuron. The monkey reports the direction of the\nstimulus by moving its eyes to a target that stands for either\nleftward or rightward motion (other behavioral reports can be\ngenerated such as moving a joystick). Provisionally, we apply the\nintentional action inference, so we assume that such reports are\nguided by conscious visual experience of the stimuli. Thus,\nchanges in behavior will be evidence for changes in conscious\ncontent. \nEarly work suggested that the activity of a single neuron provides a\nstrong correlate of the animal’s visually guided performance.\nThis can be seen by plotting both the animal and the neuron’s\nperformance across different stimulus values. In these experiments,\nthe value concerns the percent coherence of motion of a set\nof dots defined as the number of dots moving in the same direction (0%\ncoherence being random motion; 100% being all dots moving in the same\ndirection). In the first case, we construct a psychometric\ncurve that plots the animal’s percent correct reports\nrelative to percent coherence of motion of the stimuli. As one might\nexpect, percent correct reports drop as coherence drops, and the\ninflection point reflects where the subject is equally likely to\nindicate left or right motion. We can do the same for the neural\nactivity of the neuron across the same stimulus values, a\nneurometric curve. \nThe experimentalist’s window onto conscious experience is\nthrough behavior, the assumption being that report about motion\ncorrelates with perceptual experience. Correlation is assessed by\nasking the following question: would an ideal observer, using the\nactivity of the neuron in question, be able to predict the\nanimal’s visually guided performance? Essentially, do the\npsychometric and neurometric curves overlap? Strikingly, yes. MT\nneurons were observed to predict the animal’s behavior (Britten\net al. 1992). \nFigure 6 \nFigure Legend: Psychometric and Neurometric curves for a single MT\nneuron during performance of a motion direction detection task. Percent correct performance is plotted on the y-axis while percent motion coherence is plotted in a log scale on the x-axis. Figure\nmodified from Ruff & Cohen 2014 and kindly provided by Doug\nRuff. \nThis shows that the activity of a single MT neuron provides a neural\ncorrelate of the animal’s visual discrimination of motion. Note\nthat this is just a neural correlate of behavior. No one suggested\nthat this neuron was causally sufficient for the behavior or for\nperception. Later results have suggested that individual neurons are\nnot quite as sensitive as Britten suggested, but that small groups of\nMT neurons are sufficient to predict behavior (Cohen &\nNewsome 2009). \nEarlier, we worried about mere correlates. To get causal or\nexplanatory purchase, the content of the MT neurons correlated to the\nanimal’s behavior must be shown to contribute to perceptual\nguidance. This predicts that if we manipulate the content of the\nneurons, i.e., manipulate neural representations, then we should\nmanipulate the content of the animal’s visual experience of\nmotion as reflected by predicted changes in behavior. This would be to\ntest sufficiency with respect to specific consciousness. \nNewsome and colleagues demonstrated that microstimulation of MT\nneurons shifted the animal’s performance in predictable ways.\nAssume that neural population P, by encoding information about\nstimulus motion, can inform the subject’s report of motion\ndirection. This information is accessible for the control of behavior.\nActivation of P by microstimulation should shift behavior in a\nmotion selective way correlated with the direction that P is\ntuned to (represents). Metaphorically, if a downstream control system\nis sensitive to the response of P when it generates behavior,\nthen if we change P in a specific way, say amplifying its\nsignal, we should change behavior in a way biased by P’s\ncontent. This was first demonstrated by Salzman et al. (1990). They\ninserted electrodes into MT and identified neurons tuned to a\nparticular orientation. During a motion discrimination task,\nmicrostimulation of neurons with that tuning led to a shift in the\npsychometric curve as if that neuron was given more weight in driving\nbehavior. \nIn conditions of microstimulation relative to its absence, the monkey\nwas more likely to report that there was motion in the stimulated\nneuron’s preferred direction. In the original experiment, the\npsychophysical effect of microstimulation was equivalent to the\naddition of 7-20% coherence in the stimulus with respect to the\nneuron’s preferred direction, depending on the experimental\nconditions. Further, as a test of necessity, a selective lesion of MT\ndisrupted motion discrimination though the animals were able to\nrecover some function suggesting that other visual information streams\ncould be tapped so as to support performance (Newsome &\nParé 1988). \nAdopting the intentional action inference, one can conclude that the\nmicrostimulation shifted perceptual content (or again, that we have\ngood evidence for this shift). That said, given our discussion of\nunconscious vision\n (section 4),\n another possibility is that MT microstimulation only changes\nunconscious visual representations. Newsome himself asked: \nWhat is the conscious experience that accompanies the stimulation and\nthe monkey’s decision? Even if you knew everything about how the\nneurons encode and transmit information, you may not know what the\nmonkey experiences when we stimulate his MT. (Singer 2006) \nClearly, having the monkey provide an introspective report would add\nevidential weight, but obtaining such reports from non-linguistic\ncreatures is difficult. How can we get the animal to turn attention\ninward to their perceptual states in an experimental\n context?[13] \nWhat of microstimulation in the absence of a stimulus? Might we induce\nhallucinations as Penfield did in his patients? Rather than\nthe work from the Newsome group that modulated ongoing perceptual\nprocessing, the issue here is to create an internal signal that mimics\nperception. Romo et al. (1998) demonstrated that monkeys can carry out\nsensory tasks via activation triggered by microstimulation. The\nmonkeys’ task was to discriminate the frequency of two\nsequential “flutters” on their fingertips, that is,\nmechanical vibrations on the skin at specific frequencies. In an\nexperimental trial, an initial sample flutter was presented\nfor 500 ms and after a gap of 1-3 seconds, a second test\nflutter of either higher or lower frequency was presented. The animal\nreported whether the second test frequency was higher or lower than\nthe sample. \nThe experimenters examined whether direct microstimulation in the\nabsence of a stimulus could tap into the same neural representations\nthat guided the animal’s report. They isolated\nneurons in primary somatosensory cortex responsive to vibration\nfrequency on the fingers (S1, the somatosensory homunculus discovered\nby Penfield [Penfield & Boldrey 1937]; see\n figure 1).\n The investigators then stimulated the same neurons in S1 in the\nabsence of the test flutter, so used stimulation as a substitute for\nan actual vibration. Thus, the animal had to make a comparison between\nthe frequency of a mechanical sample to either a subsequent (1) real\nmechanical test vibration (i.e., the good case with an actual\nstimulus) or (2) to a microstimulation test stimulus (i.e., the\n“hallucinatory” case where direct activation of the S1\nneurons occurred in the absence of a stimulus). Romo et al.\ndemonstrated that discrimination performance based either on\nmechanical stimulation or microstimulation was equivalent. In\nother words, the animals could match either mechanical or\nmicrostimulation to a remembered mechanical\n sample.[14] \nIn subsequent work (Romo et al. 2000), the investigators inverted the\nexperiment, using the microstimulation as the sample. In this\ncase, the animals had to remember the information conveyed by the\nmicrostimulation (effectively, a hallucination) and then compare it to\neither a subsequent (a) mechanically generated stimulation on the\nfinger (actual test stimulus) or (b) a microstimulation of S1 as test\n(i.e., no stimulus). In both cases, performance was similar to earlier\nresults. The striking finding is that behavior could be driven\nentirely by microstimulation. At least for the tactile stimulations at\nissue, the animal might have been in the Matrix! \nOne might think that the intentional action inference is stronger in\nthis paradigm, given the elegant flipping of stimuli in Romo et al.\n2000. Still, the authors comment: \nThis study, therefore, has directly established a strong link between\nneural activity and perception. However, we do not know yet whether\nmicrostimulation of the QA circuit in S1 elicits a subjective flutter\nsensation in the fingertips. This can only be explored by\nmicrostimulating S1 in an attending human observer. (Romo et al. 2000:\n 277)[15] \nLike Newsome, the authors reach for introspection. Yet they might\nundersell their result, for it seems that the animals are having a\ntactile hallucination: (a) Penfield showed that stimulation of primary\nsensory cortices like S1 induces hallucinations in humans; (b) action\nis engaged not at low stimulation of S1 in monkeys but only at higher\nlevel stimulation; (c) at that point, when the stimulation grabs their\nattention, the monkeys do what they were trained to do, namely\ndiscriminate stimuli, either with (d) just mechanical stimulation\n(normal experience), or (e) with a mix of mechanical and\nmicrostimulation or with just microstimulation; (f) given the\nbehavioral equivalence of these three cases, one might then argue that\nif performance in the mechanical stimulation cases involves conscious\ntactile experience, then that same experience is involved in the other\ncases. \nIn humans, language is lateralized to the left hemisphere, and in\nvisual word recognition, the left midfusiform gyrus (lmFG; sometimes\nreferred to as the visual word form area, VWFA) is important\nfor normal processing of visual word forms during reading. For\nexample, damage to lmFG affects reading in adults (Gaillard et al.\n2006; Behrmann & Shallice 1995) while learning novel words\nsharpens representations therein (Glezer et al. 2015). Nevertheless,\nit is possible that lmFG is tuned to general visual form and is not\nspecific to visual word forms. This is a common point of contention in\naddressing the function of various areas in the human visual system,\nnotably the fusiform face area (FFA): is it a content specific area or\nis it more a general visual expertise area (Kanwisher 2000; Tarr &\nGauthier 2000)? \nIn a recent study, Hirshorn et al. (2016) used microstimulation to\ndisrupt processing in lmFG in human epileptic patients with\npreoperatively implanted electrodes spanning that area. These\nelectrodes are used to map sites that will be surgically removed to\nrelieve intractable seizures. Subjects read words or letters during\nactual or sham microstimulation. Crucially, stimulation in lmFG\nselectively disrupted word and letter reading but not general\nform perception. During lmFG stimulation, one subject when presented\nwith “illegal” reported not seeing the word (see \n Movie S1\nin Other Internet Resources). Rather, she reported thinking of\ndifferent words (still, she did not report\nseeing different words). With “message” she\nreported thinking that an “n” was present. In a\nsecond patient, the identification of letters was completely\ndisrupted (see \n  Movie S2 in Other Internet Resources). \nThe patient reported seeing an “A” when presented\nwith an “X”, and then an “F” and\n“H” when presented with “C”. The\nresults suggest that normal word reading requires lmFG processing to\nparse linguistic forms. A plausible hypothesis is that\nmicrostimulation disrupted visual experience of specific types of\nstimuli, a test of necessity. That said, the first patient’s\nintrospective reports of thinking rather than seeing words complicates\nmatters. \nTechniques for decoding information processing (using machine\nlearning) suggests that processing in lmFG becomes more finely tuned\nto word form. Initially, lmFG represents a more gist-like\nrepresentation but then develops more precise representations that\nindividuate words of similar form. These converging results provide\nevidence that the areas stimulated carry information about word form\nsuch that in disrupting that activity, word perception was selectively\ndisrupted. \nTaken together, the three cases provide examples of detailed\nmanipulations in different sensory modalities, animals, and contents\nthat test for causal sufficiency and necessity across different levels\nof the sensory processing hierarchy, from early levels (e.g., S1) to\nmid-levels (MT) and finally to higher levels (lmFG or FFA). Working\nbackwards, the experimental strategy is as follows: Given a perceptual\nexperience with a specific content, one identifies neural correlates\nthat carry equivalent content, say one’s seeing motion, feeling\nvibration, or parsing words linked to neural correlates processing\ninformation about motion, vibrational frequency and word forms. Next,\nfine-grained manipulations of the neural content are then correlated\nwith related effects on perceptual experience. One issue that remains\nopen is whether in tapping into neural processing by microstimulation,\none has simply identified an earlier causal node in the neural\nprocesses that generate perceptual experience, there being more\ninformative neural correlates later in the causal pathway. An\nimportant question is how one might identify the neural basis of the\nexperience as opposed to its cause. \nA couple of salient methodological challenges stand in the way of\nexplaining specific consciousness. The first is that much of the\ndetailed work will for the foreseeable future be done on non-human\nanimals where introspective report is not easily available and where\nthe intentional action inference will be essential. To strengthen that\ninference, we will need more detailed models that make plausible that\nconscious experience figures in the generation of the observed\nbehavior. There is an experiment-theory circle that we must break\ninto: we need a theory that supports the role of consciousness in\nbehavior but the theory itself will be supported by behavioral data.\nHow will we break into this cycle? The intentional action inference\nwill be an important means of tracking consciousness, and the limits\nof its applicability must be investigated. The second challenge is\nthat there is a need to individuate different kinds of explanatory\ncorrelates of consciousness, for some will be causes (upstream of\nconscious states), some will be enabling conditions, and some will be\nconstituents of the state itself. Dividing these cases involves not\njust gathering data, but having a clear conceptual framework to draw\ndistinctions in a principled way. Clearly joint philosophical and\nexperimental work is needed. \nIn invoking neural content, the assumption is that neural content\nmirrors perceptual content, so that if one is experiencing dots moving\nin a certain direction, there is a neural representation with the same\ncontent. This is a simplification and does not cohere with a common\ncurrent approach to neural content that takes it to be\nprobabilistic. Consider again the tuning curve from an MT\nneuron, M\n (figure 5). \nIf asked to assign a determinate content to M, one might choose\nthe value that corresponds to its peak response, here,\n 0o.[16]\n This would be the most natural option if the tuning curve were\nessentially a sharp line at 0o, i.e., were the neuron only\nactive for its preferred stimulus but otherwise not. Of course, that\nis not the neuron’s response profile, so what is the content of\nthe neural representation? \nOne approach that converges on a determinate content considers the\nactivity of a neural population. To evaluate more information, the\nbrain might integrate MT response by giving each MT neuron, \\(M_n\\),\none vote weighted according to the strength of its response.\nThus, the tuning curve represents how strongly the neuron votes for\nits preferred value (the value at the peak). The votes are tallied by\na downstream system, and the result can be represented as a\npopulation vector whose direction is understood to be the\ndirection represented by the neural population. This specific approach\nwas taken by Georgopoulos and colleagues to decode the direction of\nbodily movement from the activity of a population of motor neurons\n(Georgopoulos et al. 1982; for discussion of different coding\napproaches, see Pouget, Dayan, & Zemel 2003). \nIn recent perceptual neuroscience, an alternative picture of neural\nrepresentational content often tied to Bayesian approaches to\nperceptual computation has gained traction (for accessible\ndiscussions, see Colombo & Seriès 2012; Rescorla 2015). On\nBayesian models, extracting information from populations of (say) MT\nneurons does not yield a specific value of motion direction but rather\na probability density function, across the space of possible\nmotion directions (for a philosophical discussion of neural\nprobabilistic codes, see Shea 2014). A key idea is not the generation\nof specific values as what neurons represent, say 0 degree motion as\nproposed earlier, but rather the conceptualization of the population\nresponse as reflecting uncertainty inherent in neural activity given\nnoise. \nIf we plot the activity of all MT neurons responding to a specific\nmotion stimulus, one hypothesis is that the population response codes\nthe likelihood, \\(\\pP(r\\mid s)\\), namely the conditional\nprobability that one has the observed MT response r given the\nstimulus s. A Bayesian approach to neural population codes then\nunderstands neural processing to involve computation of the\nposterior probability, \\(\\pP(s\\mid r)\\) from the likelihood\nand prior knowledge of the probability of the stimulus \\(\\pP(s)\\) in\naccordance with Bayes Theorem (this is normalized so that the\nprobabilities sum to one). The details of Bayesian computation need\nnot concern us since our main concern is with the possibility of\nneural content as probabilistic, something that seems counterintuitive\nrelative to the approach illustrated by Georgopoulos and\ncolleagues. \nKnill and Pouget contrast the two approaches: \nThis is the basic premise on which Bayesian theories of cortical\nprocessing will succeed or fail—that the brain represents\ninformation probabilistically, by coding and computing with\nprobability density functions or approximations to probability density\nfunctions…The opposing view is that neural representations are\ndeterministic and discrete, which might be intuitive but also\nmisleading. This intuition might be due to the apparent\n‘oneness’ of our perceptual world and the need to\n‘collapse’ perceptual representations into discrete\nactions, such as decisions or motor behaviors. (Knill & Pouget\n2004) \nHow might a probabilistic account of neural representation affect our\nthinking about phenomenal consciousness via neural\nrepresentationalism? Consider this possibility: What if probabilistic\ncontent is pervasive? Pouget, Dayan and Zemel note: \ndecoding is not an essential neurobiological operation because there\nis almost never a reason to decode the stimulus explicitly. Rather,\nthe population code is used to support computations involving\ns, whose outputs are represented in the form of yet more\npopulation codes over the same or different collections of neurons.\n(2003: 385) \nPut another way, the determinacy is apparent only at the output stage,\nthe goal of processing. In the case of motor action, neural content is\nprobabilistic until the actual movement when a determinate path is\nimplemented in a specific movement trajectory. \nYet perceptual content does not seem probabilistic. This\nemphasizes a prima facie disconnect between current theories\nof neural content and those of phenomenal content. The linking\nprinciples we have deployed assume a specific view of neural content\nthat might not cohere with current approaches to neural coding,\nleaving us with the challenge of explanatorily linking probabilistic\ncontent at the neural level with more determinate, nonprobabilistic\ncontent at the phenomenal level. One option is to find\nnonprobabilistic content at the neural level (e.g., as in the\npopulation vector approach). The other is to find probabilistic\ncontent at the phenomenal level (for related ideas, see Morrison 2017\nand response by Denison 2017; also Block 2018). Either way,\nexplanations of specific content will need to deal with this prima\nfacie disconnect between phenomenal content as revealed by\nintrospection and current theories of neural content. \nTalk of the neuroscience of consciousness has, thus far, focused on\nthe neural correlates of consciousness. Not all neural correlates\nare explanatory, so finding correlates is a first step in the\nneuroscience of consciousness. The next step involves manipulation of\nrelevant correlates to test claims about sufficiency and necessity, as\nisolated in our two questions: \nGeneric Consciousness: What conditions/states N of\nnervous systems are necessary and (or) sufficient for a mental state,\nM, to be conscious as opposed to not? \nSpecific Consciousness: What neural states or properties are\nnecessary and/or sufficient for a conscious perceptual state to have\ncontent X rather than Y? \nA productive neuroscience of consciousness requires that we understand\nthe relevant neural properties at the right level of analysis. For\ngeneric consciousness, this will involve manipulation of relevant\nproperties in a way that can avoid the access/phenomenal confound, and\nrecent work focuses on pitting the many theories we have considered\nagainst each other. For specific consciousness, the critical issue\nwill be to understand neural representational content and to find ways\nto link experimentally and explanatorily neural content to phenomenal\ncontent. We have tools to manipulate neural contents to affect\nphenomenal content, and in doing so, we can begin to uncover the\nneural basis of conscious contents. There is much interesting work yet\nto be done, philosophically and empirically, and we can look forward\nto a productive interdisciplinary research program.","contact.mail":"waynewu@andrew.cmu.edu","contact.domain":"andrew.cmu.edu"}]
