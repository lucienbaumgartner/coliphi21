[{"date.published":"2012-12-05","date.changed":"2018-02-01","url":"https://plato.stanford.edu/entries/proof-theoretic-semantics/","author1":"Peter Schroeder-Heister","entry":"proof-theoretic-semantics","body.text":"\n\n\n\nProof-theoretic semantics is an alternative to truth-condition\nsemantics. It is based on the fundamental assumption that the central\nnotion in terms of which meanings are assigned to certain expressions\nof our language, in particular to logical constants, is that of\nproof rather than truth. In this sense\nproof-theoretic semantics is semantics in terms of proof .\nProof-theoretic semantics also means the semantics of proofs,\ni.e., the semantics of entities which describe how we arrive at certain\nassertions given certain assumptions. Both aspects of proof-theoretic\nsemantics can be intertwined, i.e. the semantics of proofs is itself\noften given in terms of proofs.\n\n\nProof-theoretic semantics has several roots, the most specific one\nbeing Gentzen’s remarks that the introduction rules in his\ncalculus of natural deduction define the meanings of logical constants,\nwhile the elimination rules can be obtained as a consequence of this\ndefinition (see section\n 2.2.1).\n More\nbroadly, it belongs to what Prawitz called general proof\ntheory (see section\n 1.1).\n Even more\nbroadly, it is part of the tradition according to which the meaning of\na term should be explained by reference to the way it is used\nin our language.\n\n\nWithin philosophy, proof-theoretic semantics has mostly figured\nunder the heading “theory of meaning”. This terminology\nfollows Dummett, who claimed that the theory of meaning is the basis of\ntheoretical philosophy, a view which he attributed to Frege. The term\n“proof-theoretic semantics” was proposed by\nSchroeder-Heister (1991; used already in 1987 lectures in Stockholm) in order not to leave the term\n“semantics” to denotationalism alone—after all,\n“semantics” is the standard term for investigations dealing\nwith the meaning of linguistic expressions. Furthermore, unlike\n“theory of meaning”, the term “proof-theoretic\nsemantics” covers philosophical and technical aspects likewise.\nIn 1999, the first conference with this title took place in\nTübingen, the second one in 2013. The first textbook with this title appeared in 2015. \n\n\n\nThe term “general proof theory” was coined by Prawitz.\nIn general proof theory, “proofs are studied in their own right\nin the hope of understanding their nature”, in contradistinction\nto Hilbert-style “reductive proof theory”, which is the\n“attempt to analyze the proofs of mathematical theories with the\nintention of reducing them to some more elementary part of mathematics\nsuch as finitistic or constructive mathematics” (Prawitz, 1972,\np. 123). In a similar way, Kreisel (1971) asks for a re-orientation of\nproof theory. He wants to explain “recent work in proof theory\nfrom a neglected point of view. Proofs and their representations by\nformal derivations are treated as principal objects of study, not as\nmere tools for analyzing the consequence relation.” (Kreisel,\n1971, p. 109) Whereas Kreisel focuses on the dichotomy between a theory\nof proofs and a theory of provability, Prawitz concentrates on the\ndifferent goals proof theory may pursue. However, both stress the\nnecessity of studying proofs as fundamental entities by means of which\nwe acquire demonstrative (especially mathematical) knowledge. This\nmeans in particular that proofs are epistemic entities which should not\nbe conflated with formal proofs or derivations. They are rather what\nderivations denote when they are considered to be representations of\narguments. (However, in the following we often use “proof”\nsynonymously with “derivation”, leaving it to the reader to\ndetermine whether formal proofs or proofs as epistemic entities are\nmeant.) In discussing Prawitz’s (1971) survey, Kreisel (1971,\np. 111) explicitly speaks of a “mapping” between\nderivations and mental acts and considers it as a task of proof theory\nto elucidate this mapping, including the investigation of the identity\nof proofs, a topic that Prawitz and Martin-Löf had put on the\nagenda. \n\nThis means that in general proof theory we are not solely interested\nin whether B follows from A, but in the way by means\nof which we arrive at B starting from A. In this\nsense general proof theory is intensional and epistemological in\ncharacter, whereas model theory, which is interested in the consequence\nrelation and not in the way of establishing it, is extensional and\nmetaphysical. \n\nProof-theoretic semantics is inherently inferential, as it is\ninferential activity which manifests itself in proofs. It thus belongs\nto inferentialism (see Brandom, 2000) according to which\ninferences and the rules of inference establish the meaning of\nexpressions, in contradistinction to denotationalism,\naccording to which denotations are the primary sort of meaning.\nInferentialism and the ‘meaning-as-use’ view of semantics\nis the broad philosophical framework of proof-theoretic semantics. This\ngeneral philosophical and semantical perspective merged with\nconstructive views which originated in the philosophy of mathematics,\nespecially in mathematical intuitionism. Most forms of proof-theoretic\nsemantics are intuitionistic in spirit, which means in particular that\nprinciples of classical logic such as the law of excluded middle or the\ndouble negation law are rejected or at least considered problematic.\nThis is partly due to the fact that the main tool of proof-theoretic\nsemantics, the calculus of natural deduction, is biased towards\nintuitionistic logic, in the sense that the straightforward formulation\nof its elimination rules is the intuitionistic one. There classical\nlogic is only available by means of some rule of indirect proof, which,\nat least to some extent, destroys the symmetry of the reasoning\nprinciples (see section\n 3.5).\n If one adopts\nthe standpoint of natural deduction, then intuitionistic logic is a\nnatural logical system. Also the BHK (Brouwer-Heyting-Kolmogorov)\ninterpretation of the logical signs plays a significant role. This\ninterpretation is not a unique approach to semantics, but comprises\nvarious ideas which are often more informally than formally described.\nOf particular importance is its functional view of implication,\naccording to which a proof of A → B is a\nconstructive function which, when applied to a proof of A\nyields a proof of B. This functional perspective underlies\nmany conceptions of proof-theoretic semantics, in particular those of\nLorenzen, Prawitz and Martin Löf (see sections\n 2.1.1,\n 2.2.2,\n 2.2.3). \n\nAccording to Dummett, the logical position of intuitionism\ncorresponds to the philosophical position of anti-realism. The realist\nview of a recognition independent reality is the metaphysical\ncounterpart of the view that all sentences are either true or false\nindependent of our means of recognizing it. Following Dummett, major\nparts of proof-theoretic semantics are associated with\nanti-realism. \n\nGentzen’s calculus of natural deduction and its rendering by\nPrawitz is the background to most approaches to proof-theoretic\nsemantics. Natural deduction is based on at least three major\nideas: \n\nIn Gentzen’s natural deduction system for first-order logic\nderivations are written in tree form and based on the well-known rules.\nFor example, implication has the following introduction and elimination\nrules \n\nwhere the brackets indicate the possibility to discharge occurrences\nof the assumption A. The open assumptions of a\nderivation are those assumptions on which the end-formula depends. A\nderivation is called closed, if it has no open assumption,\notherwise it is called open. If we deal with quantifiers, we\nhave to consider open individual variables (sometimes called\n“parameters”), too. Metalogical features crucial for\nproof-theoretic semantics and for the first time systematically\ninvestigated and published by Prawitz (1965) include: \n\nReduction: For every detour consisting of an introduction\nimmediately followed by an elimination there is a reduction step\nremoving this detour. \n\nNormalization: By successive applications of reductions,\nderivations can be transformed into normal forms which contain no\ndetours. \n\nFor implication the standard reduction step removing detours is the\nfollowing: \n\nA simple, but very important corollary of normalization is the\nfollowing: Every closed derivation in intuitionistic logic can be\nreduced to a derivation using an introduction rule in the last\nstep. We also say that intuitionistic natural deduction satisfies\nthe “introduction form property”. In\nproof-theoretic semantics this result figures prominently under the\nheading “fundamental assumption” (Dummett, 1991,\np. 254). The “fundamental assumption” is a typical\nexample of a philosophical re-interpretation of a technical\nproof-theoretic result. Further Reading: \n\nFor the general orientation of proof-theoretic semantics\nthe special issue of Synthese (Kahle and Schroeder-Heister,\n2006) the reader edited by Piecha and Schroeder-Heister (2016b), the textbook by Francez (2015), Schroeder-Heister (2008b, 2016a), and Wansing (2000). \n\nFor the philosophical position and development of proof\ntheory the entries on\n Hilbert’s program\nand the\n development of proof theory\nas well as Prawitz (1971). \n\nFor intuitionism the entries on\n intuitionistic logic,\n intuitionism in the philosophy of mathematics\nand the\n development of intuitionistic logic. \n\nFor anti-realism the entry on \n challenges to metaphysical realism\n as well as Tennant (1987); Tennant (1997), Tranchini (2010);\nTranchini (2012a). \n\nFor Gentzen-style proof-theory and the theory of natural\ndeduction: besides Gentzen’s (1934/35) original presentation,\nJaśkowski’s (1934) theory of suppositions and Prawitz’s (1965)\nclassic monograph, Tennant (1978), Troelstra and Schwichtenberg\n(2000), and Negri and von Plato (2001). \n\nThe semantics of implication lies at the heart of proof-theoretic\nsemantics. In contradistinction to classical truth-condition semantics,\nimplication is a logical constant in its own right. It has also the\ncharacteristic feature that it is tied to the concept of consequence.\nIt can be viewed as expressing consequence at the sentential level due\nto modus ponens and to what in Hilbert-style systems is called the\ndeduction theorem, i.e. the equivalence of Γ,A ⊢\nB and Γ ⊢ A → B. \n\nA very natural understanding of an implication A →\nB is reading it as expressing the inference rule which allows\none to pass over from A to B. Licensing the step from\nA to B on the basis of A → B\nis exactly, what modus ponens says. And the deduction theorem can be\nviewed as the means of establishing a rule: Having shown that\nB can be deduced from A justifies the rule that from\nA we may pass over to B. A rule-based semantics of\nimplication along such lines underlies several conceptions of\nproof-theoretic semantics, notably those by Lorenzen, von Kutschera and\nSchroeder-Heister. \n\nLorenzen, in his Introduction to Operative Logics and\nMathematics (1955) starts with logic-free (atomic) calculi, which\ncorrespond to production systems or grammars. He calls a rule\nadmissible in such a system if it can be added to it without\nenlarging the set of its derivable atoms. The implication arrow →\nis interpreted as expressing admissibility. An implication A\n→ B is considered to be valid, if, when read as a rule,\nit is admissible (with respect to the underlying calculus). For\niterated implications (= rules) Lorenzen develops a theory of\nadmissibility statements of higher levels. Certain statements such as\nA →A or ((A →B),\n(B →C)) → (A →C)\nhold independently of the underlying calculus. They are called\nuniversally admissible\n[“allgemeinzulässig”]), and constitute a system of\npositive implicational logic. In a related way, laws for universal\nquantification ∀ are justified using admissibility\nstatements for rules with schematic variables. \n\nFor the justification of the laws for the logical constants ∧,\n∨, ∃ and ⊥, Lorenzen uses an inversion\nprinciple (a term he coined). In a very simplified form, without\ntaking variables in rules into account, the inversion principle says\nthat everything that can be obtained from every defining condition of\nA can be obtained from A itself. For example, in the\ncase of disjunction, let A and B each be a\ndefining condition of A∨B as expressed by the\nprimitive rules A → A∨B and\nB → A∨B. Then the inversion\nprinciple says that A∨B →C is\nadmissible assuming A →C and B\n→C, which justifies the elimination rule for disjunction.\nThe remaining connectives are dealt with in a similar way. In the case\nof ⊥, the absurdity rule ⊥→ A is obtained from\nthe fact that there is no defining condition for ⊥. \n\nIn what he calls “Gentzen semantics”, von Kutschera\n(1968) gives, as Lorenzen, a semantics of logically complex\nimplication-like statements\nA1,…,An\n→ B with respect to calculi K which govern the\nreasoning with atomic sentences. The fundamental difference to Lorenzen\nis the fact that\nA1,…,An\n→ B now expresses a derivability rather than an\nadmissibility statement. \n\nIn order to turn this into a semantics of the logical constants of\npropositional logic, von Kutschera argues as follows: When giving up\nbivalence, we can no longer use classical truth-value assignments to\natomic formulas. Instead we can use calculi which prove or refute\natomic sentences. Moreover, since calculi not only generate proofs or\nrefutations but arbitrary derivability relations, the idea is to start\ndirectly with derivability in an atomic system and extend it with rules\nthat characterize the logical connectives. For that von Kutschera gives\na sequent calculus with rules for the introduction of n-ary\npropositional connectives in the succedent and antecedent, yielding a\nsequent system for generalized propositional connectives. Von Kutschera\nthen goes on to show that the generalized connectives so defined can\nall be expressed by the standard connectives of intuitionistic logic\n(conjunction, disjunction, implication, absurdity). \n\nWithin a programme of developing a general schema for rules for\narbitrary logical constants, Schroeder-Heister (1984) proposed that a\nlogically complex formula should express the content or common\ncontent of systems of rules. This means that not the\nintroduction rules are considered basic but the consequences of\ndefining conditions. A rule R is either a formula A\nor has the form\nR1,…,Rn\n⇒ A, where\nR1,…,Rn\nare themselves rules. These so-called “higher-level rules”\ngeneralize the idea that rules may discharge assumptions to the case\nwhere these assumptions can themselves be rules. For the standard\nlogical constants this means that A∧B expresses\nthe content of the pair (A,B); A → B\nexpresses the content of the rule A ⇒ B;\nA∨B expresses the common content of A and\nB; and absurdity ⊥ expresses the common content of the\nempty family of rule systems. In the case of arbitrary n-ary\npropositional connectives this leads to a natural deduction system with\ngeneralized introduction and elimination rules. These general\nconnectives are shown to be definable in terms of the standard ones,\nestablishing the expressive completeness of the standard intuitionistic\nconnectives. \n\nFor Lorenzen’s approach in relation to Prawitz-style\nproof-theoretic semantics: Schroeder-Heister (2008a). For extensions of\nexpressive completeness in the style of von Kutschera: Wansing\n(1993a). \n\nIn his Investigations into Logical Deduction, Gentzen makes\nsome, nowadays very frequently quoted, programmatic remarks on the\nsemantic relationship between introduction and elimination inferences\nin natural deduction. \n\nThis cannot mean, of course, that the elimination rules are\ndeducible from the introduction rules in the literal sense of\nthe word; in fact, they are not. It can only mean that they can be\njustified by them in some way. \n\nSo the idea underlying Gentzen’s programme is that we have\n“definitions” in the form of introduction rules and some\nsort of semantic reasoning which, by using “certain\nrequirements”, validate the elimination rules. \n\nBy adopting Lorenzen’s term and adapting its underlying idea\nto the context of natural deduction, Prawitz (1965) formulated an\n“inversion principle” to make Gentzen’s remarks more\nprecise: \n\nHere the sufficient conditions are given by the premisses of the\ncorresponding introduction rules. Thus the inversion principle says\nthat a derivation of the conclusion of an elimination rule can be\nobtained without an application of the elimination rule if its major\npremiss has been derived using an introduction rule in the last step,\nwhich means that a combination \n\nof steps, where {Di} stands for a\n(possibly empty) list of deductions of minor premisses, can be\navoided. \n\nThe relationship between introduction and elimination rules is often\ndescribed as “harmony”, or as governed by a\n“principle of harmony” (see, e.g. Tennant, 1978,\np. 74). This terminology is not uniform and sometimes not even\nfully clear. It essentially expresses what is also meant by\n“inversion”. Even if “harmony” is a term which\nsuggests a symmetric relationship, it is frequently understood as\nexpressing a conception based on introduction rules as, e.g., in\nRead’s (2010) “general elimination harmony” (although\noccasionally one includes elimination based conceptions as well).\nSometimes harmony is supposed to mean that connectives are strongest or\nweakest in a certain sense given their introduction or their\nelimination rules. This idea underlies Tennant’s (1978) harmony\nprinciple, and also Popper’s and Koslow’s structural\ncharacterizations (see section\n 2.4).\n The\nspecific relationship between introduction and elimination rules as\nformulated in an inversion principle excludes alleged inferential\ndefinitions such as that of the connective tonk, which\ncombines an introduction rule for disjunction with an elimination rule\nfor conjunction, and which has given rise to a still ongoing debate on\nthe format of inferential definitions (see Humberstone, 2010). \n\nProof-theoretic validity is the dominating approach to\nproof-theoretic semantics. As a technical concept it was developed by\nPrawitz (1971; 1973; 1974), by turning a proof-theoretic validity\nnotion based on ideas by Tait (1967) and originally used to prove\nstrong normalization, into a semantical concept. Dummett provided much\nphilosophical underpinning to this notion (see Dummett, 1991). The\nobjects which are primarily valid are proofs as representations of\narguments. In a secondary sense, single rules can be valid if they lead\nfrom valid proofs to valid proofs. In this sense, validity is a global\nrather than a local notion. It applies to arbitrary derivations over a\ngiven atomic system, which defines derivability for atoms. Calling a\nproof which uses an introduction rule in the last step\ncanonical, it is based on the following three ideas: \n\nAd 1: The definition of validity is based on\nGentzen’s idea that introduction rules are\n‘self-justifying’ and give the logical constants their\nmeaning. This self-justifying feature is only used for closed proofs,\nwhich are considered primary over open ones. \n\nAd 2: Noncanonical proofs are justified by reducing them to\ncanonical ones. Thus reduction procedures (detour reductions) as used\nin normalization proofs play a crucial role. As they justify arguments,\nthey are also called “justifications” by Prawitz. This\ndefinition again only applies to closed proofs, corresponding to the\nintroduction form property of closed normal derivations in natural\ndeduction (see section\n 1.3). \n\nAd 3: Open proofs are justified by considering their closed\ninstances. These closed instances are obtained by replacing their open\nassumptions with closed proofs of them, and their open variables with\nclosed terms. For example, a proof of B from A is\nconsidered valid, if every closed proof, which is obtained by replacing\nthe open assumption A with a closed proof of A, is\nvalid. In this way, open assumptions are considered to be placeholders\nfor closed proofs, for which reason we may speak of a substitutional\ninterpretation of open proofs. \n\nThis yields the following definition of proof-theoretic\nvalidity: \n\nFormally, this definition has to be relativized to the atomic system\nconsidered, and to the set of justifications (proof reductions)\nconsidered. Furthermore, proofs are here understood as\ncandidates of valid proofs, which means that the rules from\nwhich they are composed are not fixed. They look like proof trees, but\ntheir individual steps can have an arbitrary (finite) number of\npremisses and can eliminate arbitrary assumptions. The definition of\nvalidity singles out those proof structures which are\n‘real’ proofs on the basis of the given reduction\nprocedures. \n\nValidity with respect to every choice of an atomic system can be\nviewed as a generalized notion of logical validity. In fact, if we\nconsider the standard reductions of intuitionistic logic, then all\nderivations in intuitionistic logic are valid independent of the atomic\nsystem considered. This is semantical correctness. We may ask\nif the converse holds, viz. whether, given that a derivation is valid\nfor every atomic system, there is a corresponding derivation in\nintuitionistic logic. That intuitionistic logic is complete in this\nsense is known as Prawitz’s conjecture (see Prawitz, 1973;\nPrawitz, 2013). However, no satisfactory proof of it has been given.\nThere are considerable doubts concerning the validity of this\nconjecture for systems that go beyond implicational logic. In any case\nit will depend on the precise formulation of the notion of validity, in\nparticular on its handling of atomic systems. \n\nFor a more formal definition and detailed examples demonstrating\nvalidity, as well as some remarks on Prawitz’s conjecture\n see the \n\nMartin-Löf’s type theory (Martin-Löf, 1984) is a\nleading approach in constructive logic and mathematics.\nPhilosophically, it shares with Prawitz the three fundamental\nassumptions of standard proof-theoretic semantics, mentioned in\n section\n 2.2.2:\n the priority of closed canonical\nproofs, the reduction of closed non-canonical proofs to canonical ones\nand the substitutional view of open proofs. However,\nMartin-Löf’s type theory has at least two characteristic\nfeatures which go beyond other approaches in proof-theoretic\nsemantics: \n\nThe first idea goes back to the Curry-Howard correspondence (see de\nGroote, 1995; Sørensen and Urzyczyn, 2006), according to which\nthe fact that a formula A has a certain proof can be codified\nas the fact that a certain term t is of type A,\nwhereby the formula A is identified with the type A.\nThis can be formalized in a calculus for type assignment, whose\nstatements are of the form t : A. A proof of\nt : A in this system can be read as showing that\nt is a proof of A. Martin-Löf (1995; 1998) has\nput this into a philosophical perspective by distinguishing this\ntwo-fold sense of proof in the following way. First we have proofs of\nstatements of the form t : A. These statements are\ncalled judgements, their proofs are called\ndemonstrations. Within such judgements the term\nt represents a proof of the proposition\nA. A proof in the latter sense is also called a proof\nobject. When demonstrating a judgement t : A, we\ndemonstrate that t is a proof (object) for the proposition\nA. Within this two-layer system the demonstration\nlayer is the layer of argumentation. Unlike proof objects,\ndemonstrations have epistemic significance; their judgements carry\nassertoric force. The proof layer is the layer at which meanings are\nexplained: The meaning of a proposition A is explained by\ntelling what counts as a proof (object) for A. The distinction\nmade between canonical and non-canonical proofs is a distinction at the\npropositional and not at the judgement al layer. This implies a certain\nexplicitness requirement. When I have proved something, I must not\nonly have a justification for my proof at my disposal as in\nPrawitz’s notion of validity, but at the same time have to be\ncertain that this justification fulfills its purpose. This\ncertainty is guaranteed by a demonstration. Mathematically, this\ntwo-fold sense of proof develops its real power only when types may\nthemselves depend on terms. Dependent types are a basic ingredient of\nof Martin-Löf’s type theory and related approaches. \n\nThe second idea makes Martin-Löf’s approach strongly\ndiffer from all other definitions of proof-theoretic validity. The\ncrucial difference, for example, to Prawitz’s procedure is that\nit is not metalinguistic in character, where\n“metalinguistic” means that propositions and candidates of\nproofs are specified first and then, by means of a definition in the\nmetalanguage, it is fixed which of them are valid and which are not.\nRather, propositions and proofs come into play only in the context of\ndemonstrations. For example, if we assume that something is a proof of\nan implication A → B, we need not necessarily\nshow that both A and B are well-formed propositions\noutright, but, in addition to knowing that A is a proposition,\nwe only need to know that B is a proposition provided\nthat A has been proved. Being a proposition is\nexpressed by a specific form of judgement, which is established in the\nsame system of demonstration which is used to establish that a proof of\na proposition has been achieved. \n\nIn Martin-Löf’s theory, proof-theoretic semantics\nreceives a strongly ontological component. A recent debate deals with\nthe question of whether proof objects have a purely ontological status\nor whether they codify knowledge, even if they are not epistemic acts\nthemselves. \n\nFor inversion principles see Schroeder-Heister (2007). \n\nFor variants of proof-theoretic harmony see Francez (2015) and Schroeder-Heister (2016a). \n\nFor Prawitz’s definition of proof-theoretic validity\nsee Schroeder-Heister (2006). \n\nFor Matin-Löf’s type theory, see the entry on\n type theory\nas well as Sommaruga (2000). \n\nProof-theoretic semantics normally focuses on logical constants.\nThis focus is practically never questioned, apparently because it is\nconsidered so obvious. In proof theory, little attention has been paid\nto atomic systems, although there has been Lorenzen’s early work\n(see section\n 2.1.1),\n where the\njustification of logical rules is embedded in a theory of arbitrary\nrules, and Martin-Löf’s (1971) theory of iterated inductive\ndefinitions where introduction and elimination rules for atomic\nformulas are proposed. The rise of logic programming has widened this\nperspective. From the proof-theoretic point of view, logic programming\nis a theory of atomic reasoning with respect to clausal definitions of\natoms. Definitional reflection is an approach to proof-theoretic\nsemantics that takes up this challenge and attempts to build a theory\nwhose range of application goes beyond logical constants. \n\nIn logic programming we are dealing with program clauses of the\nform \n\nwhich define atomic formulas. Such clauses can naturally be\ninterpreted as describing introduction rules for atoms. From the point\nof view of proof-theoretic semantics the following two points are\nessential: \n\n(1)  Introduction rules (clauses) for logically compound\nformulas are not distinguished in principle from introduction rules\n(clauses) for atoms. Interpreting logic programming proof-theoretically\nmotivates an extension of proof-theoretic semantics to arbitrary atoms,\nwhich yields a semantics with a much wider realm of applications. \n\n(2)  Program clauses are not necessarily well-founded. For\nexample, the head of a clause may occur in its body. Well-founded\nprograms are just a particular sort of programs. The use of arbitrary\nclauses without further requirements in logic programming is a\nmotivation to pursue the same idea in proof-theoretic semantics,\nadmitting just any sort of introduction rules and not just those of a\nspecial form, and in particular not necessarily ones which are\nwell-founded. This carries the idea of definitional freedom, which is a\ncornerstone of logic programming, over to semantics, again widening the\nrealm of application of proof-theoretic semantics. \n\nThe idea of considering introduction rules as meaning-giving rules\nfor atoms is closely related to the theory of inductive definitions in\nits general form, according to which inductive definitions are systems\nof rules (see Aczel, 1977). \n\nThe theory of definitional reflection (Hallnäs, 1991;\nHallnäs, 2006; Hallnäs and Schroeder-Heister, 1990/91;\nSchroeder-Heister, 1993) takes up the challenge from logic programming\nand gives a proof-theoretic semantics not just for logical constants\nbut for arbitrary expressions, for which a clausal definition can be\ngiven. Formally, this approach starts with a list of clauses which is\nthe definition considered. Each clause has the form \n\nwhere the head A is an atomic formula (atom). In the\nsimplest case, the body Δ is a list of atoms\nB1,…,Bm,\nin which case a definition looks like a definite logic program. We\noften consider an extended case where Δ may also contain some\nstructural implication ‘⇒’, and sometimes even some\nstructural universal implication, which essentially is handled by\nrestricting substitution. If the definition of A has the\nform \n\nthen A has the following introduction and elimination\nrules \n\nThe introduction rules, also called rules of definitional\nclosure, express reasoning ‘along’ the clauses. The\nelimination rule is called the principle of definitional\nreflection, as it reflects upon the definition as a whole. If\nΔ1,…,\nΔn exhaust all possible conditions\nto generate A according to the given definition, and if each\nof these conditions entails the very same conclusion C, then\nA itself entails this conclusion. If the clausal definition \nis viewed as an inductive definition, this principle can be viewed as\nexpressing the extremal clause in inductive definitions: Nothing else\nbeyond the clauses given defines A. Obviously, definitional\nreflection is a generalized form of the inversion principles discussed.\nIt develops its genuine power in definitional contexts with free\nvariables that go beyond purely propositional reasoning, and in\ncontexts which are not well-founded. An example of a non-wellfounded\ndefinition is the definition of an atom R by its own\nnegation: \n\nThis example is discussed in detail in the \n\nFor non-wellfoundedness and paradoxes see the entries\non\n self-reference and\n Russell’s paradox,\nas well as the references quoted in the supplement linked to. \n\nThere is a large field of ideas and results concerning what might be\ncalled the “structural characterization” of logical\nconstants, where “structural” is here meant both in the\nproof-theoretic sense of “structural rules” and in the\nsense of a framework that bears a certain structure, where this\nframework is again proof-theoretically described. Some of its authors\nuse a semantical vocabulary and at least implicitly suggest that their\ntopic belongs to proof-theoretic semantics. Others explicitly deny\nthese connotations, emphasizing that they are interested in a\ncharacterization which establishes the logicality of a constant. The\nquestion “What is a logical constant?” can be answered in\nproof-theoretic terms, even if the semantics of the constants\nthemselves is truth-conditional: Namely by requiring that the (perhaps\ntruth-conditionally defined) constants show a certain inferential\nbehaviour that can be described in proof-theoretic terms. However, as\nsome of the authors consider their characterization at the same time as\na semantics, it is appropriate that we mention some of these approaches\nhere. \n\nThe most outspoken structuralist with respect to logical constants,\nwho explicitly understands himself as such, is Koslow. In his\nStructuralist Theory of Logic (1992) he develops a theory of\nlogical constants, in which he characterizes them by certain\n“implication relations”, where an implication relation\nroughly corresponds to a finite consequence relation in Tarski’s\nsense (which again can be described by certain structural rules of a\nsequent-style system). Koslow develops a structural theory in the\nprecise metamathematical sense, which does not specify the domain of\nobjects in any way beyond the axioms given. If a language or any other\ndomain of objects equipped with an implication relation is given, the\nstructural approach can be used to single out logical compounds by\nchecking their implicational properties. \n\nIn his early papers on the foundations of logic, Popper (1947a;\n1947b) gives inferential characterizations of logical constants in\nproof-theoretic terms. He uses a calculus of sequents and characterizes\nlogical constants by certain derivability conditions of such sequents.\nHis terminology clearly suggests that he intends a proof-theoretic\nsemantics of logical constants, as he speaks of “inferential\ndefinitions” and the “trivialization of mathematical\nlogic” achieved by defining constants in the way described.\nAlthough his presentation is not free from conceptual imprecision and\nerrors, he was the first to consider the sequent-style inferential\nbehaviour of logical constants to characterize them. This is all the\nmore remarkable as he was probably not at all, and definitely not fully\naware of Gentzen’s sequent calculus and Gentzen’s further\nachievements (he was in correspondence with Bernays, though). However,\nagainst his own opinion, his work can better be understood as an\nattempt to define the logicality of constants and to structurally\ncharacterize them, than as a proof-theoretic semantics in the genuine\nsense. He nevertheless anticipated many ideas now common in\nproof-theoretic semantics, such as the characterization of logical\nconstants by means of certain minimality or maximality conditions with\nrespect to introduction or elimination rules. \n\nImportant contributions to the logicality debate that characterize\nlogical constants inferentially in terms of sequent calculus rules are\nthose by Kneale (1956) and Hacking (1979). A thorough account of\nlogicality is proposed by Došen (1980; 1989) in\nhis theory of logical constants as “punctuation marks”,\nexpressing structural features at the logical level. He understands\nlogical constants as being characterized by certain double-line rules\nfor sequents which can be read in both directions. For example,\nconjunction and disjunction are (in classical logic, with\nmultiple-formulae succedents) characterized by the double-line\nrules \n\nDošen is able to give characterizations which include systems\nof modal logic. He explicitly considers his work as a contribution to\nthe logicality debate and not to any conception of proof-theoretic\nsemantics. Sambin et al., in their Basic Logic (Sambin, Battilotti, and\nFaggian, 2000), explicitly understand what Došen calls\ndouble-line rules as fundamental meaning giving rules. The double-line\nrules for conjunction and disjunction are read as implicit definitions\nof these constants, which by some procedure can be turned into the\nexplicit sequent-style rules we are used to. So Sambin et al. use the\nsame starting point as Došen, but interpret it not as a\nstructural description of the behaviour of constants, but semantically\nas their implicit definition (see Schroeder-Heister, 2013). \n\nThere are several other approaches to a uniform proof-theoretic\ncharacterization of logical constants, all of whom at least touch upon\nissues of proof-theoretic semantics. Such theories are Belnap’s\nDisplay Logic (Belnap, 1982), Wansing’s Logic of Information\nStructures (Wansing, 1993b), generic proof editing systems and their\nimplementations such as the Edinburgh logical framework (Harper,\nHonsell, and Plotkin, 1987) and many successors which allow the\nspecification of a variety of logical systems. Since the rise of linear\nand, more generally, substructural logics (Di Cosmo and Miller, 2010;\nRestall, 2009) there are various approaches dealing with logics that\ndiffer with respect to restrictions on their structural rules. A\nrecent movement away from singling out a particular logic as the true\none towards a more pluralist stance (see, e.g., Beall and Restall,\n2006) which is interested in what different logics have in common\nwithout any preference for a particular logic can be seen as a shift\naway from semantical justification towards structural\ncharacterization. \n\n\nThere is a considerable literature on category theory in relation to\nproof theory, and, following seminal work by Lawvere, Lambek and others\n(see Lambek and Scott, 1986, and the references therein), category\nitself can be viewed as a kind of abstract proof theory. If one looks\nat an arrow A → B in a category as a kind of\nabstract proof of B from A, we have a representation\nwhich goes beyond pure derivability of B from A (as\nthe arrow has its individuality), but does not deal with the particular\nsyntactic structure of this proof. For intuitionistic systems,\nproof-theoretic semantics in categorial form comes probably closest to\nwhat denotational semantics is in the classical case. \n\nOne of the most highly developed approaches to categorial proof theory is due to Došen. He has not only advanced the application of categorial methods in proofs theory (e.g., Došen and Petrić, 2004), but also shown how proof-theoretic methods can be used in category theory itself (Došen, 2000). Most important for categorial logic in relation to proof-theoretic semantics is that in categorial logic, arrows always come together with an identity relation, which in proof-theory corresponds to the identity of proofs. In this way, ideas and results of categorial proof theory pertain to what may be called intensional proof-theoretic semantics, that is, the study of proofs as entities in their own right, not just as vehicles to establish consequences (Došen, 2006, 2016). Another feature of categorial proof-theory is that it is inherently hypothetical in character, which means that it starts from hypothetical entities. It this way it overcomes a paradigm of standard, in particular validity-based, proof-theoretic semantics (see section 3.6 below). \nFurther Reading: \n\nFor Popper’s theory of logical constants see\nSchroeder-Heister (2005). \n\nFor logical constants and their logicality see the\nentry on\n logical constants. \n\nFor categorial approaches see  the entry on\n category theory. \n\nMost approaches to proof-theoretic semantics consider introduction\nrules as basic, meaning giving, or self-justifying, whereas the\nelimination inferences are justified as valid with respect to the given\nintroduction rules. This conception has at least three roots: The first\nis a verificationist theory of meaning according to which the\nassertibility conditions of a sentence constitute its meaning. The\nsecond is the idea that we must distinguish between what gives the\nmeaning and what are the consequences of this meaning, as not all\ninferential knowledge can consist of applications of definitions. The\nthird one is the primacy of assertion over other speech acts such as\nassuming or denying, which is implicit in all approaches considered so\nfar. \n\nOne might investigate how far one gets by considering elimination\nrules rather than introduction rules as a basis of proof-theoretic\nsemantics. Some ideas towards a proof-theoretic semantics based on\nelimination rather than introduction rules have been sketched by\nDummett (1991, Ch. 13), albeit in a very rudimentary form. A more\nprecise definition of validity based on elimination inferences is due\nto Prawitz (1971; 2007; see also Schroeder-Heister 2015). Its essential idea is that a closed proof is\nconsidered valid, if the result of applying an elimination rule to its\nend formula is a valid proof or reduces to one. For example, a closed\nproof of an implication A → B is valid, if, for\nany given closed proof of A, the result of applying modus\nponens \n\nto these two proofs is a valid proof of B, or reduces to\nsuch a proof. This conception keeps two of the three basic ingredients\nof Prawitz-style proof-theoretic semantics (see section\n 2.2.2):\n the role of proof reduction and the\nsubstitutional view of assumptions. Only the canonicity of proofs\nending with introductions is changed into the canonicity of proofs\nending with eliminations. \n\nStandard proof-theoretic semantics is assertion-centred in that\nassertibility conditions determine the meaning of logical constants.\nCorresponding to the intuitionistic way of proceeding, the negation\n¬A of a formula A is normally understood as\nimplying absurdity A →⊥, where ⊥ is a\nconstant which cannot be asserted, i.e., for which no assertibility\ncondition is defined. This is an ‘indirect’ way of\nunderstanding negation. In the literature there has been the discussion\nof what, following von Kutschera (1969), might be called\n‘direct’ negation. By that one understands a one-place\nprimitive operator of negation, which cannot be, or at least is not,\nreduced to implying absurdity. It is not classical negation either. It\nrather obeys rules which dualize the usual rules for the logical\nconstants. Sometimes it is called the “denial” of a\nsentence, sometimes also “strong negation” (see Odintsov,\n2008). Typical rules for the denial ~A of A are \n\nEssentially, the denial rules for an operator correspond to the\nassertion rules for the dual operator. Several logics of denial have\nbeen investigated, in particular Nelson’s logics of\n“constructible falsity” motivated first by Nelson (1949)\nwith respect to a certain realizability semantics. The main focus has\nbeen on his systems later called N3 and N4 which differ with respect to\nthe treatment of contradiction (N4 is N3 without ex contradictione\nquodlibet). Using denial any approach to proof-theoretic semantics\ncan be dualized by just exchanging assertion and denial and turning\nfrom logical constants to their duals. In doing so, one obtains a\nsystem based on refutation (= proof of denial) rather than proof. It\ncan be understood as applying a Popperian view to proof-theoretic\nsemantics. \n\nAnother approach would be to not just dualize assertion-centered\nproof-theoretic semantics in favour of a denial-centered\nrefutation-theoretic semantics, but to see the relation between rules\nfor assertion and for denial as governed by an inversion principle or\nprinciple of definitional reflection of its own. This would be a\nprinciple of what might be called\n“assertion-denial-harmony”. Whereas in standard\nproof-theoretic semantics, inversion principles control the\nrelationship between assertions and assumptions (or consequences), such\na principle would now govern the relationship between assertion and\ndenial. Given certain defining conditions of A, it would say\nthat the denial of every defining condition of A leads to the\ndenial of A itself. For conjunction and disjunction it leads\nto the common pairs of assertion and denial rules \n\nThis idea can easily be generalized to definitional reflection,\nyielding a reasoning system in which assertion and denial are\nintertwined. It has parallels to the deductive relations between the\nforms of judgement studied in the traditional square of opposition\n(Schroeder-Heister, 2012a; Zeilberger, 2008). It should be emphasized\nthat the denial operator is here an external sign indicating a form of\njudgement and not as a logical operator. This means in particular that\nit cannot be iterated. \n\nGentzen’s sequent calculus exhibits a symmetry between right\nand left introduction rules which suggest to look for a harmony\nprinciple that makes this symmetry significant to proof-theoretic\nsemantics. At least three lines have been pursued to deal with this\nphenomenon. (i) Either the right-introduction or or the\nleft-introduction rules are considered to be introduction rules. The\nopposite rules (left-introductions and right-introductions,\nrespectively) are then justified using the corresponding elimination\nrules. This means that the methods discussed before are applied to\nwhole sequents rather than formulas within sequents. Unlike these\nformulas, the sequents are not logically structured. Therefore this\napproach builds on definitional reflection, which applies harmony and\ninversion to rules for arbitrarily structured entities rather than for\nlogical composites only. It has been pursued by de Campos Sanz and\nPiecha (2009). (ii) The right- and left-introduction rules are derived\nfrom a characterization in the sense of Došen’s double\nline rules (section\n 2.4),\n which is then read\nas a definition of some sort. The top-down direction of a double-line\nrule is already a right- or a left-introduction rule. The other one can\nbe derived from the bottom-up direction by means of certain principles.\nThis is the basic meaning-theoretic ingredient of Sambin et al.’s\nBasic Logic (Sambin, Battilotti, and Faggian, 2000). (iii) The\nright- and left-introduction rules are seen as expressing an\ninteraction between sequents using the rule of cut. Given either the\nright- or the left-rules, the complementary rules express that\neverything that interacts with its premisses in a certain way so does\nwith its conclusion. This idea of interaction is a generalized\nsymmetric principle of definitional reflection. It can be considered to\nbe a generalization of the inversion principle, using the notion of\ninteraction rather than the derivability of consequences (see\nSchroeder-Heister, 2013). All three approaches apply to the sequent\ncalculus in its classical form, with possibly more than one formula in\nthe succedent of a sequent, including structurally restricted versions\nas investigated in linear and other logics. \n\nEven if, as in definitional reflection, we are considering\ndefinitional rules for atoms, their defining conditions do not normally\ndecompose these atoms. A proof-theoretic approach that takes the\ninternal structure of atomic sentences into account, has been proposed\nby Wieckowski (2008; 2011; 2016). He uses introduction and elimination rules\nfor atomic sentences, where these atomic sentences are not just reduced\nto other atomic sentences, but to subatomic expressions representing\nthe meaning of predicates and individual names. This can be seen as a\nfirst step towards natural language applications of proof-theoretic\nsemantics. A further step in this direction has been undertaken by\nFrancez, who developed a proof-theoretic semantics for several\nfragments of English (see Francez, Dyckhoff, and Ben-Avi, 2010; Francez\nand Dyckhoff, 2010, Francez and Ben-Avi 2015). \n\nProof-theoretic semantics is intuitionistically biased. This is due\nto the fact that natural deduction as its preferred framework has\ncertain features which make it particularly suited for intuitionistic\nlogic. In classical natural deduction the ex falso\nquodlibet \n\nis replaced with the rule of classical reductio ad\nabsurdum \n\nIn allowing to discharge A →⊥ in order to infer\nA, this rule undermines the subformula principle.\nFurthermore, in containing both ⊥ and A\n→⊥, it refers to two different logical constants in a single\nrule, so there is no separation of logical constants any more.\nFinally, as an elimination rule for ⊥ it does not follow the\ngeneral pattern of introductions and eliminations. As a consequence, it\ndestroys the introduction form property that every closed\nderivation can be reduced to one which uses an introduction rule in the\nlast step. \n\nClassical logic fits very well with the multiple-succedent sequent\ncalculus. There we do not need any additional principles beyond those\nassumed in the intuitionistic case. Just the structural feature of\nallowing for more than one formula in the succedent suffices to obtain\nclassical logic. As there are plausible approaches to establish a\nharmony between right-introductions and left-introduction in the\nsequent calculus (see  section\n 3.3),\n classical logic appears to be perfectly\njustified. However, this is only convincing if reasoning is\nappropriately framed as a multiple-conclusion process, even if this\ndoes not correspond to our standard practice where we focus on single\nconclusions. One could try to develop an appropriate intuition by\narguing that reasoning towards multiple conclusions delineates the area\nin which truth lies rather than establishing a single proposition as\ntrue. However, this intuition is hard to maintain and cannot be\nformally captured without serious difficulties. Philosophical\napproaches such as those by Shoesmith and Smiley (1978) and\nproof-theoretic approaches such as proof-nets (see Girard, 1987; Di\nCosmo and Miller, 2010) are attempts in this direction. \n\nA fundamental reason for the failure of the introduction form\nproperty in classical logic is the indeterminism inherent in the laws\nfor disjunction. A∨B can be inferred from \nA as well as from B. Therefore, if the disjunction laws were the\nonly way of inferring A∨B, the derivability of\nA∨¬A, which is a key principle of classical\nlogic, would entail that of either A or of ¬A,\nwhich is absurd. A way out of this difficulty is to abolish\nindeterministic disjunction and use instead its classical de Morgan\nequivalent ¬(¬A ∧¬B). This leads\nessentially to a logic without proper disjunction. In the quantifier\ncase, there would be no proper existential quantifier either, as\n∃xA would be understood in the sense of\n¬∀x¬A. If one is prepared to accept\nthis restriction, then certain harmony principles can be formulated for\nclassical logic. \n\nStandard approaches to proof-theoretic semantics, especially Prawitz’s\nvalidity-based approach (section 2.2.2), take closed derivations as\nbasic. The validity of open derivations is defined as the transmission\nof validity from closed derivations of the assumptions to a closed\nderivation of the assertion, where the latter is obtained by\nsubstituting a closed derivation for an open assumption. Therefore, if\none calls closed derivations ‘categorical’ and open derivations\n‘hypothetical’, one may characterize this approach as following two\nfundamental ideas: (I) The primacy of the categorical over the\nhypothetical, (II) the transmission view of consequence.  These two\nassumptions (I) and (II) may be viewed as dogmas of standard semantics\n(see Schroeder-Heister 2012c). “Standard semantics” here not only\nmeans standard proof-theoretic semantics, but also classical\nmodel-theoretic semantics, where these dogmas are assumed as\nwell. There one starts with the definition of truth, which is the\ncategorical concept, and defines consequence, the hypothetical\nconcept, as the transmission of truth from conditions to\nconsequent. From this point of view, constructive semantics, including\nproof-theoretic semantics, exchange the concept of truth with a\nconcept of construction or proof, and interpret “transmission” in\nterms of a constructive function or procedure, but otherwise leave the\nframework untouched. \n\nThere is nothing wrong in principle with these dogmas. However, there\nare phenomena that are difficult to deal with in the standard\nframework. Such a phenomenon is non-wellfoundedness, especially\ncircularity, where we may have consequences without transmission of\ntruth and provability. Another phenomenon are substructural\ndistinctions, where it is crucial to include the structuring of\nassumptions from the very beginning. Moreover, and this is most\ncrucial, we might define things in a certain way without knowing in\nadvance of whether our definition or chain of definitions is\nwell-founded or not. We do not first involve ourselves into the\nmetalinguistic study of the definition we start with, but would like\nto start to reason immediately. This problem does not obtain if we\nrestrict ourselves to the case of logical constants, where the\ndefining rules are trivially well-founded.  But the problem arises\nimmediately, when we consider more complicated cases that go beyond\nlogical constants. \n\nThis makes it worthwhile to proceed in the other direction and start with the hypothetical concept of consequence, i.e., characterize consequence directly without reducing\nit to the categorical case. Philosophically this means that the categorical concept is a\nlimiting concept of the hypothetical one. In the classical case, truth would be a limiting case of consequence, namely consequence without hypotheses. This program is closely related to the approach of categorial proof theory (section 2.5), which is based on the primacy of hypothetical entities (“arrows”). Formally, it would give preference to the sequent calculus over natural deduction, since the sequent calculus  allows the manipulation of the assumption side of a sequence by means of left-introduction rules.  \n\nor the pair \n\nas the elimination rules for conjunction. The second pair of rules would often be considered to be just a more complicated variant of the pair of projections. However, from an intensional point of view, these two pairs of rules are not identical. Identifying them corresponds to identifying A ∧ B and A ∧ (A → B), which is only extensionally, but not intensionally correct. As Došen has frequently argued (e.g., Došen 1997, 2006), formulas such as A ∧ B and A ∧ (A → B) are equivalent, but not isomorphic. Here “isomorphic” means that when proving one formula from the other and vice versa, we obtain, by combining these two proofs, the identity proof. This is not the case in this example.   Pursuing this idea leads to principles of harmony and inversion which are different from the the standard ones. As harmony and inversion lie at the heart of proof-theoretic semantics, many of its issues are touched. Taking the topic of intensionality seriously may reshape many fields of proof-theoretic semantics. And since the identity of proofs is a basic topic of categorial proof theory, the latter will need to receive stronger attention in proof-theoretic semantics than is currently the case.  \n\nFurther Reading \n\nFor negation and denial see Tranchini (2012b); Wansing\n(2001). \n\nFor natural language semantics see \n Francez (2015). \n\nFor classical logic see the entry on\n classical logic. \n\nFor hypothetical reasoning and intensional proof theoretic semantics see \n Došen (2003, 2016) and Schroeder-Heister (2016a). \n\nStandard proof-theoretic semantics has practically exclusively been\noccupied with logical constants. Logical constants play a central role\nin reasoning and inference, but are definitely not the exclusive, and\nperhaps not even the most typical sort of entities that can be defined\ninferentially. A framework is needed that deals with inferential\ndefinitions in a wider sense and covers both logical and extra-logical\ninferential definitions alike. The idea of definitional reflection with\nrespect to arbitrary definitional rules (see\n 2.3.2)\n and also natural language applications\n (see\n 3.4)\n point in this direction, but farther\nreaching conceptions can be imagined. Furthermore, the concentration on\nharmony, inversion principles, definitional reflection and the like is\nsomewhat misleading, as it might suggest that proof-theoretic semantics\nconsists of only that. It should be emphasized that already when it\ncomes to arithmetic, stronger principles are needed in addition to\ninversion. However, in spite of these limitations, proof-theoretic\nsemantics has already gained very substantial achievements that can\ncompete with more widespread approaches to semantics.","contact.mail":"psh@informatik.uni-tuebingen.de","contact.domain":"informatik.uni-tuebingen.de"}]
