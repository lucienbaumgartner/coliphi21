[{"date.published":"2003-04-09","date.changed":"2020-08-18","url":"https://plato.stanford.edu/entries/turing-test/","author1":"Graham Oppy","author1.info":"http://profiles.arts.monash.edu.au/graham-oppy/","author2.info":"http://www.csse.monash.edu.au/~dld/David.Dowe.publications.html","entry":"turing-test","body.text":"\n\n\n\nThe phrase “The Turing Test” is most properly used to\nrefer to a proposal made by Turing (1950) as a way of dealing with the\nquestion whether machines can think. According to Turing, the question\nwhether machines can think is itself “too meaningless” to\ndeserve discussion (442). However, if we consider the more\nprecise—and somehow related—question whether a digital computer\ncan do well in a certain kind of game that Turing describes\n(“The Imitation Game”), then—at least in Turing’s\neyes—we do have a question that admits of precise\ndiscussion. Moreover, as we shall see, Turing himself thought that it\nwould not be too long before we did have digital computers that could\n“do well” in the Imitation Game.\n\n\n\nThe phrase “The Turing Test” is sometimes used more\ngenerally to refer to some kinds of behavioural tests for the presence\nof mind, or thought, or intelligence in putatively minded\nentities. So, for example, it is sometimes suggested that The Turing\nTest is prefigured in Descartes’ Discourse on the\nMethod. (Copeland (2000:527) finds an anticipation of the test in\nthe 1668 writings of the Cartesian de Cordemoy. Abramson (2011a)\npresents archival evidence that Turing was aware of Descartes’\nlanguage test at the time that he wrote his 1950 paper. Gunderson\n(1964) provides an early instance of those who find that\nTuring’s work is foreshadowed in the work of Descartes.) In\nthe Discourse, Descartes says:\n\nIf there were machines which bore a resemblance to our\nbodies and imitated our actions as closely as possible for all\npractical purposes, we should still have two very certain means of\nrecognizing that they were not real men. The first is that they could\nnever use words, or put together signs, as we do in order to declare\nour thoughts to others. For we can certainly conceive of a machine so\nconstructed that it utters words, and even utters words that correspond\nto bodily actions causing a change in its organs. … But it is\nnot conceivable that such a machine should produce different\narrangements of words so as to give an appropriately meaningful answer\nto whatever is said in its presence, as the dullest of men can do.\nSecondly, even though some machines might do some things as well as we\ndo them, or perhaps even better, they would inevitably fail in others,\nwhich would reveal that they are acting not from understanding, but\nonly from the disposition of their organs. For whereas reason is a\nuniversal instrument, which can be used in all kinds of situations,\nthese organs need some particular action; hence it is for all practical\npurposes impossible for a machine to have enough different organs to\nmake it act in all the contingencies of life in the way in which our\nreason makes us act. (Translation by Robert Stoothoff)\n\n\n\nAlthough not everything about this passage is perfectly clear, it\ndoes seem that Descartes gives a negative answer to the question\nwhether machines can think; and, moreover, it seems that his giving\nthis negative answer is tied to his confidence that no mere machine\ncould pass The Turing Test: no mere machine could talk and act in the\nway in which adult human beings do. Since Descartes explicitly says\nthat there are “two very certain means” by which we can\nrule out that something is a machine—it is, according to Descartes,\ninconceivable that a mere machine could produce different arrangements\nof words so as to give an appropriately meaningful answer to whatever\nis said in its presence; and it is for all practical purposes\nimpossible for a machine to have enough different organs to make it act\nin all the contingencies of life in the way in which our reason makes\nus act—it seems that he must agree with the further claim that\nnothing that can produce different arrangements of words so as to give\nan appropriately meaningful answer to whatever is said in its presence\ncan be a machine. Given the further assumption—which one suspects\nthat Descartes would have been prepared to grant—that only things\nthat think can produce different arrangements of words so as to give an\nappropriately meaningful answer to whatever is said in their presence,\nit seems to follow that Descartes would have agreed that the Turing\nTest would be a good test of his confident assumption that there cannot\nbe thinking machines. Given the knowledge that something is indeed a\nmachine, evidence that that thing can produce different arrangements of\nwords so as to give an appropriately meaningful answer to whatever is\nsaid in its presence is evidence that there can be thinking machines.\n\n\n\nThe phrase “The Turing Test” is also sometimes used to\nrefer to certain kinds of purely behavioural allegedly logically\nsufficient conditions for the presence of mind, or thought, or\nintelligence, in putatively minded entities. So, for example, Ned\nBlock’s “Blockhead” thought experiment is often said to be\na (putative) knockdown objection to The Turing Test. (Block (1981)\ncontains a direct discussion of The Turing Test in this context.) Here,\nwhat a proponent of this view has in mind is the idea that it is\nlogically possible for an entity to pass the kinds of tests\nthat Descartes and (at least allegedly) Turing have in mind—to use\nwords (and, perhaps, to act) in just the kind of way that human beings\ndo—and yet to be entirely lacking in intelligence, not possessed of\na mind, etc.\n\n\n\nThe subsequent discussion takes up the preceding ideas in the order\nin which they have been introduced. First, there is a discussion of\nTuring’s paper (1950), and of the arguments contained therein. Second,\nthere is a discussion of current assessments of various proposals that\nhave been called “The Turing Test” (whether or not there is\nmuch merit in the application of this label to the proposals in\nquestion). Third, there is a brief discussion of some recent writings\non The Turing Test, including some discussion of the question whether\nThe Turing Test sets an appropriate goal for research into artificial\nintelligence. Finally, there is a very short discussion of Searle’s\nChinese Room argument, and, in particular, of the bearing of this\nargument on The Turing Test.\n\n\n\nTuring (1950) describes the following kind of game. Suppose that we\nhave a person, a machine, and an interrogator. The interrogator is in a\nroom separated from the other person and the machine. The object of the\ngame is for the interrogator to determine which of the other two is the\nperson, and which is the machine. The interrogator knows the other\nperson and the machine by the labels ‘X’ and\n‘Y’—but, at least at the beginning of the game,\ndoes not know which of the other person and the machine is\n‘X’—and at the end of the game says either\n‘X is the person and Y is the machine’ or\n‘X is the machine and Y is the person’.\nThe interrogator is allowed to put questions to the person and the\nmachine of the following kind: “Will X please tell me\nwhether X plays chess?” Whichever of the machine and the\nother person is X must answer questions that are addressed to\nX. The object of the machine is to try to cause the\ninterrogator to mistakenly conclude that the machine is the other\nperson; the object of the other person is to try to help the\ninterrogator to correctly identify the machine. About this game, Turing\n(1950) says:   There are at least two kinds of questions that can be raised about\nTuring’s predictions concerning his Imitation Game. First, there are\nempirical questions, e.g., Is it true that we now—or will\nsoon—have made computers that can play the imitation game so\nwell that an average interrogator has no more than a 70 percent chance\nof making the right identification after five minutes of questioning?\nSecond, there are conceptual questions, e.g., Is it true that, if an\naverage interrogator had no more than a 70 percent chance of making\nthe right identification after five minutes of questioning, we should\nconclude that the machine exhibits some level of thought, or\nintelligence, or mentality?  There is little doubt that Turing would have been disappointed by\nthe state of play at the end of the twentieth century.  Participants\nin the Loebner Prize Competition—an annual event in which\ncomputer programmes are submitted to the Turing Test— had come\nnowhere near the standard that Turing envisaged. A quick look at the\ntranscripts of the participants for the preceding decade reveals that\nthe entered programs were all easily detected by a range of\nnot-very-subtle lines of questioning. Moreover, major players in the\nfield regularly claimed that the Loebner Prize Competition was an\nembarrassment precisely because we were still so far from having a\ncomputer programme that could carry out a decent conversation for a\nperiod of five minutes—see, for example, Shieber (1994). It was\nwidely conceded on all sides that the programs entered in the Loebner\nPrize Competition were designed solely with the aim of winning the\nminor prize of best competitor for the year, with no thought that the\nembodied strategies would actually yield something capable of passing\nthe Turing Test. \n\nAt the end of the second decade of the twenty-first century, it is\nunclear how much has changed. On the one hand, there have been\ninteresting developments in language generators. In particular, the\nrelease of Open AI’s GPT-3 (Brown, et al. 2020, Other Internet\nResources) has prompted a flurry of excitement. GPT-3 is quite good at\ngenerating fiction, poetry, press releases, code, music, jokes,\ntechnical manuals, and news articles. Perhaps, as Chalmers speculates\n(2020, Other Internet Resources), GPT-3 “suggests a potential\nmindless path to artificial general intelligence”. But, of\ncourse, GPT-3 is not close to passing the Turing Test: GPT-3 neither\nperceives nor acts, and it is, at best, highly contentious whether it\nis a site of understanding. What remains to be seen is whether, within\nthe next couple of generations of language generators – GPT-4 or\nGPT-5 – we have something that can be linked to perceptual\ninputs and behavioural outputs in a way that does produce something\ncapable of passing the Turing Test. \n\nOn the other hand, as, for example, Floridi (2008) complains, there\nare other ways in which progress has been frustratingly slow. In 2014,\nclaims emerged that, because the computer program Eugene\nGoostman had fooled 33% of judges in the Turing Test 2014\ncompetition, it had “passed the Turing Test”. But there\nhave been other one-off competitions in which similar results have\nbeen achieved. Back in 1991, PC Therapist had 50% of judges\nfooled. And, in a 2011 demonstration, Cleverbot had an even\nhigher success rate. In all three of these cases, the size of the\ntrial was very small, and the result was not reliably projectible: in\nno case were there strong grounds for holding that an average\ninterrogator had no more than a 70% chance of making the right\ndetermination about the relevant program after five minutes of\nquestioning. Moreover—and much more importantly—we must\ndistinguish between the test the Turing proposed, and the particular\nprediction that he made about how things would be by the end of the\ntwentieth century. The percentage chance of making the correct\nidentification, the time interval over which the test takes place, and\nthe number of conversational exchanges required are all adjustable\nparameters in the Test, despite the fact that they are fixed in the\nparticular prediction that Turing made. Even if Turing was very far\nout in the prediction that he made about how things would be by the\nend of the twentieth century, it remains possible that the test that\nhe proposes is a good one. However, before one can endorse the\nsuggestion that the Turing Test is good, there are various objections\nthat ought to be addressed. \n\nSome people have suggested that the Turing Test is chauvinistic: it\nonly recognizes intelligence in things that are able to sustain a\nconversation with us. Why couldn’t it be the case that there are\nintelligent things that are unable to carry on a conversation, or, at\nany rate, unable to carry on a conversation with creatures like us?\n(See, for example, French (1990).) Perhaps the intuition behind this\nquestion can be granted; perhaps it is unduly chauvinistic to insist\nthat anything that is intelligent has to be capable of sustaining a\nconversation with us. (On the other hand, one might think that, given\nthe availability of suitably qualified translators, it ought to be\npossible for any two intelligent agents that speak different languages\nto carry on some kind of conversation.) But, in any case, the charge of\nchauvinism is completely beside the point. What Turing claims is only\nthat, if something can carry out a conversation with us, then we have\ngood grounds to suppose that that thing has intelligence of the kind\nthat we possess; he does not claim that only something that can carry\nout a conversation with us can possess the kind of intelligence that we\nhave. \n\nOther people have thought that the Turing Test is not sufficiently\ndemanding: we already have anecdotal evidence that quite unintelligent\nprograms (e.g., ELIZA—for details of which, see Weizenbaum\n(1966)) can seem to ordinary observers to be loci of intelligence for\nquite extended periods of time. Moreover, over a short period of\ntime—such as the five minutes that Turing mentions in his\nprediction about how things will be in the year 2000—it might\nwell be the case that almost all human observers could be taken in by\ncunningly designed but quite unintelligent programs. However, it is\nimportant to recall that, in order to pass Turing’s Test, it is not\nenough for the computer program to fool “ordinary\nobservers” in circumstances other than those in which the test\nis supposed to take place. What the computer program has to be able to\ndo is to survive interrogation by someone who knows that one of the\nother two participants in the conversation is a machine. Moreover, the\ncomputer program has to be able to survive such interrogation with a\nhigh degree of success over a repeated number of trials. (Turing says\nnothing about how many trials he would require. However, we can safely\nassume that, in order to get decent evidence that there is no more\nthan a 70% chance that a machine will be correctly identified as a\nmachine after five minutes of conversation, there will have to be a\nreasonably large number of trials.) If a computer program could do\nthis quite demanding thing, then it does seem plausible to claim that\nwe would have at least prima facie reason for thinking that\nwe are in the presence of intelligence. (Perhaps it is worth\nemphasizing again that there might be all kinds of intelligent\nthings—including intelligent machines—that would not pass this\ntest. It is conceivable, for example, that there might be machines\nthat, as a result of moral considerations, refused to lie or to engage\nin pretence. Since the human participant is supposed to do everything\nthat he or she can to help the interrogator, the question “Are\nyou a machine?” would quickly allow the interrogator to sort\nsuch (pathological?) truth-telling machines from humans.) \n\nAnother contentious aspect of Turing’s paper (1950) concerns his\nrestriction of the discussion to the case of “digital\ncomputers.” On the one hand, it seems clear that this restriction\nis really only significant for the prediction that Turing makes about\nhow things will be in the year 2000, and not for the details of the\ntest itself. (Indeed, it seems that if the test that Turing proposes is\na good one, then it will be a good test for any kinds of entities,\nincluding, for example, animals, aliens, and analog computers. That is:\nif animals, aliens, analog computers, or any other kinds of things,\npass the test that Turing proposes, then there will be as much reason\nto think that these things exhibit intelligence as there is reason to\nthink that digital computers that pass the test exhibit intelligence.)\nOn the other hand, it is actually a highly controversial question\nwhether “thinking machines” would have to be digital\ncomputers; and it is also a controversial question whether Turing\nhimself assumed that this would be the case. In particular, it is worth\nnoting that the seventh of the objections that Turing (1950) considers\naddresses the possibility of continuous state machines, which Turing\nexplicitly acknowledges to be different from discrete state machines.\nTuring appears to claim that, even if we are continuous state machines,\na discrete state machine would be able to imitate us sufficiently well\nfor the purposes of the Imitation Game. However, it seems doubtful that\nthe considerations that he gives are sufficient to establish that, if\nthere are continuous state machines that pass the Turing Test, then it\nis possible to make discrete state machines that pass the test as well.\n(Turing himself was keen to point out that some limits had to be set on\nthe notion of “machine” in order to make the question about\n“thinking machines” interesting: \n\nBut, of course, as Turing himself recognized, there is a large class\nof possible “machines” that are neither digital nor\nbiotechnological.) More generally, the crucial point seems to be that,\nwhile Turing recognized that the class of machines is potentially much\nlarger than the class of discrete state machines, he was himself\nvery confident that properly engineered discrete state\nmachines could succeed in the Imitation Game (and, moreover, at the\ntime that he was writing, there were certain discrete state\nmachines—“electronic computers”—that loomed very\nlarge in the public imagination). \n\nAlthough Turing (1950) is pretty informal, and, in some ways rather\nidiosyncratic, there is much to be gained by considering the\ndiscussion that Turing gives of potential objections to his claim that\nmachinese—and, in particular, digital computers—can\n“think”.  Turing gives the following labels to the\nobjections that he considers: (1) The Theological Objection; (2) The\n“Heads in the Sand” Objection; (3) The Mathematical\nObjection; (4) The Argument from Consciousness; (5) Arguments from\nVarious Disabilities; (6) Lady Lovelace’s Objection; (7) Argument from\nContinuity of the Nervous System; (8) The Argument from Informality of\nBehavior; and (9) The Argument from Extra-Sensory Perception. We shall\nconsider these objections in the corresponding subsections below. (In\nsome—but not all—cases, the counter-arguments to these objections\nthat we discuss are also provided by Turing.) \n\nSubstance dualists believe that thinking is a function of a\nnon-material, separately existing, substance that somehow\n“combines” with the body to make a person. So—the\nargument might go—making a body can never be sufficient to guarantee\nthe presence of thought: in themselves, digital computers are no\ndifferent from any other merely material bodies in being utterly unable\nto think. Moreover—to introduce the “theological”\nelement—it might be further added that, where a “soul”\nis suitably combined with a body, this is always the work of the divine\ncreator of the universe: it is entirely up to God whether or not a\nparticular kind of body is imbued with a thinking soul. (There is well\nknown scriptural support for the proposition that human beings are\n“made in God’s image”. Perhaps there is also theological\nsupport for the claim that only God can make things in God’s\nimage.) \n\nThere are several different kinds of remarks to make here. First,\nthere are many serious objections to substance dualism. Second, there\nare many serious objections to theism. Third, even if theism and\nsubstance dualism are both allowed to pass, it remains quite unclear\nwhy thinking machines are supposed to be ruled out by this combination\nof views. Given that God can unite souls with human bodies, it is hard\nto see what reason there is for thinking that God could not unite souls\nwith digital computers (or rocks, for that matter!). Perhaps, on this\ncombination of views, there is no especially good reason why, amongst\nthe things that we can make, certain kinds of digital computers turn\nout to be the only ones to which God gives souls—but it seems pretty\nclear that there is also no particularly good reason for ruling out the\npossibility that God would choose to give souls to certain kinds of\ndigital computers. Evidence that God is dead set against the idea of\ngiving souls to certain kinds of digital computers is not particularly\nthick on the ground. \n\nIf there were thinking machines, then various consequences would\nfollow. First, we would lose the best reasons that we have for thinking\nthat we are superior to everything else in the universe (since our\ncherished “reason” would no longer be something that we\nalone possess). Second, the possibility that we might be\n“supplanted” by machines would become a genuine worry: if\nthere were thinking machines, then very likely there would be machines\nthat could think much better than we can. Third, the possibility that\nwe might be “dominated” by machines would also become a\ngenuine worry: if there were thinking machines, who’s to say that they\nwould not take over the universe, and either enslave or exterminate us? \n\nAs it stands, what we have here is not an argument against the claim\nthat machines can think; rather, we have the expression of various\nfears about what might follow if there were thinking machines. Someone\nwho took these worries seriously—and who was persuaded that it is\nindeed possible for us to construct thinking machines—might well\nthink that we have here reasons for giving up on the project of\nattempting to construct thinking machines. However, it would be a major\ntask—which we do not intend to pursue here—to determine whether\nthere really are any good reasons for taking these worries\nseriously. \n\nSome people have supposed that certain fundamental results in\nmathematical logic that were discovered during the 1930s—by\nGödel (first incompleteness theorem) and Turing (the halting\nproblem)—have important consequences for questions about digital\ncomputation and intelligent thought. (See, for example, Lucas (1961)\nand Penrose (1989); see, too, Hodges (1983:414) who mentions Polanyi’s\ndiscussions with Turing on this matter.) Essentially, these results\nshow that within a formal system that is strong enough, there are a\nclass of true statements that can be expressed but not proven within\nthe system (see the entry on\n Gödel’s incompleteness theorems).\n Let us say that\nsuch a system is “subject to the Lucas-Penrose constraint” because it\nis constrained from being able to prove a class of true statements\nexpressible within the system.  \n\nTuring (1950:444) himself observes that these results from\nmathematical logic might have implications for the Turing test: \n\nSo, in the context of the Turing test, “being subject to the\nLucas-Penrose constraint” implies the existence of a class of\n“unanswerable” questions. However Turing noted that in the\ncontext of the Turing test, these “unanswerable” questions\nare only a concern if humans can answer them. His “short”\nreply was that it is not clear that humans are free from such a\nconstraint themselves. Turing then goes on to add that he does not\nthink that the argument can be dismissed “quite so\nlightly.”  \n\nTo make the argument more precise, we can write it as follows: \n\nOnce the argument is laid out as above, it becomes clear that premise\n(3) should be challenged. Putting that aside, we note that one\ninterpretation of Turing’s “short” reply is that claim (4)\nis merely asserted—without any kind of proof. The\n“short” reply then leads us to examine whether humans are\nfree from the Lucas-Penrose constraint.  \n\nIf humans are subject to the Lucas-Penrose constraint then the\nconstraint does not provide any basis for distinguishing humans from\ndigital computers. If humans are free from the Lucas-Penrose\nconstraint, then (granting premise 3) it follows that digital computers\nmay fail the Turing test and thus, it seems, cannot think. \n\nHowever, there remains a question as to whether being free from the\nconstraint is necessary for the capacity to think. It may be that the\nTuring test is too strict. Since, by hypothesis, we are free from the\nLucas-Penrose constraint, we are, in some sense, too good at asking\nand answering questions. Suppose there is a thinking entity that is\nsubject to the Lucas-Penrose constraint. By an argument analogous to\nthe one above, it can fail the Turing test. Thus, an entity which can\nthink would fail the Turing test. \n\nWe can respond to this concern by noting that the construction of\nquestions suggested by the results from mathematical\nlogic—Gödel, Turing, etc.—are extremely complicated,\nand require extremely detailed information about the language and\ninternal programming of the digital computer (which, of course, is not\navailable to the interrogators in the Imitation Game). At the very\nleast, much more argument is required to overthrow the view that the\nTuring Test could remain a very high quality statistical test for the\npresence of mind and intelligence even if digital computers differ\nfrom human beings in being subject to the Lucas-Penrose\nconstraint. (See Bowie 1982, Dietrich 1994, Feferman 1996, \nAbramson 2008, and Section 6.3 of the entry on \n  Gödel’s incompleteness theorems,\n for further discussion.) \n\nTuring cites Professor Jefferson’s Lister Oration for 1949 as\na source for the kind of objection that he takes to fall under this\nlabel:  \n\nThere are several different ideas that are being run together here, and\nthat it is profitable to disentangle. One idea—the one upon which\nTuring first focuses—is the idea that the only way in which one\ncould be certain that a machine thinks is to be the machine, and to\nfeel oneself thinking. A second idea, perhaps, is that the presence of\nmind requires the presence of a certain kind of self-consciousness\n(“not only write it but know that it had written it”). A\nthird idea is that it is a mistake to take a narrow view of the mind,\ni.e. to suppose that there could be a believing intellect divorced from\nthe kinds of desires and emotions that play such a central role in the\ngeneration of human behavior (“no mechanism could feel\n…”).  \n\nAgainst the solipsistic line of thought, Turing makes the effective\nreply that he would be satisfied if he could secure agreement on the\nclaim that we might each have just as much reason to suppose that\nmachines think as we have reason to suppose that other people\nthink. (The point isn’t that Turing thinks that solipsism is a serious\noption; rather, the point is that following this line of argument isn’t\ngoing to lead to the conclusion that there are respects in which\ndigital computers could not be our intellectual equals or\nsuperiors.) \n\nAgainst the other lines of thought, Turing provides a little\n“viva voce” that is intended to illustrate the\nkind of evidence that he supposes one might have that a machine is\nintelligent. Given the right kinds of responses from the machine, we\nwould naturally interpret its utterances as evidence of\npleasure, grief, warmth, misery, anger, depression,\netc. Perhaps—though Turing doesn’t say this—the only way to\nmake a machine of this kind would be to equip it with sensors,\naffective states, etc., i.e., in effect, to make an artificial\nperson. However, the important point is that if the claims\nabout self-consciousness, desires, emotions, etc. are right, then\nTuring can accept these claims with equanimity: his claim is\nthen that a machine with a digital computing “brain” can\nhave the full range of mental states that can be enjoyed by adult\nhuman beings. \n\nTuring considers a list of things that some people have claimed\nmachines will never be able to do: (1) be kind; (2) be resourceful; (3)\nbe beautiful; (4) be friendly; (5) have initiative; (6) have a sense of\nhumor; (7) tell right from wrong; (8) make mistakes; (9) fall in love;\n(10) enjoy strawberries and cream; (11) make someone fall in love with\none; (12) learn from experience; (13) use words properly; (14) be the\nsubject of one’s own thoughts; (15) have as much diversity of behavior\nas a man; (16) do something really new.  \n\nAn interesting question to ask, before we address these claims\ndirectly, is whether we should suppose that intelligent creatures from\nsome other part of the universe would necessarily be able to do these\nthings. Why, for example, should we suppose that there must be\nsomething deficient about a creature that does not enjoy—or that is\nnot able to enjoy—strawberries and cream? True enough, we might\nsuppose that an intelligent creature ought to have the capacity to\nenjoy some kinds of things—but it seems unduly chauvinistic to\ninsist that intelligent creatures must be able to enjoy just the kinds\nof things that we do. (No doubt, similar considerations apply to the\nclaim that an intelligent creature must be the kind of thing that can\nmake a human being fall in love with it. Yes, perhaps, an intelligent\ncreature should be the kind of thing that can love and be loved; but\nwhat is so special about us?) \n\nSetting aside those tasks that we deem to be unduly chauvinistic, we\nshould then ask what grounds there are for supposing that no digital\ncomputing machine could do the other things on the list.\nTuring suggests that the most likely ground lies in our prior\nacquaintance with machines of all kinds: none of the machines that any\nof us has hitherto encountered has been able to do these things. In\nparticular, the digital computers with which we are now familiar cannot\ndo these things. (Except perhaps for make mistakes: after all, even\ndigital computers are subject to “errors of functioning.”\nBut this might be set aside as an irrelevant case.) However, given the\nlimitations of storage capacity and processing speed of even the most\nrecent digital computers, there are obvious reasons for being cautious\nin assessing the merits of this inductive argument. \n\n(A different question worth asking concerns the progress that has\nbeen made until now in constructing machines that can do the kinds of\nthings that appear on Turing’s list. There is at least room for debate\nabout the extent to which current computers can: make mistakes, use\nwords properly, learn from experience, be beautiful, etc. Moreover,\nthere is also room for debate about the extent to which recent advances\nin other areas may be expected to lead to further advancements in\novercoming these alleged disabilities. Perhaps, for example, recent\nadvances in work on artificial sensors may one day contribute to the\nproduction of machines that can enjoy strawberries and cream. Of\ncourse, if the intended objection is to the notion that machines can\nexperience any kind of feeling of enjoyment, then it is not clear that\nwork on particular kinds of artificial sensors is to the point.) \n\nOne of the most popular objections to the claim that there can be\nthinking machines is suggested by a remark made by Lady Lovelace in her\nmemoir on Babbage’s Analytical Engine:  \n\nThe key idea is that machines can only do what we know how to\norder them to do (or that machines can never do anything really new,\nor anything that would take us by surprise). As Turing says, one way\nto respond to these challenges is to ask whether we can ever do\nanything “really new.” Suppose, for instance, that the\nworld is deterministic, so that everything that we do is fully\ndetermined by the laws of nature and the boundary conditions of the\nuniverse. There is a sense in which nothing “really new”\nhappens in a deterministic universe—though, of course, the\nuniverse’s being deterministic would be entirely compatible with our\nbeing surprised by events that occur within it. Moreover—as\nTuring goes on to point out—there are many ways in which even digital\ncomputers do things that take us by surprise; more needs to be said to\nmake clear exactly what the nature of this suggestion is. (Yes, we\nmight suppose, digital computers are “constrained” by\ntheir programs: they can’t do anything that is not permitted by the\nprograms that they have. But human beings are\n“constrained” by their biology and their genetic\ninheritance in what might be argued to be just the same kind of way:\nthey can’t do anything that is not permitted by the biology and\ngenetic inheritance that they have. If a program were sufficiently\ncomplex—and if the processor(s) on which it ran were\nsufficiently fast—then it is not easy to say whether the kinds\nof “constraints” that would remain would necessarily\ndiffer in kind from the kinds of constraints that are imposed by\nbiology and genetic inheritance.) \n\nBringsjord et al. (2001) claim that Turing’s response to the\nLovelace Objection is “mysterious” at best, and\n“incompetent” at worst (p.4). In their view, Turing’s claim\nthat “computers do take us by surprise” is only true when\n“surprise” is given a very superficial interpretation. For,\nwhile it is true that computers do things that we don’t intend them to\ndo—because we’re not smart enough, or because we’re not careful\nenough, or because there are rare hardware errors, or whatever—it\nisn’t true that there are any cases in which we should want to say that\na computer has originated something. Whatever merit might be\nfound in this objection, it seems worth pointing out that, in the\nrelevant sense of origination, human beings “originate\nsomething” on more or less every occasion in which they engage in\nconversation: they produce new sentences of natural language that it is\nappropriate for them to produce in the circumstances in which they find\nthemselves. Thus, on the one hand—for all that Bringsjord et al.\nhave argued—The Turing Test is a perfectly good test for the\npresence of “origination” (or “creativity,” or\nwhatever). Moreover, on the other hand, for all that Bringsjord et al.\nhave argued, it remains an open question whether a digital computing\ndevice is capable of “origination” in this sense (i.e.\ncapable of producing new sentences that are appropriate to the\ncircumstances in which the computer finds itself). So we are not overly\ninclined to think that Turing’s response to the Lovelace Objection is\npoor; and we are even less inclined to think that Turing lacked the\nresources to provide a satisfactory response on this point. \n\nThe human brain and nervous system is not much like a digital computer.\nIn particular, there are reasons for being skeptical of the claim that\nthe brain is a discrete-state machine. Turing observes that a small\nerror in the information about the size of a nervous impulse impinging\non a neuron may make a large difference to the size of the outgoing\nimpulse. From this, Turing infers that the brain is likely to be a\ncontinuous-state machine; and he then notes that, since discrete-state\nmachines are not continuous-state machines, there might be reason here\nfor thinking that no discrete-state machine can be intelligent.  \n\nTuring’s response to this kind of argument seems to be that a\ncontinuous-state machine can be imitated by discrete-state machines\nwith very small levels of error. Just as differential analyzers can be\nimitated by digital computers to within quite small margins of error,\nso too, the conversation of human beings can be imitated by digital\ncomputers to margins of error that would not be detected by ordinary\ninterrogators playing the imitation game. It is not clear that this is\nthe right kind of response for Turing to make. If someone thinks that\nreal thought (or intelligence, or mind, or whatever) can only be\nlocated in a continuous-state machine, then the fact—if, indeed, it\nis a fact—that it is possible for discrete-state machines to pass\nthe Turing Test shows only that the Turing Test is no good. A better\nreply is to ask why one should be so confident that real thought, etc.\ncan only be located in continuous-state machines (if, indeed, it is\nright to suppose that we are not discrete-state machines). And, before\nwe ask this question, we would do well to consider whether we really do\nhave such good reason to suppose that, from the standpoint of our\nability to think, we are not essentially discrete-state machines. (As\nBlock (1981) points out, it seems that there is nothing in our concept\nof intelligence that rules out intelligent beings with quantised\nsensory devices; and nor is there anything in our concept of\nintelligence that rules out intelligent beings with digital working\nparts.) \n\nThis argument relies on the assumption that there is no set of rules\nthat describes what a person ought to do in every possible set of\ncircumstances, and on the further assumption that there is a set of\nrules that describes what a machine will do in every possible set of\ncircumstances. From these two assumptions, it is supposed to\nfollow—somehow!—that people are not machines. As Turing notes,\nthere is some slippage between “ought” and\n“will” in this formulation of the argument. However, once\nwe make the appropriate adjustments, it is not clear that an obvious\ndifference between people and digital computers emerges. \n\nSuppose, first, that we focus on the question of whether there are\nsets of rules that describe what a person and a machine\n“will” do in every possible set of circumstances. If the\nworld is deterministic, then there are such rules for both persons and\nmachines (though perhaps it is not possible to write down the rules).\nIf the world is not deterministic, then there are no such rules for\neither persons or machines (since both persons and machines can be\nsubject to non-deterministic processes in the production of their\nbehavior). Either way, it is hard to see any reason for supposing that\nthere is a relevant difference between people and machines that bears\non the description of what they will do in all possible sets of\ncircumstances. (Perhaps it might be said that what the objection\ninvites us to suppose is that, even though the world is not\ndeterministic, humans differ from digital machines precisely because\nthe operations of the latter are indeed deterministic. But, if the\nworld is non-deterministic, then there is no reason why digital\nmachines cannot be programmed to behave non-deterministically, by\nallowing them to access input from non-deterministic features of the\nworld.) \n\nSuppose, instead, that we focus on the question of whether there are\nsets of rules that describe what a person and a machine\n“ought” to do in every possible set of circumstances.\nWhether or not we suppose that norms can be codified—and quite apart\nfrom the question of which kinds of norms are in question—it is hard\nto see what grounds there could be for this judgment, other than the\nquestion-begging claim that machines are not the kinds of things whose\nbehavior could be subject to norms. (And, in that case, the initial\nargument is badly mis-stated: the claim ought to be that, whereas there\nare sets of rules that describe what a person ought to do in every\npossible set of circumstances, there are no sets of rules that describe\nwhat machines ought to do in all possible sets of\ncircumstances!) \n\nThe strangest part of Turing’s paper is the few paragraphs on ESP.\nPerhaps it is intended to be tongue-in-cheek, though, if it is, this\nfact is poorly signposted by Turing. Perhaps, instead, Turing was\ninfluenced by the apparently scientifically respectable results of J.\nB. Rhine. At any rate, taking the text at face value, Turing seems to\nhave thought that there was overwhelming empirical evidence for\ntelepathy (and he was also prepared to take clairvoyance, precognition\nand psychokinesis seriously). Moreover, he also seems to have thought\nthat if the human participant in the game was telepathic, then the\ninterrogator could exploit this fact in order to determine the\nidentity of the machine—and, in order to circumvent this\ndifficulty, Turing proposes that the competitors should be housed in a\n“telepathy-proof room.” Leaving aside the point that, as a\nmatter of fact, there is no current statistical support for\ntelepathy—or clairvoyance, or precognition, or telekinesis—it\nis worth asking what kind of theory of the nature of telepathy would\nhave appealed to Turing. After all, if humans can be telepathic, why\nshouldn’t digital computers be so as well? If the capacity for\ntelepathy were a standard feature of any sufficiently advanced system\nthat is able to carry out human conversation, then there is no\nin-principle reason why digital computers could not be the equals of\nhuman beings in this respect as well. (Perhaps this response assumes\nthat a successful machine participant in the imitation game will need\nto be equipped with sensors, etc. However, as we noted above, this\nassumption is not terribly controversial. A plausible\nconversationalist has to keep up to date with goings-on in the world.) \n\nAfter discussing the nine objections mentioned above, Turing goes on\nto say that he has “no very convincing arguments of a positive\nnature to support my views. If I had I should not have taken such\npains to point out the fallacies in contrary views.” (454)\nPerhaps Turing sells himself a little short in this\nself-assessment. First of all—as his brief discussion of\nsolipsism makes clear—it is worth asking what grounds we have\nfor attributing intelligence (thought, mind) to other people. If it is\nplausible to suppose that we base our attributions on behavioral tests\nor behavioral criteria, then his claim about the appropriate test to\napply in the case of machines seems apt, and his conjecture that\ndigital computing machines might pass the test seems like a\nreasonable—though controversial—empirical\nconjecture. Second, subsequent developments in the philosophy of\nmind—and, in particular, the fashioning of functionalist theories of\nthe mind—have provided a more secure theoretical environment in\nwhich to place speculations about the possibility of thinking\nmachines. If mental states are functional states—and if mental\nstates are capable of realisation in vastly different kinds of\nmaterials—then there is some reason to think that it is an\nempirical question whether minds can be realised in digital computing\nmachines. Of course, this kind of suggestion is open to challenge; we\nshall consider some important philosophical objections in the later\nparts of this review. \n\nThere are a number of much-debated issues that arise in connection with\nthe interpretation of various parts of Turing (1950), and that we have\nhitherto neglected to discuss. What has been said in the first two\nsections of this document amounts to our interpretation of what Turing\nhas to say (perhaps bolstered with what we take to be further relevant\nconsiderations in those cases where Turing’s remarks can be fairly\nreadily improved upon). But since some of this interpretation has been\ncontested, it is probably worth noting where the major points of\ncontroversy have been.  \n\nTuring (1950) introduces the imitation game by describing a game in\nwhich the participants are a man, a woman, and a human interrogator.\nThe interrogator is in a room apart from the other two, and is set the\ntask of determining which of the other two is a man and which is a\nwoman. Both the man and the woman are set the task of trying to\nconvince the interrogator that they are the woman. Turing recommends\nthat the best strategy for the woman is to answer all questions\ntruthfully; of course, the best strategy for the man will require some\nlying. The participants in this game also use teletypewriter to\ncommunicate with one another—to avoid clues that might be offered by\ntone of voice, etc. Turing then says: “We now ask the question,\n‘What will happen when a machine takes the part of A in this game?’\nWill the interrogator decide wrongly as often when the game is played\nlike this as he does when the game is played between a man and a woman?”\n(434). \n\nNow, of course, it is possible to interpret Turing as here\nintending to say what he seems literally to say, namely, that the new\ngame is one in which the computer must pretend to be a woman, and the\nother participant in the game is a woman. (See, for example, Genova\n(1994), and Traiger (2000).) And it is also possible to\ninterpret Turing as intending to say that the new game is one in which\nthe computer must pretend to be a woman, and the other participant in\nthe game is a man who must also pretend to be a woman. However, as\nCopeland (2000), Piccinini (2000), and Moor (2001) convincingly argue,\nthe rest of Turing’s article, and material in other articles that\nTuring wrote at around the same time, very strongly support the claim\nthat Turing actually intended the standard interpretation that we gave\nabove, viz. that the computer is to pretend to be a human being, and\nthe other participant in the game is a human being of unspecified\ngender. Moreover, as Moor (2001) argues, there is no reason to think\nthat one would get a better test if the computer must pretend to be a\nwoman and the other participant in the game is a man pretending to be\na woman (and, indeed, there is some reason to think that one would get\na worse test). Perhaps it would make no difference to the\neffectiveness of the test if the computer must pretend to be a woman,\nand the other participant is a woman (any more than it would make a\ndifference if the computer must pretend to be an accountant and the\nother participant is an accountant); however, this consideration is\nsimply insufficient to outweigh the strong textual evidence that\nsupports the standard interpretation of the imitation game that we\ngave at the beginning of our discussion of Turing (1950). \n\nAs we noted earlier, Turing (1950) makes the claim that:  \n\nMost commentators contend that this claim has been shown to be\nmistaken: in the year 2000, no-one was able to program\ncomputers to make them play the imitation game so well that an average\ninterrogator had no more than a 70% chance of making the correct\nidentification after five minutes of questioning. Copeland (2000)\nargues that this contention is seriously mistaken: “about fifty\nyears” is by no means “exactly fifty years,” and it\nremains open that we may soon be able to do the required programming.\nAgainst this, it should be noted that Turing (1950) goes on\nimmediately to refer to how things will be “at the end of the\ncentury,” which suggests that not too much can be read into the\nqualifying “about.” However, as Copeland (2000) points\nout, there are other more cautious predictions that Turing makes\nelsewhere (e.g., that it would be “at least 100 years”\nbefore a machine was able to pass an unrestricted version of his\ntest); and there are other predictions that are made in Turing (1950)\nthat seem to have been vindicated. In particular, it is plausible to\nclaim that, in the year 2000, educated opinion had altered to the\nextent that, in many quarters, one could speak of the possibility of\nmachines’ thinking—and of machines’ learning—without expecting\nto be contradicted. As Moor (2001) points out, “machine\nintelligence” is not the oxymoron that it might have been taken\nto be when Turing first started thinking about these matters. \n\nThere are two different theoretical claims that are run together in\nmany discussions of The Turing Test that can profitably be separated.\nOne claim holds that the general scheme that is described in Turing’s\nImitation Game provides a good test for the presence of intelligence.\n(If something can pass itself off as a person under sufficiently\ndemanding test conditions, then we have very good reason to suppose\nthat that thing is intelligent.) Another claim holds that an\nappropriately programmed computer could pass the kind of test that is\ndescribed in the first claim. We might call the first claim “The\nTuring Test Claim” and the second claim “The Thinking\nMachine Claim”. Some objections to the claims made in Turing\n(1950) are objections to the Thinking Machine Claim, but not objections\nto the Turing Test Claim. (Consider, for example, the argument of\nSearle (1982), which we discuss further in Section 6.) However, other\nobjections are objections to the Turing Test Claim. Until we get to\nSection 6, we shall be confining our attention to discussions of the\nTuring Test Claim.  \n\nIn this article, we follow the standard philosophical convention\naccording to which “a mind” means “at least one\nmind”. If “passing the Turing Test” implies\nintelligence, then “passing the Turing Test” implies the\npresence of at least one mind. We cannot here explore recent\ndiscussions of “swarm intelligence”, “collective\nintelligence”, and the like. However, it is surely clear that\ntwo people taking turns could “pass the Turing Test” in\ncircumstances in which we should be very reluctant to say that there\nis a “collective mind” that has the minds of the two as\ncomponents.  \n\nGiven the initial distinction that we made between different ways in\nwhich the expression The Turing Test gets interpreted in the\nliterature, it is probably best to approach the question of the\nassessment of the current standing of The Turing Test by dividing\ncases. True enough, we think that there is a correct interpretation of\nexactly what test it is that is proposed by Turing (1950); but a\ncomplete discussion of the current standing of The Turing Test should\npay at least some attention to the current standing of other tests that\nhave been mistakenly supposed to be proposed by Turing (1950).  \n\nThere are a number of main ideas to be investigated. First, there is\nthe suggestion that The Turing Test provides logically necessary and\nsufficient conditions for the attribution of intelligence. Second,\nthere is the suggestion that The Turing Test provides logically\nsufficient—but not logically necessary—conditions for the\nattribution of intelligence. Third, there is the suggestion that The\nTuring Test provides “criteria”—defeasible sufficient\nconditions—for the attribution of intelligence. Fourth—and\nperhaps not importantly distinct from the previous claim—there is\nthe suggestion that The Turing Test provides (more or less strong)\nprobabilistic support for the attribution of intelligence. We shall\nconsider each of these suggestions in turn. \n\nIt is doubtful whether there are very many examples of people who have\nexplicitly claimed that The Turing Test is meant to provide conditions\nthat are both logically necessary and logically sufficient for the\nattribution of intelligence. (Perhaps Block (1981) is one such case.)\nHowever, some of the objections that have been proposed against The\nTuring Test only make sense under the assumption that The Turing Test\ndoes indeed provide logically necessary and logically sufficient\nconditions for the attribution of intelligence; and many more of the\nobjections that have been proposed against The Turing Test only make\nsense under the assumption that The Turing Test provides necessary and\nsufficient conditions for the attribution of intelligence, where the\nmodality in question is weaker than the strictly logical, e.g., nomic\nor causal.  \n\nConsider, for example, those people who have claimed that The Turing\nTest is chauvinistic; and, in particular, those people who have claimed\nthat it is surely logically possible for there to be something that\npossesses considerable intelligence, and yet that is not able to pass\nThe Turing Test. (Examples: Intelligent creatures might fail to pass\nThe Turing Test because they do not share our way of life; intelligent\ncreatures might fail to pass The Turing Test because they refuse to\nengage in games of pretence; intelligent creatures might fail to pass\nThe Turing Test because the pragmatic conventions that govern the\nlanguages that they speak are so very different from the pragmatic\nconventions that govern human languages. Etc.) None of this can\nconstitute objections to The Turing Test unless The Turing Test\ndelivers necessary conditions for the attribution of\nintelligence. \n\nFrench (1990) offers ingenious arguments that are intended to show\nthat “the Turing Test provides a guarantee not of intelligence,\nbut of culturally-oriented intelligence.” But, of course,\nanything that has culturally-oriented intelligence has\nintelligence; so French’s objections cannot be taken to be directed\ntowards the idea that The Turing Test provides sufficient conditions\nfor the attribution of intelligence. Rather—as we shall see\nlater—French supposes that The Turing Test establishes sufficient\nconditions that no machine will ever satisfy. That is, in French’s\nview, what is wrong with The Turing Test is that it establishes\nutterly uninteresting sufficient conditions for the attribution of\nintelligence. \n\nThere are many philosophers who have supposed that The Turing Test is\nintended to provide logically sufficient conditions for the attribution\nof intelligence. That is, there are many philosophers who have supposed\nthat The Turing Test claims that it is logically impossible for\nsomething that lacks intelligence to pass The Turing Test. (Often, this\nsupposition goes with an interpretation according to which passing The\nTuring Test requires rather a lot, e.g., producing behavior that is\nindistinguishable from human behavior over an entire lifetime.)  \n\nThere are well-known arguments against the claim that passing The\nTuring Test—or any other purely behavioral test—provides\nlogically sufficient conditions for the attribution of intelligence.\nThe standard objection to this kind of analysis of\nintelligence (mind, thought) is that a being whose behavior was\nproduced by “brute force” methods ought not to count as\nintelligent (as possessing a mind, as having thoughts). \n\nConsider, for example, Ned Block’s Blockhead. Blockhead is\na creature that looks just like a human being, but that is controlled\nby a “game-of-life look-up tree,” i.e. by a tree that\ncontains a programmed response for every discriminable input at each\nstage in the creature’s life. If we agree that Blockhead is logically\npossible, and if we agree that Blockhead is not intelligent (does not\nhave a mind, does not think), then Blockhead is a counterexample to the\nclaim that the Turing Test provides a logically sufficient condition\nfor the ascription of intelligence. After all, Blockhead could be\nprogrammed with a look-up tree that produces responses identical with\nthe ones that you would give over the entire course of\nyour life (given the same inputs). \n\nThere are perhaps only two ways in which someone who claims that The\nTuring Test offers logically sufficient conditions for the attribution\nof intelligence can respond to Block’s argument. First, it could be\ndenied that Blockhead is a logical possibility; second, it could be\nclaimed that Blockhead would be intelligent (have a mind, think). \n\nIn order to deny that Blockhead is a logical possibility, it seems\nthat what needs to be denied is the commonly accepted link between\nconceivability and logical possibility: it certainly seems that\nBlockhead is conceivable, and so, if (properly circumscribed)\nconceivability is sufficient for logical possibility, then it seems\nthat we have good reason to accept that Blockhead is a logical\npossibility. Since it would take us too far away from our present\nconcerns to explore this issue properly, we merely note that it remains\na controversial question whether (properly circumscribed)\nconceivability is sufficient for logical possibility. (For further\ndiscussion of this issue, see Crooke (2002).) \n\nThe question of whether Blockhead is intelligent (has a mind, thinks)\nmay seem straightforward, but—despite Block’s confident\nassertion that Blockhead “has all of the intelligence of a\ntoaster”—it is not obvious that we should deny that\nBlockhead is intelligent. Blockhead may not be a particularly\nefficient processor of information; but it is at least a processor of\ninformation, and that—in combination with the behavior that is\nproduced as a result of the processing of information—might well\nbe taken to be sufficient grounds for the attribution of some\nlevel of intelligence to Blockhead. For further critical discussion of\nthe argument of Block (1981), see McDermott (2014), and Pautz and Stoljar (2019).  \n\nIn his Philosophical Investigations, Wittgenstein famously\nwrites: “An ‘inner process’ stands in need of outward\ncriteria” (580). Exactly what Wittgenstein meant by this remark\nis unclear, but one way in which it might be interpreted is as follows:\nin order to be justified in ascribing a “mental state” to\nsome entity, there must be some true claims about the observable\nbehavior of that entity that, (perhaps) together with other true claims\nabout that entity (not themselves couched in “mentalistic”\nvocabulary), entail that the entity has the mental state in question.\nIf no true claims about the observable behavior of the entity can play\nany role in the justification of the ascription of the mental state in\nquestion to the entity, then there are no grounds for attributing that\nkind of mental state to the entity.  \n\nThe claim that, in order to be justified in ascribing a mental state\nto an entity, there must be some true claims about the observable\nbehavior of that entity that alone—i.e. without the addition of any\nother true claims about that entity—entail that the entity has the\nmental state in question, is a piece of philosophical behaviorism. It\nmay be—for all that we are able to argue—that Wittgenstein was a\nphilosophical behaviorist; it may be—for all that we are able to\nargue—that Turing was one, too. However, if we go by the letter of\nthe account given in the previous paragraph, then all that need follow\nfrom the claim that the Turing Test is criterial for the ascription of\nintelligence (thought, mind) is that, when other true claims (not\nthemselves couched in terms of mentalistic vocabulary) are conjoined\nwith the claim that an entity has passed the Turing Test, it then\nfollows that the entity in question has intelligence (thought,\nmind). \n\n(Note that the parenthetical qualification that the additional true\nclaims not be couched in terms of mentalistic vocabulary is only one\nway in which one might try to avoid the threat of trivialization. The\ndifficulty is that the addition of the true claim that an entity has a\nmind will always produce a set of claims that entails that that entity\nhas a mind, no matter what other claims belong to the set!) \n\nTo see how the claim that the Turing Test is merely criterial for\nthe ascription of intelligence differs from the logical behaviorist\nclaim that the Turing Test provides logically sufficient conditions for\nthe ascription of intelligence, it suffices to consider the question of\nwhether it is nomically possible for there to be a “hand\nsimulation” of a Turing Test program. Many people have supposed\nthat there is good reason to deny that Blockhead is a nomic (or\nphysical) possibility. For example, in The Physics of\nImmortality, Frank Tipler provides the following argument in\ndefence of the claim that it is physically impossible to “hand\nsimulate” a Turing-Test-passing program: \n\nWhile there might be ways in which the details of Tipler’s argument\ncould be improved, the general point seems clearly right: the kind of\ncombinatorial explosion that is required for a look-up tree for a\nhuman being is ruled out by the laws and boundary conditions that\ngovern the operations of the physical world. But, if this is right,\nthen, while it may be true that Blockhead is a logical\npossibility, it follows that Blockhead is not a nomic or\nphysical possibility. And then it seems natural to hold that\nThe Turing Test does indeed provide nomically sufficient\nconditions for the attribution of intelligence: given everything else\nthat we already know—or, at any rate, take ourselves to\nknow—about the universe in which we live, we would be fully\njustified in concluding that anything that succeeds in passing The\nTuring Test is, indeed, intelligent (possessed of a mind, and so\nforth). \n\nThere are ways in which the argument in the previous paragraph might\nbe resisted. At the very least, it is worth noting that there is a\nserious gap in the argument that we have just rehearsed. Even if we can\nrule out “hand simulation” of intelligence, it does not\nfollow that we have ruled out all other kinds of mere simulation of\nintelligence. Perhaps—for all that has been argued so far—there\nare nomically possible ways of producing mere simulations of\nintelligence. But, if that’s right, then passing The Turing Test need\nnot be so much as criterial for the possession of intelligence: it need\nnot be that given everything else that we already know—or, at any\nrate, take ourselves to know—about the universe in which we live, we\nwould be fully justified in concluding that anything that succeeds in\npassing The Turing Test is, indeed, intelligent (possessed of a mind,\nand so forth). \n\n(McDermott (2014) calculates that a look-up table for a participant\nwho makes 50 conversational exchanges would have about\n1022278 nodes. It is tempting to take this calculation to\nestablish that it is neither nomically nor physically possible for\nthere to be a “hand simulation” of a Turing Test program, on the\ngrounds that the required number of nodes could not be fitted into a\nspace much much larger than the entire observable universe.)  \n\nWhen we look at the initial formulation that Turing provides of his\ntest, it is clear that he thought that the passing of the test would\nprovide probabilistic support for the hypothesis of intelligence. There\nare at least two different points to make here. First, the\nprediction that Turing makes is itself probabilistic: Turing\npredicts that, in about fifty years from the time of his writing, it\nwill be possible to programme digital computers to make them play the\nimitation game so well that an average interrogator will have no more\nthan a seventy per cent chance of making the right identification after\nfive minutes of questioning. Second, the probabilistic nature of\nTuring’s prediction provides good reason to think that the\ntest that Turing proposes is itself of a probabilistic nature:\na given level of success in the imitation game produces—or, at any\nrate, should produce—a specifiable level of increase in confidence\nthat the participant in question is intelligent (has thoughts, is\npossessed of a mind). Since Turing doesn’t tell us how he supposes that\nlevels of success in the imitation game correlate with increases in\nconfidence that the participant in question is intelligent, there is a\nsense in which The Turing Test is greatly underspecified. Relevant\nvariables clearly include: the length of the period of time over which\nthe questioning in the game takes place (or, at any rate, the\n“amount” of questioning that takes place); the skills and\nexpertise of the interrogator (this bears, for example, on the\n“depth” and “difficulty” of the questioning\nthat takes place); the skills and expertise of the third player in the\ngame; and the number of independent sessions of the game that are run\n(particularly when the other participants in the game differ from one\nrun to the next). Clearly, a machine that is very successful in many\ndifferent runs of the game that last for quite extended periods of time\nand that involve highly skilled participants in the other roles has a\nmuch stronger claim to intelligence than a machine that has been\nsuccessful in a single, short run of the game with highly inexpert\nparticipants. That a machine has succeeded in one short run of the game\nagainst inexpert opponents might provide some reason for increase in\nconfidence that the machine in question is intelligent: but it is clear\nthat results on subsequent runs of the game could quickly overturn this\ninitial increase in confidence. That a machine has done much better\nthan chance over many long runs of the imitation game against a variety\nof skilled participants surely provides much stronger evidence that the\nmachine is intelligent. (Given enough evidence of this kind, it seems\nthat one could be quite confident indeed that the machine is\nintelligent, while still—of course—recognizing that one’s\njudgment could be overturned by further evidence, such as a series of\nshort runs in which it does much worse than chance against participants\nwho use the same strategy over and over to expose the machine as a\nmachine.)  \n\nThe probabilistic nature of The Turing Test is often overlooked.  True\nenough, Moor (1976, 2001)—along with various other\ncommentators—has noted that The Turing Test is\n“inductive,” i.e. that “The Turing Test”\nprovides no more than defeasible evidence of intelligence. However, it\nis one thing to say that success in “a rigorous Turing\ntest” provides no more than defeasible evidence of intelligence;\nit is quite another to note the probabilistic features to which we\nhave drawn attention in the preceding paragraph.  Consider, for\nexample, Moor’s observation (Moor 2001:83) that “…\ninductive evidence gathered in a Turing test can be outweighed by new\nevidence. … If new evidence shows that a machine passed the\nTuring Test by remote control run by a human behind the scenes, then\nreassessment is called for.” This—and other similar\npassages—seems to us to suggest that Moor supposes that a\n“rigorous Turing test” is a one-off event in which the\nmachine either succeeds or fails. But this interpretation of The\nTuring Test is vulnerable to the kind of objection lodged by\nBringsjord (1994): even on a moderately long single run with\nrelatively expert participants, it may not be all that unlikely that\nan unintelligent machine serendipitously succeeds in the imitation\ngame. In our view, given enough sufficiently long runs with different\nsufficiently expert participants, the likelihood of serendipitous\nsuccess can be made as small as one wishes. Thus, while Bringsjord’s\n“argument from serendipity” has force against some\nversions of The Turing Test, it has no force against the most\nplausible interpretation of the test that Turing actually\nproposed. \n\nIt is worth noting that it is quite easy to construct more\nsophisticated versions of “The Imitation Game” that yield\nmore fine-grained statistical data. For example, rather than getting\nthe judges to issue Yes/No verdicts about both of the participants in\nthe game, one could get the judges to provide probabilistic answers.\n(“I give a 75% probability to the claim that A is the machine, and only\n25% probability to the claim that B is the machine.”) This point is\nimportant when one comes to consider criticisms of the\n“methodology” implicit in “The Turing\nTest”. (For further discussion of the probabilistic nature of “The Turing\nTest”, see Shieber (2007).)  \n\nSome of the literature about The Turing Test is concerned with\nquestions about the framing of a test that can provide a suitable guide\nto future research in the area of Artificial Intelligence. The idea\nhere is very simple. Suppose that we have the ambition to produce an\nartificially intelligent entity. What tests should we take as setting\nthe goals that putatively intelligent artificial systems should\nachieve? Should we suppose that The Turing Test provides an appropriate\ngoal for research in this field? In assessing these proposals, there\nare two different questions that need to be borne in mind. First, there\nis the question whether it is a useful goal for AI research to aim to\nmake a machine that can pass the given test (administered over the\nspecified length of time, at the specified degree of success). Second,\nthere is the question of the appropriate conclusion to draw about the\nmental capacities of a machine that does manage to pass the test\n(administered over the specified length of time, at the specified\ndegree of success).  \n\nOpinion on these questions is deeply divided. Some people suppose\nthat The Turing Test does not provide a useful goal for research in AI\nbecause it is far too difficult to produce a system that can pass the\ntest. Other people suppose that The Turing Test does not provide a\nuseful goal for research in AI because it sets a very narrow target\n(and thus sets unnecessary restrictions on the kind of research that\ngets done). Some people think that The Turing Test provides an entirely\nappropriate goal for research in AI; while other people think that\nthere is a sense in which The Turing Test is not really demanding\nenough, and who suppose that The Turing Test needs to be extended in\nvarious ways in order to provide an appropriate goal for AI. We shall\nconsider some representatives of each of these positions in turn. \n\nSome people have claimed that The Turing Test doesn’t set an\nappropriate goal for current research in AI because we are plainly so\nfar away from attaining this goal. Amongst these people there are some\nwho have gone on to offer reasons for thinking that it is doubtful\nthat we shall ever be able to create a machine that can pass The\nTuring Test—or, at any rate, that it is doubtful that we shall\nbe able to do this at any time in the foreseeable future. Perhaps the\nmost interesting arguments of this kind are due to French (1990); at\nany rate, these are the arguments that we shall go on to\nconsider. (Cullen (2009) sets out similar considerations.)  \n\nAccording to French, The Turing Test is “virtually\nuseless” as a real test of intelligence, because nothing without\na “human subcognitive substrate” could pass the test, and\nyet the development of an artificial “human cognitive\nsubstrate” is almost impossibly difficult. At the very least,\nthere are straightforward sets of questions that reveal\n“low-level cognitive structure” and that—in French’s\nview—are almost certain to be successful in separating human beings\nfrom machines. \n\nFirst, if interrogators are allowed to draw on the results of\nresearch into, say, associative priming, then there is data\nthat will very plausibly separate human beings from machines. For\nexample, there is research that shows that, if humans are presented\nwith series of strings of letters, they require less time to recognize\nthat a string is a word (in a language that they speak) if it is\npreceded by a related word (in the language that they speak), rather\nthan by an unrelated word (in the language that they speak) or a string\nof letters that is not a word (in the language that they speak).\nProvided that the interrogator has accurate data about average\nrecognition times for subjects who speak the language in question, the\ninterrogator can distinguish between the machine and the human simply\nby looking at recognition times for appropriate series of strings of\nletters. Or so says French. It isn’t clear to us that this is right.\nAfter all, the design of The Turing Test makes it hard to see how the\ninterrogator will get reliable information about response times to\nseries of strings of symbols. The point of putting the computer in a\nseparate room and requiring communication by teletype was precisely to\nrule out certain irrelevant ways of identifying the computer. If these\nrequirements don’t already rule out identification of the computer by\nthe application of tests of associative priming, then the requirements\ncan surely be altered to bring it about that this is the case. (Perhaps\nit is also worth noting that administration of the kind of test that\nFrench imagines is not ordinary conversation; nor is it something that\none would expect that any but a few expert interrogators would happen\nupon. So, even if the circumstances of The Turing Test do not rule out\nthe kind of procedure that French here envisages, it is not clear that\nThe Turing Test will be impossibly hard for machines to pass.) \n\nSecond, at a slightly higher cognitive level, there are certain\nkinds of “ratings games” that French supposes will be very\nreliable discriminators between humans and machines. For instance, the\n“Neologism Ratings Game”—which asks participants to rank\nmade-up words on their appropriateness as names for given kinds of\nentities—and the “Category Rating Game”—which asks\nparticipants to rate things of one category as things of another\ncategory—are both, according to French, likely to prove highly\nreliable in discriminating between humans and machines. For, in the\nfirst case, the ratings that humans make depend upon large numbers of\nculturally acquired associations (which it would be well-nigh\nimpossible to identify and describe, and hence which it would\n(arguably) be well-nigh impossible to program into a computer). And, in\nthe second case, the ratings that people actually make are highly\ndependent upon particular social and cultural settings (and upon the\nparticular ways in which human life is experienced). To take French’s\nexamples, there would be widespread agreement amongst competent English\nspeakers in the technologically developed Western world that\n“Flugblogs” is not an appropriate name for a breakfast\ncereal, while “Flugly” is an appropriate name for a child’s\nteddy bear. And there would also be widespread agreement amongst\ncompetent speakers of English in the developed world that pens rate\nhigher as weapons than grand pianos rate as wheelbarrows. Again, there\nare questions that can be raised about French’s argument here. It is\nnot clear to us that the data upon which the ratings games rely is as\nreliable as French would have us suppose. (At least one of us thinks\nthat “Flugly” would be an entirely inappropriate name for a\nchild’s teddy bear, a response that is due to the similarity between\nthe made-up word “Flugly” and the word “Fugly,”\nthat had some currency in the primarily undergraduate University\ncollege that we both attended. At least one of us also thinks that\nyoung children would very likely be delighted to eat a cereal called\n“Flugblogs,” and that a good answer to the question about\nratings pens and grand pianos is that it all depends upon the pens and\ngrand pianos in question. What if the grand piano has wheels? What if\nthe opponent has a sword or a sub-machine gun? It isn’t obvious that a\nrefusal to play this kind of ratings game would necessarily be a\ngive-away that one is a machine.) Moreover, even if the data is\nreliable, it is not obvious that any but a select group of\ninterrogators will hit upon this kind of strategy for trying to unmask\nthe machine; nor is it obvious that it is impossibly hard to build a\nmachine that is able to perform in the way in which typical humans do\non these kinds of tests. In particular, if—as Turing assumes—it\nis possible to make learning machines that can be “trained\nup” to learn how to do various kinds of tasks, then it is quite\nunclear why these machines couldn’t acquire just the same kinds of\n“subcognitive competencies” that human children acquire\nwhen they are “trained up” in the use of language. \n\nThere are other reasons that have been given for thinking that The\nTuring Test is too hard (and, for this reason, inappropriate in setting\ngoals for current research into artificial intelligence). In general,\nthe idea is that there may well be features of human cognition that are\nparticularly hard to simulate, but that are not in any sense essential\nfor intelligence (or thought, or possession of a mind). The problem\nhere is not merely that The Turing Test really does test for\nhuman intelligence; rather, the problem here is the fact—if\nindeed it is a fact—that there are quite inessential features of\nhuman intelligence that are extraordinarily difficult to replicate in a\nmachine. If this complaint is justified—if, indeed, there are\nfeatures of human intelligence that are extraordinarily difficult to\nreplicate in machines, and that could and would be reliably\nused to unmask machines in runs of The Turing Test—then there is\nreason to worry about the idea that The Turing Test sets an appropriate\ndirection for research in artificial intelligence. However, as our\ndiscussion of French shows, there may be reason for caution in\nsupposing that the kinds of considerations discussed in the present\nsection show that we are already in a position to say that The Turing\nTest does indeed set inappropriate goals for research in artificial\nintelligence. \n\nThere are authors who have suggested that The Turing Test does not set\na sufficiently broad goal for research in the area of artificial\nintelligence. Amongst these authors, there are many who suppose that\nThe Turing Test is too easy. (We go on to consider some of these\nauthors in the next sub-section.) But there are also some authors who\nhave supposed that, even if the goal that is set by The Turing Test is\nvery demanding indeed, it is nonetheless too restrictive.  \n\nObjection to the notion that the Turing Test provides a logically\nsufficient condition for intelligence can be adapted to the goal of\nshowing that the Turing Test is too restrictive. Consider, for\nexample, Gunderson (1964). Gunderson has two major complaints to make\nagainst The Turing Test. First, he thinks that success in Turing’s\nImitation Game might come for reasons other than the possession of\nintelligence.  But, second, he thinks that success in the Imitation\nGame would be but one example of the kinds of things that intelligent\nbeings can do and—hence—in itself could not be taken as a\nreliable indicator of intelligence. By way of analogy, Gunderson\noffers the case of a vacuum cleaner salesman who claims that his\nproduct is “all-purpose” when, in fact, all it does is to\nsuck up dust. According to Gunderson, Turing is in the same position\nas the vacuum cleaner salesman if he is prepared to say that\na machine is intelligent merely on the basis of its success in the\nImitation Game. Just as “all purpose” entails the ability\nto do a range of things, so, too, “thinking” entails the\npossession of a range of abilities (beyond the mere ability to succeed\nin the Imitation Game). \n\nThere is an obvious reply to the argument that we have here attributed\nto Gunderson, viz. that a machine that is capable of success in the\nImitation Game is capable of doing a large range of different kinds of\nthings. In order to carry out a conversation, one needs to have many\ndifferent kinds of cognitive skills, each of which is capable of\napplication in other areas. Apart from the obvious general cognitive\ncompetencies—memory, perception, etc.—there are many\nparticular competencies—rudimentary arithmetic abilities,\nunderstanding of the rules of games, rudimentary understanding of\nnational politics, etc.—which are tested in the course of repeated\nruns of the Imitation Game.  It is inconceivable that that there be a\nmachine that is startlingly good at playing the Imitation Game, and\nyet unable to do well at any other tasks that might be\nassigned to it; and it is equally inconceivable that there is a\nmachine that is startlingly good at the Imitation Game and yet that\ndoes not have a wide range of competencies that can be displayed in a\nrange of quite disparate areas.  To the extent that Gunderson\nconsiders this line of reply, all that he says is that there is no\nreason to think that a machine that can succeed in the Imitation Game\nmust have more than a narrow range of abilities; we think\nthat there is no reason to believe that this reply should be taken\nseriously. \n\nMore recently, Erion (2001) has defended a position that has some\naffinity to that of Gunderson. According to Erion, machines might be\n“capable of outperforming human beings in limited tasks in\nspecific environments, [and yet] still be unable to act skillfully in\nthe diverse range of situations that a person with common sense\ncan” (36). On one way of understanding the claim that Erion\nmakes, he too believes that The Turing Test only identifies one amongst\na range of independent competencies that are possessed by intelligent\nhuman beings, and it is for this reason that he proposes a more\ncomprehensive “Cartesian Test” that “involves a more\ncareful examination of a creature’s language, [and] also tests the\ncreature’s ability to solve problems in a wide variety of everyday\ncircumstances” (37). In our view, at least when The Turing Test\nis properly understood, it is clear that anything that passes The\nTuring Test must have the ability to solve problems in a wide variety\nof everyday circumstances (because the interrogators will use their\nquestions to probe these—and other—kinds of abilities in those\nwho play the Imitation Game). \n\nThere are authors who have suggested that The Turing Test should be\nreplaced with a more demanding test of one kind or another. It is not\nat all clear that any of these tests actually proposes a better goal\nfor research in AI than is set by The Turing Test. However, in this\nsection, we shall not attempt to defend that claim; rather, we shall\nsimply describe some of the further tests that have been proposed, and\nmake occasional comments upon them. (One preliminary point upon which\nwe wish to insist is that Turing’s Imitation Game was devised against\nthe background of the limitations imposed by then current technology.\nIt is, of course, not essential to the game that tele-text devices be\nused to prevent direct access to information about the sex or genus of\nparticipants in the game. We shall not advert to these relatively\nmundane kinds of considerations in what follows.)  \n\nHarnad (1989, 1991) claims that a better test than The Turing Test will\nbe one that requires responses to all of our inputs, and not merely to\ntext-formatted linguistic inputs. That is, according to Harnad, the\nappropriate goal for research in AI has to be to construct a robot with\nsomething like human sensorimotor capabilities. Harnad also considers\nthe suggestion that it might be an appropriate goal for AI to aim for\n“neuromolecular indistinguishability,” but rejects this\nsuggestion on the grounds that once we know how to make a robot that\ncan pass his Total Turing Test, there will be no problems about\nmind-modeling that remain unsolved. It is an interesting question\nwhether the test that Harnad proposes sets a more appropriate goal for\nAI research. In particular, it seems worth noting that it is not clear\nthat there could be a system that was able to pass The Turing Test and\nyet that was not able to pass The Total Turing Test. Since Harnad\nhimself seems to think that it is quite likely that “full robotic\ncapacities [are] … necessary to generate … successful\nlinguistic performance,” it is unclear why there is reason to\nreplace The Turing Test with his extended test. (This point against\nHarnad can be found in Hauser (1993:227), and elsewhere.)  \n\nBringsjord et al. (2001) propose that a more satisfactory aim for AI is\nprovided by a certain kind of meta-test that they call the Lovelace\nTest. They say that an artificial agent A, designed by human\nH, passes the Lovelace Test just in case three conditions are jointly\nsatisfied: (1) the artificial agent A produces output\nO; (2) A’s outputting O is not the result of\na fluke hardware error, but rather the result of processes that\nA can repeat; and (3) H—or someone who knows what\nH knows and who has H’s resources—cannot explain\nhow A produced O by appeal to A’s\narchitecture, knowledge-base and core functions. Against this proposal,\nit seems worth noting that there are questions to be raised about the\ninterpretation of the third condition. If a computer program is long\nand complex, then no human agent can explain in complete\ndetail how the output was produced. (Why did the computer output 3.16\nrather than 3.17?) But if we are allowed to give a highly schematic\nexplanation—the computer took the input, did some internal\nprocessing and then produced an answer—then it seems that it will\nturn out to be very hard to support the claim that human agents ever do\nanything genuinely creative. (After all, we too take external input,\nperform internal processing, and produce outputs.) What is missing from\nthe account that we are considering is any suggestion about the\nappropriate level of explanation that is to be provided. It is\nquite unclear why we should suppose that there is a relevant difference\nbetween people and machines at any level of explanation; but, if that’s\nright, then the test in question is trivial. (One might also worry that\nthe proposed test rules out by fiat the possibility that\ncreativity can be best achieved by using genuine randomising\ndevices.)  \n\nSchweizer (1998) claims that a better test than The Turing Test will\nadvert to the evolutionary history of the subjects of the test. When we\nattribute intelligence to human beings, we rely on an extensive\nhistorical record of the intellectual achievements of human beings. On\nthe basis of this historical record, we are able to claim that human\nbeings are intelligent; and we can rely upon this claim when we\nattribute intelligence to individual human beings on the basis of their\nbehavior. According to Schweizer, if we are to attribute intelligence\nto machines, we need to be able to advert to a comparable historical\nrecord of cognitive achievements. So, it will only be when machines\nhave developed languages, written scientific treatises, composed\nsymphonies, invented games, and the like, that we shall be in a\nposition to attribute intelligence to individual machines on the basis\nof their behavior. Of course, we can still use The Turing Test to\ndetermine whether an individual machine is intelligent: but our answer\nto the question won’t depend merely upon whether or not the machine is\nsuccessful in The Turing Test; there is the further\n“evolutionary” condition that also must be satisfied.\nAgainst Schweizer, it seems worth noting that it is not at all clear\nthat our reason for granting intelligence to other humans on the basis\nof their behavior is that we have prior knowledge of the collective\ncognitive achievements of human beings.  \n\nPerhaps the best known attack on the suggestion that The Turing Test\nprovides an appropriate research goal for AI is due to Hayes and Ford\n(1995). Among the controversial claims that Hayes and Ford make, there\nare at least the following:  \n\nSome of these claims seem straightforwardly incorrect. Consider (h),\nfor example. In what sense can it be claimed that 50% of the human\npopulation would fail “the species test”? If “the\nspecies test” requires the interrogator to decide which of two\npeople is a machine, why should it be thought that the verdict of the\ninterrogator has any consequences for the assessment of the\nintelligence of the person who is judged to be a machine? (Remember,\ntoo, that one of the conditions for “the species test”—as\nit is originally described by Hayes and Ford—is that one of the\ncontestants is a machine. While the machine can\n“demonstrate” its intelligence by winning the imitation\ngame, a person cannot “demonstrate” their lack of\nintelligence by failing to win.) \n\nIt seems wrong to say that The Turing Test is defective because it\nis a “null effect experiment”. True enough, there is a\nsense in which The Turing Test does look for a “null\nresult”: if ordinary judges in the specified circumstances fail\nto identify the machine (at a given level of success), then there is a\ngiven likelihood that the machine is intelligent. But the point of\ninsisting on “ordinary judges” in the specified\ncircumstances is precisely to rule out irrelevant ways of identifying\nthe machine (i.e. ways of identifying the machine that are not relevant\nto the question whether it is intelligent). There might be all kinds of\nirrelevant differences between a given kind of machine and a human\nbeing—not all of them rendered undetectable by the experimental\nset-up that Turing describes—but The Turing Test will remain a good\ntest provided that it is able to ignore these irrelevant\ndifferences. \n\nIt also seems doubtful that it is a serious failing of The Turing Test\nthat it can only test for “complete success”. On the one\nhand, if a man has a one in ten chance of producing a claim that is\nplainly not feminine, then we can compute the chance that he will be\ndiscovered in a game in which he answers N\nquestions—and, if N is sufficiently small, then it\nwon’t turn out that “he would almost always fail to\nwin”. On the other hand, as we noted at the end of Section 4.4\nabove, if one were worried about the “YES/NO” nature of\n“The Turing Test”, then one could always get the judges to\nproduce probabilistic verdicts instead. This change preserves the\ncharacter of The Turing Test, but gives it scope for greater\nstatistical sophistication. \n\nWhile there are (many) other criticisms that can be made of the\nclaims defended by Hayes and Ford (1995), it should be acknowledged\nthat they are right to worry about the suggestion that The Turing Test\nprovides the defining goal for research in AI. There are various\nreasons why one should be loathe to accept the proposition that the one\ncentral ambition of AI research is to produce artificial people.\nHowever it is worth pointing out that there is no reason to think that\nTuring supposed that The Turing Test defined the field of AI research\n(and there is not much evidence that any other serious thinkers have\nthought so either). Turing himself was well aware that there might be\nnon-human forms of intelligence—cf. (j) above. However, all of this\nremains consistent with the suggestion that it is quite appropriate to\nsuppose that The Turing Test sets one long term goal for AI\nresearch: one thing that we might well aim to do eventually is\nto produce artificial people. If—as Hayes and Ford claim—that\ntask is almost impossibly difficult, then there is no harm in supposing\nthat the goal is merely an ambit goal to which few resources\nshould be committed; but we might still have good reason to allow that\nit is a goal. \n\nThere are many different objections to The Turing Test which have\nsurfaced in the literature during the past fifty years, but which we\nhave not yet discussed. We cannot hope to canvass all of these\nobjections here. However, there is one argument—Searle’s\n“Chinese Room” argument—that is mentioned so often in\nconnection with the Turing Test that we feel obliged to end with some\ndiscussion of it.  \n\nIn Minds, Brains and Programs and elsewhere, John Searle\nargues against the claim that “appropriately programmed computers\nliterally have cognitive states” (64). Clearly enough, Searle is\nhere disagreeing with Turing’s claim that an appropriately programmed\ncomputer could think. There is much that is controversial about\nSearle’s argument; we shall just consider one way of\nunderstanding what it is that he is arguing for. \n\nThe basic structure of Searle’s argument is very well known. We can\nimagine a “hand simulation” of an intelligent agent—in\nthe case described, a speaker of a Chinese language—in circumstances\nin which we might well be very reluctant to allow that there is any\nappropriate intelligence lying behind the simulated behavior. (Thus,\nwhat we are invited to suppose is a logical possibility is not so very\ndifferent from what Block invites us to suppose is a logical\npossibility. However, the argument that Searle goes on to develop is\nrather different from the argument that Block defends.)\nMoreover—and this is really the key point for Searle’s\nargument—the “hand simulation” in question is, in all\nrelevant respects, simply a special kind of digital computation. So,\nthere is a possible world—doubtless one quite remote from the actual\nworld—in which a digital computer simulates intelligence but in\nwhich the digital computer does not itself possess intelligence. But,\nif we consider any digital computer in the actual world, it will not\ndiffer from the computer in that remote possible world in any way which\ncould make it the case that the computer in the actual world is more\nintelligent than the computer in that remote possible world. Given that\nwe agree that the “hand simulating” computer in the Chinese\nRoom is not intelligent, we have no option but to conclude that digital\ncomputers are simply not the kinds of things that can be\nintelligent. \n\nSo far, the argument that we have described arrives at the\nconclusion that no appropriately programmed computer can think. While\nthis conclusion is not one that Turing accepted, it is important to\nnote that it is compatible with the claim that The Turing Test is a\ngood test for intelligence. This is because, for all that has been\nargued, it may be that it is not nomically possible to provide\nany “hand simulation” of intelligence (and, in particular,\nthat it is not possible to simulate intelligence using any kind of\ncomputer). In order to turn Searle’s argument—at least in the way in\nwhich we have developed it—into an objection to The Turing Test, we\nneed to have some reason for thinking that it is at least\nnomically possible to simulate intelligence using computers.\n(If it is nomically impossible to simulate intelligence using\ncomputers, then the alleged fact that digital computers cannot\ngenuinely possess intelligence casts no doubt at all on the usefulness\nof the Turing Test, since digital computers are nomically disqualified\nfrom the range of cases in which there is mere simulation of\nintelligence.) In the absence of reason to believe this, the most that\nSearle’s argument yields is an objection to Turing’s confidently held\nbelief that digital computing machines will one day pass The Turing\nTest. (Here, as elsewhere, we are supposing that, for any kind of\ncreature C, there is a version of The Turing Test in which C takes the\nrole of the machine in the specific test that Turing describes. This\ngeneral format for testing for the presence of intelligence would not\nnecessarily be undermined by the success of Searle’s Chinese Room\nargument.) \n\nThere are various responses that might be made to the argument that\nwe have attributed to Searle. One kind of response is to dispute the\nclaim that there is no intelligence present in the case of the Chinese\nRoom. (Suppose that the “hand simulation” is embedded in a\nrobot that is equipped with appropriate sensors, etc. Suppose, further,\nthat the “hand simulation” involves updating the process of\n“hand simulation,” etc. If enough details of this kind are\nadded, then it becomes quite unclear whether we do want to say that we\nstill haven’t described an intelligent system.) Another kind of\nresponse is to dispute the claim that digital computers in the actual\nworld could not be relevantly different from the system that operates\nin the Chinese Room in that remote possible world. (If we suppose that\nthe core of the Chinese Room is a kind of giant look-up table, then it\nmay well be important to note that digital computers in the actual\nworld do not work with look-up tables in that kind of way.) Doubtless\nthere are other possible lines of response as well. However, it would\ntake us out of our way to try to take this discussion further. (One\ngood place to look for further discussion of these matters is\nBraddon-Mitchell and Jackson (1996).) \n\nThere are radically different views about the measurement of\nintelligence that have not been canvassed in this article. Our concern\nhas been to discuss Turing (1950) and its legacy. But, of course, a\nmore wide-ranging discussion would also consider, for example,\nresearch on the measurement of intelligence using the mathematical and\ncomputational resources of Algorithmic Information Theory, Kolmogorov\nComplexity Theory, Minimum Message Length (MML) Theory, and so forth. (For\nan introduction to this literature, see Hernandez-Orallo and Dowe\n(2010), and the list of references contained therein. For a more general introduction to research into AI, see Marquis et al. (2020).) ","contact.mail":"Graham.Oppy@monash.edu","contact.domain":"monash.edu"},{"date.published":"2003-04-09","date.changed":"2020-08-18","url":"https://plato.stanford.edu/entries/turing-test/","author1":"Graham Oppy","author1.info":"http://profiles.arts.monash.edu.au/graham-oppy/","author2.info":"http://www.csse.monash.edu.au/~dld/David.Dowe.publications.html","entry":"turing-test","body.text":"\n\n\n\nThe phrase “The Turing Test” is most properly used to\nrefer to a proposal made by Turing (1950) as a way of dealing with the\nquestion whether machines can think. According to Turing, the question\nwhether machines can think is itself “too meaningless” to\ndeserve discussion (442). However, if we consider the more\nprecise—and somehow related—question whether a digital computer\ncan do well in a certain kind of game that Turing describes\n(“The Imitation Game”), then—at least in Turing’s\neyes—we do have a question that admits of precise\ndiscussion. Moreover, as we shall see, Turing himself thought that it\nwould not be too long before we did have digital computers that could\n“do well” in the Imitation Game.\n\n\n\nThe phrase “The Turing Test” is sometimes used more\ngenerally to refer to some kinds of behavioural tests for the presence\nof mind, or thought, or intelligence in putatively minded\nentities. So, for example, it is sometimes suggested that The Turing\nTest is prefigured in Descartes’ Discourse on the\nMethod. (Copeland (2000:527) finds an anticipation of the test in\nthe 1668 writings of the Cartesian de Cordemoy. Abramson (2011a)\npresents archival evidence that Turing was aware of Descartes’\nlanguage test at the time that he wrote his 1950 paper. Gunderson\n(1964) provides an early instance of those who find that\nTuring’s work is foreshadowed in the work of Descartes.) In\nthe Discourse, Descartes says:\n\nIf there were machines which bore a resemblance to our\nbodies and imitated our actions as closely as possible for all\npractical purposes, we should still have two very certain means of\nrecognizing that they were not real men. The first is that they could\nnever use words, or put together signs, as we do in order to declare\nour thoughts to others. For we can certainly conceive of a machine so\nconstructed that it utters words, and even utters words that correspond\nto bodily actions causing a change in its organs. … But it is\nnot conceivable that such a machine should produce different\narrangements of words so as to give an appropriately meaningful answer\nto whatever is said in its presence, as the dullest of men can do.\nSecondly, even though some machines might do some things as well as we\ndo them, or perhaps even better, they would inevitably fail in others,\nwhich would reveal that they are acting not from understanding, but\nonly from the disposition of their organs. For whereas reason is a\nuniversal instrument, which can be used in all kinds of situations,\nthese organs need some particular action; hence it is for all practical\npurposes impossible for a machine to have enough different organs to\nmake it act in all the contingencies of life in the way in which our\nreason makes us act. (Translation by Robert Stoothoff)\n\n\n\nAlthough not everything about this passage is perfectly clear, it\ndoes seem that Descartes gives a negative answer to the question\nwhether machines can think; and, moreover, it seems that his giving\nthis negative answer is tied to his confidence that no mere machine\ncould pass The Turing Test: no mere machine could talk and act in the\nway in which adult human beings do. Since Descartes explicitly says\nthat there are “two very certain means” by which we can\nrule out that something is a machine—it is, according to Descartes,\ninconceivable that a mere machine could produce different arrangements\nof words so as to give an appropriately meaningful answer to whatever\nis said in its presence; and it is for all practical purposes\nimpossible for a machine to have enough different organs to make it act\nin all the contingencies of life in the way in which our reason makes\nus act—it seems that he must agree with the further claim that\nnothing that can produce different arrangements of words so as to give\nan appropriately meaningful answer to whatever is said in its presence\ncan be a machine. Given the further assumption—which one suspects\nthat Descartes would have been prepared to grant—that only things\nthat think can produce different arrangements of words so as to give an\nappropriately meaningful answer to whatever is said in their presence,\nit seems to follow that Descartes would have agreed that the Turing\nTest would be a good test of his confident assumption that there cannot\nbe thinking machines. Given the knowledge that something is indeed a\nmachine, evidence that that thing can produce different arrangements of\nwords so as to give an appropriately meaningful answer to whatever is\nsaid in its presence is evidence that there can be thinking machines.\n\n\n\nThe phrase “The Turing Test” is also sometimes used to\nrefer to certain kinds of purely behavioural allegedly logically\nsufficient conditions for the presence of mind, or thought, or\nintelligence, in putatively minded entities. So, for example, Ned\nBlock’s “Blockhead” thought experiment is often said to be\na (putative) knockdown objection to The Turing Test. (Block (1981)\ncontains a direct discussion of The Turing Test in this context.) Here,\nwhat a proponent of this view has in mind is the idea that it is\nlogically possible for an entity to pass the kinds of tests\nthat Descartes and (at least allegedly) Turing have in mind—to use\nwords (and, perhaps, to act) in just the kind of way that human beings\ndo—and yet to be entirely lacking in intelligence, not possessed of\na mind, etc.\n\n\n\nThe subsequent discussion takes up the preceding ideas in the order\nin which they have been introduced. First, there is a discussion of\nTuring’s paper (1950), and of the arguments contained therein. Second,\nthere is a discussion of current assessments of various proposals that\nhave been called “The Turing Test” (whether or not there is\nmuch merit in the application of this label to the proposals in\nquestion). Third, there is a brief discussion of some recent writings\non The Turing Test, including some discussion of the question whether\nThe Turing Test sets an appropriate goal for research into artificial\nintelligence. Finally, there is a very short discussion of Searle’s\nChinese Room argument, and, in particular, of the bearing of this\nargument on The Turing Test.\n\n\n\nTuring (1950) describes the following kind of game. Suppose that we\nhave a person, a machine, and an interrogator. The interrogator is in a\nroom separated from the other person and the machine. The object of the\ngame is for the interrogator to determine which of the other two is the\nperson, and which is the machine. The interrogator knows the other\nperson and the machine by the labels ‘X’ and\n‘Y’—but, at least at the beginning of the game,\ndoes not know which of the other person and the machine is\n‘X’—and at the end of the game says either\n‘X is the person and Y is the machine’ or\n‘X is the machine and Y is the person’.\nThe interrogator is allowed to put questions to the person and the\nmachine of the following kind: “Will X please tell me\nwhether X plays chess?” Whichever of the machine and the\nother person is X must answer questions that are addressed to\nX. The object of the machine is to try to cause the\ninterrogator to mistakenly conclude that the machine is the other\nperson; the object of the other person is to try to help the\ninterrogator to correctly identify the machine. About this game, Turing\n(1950) says:   There are at least two kinds of questions that can be raised about\nTuring’s predictions concerning his Imitation Game. First, there are\nempirical questions, e.g., Is it true that we now—or will\nsoon—have made computers that can play the imitation game so\nwell that an average interrogator has no more than a 70 percent chance\nof making the right identification after five minutes of questioning?\nSecond, there are conceptual questions, e.g., Is it true that, if an\naverage interrogator had no more than a 70 percent chance of making\nthe right identification after five minutes of questioning, we should\nconclude that the machine exhibits some level of thought, or\nintelligence, or mentality?  There is little doubt that Turing would have been disappointed by\nthe state of play at the end of the twentieth century.  Participants\nin the Loebner Prize Competition—an annual event in which\ncomputer programmes are submitted to the Turing Test— had come\nnowhere near the standard that Turing envisaged. A quick look at the\ntranscripts of the participants for the preceding decade reveals that\nthe entered programs were all easily detected by a range of\nnot-very-subtle lines of questioning. Moreover, major players in the\nfield regularly claimed that the Loebner Prize Competition was an\nembarrassment precisely because we were still so far from having a\ncomputer programme that could carry out a decent conversation for a\nperiod of five minutes—see, for example, Shieber (1994). It was\nwidely conceded on all sides that the programs entered in the Loebner\nPrize Competition were designed solely with the aim of winning the\nminor prize of best competitor for the year, with no thought that the\nembodied strategies would actually yield something capable of passing\nthe Turing Test. \n\nAt the end of the second decade of the twenty-first century, it is\nunclear how much has changed. On the one hand, there have been\ninteresting developments in language generators. In particular, the\nrelease of Open AI’s GPT-3 (Brown, et al. 2020, Other Internet\nResources) has prompted a flurry of excitement. GPT-3 is quite good at\ngenerating fiction, poetry, press releases, code, music, jokes,\ntechnical manuals, and news articles. Perhaps, as Chalmers speculates\n(2020, Other Internet Resources), GPT-3 “suggests a potential\nmindless path to artificial general intelligence”. But, of\ncourse, GPT-3 is not close to passing the Turing Test: GPT-3 neither\nperceives nor acts, and it is, at best, highly contentious whether it\nis a site of understanding. What remains to be seen is whether, within\nthe next couple of generations of language generators – GPT-4 or\nGPT-5 – we have something that can be linked to perceptual\ninputs and behavioural outputs in a way that does produce something\ncapable of passing the Turing Test. \n\nOn the other hand, as, for example, Floridi (2008) complains, there\nare other ways in which progress has been frustratingly slow. In 2014,\nclaims emerged that, because the computer program Eugene\nGoostman had fooled 33% of judges in the Turing Test 2014\ncompetition, it had “passed the Turing Test”. But there\nhave been other one-off competitions in which similar results have\nbeen achieved. Back in 1991, PC Therapist had 50% of judges\nfooled. And, in a 2011 demonstration, Cleverbot had an even\nhigher success rate. In all three of these cases, the size of the\ntrial was very small, and the result was not reliably projectible: in\nno case were there strong grounds for holding that an average\ninterrogator had no more than a 70% chance of making the right\ndetermination about the relevant program after five minutes of\nquestioning. Moreover—and much more importantly—we must\ndistinguish between the test the Turing proposed, and the particular\nprediction that he made about how things would be by the end of the\ntwentieth century. The percentage chance of making the correct\nidentification, the time interval over which the test takes place, and\nthe number of conversational exchanges required are all adjustable\nparameters in the Test, despite the fact that they are fixed in the\nparticular prediction that Turing made. Even if Turing was very far\nout in the prediction that he made about how things would be by the\nend of the twentieth century, it remains possible that the test that\nhe proposes is a good one. However, before one can endorse the\nsuggestion that the Turing Test is good, there are various objections\nthat ought to be addressed. \n\nSome people have suggested that the Turing Test is chauvinistic: it\nonly recognizes intelligence in things that are able to sustain a\nconversation with us. Why couldn’t it be the case that there are\nintelligent things that are unable to carry on a conversation, or, at\nany rate, unable to carry on a conversation with creatures like us?\n(See, for example, French (1990).) Perhaps the intuition behind this\nquestion can be granted; perhaps it is unduly chauvinistic to insist\nthat anything that is intelligent has to be capable of sustaining a\nconversation with us. (On the other hand, one might think that, given\nthe availability of suitably qualified translators, it ought to be\npossible for any two intelligent agents that speak different languages\nto carry on some kind of conversation.) But, in any case, the charge of\nchauvinism is completely beside the point. What Turing claims is only\nthat, if something can carry out a conversation with us, then we have\ngood grounds to suppose that that thing has intelligence of the kind\nthat we possess; he does not claim that only something that can carry\nout a conversation with us can possess the kind of intelligence that we\nhave. \n\nOther people have thought that the Turing Test is not sufficiently\ndemanding: we already have anecdotal evidence that quite unintelligent\nprograms (e.g., ELIZA—for details of which, see Weizenbaum\n(1966)) can seem to ordinary observers to be loci of intelligence for\nquite extended periods of time. Moreover, over a short period of\ntime—such as the five minutes that Turing mentions in his\nprediction about how things will be in the year 2000—it might\nwell be the case that almost all human observers could be taken in by\ncunningly designed but quite unintelligent programs. However, it is\nimportant to recall that, in order to pass Turing’s Test, it is not\nenough for the computer program to fool “ordinary\nobservers” in circumstances other than those in which the test\nis supposed to take place. What the computer program has to be able to\ndo is to survive interrogation by someone who knows that one of the\nother two participants in the conversation is a machine. Moreover, the\ncomputer program has to be able to survive such interrogation with a\nhigh degree of success over a repeated number of trials. (Turing says\nnothing about how many trials he would require. However, we can safely\nassume that, in order to get decent evidence that there is no more\nthan a 70% chance that a machine will be correctly identified as a\nmachine after five minutes of conversation, there will have to be a\nreasonably large number of trials.) If a computer program could do\nthis quite demanding thing, then it does seem plausible to claim that\nwe would have at least prima facie reason for thinking that\nwe are in the presence of intelligence. (Perhaps it is worth\nemphasizing again that there might be all kinds of intelligent\nthings—including intelligent machines—that would not pass this\ntest. It is conceivable, for example, that there might be machines\nthat, as a result of moral considerations, refused to lie or to engage\nin pretence. Since the human participant is supposed to do everything\nthat he or she can to help the interrogator, the question “Are\nyou a machine?” would quickly allow the interrogator to sort\nsuch (pathological?) truth-telling machines from humans.) \n\nAnother contentious aspect of Turing’s paper (1950) concerns his\nrestriction of the discussion to the case of “digital\ncomputers.” On the one hand, it seems clear that this restriction\nis really only significant for the prediction that Turing makes about\nhow things will be in the year 2000, and not for the details of the\ntest itself. (Indeed, it seems that if the test that Turing proposes is\na good one, then it will be a good test for any kinds of entities,\nincluding, for example, animals, aliens, and analog computers. That is:\nif animals, aliens, analog computers, or any other kinds of things,\npass the test that Turing proposes, then there will be as much reason\nto think that these things exhibit intelligence as there is reason to\nthink that digital computers that pass the test exhibit intelligence.)\nOn the other hand, it is actually a highly controversial question\nwhether “thinking machines” would have to be digital\ncomputers; and it is also a controversial question whether Turing\nhimself assumed that this would be the case. In particular, it is worth\nnoting that the seventh of the objections that Turing (1950) considers\naddresses the possibility of continuous state machines, which Turing\nexplicitly acknowledges to be different from discrete state machines.\nTuring appears to claim that, even if we are continuous state machines,\na discrete state machine would be able to imitate us sufficiently well\nfor the purposes of the Imitation Game. However, it seems doubtful that\nthe considerations that he gives are sufficient to establish that, if\nthere are continuous state machines that pass the Turing Test, then it\nis possible to make discrete state machines that pass the test as well.\n(Turing himself was keen to point out that some limits had to be set on\nthe notion of “machine” in order to make the question about\n“thinking machines” interesting: \n\nBut, of course, as Turing himself recognized, there is a large class\nof possible “machines” that are neither digital nor\nbiotechnological.) More generally, the crucial point seems to be that,\nwhile Turing recognized that the class of machines is potentially much\nlarger than the class of discrete state machines, he was himself\nvery confident that properly engineered discrete state\nmachines could succeed in the Imitation Game (and, moreover, at the\ntime that he was writing, there were certain discrete state\nmachines—“electronic computers”—that loomed very\nlarge in the public imagination). \n\nAlthough Turing (1950) is pretty informal, and, in some ways rather\nidiosyncratic, there is much to be gained by considering the\ndiscussion that Turing gives of potential objections to his claim that\nmachinese—and, in particular, digital computers—can\n“think”.  Turing gives the following labels to the\nobjections that he considers: (1) The Theological Objection; (2) The\n“Heads in the Sand” Objection; (3) The Mathematical\nObjection; (4) The Argument from Consciousness; (5) Arguments from\nVarious Disabilities; (6) Lady Lovelace’s Objection; (7) Argument from\nContinuity of the Nervous System; (8) The Argument from Informality of\nBehavior; and (9) The Argument from Extra-Sensory Perception. We shall\nconsider these objections in the corresponding subsections below. (In\nsome—but not all—cases, the counter-arguments to these objections\nthat we discuss are also provided by Turing.) \n\nSubstance dualists believe that thinking is a function of a\nnon-material, separately existing, substance that somehow\n“combines” with the body to make a person. So—the\nargument might go—making a body can never be sufficient to guarantee\nthe presence of thought: in themselves, digital computers are no\ndifferent from any other merely material bodies in being utterly unable\nto think. Moreover—to introduce the “theological”\nelement—it might be further added that, where a “soul”\nis suitably combined with a body, this is always the work of the divine\ncreator of the universe: it is entirely up to God whether or not a\nparticular kind of body is imbued with a thinking soul. (There is well\nknown scriptural support for the proposition that human beings are\n“made in God’s image”. Perhaps there is also theological\nsupport for the claim that only God can make things in God’s\nimage.) \n\nThere are several different kinds of remarks to make here. First,\nthere are many serious objections to substance dualism. Second, there\nare many serious objections to theism. Third, even if theism and\nsubstance dualism are both allowed to pass, it remains quite unclear\nwhy thinking machines are supposed to be ruled out by this combination\nof views. Given that God can unite souls with human bodies, it is hard\nto see what reason there is for thinking that God could not unite souls\nwith digital computers (or rocks, for that matter!). Perhaps, on this\ncombination of views, there is no especially good reason why, amongst\nthe things that we can make, certain kinds of digital computers turn\nout to be the only ones to which God gives souls—but it seems pretty\nclear that there is also no particularly good reason for ruling out the\npossibility that God would choose to give souls to certain kinds of\ndigital computers. Evidence that God is dead set against the idea of\ngiving souls to certain kinds of digital computers is not particularly\nthick on the ground. \n\nIf there were thinking machines, then various consequences would\nfollow. First, we would lose the best reasons that we have for thinking\nthat we are superior to everything else in the universe (since our\ncherished “reason” would no longer be something that we\nalone possess). Second, the possibility that we might be\n“supplanted” by machines would become a genuine worry: if\nthere were thinking machines, then very likely there would be machines\nthat could think much better than we can. Third, the possibility that\nwe might be “dominated” by machines would also become a\ngenuine worry: if there were thinking machines, who’s to say that they\nwould not take over the universe, and either enslave or exterminate us? \n\nAs it stands, what we have here is not an argument against the claim\nthat machines can think; rather, we have the expression of various\nfears about what might follow if there were thinking machines. Someone\nwho took these worries seriously—and who was persuaded that it is\nindeed possible for us to construct thinking machines—might well\nthink that we have here reasons for giving up on the project of\nattempting to construct thinking machines. However, it would be a major\ntask—which we do not intend to pursue here—to determine whether\nthere really are any good reasons for taking these worries\nseriously. \n\nSome people have supposed that certain fundamental results in\nmathematical logic that were discovered during the 1930s—by\nGödel (first incompleteness theorem) and Turing (the halting\nproblem)—have important consequences for questions about digital\ncomputation and intelligent thought. (See, for example, Lucas (1961)\nand Penrose (1989); see, too, Hodges (1983:414) who mentions Polanyi’s\ndiscussions with Turing on this matter.) Essentially, these results\nshow that within a formal system that is strong enough, there are a\nclass of true statements that can be expressed but not proven within\nthe system (see the entry on\n Gödel’s incompleteness theorems).\n Let us say that\nsuch a system is “subject to the Lucas-Penrose constraint” because it\nis constrained from being able to prove a class of true statements\nexpressible within the system.  \n\nTuring (1950:444) himself observes that these results from\nmathematical logic might have implications for the Turing test: \n\nSo, in the context of the Turing test, “being subject to the\nLucas-Penrose constraint” implies the existence of a class of\n“unanswerable” questions. However Turing noted that in the\ncontext of the Turing test, these “unanswerable” questions\nare only a concern if humans can answer them. His “short”\nreply was that it is not clear that humans are free from such a\nconstraint themselves. Turing then goes on to add that he does not\nthink that the argument can be dismissed “quite so\nlightly.”  \n\nTo make the argument more precise, we can write it as follows: \n\nOnce the argument is laid out as above, it becomes clear that premise\n(3) should be challenged. Putting that aside, we note that one\ninterpretation of Turing’s “short” reply is that claim (4)\nis merely asserted—without any kind of proof. The\n“short” reply then leads us to examine whether humans are\nfree from the Lucas-Penrose constraint.  \n\nIf humans are subject to the Lucas-Penrose constraint then the\nconstraint does not provide any basis for distinguishing humans from\ndigital computers. If humans are free from the Lucas-Penrose\nconstraint, then (granting premise 3) it follows that digital computers\nmay fail the Turing test and thus, it seems, cannot think. \n\nHowever, there remains a question as to whether being free from the\nconstraint is necessary for the capacity to think. It may be that the\nTuring test is too strict. Since, by hypothesis, we are free from the\nLucas-Penrose constraint, we are, in some sense, too good at asking\nand answering questions. Suppose there is a thinking entity that is\nsubject to the Lucas-Penrose constraint. By an argument analogous to\nthe one above, it can fail the Turing test. Thus, an entity which can\nthink would fail the Turing test. \n\nWe can respond to this concern by noting that the construction of\nquestions suggested by the results from mathematical\nlogic—Gödel, Turing, etc.—are extremely complicated,\nand require extremely detailed information about the language and\ninternal programming of the digital computer (which, of course, is not\navailable to the interrogators in the Imitation Game). At the very\nleast, much more argument is required to overthrow the view that the\nTuring Test could remain a very high quality statistical test for the\npresence of mind and intelligence even if digital computers differ\nfrom human beings in being subject to the Lucas-Penrose\nconstraint. (See Bowie 1982, Dietrich 1994, Feferman 1996, \nAbramson 2008, and Section 6.3 of the entry on \n  Gödel’s incompleteness theorems,\n for further discussion.) \n\nTuring cites Professor Jefferson’s Lister Oration for 1949 as\na source for the kind of objection that he takes to fall under this\nlabel:  \n\nThere are several different ideas that are being run together here, and\nthat it is profitable to disentangle. One idea—the one upon which\nTuring first focuses—is the idea that the only way in which one\ncould be certain that a machine thinks is to be the machine, and to\nfeel oneself thinking. A second idea, perhaps, is that the presence of\nmind requires the presence of a certain kind of self-consciousness\n(“not only write it but know that it had written it”). A\nthird idea is that it is a mistake to take a narrow view of the mind,\ni.e. to suppose that there could be a believing intellect divorced from\nthe kinds of desires and emotions that play such a central role in the\ngeneration of human behavior (“no mechanism could feel\n…”).  \n\nAgainst the solipsistic line of thought, Turing makes the effective\nreply that he would be satisfied if he could secure agreement on the\nclaim that we might each have just as much reason to suppose that\nmachines think as we have reason to suppose that other people\nthink. (The point isn’t that Turing thinks that solipsism is a serious\noption; rather, the point is that following this line of argument isn’t\ngoing to lead to the conclusion that there are respects in which\ndigital computers could not be our intellectual equals or\nsuperiors.) \n\nAgainst the other lines of thought, Turing provides a little\n“viva voce” that is intended to illustrate the\nkind of evidence that he supposes one might have that a machine is\nintelligent. Given the right kinds of responses from the machine, we\nwould naturally interpret its utterances as evidence of\npleasure, grief, warmth, misery, anger, depression,\netc. Perhaps—though Turing doesn’t say this—the only way to\nmake a machine of this kind would be to equip it with sensors,\naffective states, etc., i.e., in effect, to make an artificial\nperson. However, the important point is that if the claims\nabout self-consciousness, desires, emotions, etc. are right, then\nTuring can accept these claims with equanimity: his claim is\nthen that a machine with a digital computing “brain” can\nhave the full range of mental states that can be enjoyed by adult\nhuman beings. \n\nTuring considers a list of things that some people have claimed\nmachines will never be able to do: (1) be kind; (2) be resourceful; (3)\nbe beautiful; (4) be friendly; (5) have initiative; (6) have a sense of\nhumor; (7) tell right from wrong; (8) make mistakes; (9) fall in love;\n(10) enjoy strawberries and cream; (11) make someone fall in love with\none; (12) learn from experience; (13) use words properly; (14) be the\nsubject of one’s own thoughts; (15) have as much diversity of behavior\nas a man; (16) do something really new.  \n\nAn interesting question to ask, before we address these claims\ndirectly, is whether we should suppose that intelligent creatures from\nsome other part of the universe would necessarily be able to do these\nthings. Why, for example, should we suppose that there must be\nsomething deficient about a creature that does not enjoy—or that is\nnot able to enjoy—strawberries and cream? True enough, we might\nsuppose that an intelligent creature ought to have the capacity to\nenjoy some kinds of things—but it seems unduly chauvinistic to\ninsist that intelligent creatures must be able to enjoy just the kinds\nof things that we do. (No doubt, similar considerations apply to the\nclaim that an intelligent creature must be the kind of thing that can\nmake a human being fall in love with it. Yes, perhaps, an intelligent\ncreature should be the kind of thing that can love and be loved; but\nwhat is so special about us?) \n\nSetting aside those tasks that we deem to be unduly chauvinistic, we\nshould then ask what grounds there are for supposing that no digital\ncomputing machine could do the other things on the list.\nTuring suggests that the most likely ground lies in our prior\nacquaintance with machines of all kinds: none of the machines that any\nof us has hitherto encountered has been able to do these things. In\nparticular, the digital computers with which we are now familiar cannot\ndo these things. (Except perhaps for make mistakes: after all, even\ndigital computers are subject to “errors of functioning.”\nBut this might be set aside as an irrelevant case.) However, given the\nlimitations of storage capacity and processing speed of even the most\nrecent digital computers, there are obvious reasons for being cautious\nin assessing the merits of this inductive argument. \n\n(A different question worth asking concerns the progress that has\nbeen made until now in constructing machines that can do the kinds of\nthings that appear on Turing’s list. There is at least room for debate\nabout the extent to which current computers can: make mistakes, use\nwords properly, learn from experience, be beautiful, etc. Moreover,\nthere is also room for debate about the extent to which recent advances\nin other areas may be expected to lead to further advancements in\novercoming these alleged disabilities. Perhaps, for example, recent\nadvances in work on artificial sensors may one day contribute to the\nproduction of machines that can enjoy strawberries and cream. Of\ncourse, if the intended objection is to the notion that machines can\nexperience any kind of feeling of enjoyment, then it is not clear that\nwork on particular kinds of artificial sensors is to the point.) \n\nOne of the most popular objections to the claim that there can be\nthinking machines is suggested by a remark made by Lady Lovelace in her\nmemoir on Babbage’s Analytical Engine:  \n\nThe key idea is that machines can only do what we know how to\norder them to do (or that machines can never do anything really new,\nor anything that would take us by surprise). As Turing says, one way\nto respond to these challenges is to ask whether we can ever do\nanything “really new.” Suppose, for instance, that the\nworld is deterministic, so that everything that we do is fully\ndetermined by the laws of nature and the boundary conditions of the\nuniverse. There is a sense in which nothing “really new”\nhappens in a deterministic universe—though, of course, the\nuniverse’s being deterministic would be entirely compatible with our\nbeing surprised by events that occur within it. Moreover—as\nTuring goes on to point out—there are many ways in which even digital\ncomputers do things that take us by surprise; more needs to be said to\nmake clear exactly what the nature of this suggestion is. (Yes, we\nmight suppose, digital computers are “constrained” by\ntheir programs: they can’t do anything that is not permitted by the\nprograms that they have. But human beings are\n“constrained” by their biology and their genetic\ninheritance in what might be argued to be just the same kind of way:\nthey can’t do anything that is not permitted by the biology and\ngenetic inheritance that they have. If a program were sufficiently\ncomplex—and if the processor(s) on which it ran were\nsufficiently fast—then it is not easy to say whether the kinds\nof “constraints” that would remain would necessarily\ndiffer in kind from the kinds of constraints that are imposed by\nbiology and genetic inheritance.) \n\nBringsjord et al. (2001) claim that Turing’s response to the\nLovelace Objection is “mysterious” at best, and\n“incompetent” at worst (p.4). In their view, Turing’s claim\nthat “computers do take us by surprise” is only true when\n“surprise” is given a very superficial interpretation. For,\nwhile it is true that computers do things that we don’t intend them to\ndo—because we’re not smart enough, or because we’re not careful\nenough, or because there are rare hardware errors, or whatever—it\nisn’t true that there are any cases in which we should want to say that\na computer has originated something. Whatever merit might be\nfound in this objection, it seems worth pointing out that, in the\nrelevant sense of origination, human beings “originate\nsomething” on more or less every occasion in which they engage in\nconversation: they produce new sentences of natural language that it is\nappropriate for them to produce in the circumstances in which they find\nthemselves. Thus, on the one hand—for all that Bringsjord et al.\nhave argued—The Turing Test is a perfectly good test for the\npresence of “origination” (or “creativity,” or\nwhatever). Moreover, on the other hand, for all that Bringsjord et al.\nhave argued, it remains an open question whether a digital computing\ndevice is capable of “origination” in this sense (i.e.\ncapable of producing new sentences that are appropriate to the\ncircumstances in which the computer finds itself). So we are not overly\ninclined to think that Turing’s response to the Lovelace Objection is\npoor; and we are even less inclined to think that Turing lacked the\nresources to provide a satisfactory response on this point. \n\nThe human brain and nervous system is not much like a digital computer.\nIn particular, there are reasons for being skeptical of the claim that\nthe brain is a discrete-state machine. Turing observes that a small\nerror in the information about the size of a nervous impulse impinging\non a neuron may make a large difference to the size of the outgoing\nimpulse. From this, Turing infers that the brain is likely to be a\ncontinuous-state machine; and he then notes that, since discrete-state\nmachines are not continuous-state machines, there might be reason here\nfor thinking that no discrete-state machine can be intelligent.  \n\nTuring’s response to this kind of argument seems to be that a\ncontinuous-state machine can be imitated by discrete-state machines\nwith very small levels of error. Just as differential analyzers can be\nimitated by digital computers to within quite small margins of error,\nso too, the conversation of human beings can be imitated by digital\ncomputers to margins of error that would not be detected by ordinary\ninterrogators playing the imitation game. It is not clear that this is\nthe right kind of response for Turing to make. If someone thinks that\nreal thought (or intelligence, or mind, or whatever) can only be\nlocated in a continuous-state machine, then the fact—if, indeed, it\nis a fact—that it is possible for discrete-state machines to pass\nthe Turing Test shows only that the Turing Test is no good. A better\nreply is to ask why one should be so confident that real thought, etc.\ncan only be located in continuous-state machines (if, indeed, it is\nright to suppose that we are not discrete-state machines). And, before\nwe ask this question, we would do well to consider whether we really do\nhave such good reason to suppose that, from the standpoint of our\nability to think, we are not essentially discrete-state machines. (As\nBlock (1981) points out, it seems that there is nothing in our concept\nof intelligence that rules out intelligent beings with quantised\nsensory devices; and nor is there anything in our concept of\nintelligence that rules out intelligent beings with digital working\nparts.) \n\nThis argument relies on the assumption that there is no set of rules\nthat describes what a person ought to do in every possible set of\ncircumstances, and on the further assumption that there is a set of\nrules that describes what a machine will do in every possible set of\ncircumstances. From these two assumptions, it is supposed to\nfollow—somehow!—that people are not machines. As Turing notes,\nthere is some slippage between “ought” and\n“will” in this formulation of the argument. However, once\nwe make the appropriate adjustments, it is not clear that an obvious\ndifference between people and digital computers emerges. \n\nSuppose, first, that we focus on the question of whether there are\nsets of rules that describe what a person and a machine\n“will” do in every possible set of circumstances. If the\nworld is deterministic, then there are such rules for both persons and\nmachines (though perhaps it is not possible to write down the rules).\nIf the world is not deterministic, then there are no such rules for\neither persons or machines (since both persons and machines can be\nsubject to non-deterministic processes in the production of their\nbehavior). Either way, it is hard to see any reason for supposing that\nthere is a relevant difference between people and machines that bears\non the description of what they will do in all possible sets of\ncircumstances. (Perhaps it might be said that what the objection\ninvites us to suppose is that, even though the world is not\ndeterministic, humans differ from digital machines precisely because\nthe operations of the latter are indeed deterministic. But, if the\nworld is non-deterministic, then there is no reason why digital\nmachines cannot be programmed to behave non-deterministically, by\nallowing them to access input from non-deterministic features of the\nworld.) \n\nSuppose, instead, that we focus on the question of whether there are\nsets of rules that describe what a person and a machine\n“ought” to do in every possible set of circumstances.\nWhether or not we suppose that norms can be codified—and quite apart\nfrom the question of which kinds of norms are in question—it is hard\nto see what grounds there could be for this judgment, other than the\nquestion-begging claim that machines are not the kinds of things whose\nbehavior could be subject to norms. (And, in that case, the initial\nargument is badly mis-stated: the claim ought to be that, whereas there\nare sets of rules that describe what a person ought to do in every\npossible set of circumstances, there are no sets of rules that describe\nwhat machines ought to do in all possible sets of\ncircumstances!) \n\nThe strangest part of Turing’s paper is the few paragraphs on ESP.\nPerhaps it is intended to be tongue-in-cheek, though, if it is, this\nfact is poorly signposted by Turing. Perhaps, instead, Turing was\ninfluenced by the apparently scientifically respectable results of J.\nB. Rhine. At any rate, taking the text at face value, Turing seems to\nhave thought that there was overwhelming empirical evidence for\ntelepathy (and he was also prepared to take clairvoyance, precognition\nand psychokinesis seriously). Moreover, he also seems to have thought\nthat if the human participant in the game was telepathic, then the\ninterrogator could exploit this fact in order to determine the\nidentity of the machine—and, in order to circumvent this\ndifficulty, Turing proposes that the competitors should be housed in a\n“telepathy-proof room.” Leaving aside the point that, as a\nmatter of fact, there is no current statistical support for\ntelepathy—or clairvoyance, or precognition, or telekinesis—it\nis worth asking what kind of theory of the nature of telepathy would\nhave appealed to Turing. After all, if humans can be telepathic, why\nshouldn’t digital computers be so as well? If the capacity for\ntelepathy were a standard feature of any sufficiently advanced system\nthat is able to carry out human conversation, then there is no\nin-principle reason why digital computers could not be the equals of\nhuman beings in this respect as well. (Perhaps this response assumes\nthat a successful machine participant in the imitation game will need\nto be equipped with sensors, etc. However, as we noted above, this\nassumption is not terribly controversial. A plausible\nconversationalist has to keep up to date with goings-on in the world.) \n\nAfter discussing the nine objections mentioned above, Turing goes on\nto say that he has “no very convincing arguments of a positive\nnature to support my views. If I had I should not have taken such\npains to point out the fallacies in contrary views.” (454)\nPerhaps Turing sells himself a little short in this\nself-assessment. First of all—as his brief discussion of\nsolipsism makes clear—it is worth asking what grounds we have\nfor attributing intelligence (thought, mind) to other people. If it is\nplausible to suppose that we base our attributions on behavioral tests\nor behavioral criteria, then his claim about the appropriate test to\napply in the case of machines seems apt, and his conjecture that\ndigital computing machines might pass the test seems like a\nreasonable—though controversial—empirical\nconjecture. Second, subsequent developments in the philosophy of\nmind—and, in particular, the fashioning of functionalist theories of\nthe mind—have provided a more secure theoretical environment in\nwhich to place speculations about the possibility of thinking\nmachines. If mental states are functional states—and if mental\nstates are capable of realisation in vastly different kinds of\nmaterials—then there is some reason to think that it is an\nempirical question whether minds can be realised in digital computing\nmachines. Of course, this kind of suggestion is open to challenge; we\nshall consider some important philosophical objections in the later\nparts of this review. \n\nThere are a number of much-debated issues that arise in connection with\nthe interpretation of various parts of Turing (1950), and that we have\nhitherto neglected to discuss. What has been said in the first two\nsections of this document amounts to our interpretation of what Turing\nhas to say (perhaps bolstered with what we take to be further relevant\nconsiderations in those cases where Turing’s remarks can be fairly\nreadily improved upon). But since some of this interpretation has been\ncontested, it is probably worth noting where the major points of\ncontroversy have been.  \n\nTuring (1950) introduces the imitation game by describing a game in\nwhich the participants are a man, a woman, and a human interrogator.\nThe interrogator is in a room apart from the other two, and is set the\ntask of determining which of the other two is a man and which is a\nwoman. Both the man and the woman are set the task of trying to\nconvince the interrogator that they are the woman. Turing recommends\nthat the best strategy for the woman is to answer all questions\ntruthfully; of course, the best strategy for the man will require some\nlying. The participants in this game also use teletypewriter to\ncommunicate with one another—to avoid clues that might be offered by\ntone of voice, etc. Turing then says: “We now ask the question,\n‘What will happen when a machine takes the part of A in this game?’\nWill the interrogator decide wrongly as often when the game is played\nlike this as he does when the game is played between a man and a woman?”\n(434). \n\nNow, of course, it is possible to interpret Turing as here\nintending to say what he seems literally to say, namely, that the new\ngame is one in which the computer must pretend to be a woman, and the\nother participant in the game is a woman. (See, for example, Genova\n(1994), and Traiger (2000).) And it is also possible to\ninterpret Turing as intending to say that the new game is one in which\nthe computer must pretend to be a woman, and the other participant in\nthe game is a man who must also pretend to be a woman. However, as\nCopeland (2000), Piccinini (2000), and Moor (2001) convincingly argue,\nthe rest of Turing’s article, and material in other articles that\nTuring wrote at around the same time, very strongly support the claim\nthat Turing actually intended the standard interpretation that we gave\nabove, viz. that the computer is to pretend to be a human being, and\nthe other participant in the game is a human being of unspecified\ngender. Moreover, as Moor (2001) argues, there is no reason to think\nthat one would get a better test if the computer must pretend to be a\nwoman and the other participant in the game is a man pretending to be\na woman (and, indeed, there is some reason to think that one would get\na worse test). Perhaps it would make no difference to the\neffectiveness of the test if the computer must pretend to be a woman,\nand the other participant is a woman (any more than it would make a\ndifference if the computer must pretend to be an accountant and the\nother participant is an accountant); however, this consideration is\nsimply insufficient to outweigh the strong textual evidence that\nsupports the standard interpretation of the imitation game that we\ngave at the beginning of our discussion of Turing (1950). \n\nAs we noted earlier, Turing (1950) makes the claim that:  \n\nMost commentators contend that this claim has been shown to be\nmistaken: in the year 2000, no-one was able to program\ncomputers to make them play the imitation game so well that an average\ninterrogator had no more than a 70% chance of making the correct\nidentification after five minutes of questioning. Copeland (2000)\nargues that this contention is seriously mistaken: “about fifty\nyears” is by no means “exactly fifty years,” and it\nremains open that we may soon be able to do the required programming.\nAgainst this, it should be noted that Turing (1950) goes on\nimmediately to refer to how things will be “at the end of the\ncentury,” which suggests that not too much can be read into the\nqualifying “about.” However, as Copeland (2000) points\nout, there are other more cautious predictions that Turing makes\nelsewhere (e.g., that it would be “at least 100 years”\nbefore a machine was able to pass an unrestricted version of his\ntest); and there are other predictions that are made in Turing (1950)\nthat seem to have been vindicated. In particular, it is plausible to\nclaim that, in the year 2000, educated opinion had altered to the\nextent that, in many quarters, one could speak of the possibility of\nmachines’ thinking—and of machines’ learning—without expecting\nto be contradicted. As Moor (2001) points out, “machine\nintelligence” is not the oxymoron that it might have been taken\nto be when Turing first started thinking about these matters. \n\nThere are two different theoretical claims that are run together in\nmany discussions of The Turing Test that can profitably be separated.\nOne claim holds that the general scheme that is described in Turing’s\nImitation Game provides a good test for the presence of intelligence.\n(If something can pass itself off as a person under sufficiently\ndemanding test conditions, then we have very good reason to suppose\nthat that thing is intelligent.) Another claim holds that an\nappropriately programmed computer could pass the kind of test that is\ndescribed in the first claim. We might call the first claim “The\nTuring Test Claim” and the second claim “The Thinking\nMachine Claim”. Some objections to the claims made in Turing\n(1950) are objections to the Thinking Machine Claim, but not objections\nto the Turing Test Claim. (Consider, for example, the argument of\nSearle (1982), which we discuss further in Section 6.) However, other\nobjections are objections to the Turing Test Claim. Until we get to\nSection 6, we shall be confining our attention to discussions of the\nTuring Test Claim.  \n\nIn this article, we follow the standard philosophical convention\naccording to which “a mind” means “at least one\nmind”. If “passing the Turing Test” implies\nintelligence, then “passing the Turing Test” implies the\npresence of at least one mind. We cannot here explore recent\ndiscussions of “swarm intelligence”, “collective\nintelligence”, and the like. However, it is surely clear that\ntwo people taking turns could “pass the Turing Test” in\ncircumstances in which we should be very reluctant to say that there\nis a “collective mind” that has the minds of the two as\ncomponents.  \n\nGiven the initial distinction that we made between different ways in\nwhich the expression The Turing Test gets interpreted in the\nliterature, it is probably best to approach the question of the\nassessment of the current standing of The Turing Test by dividing\ncases. True enough, we think that there is a correct interpretation of\nexactly what test it is that is proposed by Turing (1950); but a\ncomplete discussion of the current standing of The Turing Test should\npay at least some attention to the current standing of other tests that\nhave been mistakenly supposed to be proposed by Turing (1950).  \n\nThere are a number of main ideas to be investigated. First, there is\nthe suggestion that The Turing Test provides logically necessary and\nsufficient conditions for the attribution of intelligence. Second,\nthere is the suggestion that The Turing Test provides logically\nsufficient—but not logically necessary—conditions for the\nattribution of intelligence. Third, there is the suggestion that The\nTuring Test provides “criteria”—defeasible sufficient\nconditions—for the attribution of intelligence. Fourth—and\nperhaps not importantly distinct from the previous claim—there is\nthe suggestion that The Turing Test provides (more or less strong)\nprobabilistic support for the attribution of intelligence. We shall\nconsider each of these suggestions in turn. \n\nIt is doubtful whether there are very many examples of people who have\nexplicitly claimed that The Turing Test is meant to provide conditions\nthat are both logically necessary and logically sufficient for the\nattribution of intelligence. (Perhaps Block (1981) is one such case.)\nHowever, some of the objections that have been proposed against The\nTuring Test only make sense under the assumption that The Turing Test\ndoes indeed provide logically necessary and logically sufficient\nconditions for the attribution of intelligence; and many more of the\nobjections that have been proposed against The Turing Test only make\nsense under the assumption that The Turing Test provides necessary and\nsufficient conditions for the attribution of intelligence, where the\nmodality in question is weaker than the strictly logical, e.g., nomic\nor causal.  \n\nConsider, for example, those people who have claimed that The Turing\nTest is chauvinistic; and, in particular, those people who have claimed\nthat it is surely logically possible for there to be something that\npossesses considerable intelligence, and yet that is not able to pass\nThe Turing Test. (Examples: Intelligent creatures might fail to pass\nThe Turing Test because they do not share our way of life; intelligent\ncreatures might fail to pass The Turing Test because they refuse to\nengage in games of pretence; intelligent creatures might fail to pass\nThe Turing Test because the pragmatic conventions that govern the\nlanguages that they speak are so very different from the pragmatic\nconventions that govern human languages. Etc.) None of this can\nconstitute objections to The Turing Test unless The Turing Test\ndelivers necessary conditions for the attribution of\nintelligence. \n\nFrench (1990) offers ingenious arguments that are intended to show\nthat “the Turing Test provides a guarantee not of intelligence,\nbut of culturally-oriented intelligence.” But, of course,\nanything that has culturally-oriented intelligence has\nintelligence; so French’s objections cannot be taken to be directed\ntowards the idea that The Turing Test provides sufficient conditions\nfor the attribution of intelligence. Rather—as we shall see\nlater—French supposes that The Turing Test establishes sufficient\nconditions that no machine will ever satisfy. That is, in French’s\nview, what is wrong with The Turing Test is that it establishes\nutterly uninteresting sufficient conditions for the attribution of\nintelligence. \n\nThere are many philosophers who have supposed that The Turing Test is\nintended to provide logically sufficient conditions for the attribution\nof intelligence. That is, there are many philosophers who have supposed\nthat The Turing Test claims that it is logically impossible for\nsomething that lacks intelligence to pass The Turing Test. (Often, this\nsupposition goes with an interpretation according to which passing The\nTuring Test requires rather a lot, e.g., producing behavior that is\nindistinguishable from human behavior over an entire lifetime.)  \n\nThere are well-known arguments against the claim that passing The\nTuring Test—or any other purely behavioral test—provides\nlogically sufficient conditions for the attribution of intelligence.\nThe standard objection to this kind of analysis of\nintelligence (mind, thought) is that a being whose behavior was\nproduced by “brute force” methods ought not to count as\nintelligent (as possessing a mind, as having thoughts). \n\nConsider, for example, Ned Block’s Blockhead. Blockhead is\na creature that looks just like a human being, but that is controlled\nby a “game-of-life look-up tree,” i.e. by a tree that\ncontains a programmed response for every discriminable input at each\nstage in the creature’s life. If we agree that Blockhead is logically\npossible, and if we agree that Blockhead is not intelligent (does not\nhave a mind, does not think), then Blockhead is a counterexample to the\nclaim that the Turing Test provides a logically sufficient condition\nfor the ascription of intelligence. After all, Blockhead could be\nprogrammed with a look-up tree that produces responses identical with\nthe ones that you would give over the entire course of\nyour life (given the same inputs). \n\nThere are perhaps only two ways in which someone who claims that The\nTuring Test offers logically sufficient conditions for the attribution\nof intelligence can respond to Block’s argument. First, it could be\ndenied that Blockhead is a logical possibility; second, it could be\nclaimed that Blockhead would be intelligent (have a mind, think). \n\nIn order to deny that Blockhead is a logical possibility, it seems\nthat what needs to be denied is the commonly accepted link between\nconceivability and logical possibility: it certainly seems that\nBlockhead is conceivable, and so, if (properly circumscribed)\nconceivability is sufficient for logical possibility, then it seems\nthat we have good reason to accept that Blockhead is a logical\npossibility. Since it would take us too far away from our present\nconcerns to explore this issue properly, we merely note that it remains\na controversial question whether (properly circumscribed)\nconceivability is sufficient for logical possibility. (For further\ndiscussion of this issue, see Crooke (2002).) \n\nThe question of whether Blockhead is intelligent (has a mind, thinks)\nmay seem straightforward, but—despite Block’s confident\nassertion that Blockhead “has all of the intelligence of a\ntoaster”—it is not obvious that we should deny that\nBlockhead is intelligent. Blockhead may not be a particularly\nefficient processor of information; but it is at least a processor of\ninformation, and that—in combination with the behavior that is\nproduced as a result of the processing of information—might well\nbe taken to be sufficient grounds for the attribution of some\nlevel of intelligence to Blockhead. For further critical discussion of\nthe argument of Block (1981), see McDermott (2014), and Pautz and Stoljar (2019).  \n\nIn his Philosophical Investigations, Wittgenstein famously\nwrites: “An ‘inner process’ stands in need of outward\ncriteria” (580). Exactly what Wittgenstein meant by this remark\nis unclear, but one way in which it might be interpreted is as follows:\nin order to be justified in ascribing a “mental state” to\nsome entity, there must be some true claims about the observable\nbehavior of that entity that, (perhaps) together with other true claims\nabout that entity (not themselves couched in “mentalistic”\nvocabulary), entail that the entity has the mental state in question.\nIf no true claims about the observable behavior of the entity can play\nany role in the justification of the ascription of the mental state in\nquestion to the entity, then there are no grounds for attributing that\nkind of mental state to the entity.  \n\nThe claim that, in order to be justified in ascribing a mental state\nto an entity, there must be some true claims about the observable\nbehavior of that entity that alone—i.e. without the addition of any\nother true claims about that entity—entail that the entity has the\nmental state in question, is a piece of philosophical behaviorism. It\nmay be—for all that we are able to argue—that Wittgenstein was a\nphilosophical behaviorist; it may be—for all that we are able to\nargue—that Turing was one, too. However, if we go by the letter of\nthe account given in the previous paragraph, then all that need follow\nfrom the claim that the Turing Test is criterial for the ascription of\nintelligence (thought, mind) is that, when other true claims (not\nthemselves couched in terms of mentalistic vocabulary) are conjoined\nwith the claim that an entity has passed the Turing Test, it then\nfollows that the entity in question has intelligence (thought,\nmind). \n\n(Note that the parenthetical qualification that the additional true\nclaims not be couched in terms of mentalistic vocabulary is only one\nway in which one might try to avoid the threat of trivialization. The\ndifficulty is that the addition of the true claim that an entity has a\nmind will always produce a set of claims that entails that that entity\nhas a mind, no matter what other claims belong to the set!) \n\nTo see how the claim that the Turing Test is merely criterial for\nthe ascription of intelligence differs from the logical behaviorist\nclaim that the Turing Test provides logically sufficient conditions for\nthe ascription of intelligence, it suffices to consider the question of\nwhether it is nomically possible for there to be a “hand\nsimulation” of a Turing Test program. Many people have supposed\nthat there is good reason to deny that Blockhead is a nomic (or\nphysical) possibility. For example, in The Physics of\nImmortality, Frank Tipler provides the following argument in\ndefence of the claim that it is physically impossible to “hand\nsimulate” a Turing-Test-passing program: \n\nWhile there might be ways in which the details of Tipler’s argument\ncould be improved, the general point seems clearly right: the kind of\ncombinatorial explosion that is required for a look-up tree for a\nhuman being is ruled out by the laws and boundary conditions that\ngovern the operations of the physical world. But, if this is right,\nthen, while it may be true that Blockhead is a logical\npossibility, it follows that Blockhead is not a nomic or\nphysical possibility. And then it seems natural to hold that\nThe Turing Test does indeed provide nomically sufficient\nconditions for the attribution of intelligence: given everything else\nthat we already know—or, at any rate, take ourselves to\nknow—about the universe in which we live, we would be fully\njustified in concluding that anything that succeeds in passing The\nTuring Test is, indeed, intelligent (possessed of a mind, and so\nforth). \n\nThere are ways in which the argument in the previous paragraph might\nbe resisted. At the very least, it is worth noting that there is a\nserious gap in the argument that we have just rehearsed. Even if we can\nrule out “hand simulation” of intelligence, it does not\nfollow that we have ruled out all other kinds of mere simulation of\nintelligence. Perhaps—for all that has been argued so far—there\nare nomically possible ways of producing mere simulations of\nintelligence. But, if that’s right, then passing The Turing Test need\nnot be so much as criterial for the possession of intelligence: it need\nnot be that given everything else that we already know—or, at any\nrate, take ourselves to know—about the universe in which we live, we\nwould be fully justified in concluding that anything that succeeds in\npassing The Turing Test is, indeed, intelligent (possessed of a mind,\nand so forth). \n\n(McDermott (2014) calculates that a look-up table for a participant\nwho makes 50 conversational exchanges would have about\n1022278 nodes. It is tempting to take this calculation to\nestablish that it is neither nomically nor physically possible for\nthere to be a “hand simulation” of a Turing Test program, on the\ngrounds that the required number of nodes could not be fitted into a\nspace much much larger than the entire observable universe.)  \n\nWhen we look at the initial formulation that Turing provides of his\ntest, it is clear that he thought that the passing of the test would\nprovide probabilistic support for the hypothesis of intelligence. There\nare at least two different points to make here. First, the\nprediction that Turing makes is itself probabilistic: Turing\npredicts that, in about fifty years from the time of his writing, it\nwill be possible to programme digital computers to make them play the\nimitation game so well that an average interrogator will have no more\nthan a seventy per cent chance of making the right identification after\nfive minutes of questioning. Second, the probabilistic nature of\nTuring’s prediction provides good reason to think that the\ntest that Turing proposes is itself of a probabilistic nature:\na given level of success in the imitation game produces—or, at any\nrate, should produce—a specifiable level of increase in confidence\nthat the participant in question is intelligent (has thoughts, is\npossessed of a mind). Since Turing doesn’t tell us how he supposes that\nlevels of success in the imitation game correlate with increases in\nconfidence that the participant in question is intelligent, there is a\nsense in which The Turing Test is greatly underspecified. Relevant\nvariables clearly include: the length of the period of time over which\nthe questioning in the game takes place (or, at any rate, the\n“amount” of questioning that takes place); the skills and\nexpertise of the interrogator (this bears, for example, on the\n“depth” and “difficulty” of the questioning\nthat takes place); the skills and expertise of the third player in the\ngame; and the number of independent sessions of the game that are run\n(particularly when the other participants in the game differ from one\nrun to the next). Clearly, a machine that is very successful in many\ndifferent runs of the game that last for quite extended periods of time\nand that involve highly skilled participants in the other roles has a\nmuch stronger claim to intelligence than a machine that has been\nsuccessful in a single, short run of the game with highly inexpert\nparticipants. That a machine has succeeded in one short run of the game\nagainst inexpert opponents might provide some reason for increase in\nconfidence that the machine in question is intelligent: but it is clear\nthat results on subsequent runs of the game could quickly overturn this\ninitial increase in confidence. That a machine has done much better\nthan chance over many long runs of the imitation game against a variety\nof skilled participants surely provides much stronger evidence that the\nmachine is intelligent. (Given enough evidence of this kind, it seems\nthat one could be quite confident indeed that the machine is\nintelligent, while still—of course—recognizing that one’s\njudgment could be overturned by further evidence, such as a series of\nshort runs in which it does much worse than chance against participants\nwho use the same strategy over and over to expose the machine as a\nmachine.)  \n\nThe probabilistic nature of The Turing Test is often overlooked.  True\nenough, Moor (1976, 2001)—along with various other\ncommentators—has noted that The Turing Test is\n“inductive,” i.e. that “The Turing Test”\nprovides no more than defeasible evidence of intelligence. However, it\nis one thing to say that success in “a rigorous Turing\ntest” provides no more than defeasible evidence of intelligence;\nit is quite another to note the probabilistic features to which we\nhave drawn attention in the preceding paragraph.  Consider, for\nexample, Moor’s observation (Moor 2001:83) that “…\ninductive evidence gathered in a Turing test can be outweighed by new\nevidence. … If new evidence shows that a machine passed the\nTuring Test by remote control run by a human behind the scenes, then\nreassessment is called for.” This—and other similar\npassages—seems to us to suggest that Moor supposes that a\n“rigorous Turing test” is a one-off event in which the\nmachine either succeeds or fails. But this interpretation of The\nTuring Test is vulnerable to the kind of objection lodged by\nBringsjord (1994): even on a moderately long single run with\nrelatively expert participants, it may not be all that unlikely that\nan unintelligent machine serendipitously succeeds in the imitation\ngame. In our view, given enough sufficiently long runs with different\nsufficiently expert participants, the likelihood of serendipitous\nsuccess can be made as small as one wishes. Thus, while Bringsjord’s\n“argument from serendipity” has force against some\nversions of The Turing Test, it has no force against the most\nplausible interpretation of the test that Turing actually\nproposed. \n\nIt is worth noting that it is quite easy to construct more\nsophisticated versions of “The Imitation Game” that yield\nmore fine-grained statistical data. For example, rather than getting\nthe judges to issue Yes/No verdicts about both of the participants in\nthe game, one could get the judges to provide probabilistic answers.\n(“I give a 75% probability to the claim that A is the machine, and only\n25% probability to the claim that B is the machine.”) This point is\nimportant when one comes to consider criticisms of the\n“methodology” implicit in “The Turing\nTest”. (For further discussion of the probabilistic nature of “The Turing\nTest”, see Shieber (2007).)  \n\nSome of the literature about The Turing Test is concerned with\nquestions about the framing of a test that can provide a suitable guide\nto future research in the area of Artificial Intelligence. The idea\nhere is very simple. Suppose that we have the ambition to produce an\nartificially intelligent entity. What tests should we take as setting\nthe goals that putatively intelligent artificial systems should\nachieve? Should we suppose that The Turing Test provides an appropriate\ngoal for research in this field? In assessing these proposals, there\nare two different questions that need to be borne in mind. First, there\nis the question whether it is a useful goal for AI research to aim to\nmake a machine that can pass the given test (administered over the\nspecified length of time, at the specified degree of success). Second,\nthere is the question of the appropriate conclusion to draw about the\nmental capacities of a machine that does manage to pass the test\n(administered over the specified length of time, at the specified\ndegree of success).  \n\nOpinion on these questions is deeply divided. Some people suppose\nthat The Turing Test does not provide a useful goal for research in AI\nbecause it is far too difficult to produce a system that can pass the\ntest. Other people suppose that The Turing Test does not provide a\nuseful goal for research in AI because it sets a very narrow target\n(and thus sets unnecessary restrictions on the kind of research that\ngets done). Some people think that The Turing Test provides an entirely\nappropriate goal for research in AI; while other people think that\nthere is a sense in which The Turing Test is not really demanding\nenough, and who suppose that The Turing Test needs to be extended in\nvarious ways in order to provide an appropriate goal for AI. We shall\nconsider some representatives of each of these positions in turn. \n\nSome people have claimed that The Turing Test doesn’t set an\nappropriate goal for current research in AI because we are plainly so\nfar away from attaining this goal. Amongst these people there are some\nwho have gone on to offer reasons for thinking that it is doubtful\nthat we shall ever be able to create a machine that can pass The\nTuring Test—or, at any rate, that it is doubtful that we shall\nbe able to do this at any time in the foreseeable future. Perhaps the\nmost interesting arguments of this kind are due to French (1990); at\nany rate, these are the arguments that we shall go on to\nconsider. (Cullen (2009) sets out similar considerations.)  \n\nAccording to French, The Turing Test is “virtually\nuseless” as a real test of intelligence, because nothing without\na “human subcognitive substrate” could pass the test, and\nyet the development of an artificial “human cognitive\nsubstrate” is almost impossibly difficult. At the very least,\nthere are straightforward sets of questions that reveal\n“low-level cognitive structure” and that—in French’s\nview—are almost certain to be successful in separating human beings\nfrom machines. \n\nFirst, if interrogators are allowed to draw on the results of\nresearch into, say, associative priming, then there is data\nthat will very plausibly separate human beings from machines. For\nexample, there is research that shows that, if humans are presented\nwith series of strings of letters, they require less time to recognize\nthat a string is a word (in a language that they speak) if it is\npreceded by a related word (in the language that they speak), rather\nthan by an unrelated word (in the language that they speak) or a string\nof letters that is not a word (in the language that they speak).\nProvided that the interrogator has accurate data about average\nrecognition times for subjects who speak the language in question, the\ninterrogator can distinguish between the machine and the human simply\nby looking at recognition times for appropriate series of strings of\nletters. Or so says French. It isn’t clear to us that this is right.\nAfter all, the design of The Turing Test makes it hard to see how the\ninterrogator will get reliable information about response times to\nseries of strings of symbols. The point of putting the computer in a\nseparate room and requiring communication by teletype was precisely to\nrule out certain irrelevant ways of identifying the computer. If these\nrequirements don’t already rule out identification of the computer by\nthe application of tests of associative priming, then the requirements\ncan surely be altered to bring it about that this is the case. (Perhaps\nit is also worth noting that administration of the kind of test that\nFrench imagines is not ordinary conversation; nor is it something that\none would expect that any but a few expert interrogators would happen\nupon. So, even if the circumstances of The Turing Test do not rule out\nthe kind of procedure that French here envisages, it is not clear that\nThe Turing Test will be impossibly hard for machines to pass.) \n\nSecond, at a slightly higher cognitive level, there are certain\nkinds of “ratings games” that French supposes will be very\nreliable discriminators between humans and machines. For instance, the\n“Neologism Ratings Game”—which asks participants to rank\nmade-up words on their appropriateness as names for given kinds of\nentities—and the “Category Rating Game”—which asks\nparticipants to rate things of one category as things of another\ncategory—are both, according to French, likely to prove highly\nreliable in discriminating between humans and machines. For, in the\nfirst case, the ratings that humans make depend upon large numbers of\nculturally acquired associations (which it would be well-nigh\nimpossible to identify and describe, and hence which it would\n(arguably) be well-nigh impossible to program into a computer). And, in\nthe second case, the ratings that people actually make are highly\ndependent upon particular social and cultural settings (and upon the\nparticular ways in which human life is experienced). To take French’s\nexamples, there would be widespread agreement amongst competent English\nspeakers in the technologically developed Western world that\n“Flugblogs” is not an appropriate name for a breakfast\ncereal, while “Flugly” is an appropriate name for a child’s\nteddy bear. And there would also be widespread agreement amongst\ncompetent speakers of English in the developed world that pens rate\nhigher as weapons than grand pianos rate as wheelbarrows. Again, there\nare questions that can be raised about French’s argument here. It is\nnot clear to us that the data upon which the ratings games rely is as\nreliable as French would have us suppose. (At least one of us thinks\nthat “Flugly” would be an entirely inappropriate name for a\nchild’s teddy bear, a response that is due to the similarity between\nthe made-up word “Flugly” and the word “Fugly,”\nthat had some currency in the primarily undergraduate University\ncollege that we both attended. At least one of us also thinks that\nyoung children would very likely be delighted to eat a cereal called\n“Flugblogs,” and that a good answer to the question about\nratings pens and grand pianos is that it all depends upon the pens and\ngrand pianos in question. What if the grand piano has wheels? What if\nthe opponent has a sword or a sub-machine gun? It isn’t obvious that a\nrefusal to play this kind of ratings game would necessarily be a\ngive-away that one is a machine.) Moreover, even if the data is\nreliable, it is not obvious that any but a select group of\ninterrogators will hit upon this kind of strategy for trying to unmask\nthe machine; nor is it obvious that it is impossibly hard to build a\nmachine that is able to perform in the way in which typical humans do\non these kinds of tests. In particular, if—as Turing assumes—it\nis possible to make learning machines that can be “trained\nup” to learn how to do various kinds of tasks, then it is quite\nunclear why these machines couldn’t acquire just the same kinds of\n“subcognitive competencies” that human children acquire\nwhen they are “trained up” in the use of language. \n\nThere are other reasons that have been given for thinking that The\nTuring Test is too hard (and, for this reason, inappropriate in setting\ngoals for current research into artificial intelligence). In general,\nthe idea is that there may well be features of human cognition that are\nparticularly hard to simulate, but that are not in any sense essential\nfor intelligence (or thought, or possession of a mind). The problem\nhere is not merely that The Turing Test really does test for\nhuman intelligence; rather, the problem here is the fact—if\nindeed it is a fact—that there are quite inessential features of\nhuman intelligence that are extraordinarily difficult to replicate in a\nmachine. If this complaint is justified—if, indeed, there are\nfeatures of human intelligence that are extraordinarily difficult to\nreplicate in machines, and that could and would be reliably\nused to unmask machines in runs of The Turing Test—then there is\nreason to worry about the idea that The Turing Test sets an appropriate\ndirection for research in artificial intelligence. However, as our\ndiscussion of French shows, there may be reason for caution in\nsupposing that the kinds of considerations discussed in the present\nsection show that we are already in a position to say that The Turing\nTest does indeed set inappropriate goals for research in artificial\nintelligence. \n\nThere are authors who have suggested that The Turing Test does not set\na sufficiently broad goal for research in the area of artificial\nintelligence. Amongst these authors, there are many who suppose that\nThe Turing Test is too easy. (We go on to consider some of these\nauthors in the next sub-section.) But there are also some authors who\nhave supposed that, even if the goal that is set by The Turing Test is\nvery demanding indeed, it is nonetheless too restrictive.  \n\nObjection to the notion that the Turing Test provides a logically\nsufficient condition for intelligence can be adapted to the goal of\nshowing that the Turing Test is too restrictive. Consider, for\nexample, Gunderson (1964). Gunderson has two major complaints to make\nagainst The Turing Test. First, he thinks that success in Turing’s\nImitation Game might come for reasons other than the possession of\nintelligence.  But, second, he thinks that success in the Imitation\nGame would be but one example of the kinds of things that intelligent\nbeings can do and—hence—in itself could not be taken as a\nreliable indicator of intelligence. By way of analogy, Gunderson\noffers the case of a vacuum cleaner salesman who claims that his\nproduct is “all-purpose” when, in fact, all it does is to\nsuck up dust. According to Gunderson, Turing is in the same position\nas the vacuum cleaner salesman if he is prepared to say that\na machine is intelligent merely on the basis of its success in the\nImitation Game. Just as “all purpose” entails the ability\nto do a range of things, so, too, “thinking” entails the\npossession of a range of abilities (beyond the mere ability to succeed\nin the Imitation Game). \n\nThere is an obvious reply to the argument that we have here attributed\nto Gunderson, viz. that a machine that is capable of success in the\nImitation Game is capable of doing a large range of different kinds of\nthings. In order to carry out a conversation, one needs to have many\ndifferent kinds of cognitive skills, each of which is capable of\napplication in other areas. Apart from the obvious general cognitive\ncompetencies—memory, perception, etc.—there are many\nparticular competencies—rudimentary arithmetic abilities,\nunderstanding of the rules of games, rudimentary understanding of\nnational politics, etc.—which are tested in the course of repeated\nruns of the Imitation Game.  It is inconceivable that that there be a\nmachine that is startlingly good at playing the Imitation Game, and\nyet unable to do well at any other tasks that might be\nassigned to it; and it is equally inconceivable that there is a\nmachine that is startlingly good at the Imitation Game and yet that\ndoes not have a wide range of competencies that can be displayed in a\nrange of quite disparate areas.  To the extent that Gunderson\nconsiders this line of reply, all that he says is that there is no\nreason to think that a machine that can succeed in the Imitation Game\nmust have more than a narrow range of abilities; we think\nthat there is no reason to believe that this reply should be taken\nseriously. \n\nMore recently, Erion (2001) has defended a position that has some\naffinity to that of Gunderson. According to Erion, machines might be\n“capable of outperforming human beings in limited tasks in\nspecific environments, [and yet] still be unable to act skillfully in\nthe diverse range of situations that a person with common sense\ncan” (36). On one way of understanding the claim that Erion\nmakes, he too believes that The Turing Test only identifies one amongst\na range of independent competencies that are possessed by intelligent\nhuman beings, and it is for this reason that he proposes a more\ncomprehensive “Cartesian Test” that “involves a more\ncareful examination of a creature’s language, [and] also tests the\ncreature’s ability to solve problems in a wide variety of everyday\ncircumstances” (37). In our view, at least when The Turing Test\nis properly understood, it is clear that anything that passes The\nTuring Test must have the ability to solve problems in a wide variety\nof everyday circumstances (because the interrogators will use their\nquestions to probe these—and other—kinds of abilities in those\nwho play the Imitation Game). \n\nThere are authors who have suggested that The Turing Test should be\nreplaced with a more demanding test of one kind or another. It is not\nat all clear that any of these tests actually proposes a better goal\nfor research in AI than is set by The Turing Test. However, in this\nsection, we shall not attempt to defend that claim; rather, we shall\nsimply describe some of the further tests that have been proposed, and\nmake occasional comments upon them. (One preliminary point upon which\nwe wish to insist is that Turing’s Imitation Game was devised against\nthe background of the limitations imposed by then current technology.\nIt is, of course, not essential to the game that tele-text devices be\nused to prevent direct access to information about the sex or genus of\nparticipants in the game. We shall not advert to these relatively\nmundane kinds of considerations in what follows.)  \n\nHarnad (1989, 1991) claims that a better test than The Turing Test will\nbe one that requires responses to all of our inputs, and not merely to\ntext-formatted linguistic inputs. That is, according to Harnad, the\nappropriate goal for research in AI has to be to construct a robot with\nsomething like human sensorimotor capabilities. Harnad also considers\nthe suggestion that it might be an appropriate goal for AI to aim for\n“neuromolecular indistinguishability,” but rejects this\nsuggestion on the grounds that once we know how to make a robot that\ncan pass his Total Turing Test, there will be no problems about\nmind-modeling that remain unsolved. It is an interesting question\nwhether the test that Harnad proposes sets a more appropriate goal for\nAI research. In particular, it seems worth noting that it is not clear\nthat there could be a system that was able to pass The Turing Test and\nyet that was not able to pass The Total Turing Test. Since Harnad\nhimself seems to think that it is quite likely that “full robotic\ncapacities [are] … necessary to generate … successful\nlinguistic performance,” it is unclear why there is reason to\nreplace The Turing Test with his extended test. (This point against\nHarnad can be found in Hauser (1993:227), and elsewhere.)  \n\nBringsjord et al. (2001) propose that a more satisfactory aim for AI is\nprovided by a certain kind of meta-test that they call the Lovelace\nTest. They say that an artificial agent A, designed by human\nH, passes the Lovelace Test just in case three conditions are jointly\nsatisfied: (1) the artificial agent A produces output\nO; (2) A’s outputting O is not the result of\na fluke hardware error, but rather the result of processes that\nA can repeat; and (3) H—or someone who knows what\nH knows and who has H’s resources—cannot explain\nhow A produced O by appeal to A’s\narchitecture, knowledge-base and core functions. Against this proposal,\nit seems worth noting that there are questions to be raised about the\ninterpretation of the third condition. If a computer program is long\nand complex, then no human agent can explain in complete\ndetail how the output was produced. (Why did the computer output 3.16\nrather than 3.17?) But if we are allowed to give a highly schematic\nexplanation—the computer took the input, did some internal\nprocessing and then produced an answer—then it seems that it will\nturn out to be very hard to support the claim that human agents ever do\nanything genuinely creative. (After all, we too take external input,\nperform internal processing, and produce outputs.) What is missing from\nthe account that we are considering is any suggestion about the\nappropriate level of explanation that is to be provided. It is\nquite unclear why we should suppose that there is a relevant difference\nbetween people and machines at any level of explanation; but, if that’s\nright, then the test in question is trivial. (One might also worry that\nthe proposed test rules out by fiat the possibility that\ncreativity can be best achieved by using genuine randomising\ndevices.)  \n\nSchweizer (1998) claims that a better test than The Turing Test will\nadvert to the evolutionary history of the subjects of the test. When we\nattribute intelligence to human beings, we rely on an extensive\nhistorical record of the intellectual achievements of human beings. On\nthe basis of this historical record, we are able to claim that human\nbeings are intelligent; and we can rely upon this claim when we\nattribute intelligence to individual human beings on the basis of their\nbehavior. According to Schweizer, if we are to attribute intelligence\nto machines, we need to be able to advert to a comparable historical\nrecord of cognitive achievements. So, it will only be when machines\nhave developed languages, written scientific treatises, composed\nsymphonies, invented games, and the like, that we shall be in a\nposition to attribute intelligence to individual machines on the basis\nof their behavior. Of course, we can still use The Turing Test to\ndetermine whether an individual machine is intelligent: but our answer\nto the question won’t depend merely upon whether or not the machine is\nsuccessful in The Turing Test; there is the further\n“evolutionary” condition that also must be satisfied.\nAgainst Schweizer, it seems worth noting that it is not at all clear\nthat our reason for granting intelligence to other humans on the basis\nof their behavior is that we have prior knowledge of the collective\ncognitive achievements of human beings.  \n\nPerhaps the best known attack on the suggestion that The Turing Test\nprovides an appropriate research goal for AI is due to Hayes and Ford\n(1995). Among the controversial claims that Hayes and Ford make, there\nare at least the following:  \n\nSome of these claims seem straightforwardly incorrect. Consider (h),\nfor example. In what sense can it be claimed that 50% of the human\npopulation would fail “the species test”? If “the\nspecies test” requires the interrogator to decide which of two\npeople is a machine, why should it be thought that the verdict of the\ninterrogator has any consequences for the assessment of the\nintelligence of the person who is judged to be a machine? (Remember,\ntoo, that one of the conditions for “the species test”—as\nit is originally described by Hayes and Ford—is that one of the\ncontestants is a machine. While the machine can\n“demonstrate” its intelligence by winning the imitation\ngame, a person cannot “demonstrate” their lack of\nintelligence by failing to win.) \n\nIt seems wrong to say that The Turing Test is defective because it\nis a “null effect experiment”. True enough, there is a\nsense in which The Turing Test does look for a “null\nresult”: if ordinary judges in the specified circumstances fail\nto identify the machine (at a given level of success), then there is a\ngiven likelihood that the machine is intelligent. But the point of\ninsisting on “ordinary judges” in the specified\ncircumstances is precisely to rule out irrelevant ways of identifying\nthe machine (i.e. ways of identifying the machine that are not relevant\nto the question whether it is intelligent). There might be all kinds of\nirrelevant differences between a given kind of machine and a human\nbeing—not all of them rendered undetectable by the experimental\nset-up that Turing describes—but The Turing Test will remain a good\ntest provided that it is able to ignore these irrelevant\ndifferences. \n\nIt also seems doubtful that it is a serious failing of The Turing Test\nthat it can only test for “complete success”. On the one\nhand, if a man has a one in ten chance of producing a claim that is\nplainly not feminine, then we can compute the chance that he will be\ndiscovered in a game in which he answers N\nquestions—and, if N is sufficiently small, then it\nwon’t turn out that “he would almost always fail to\nwin”. On the other hand, as we noted at the end of Section 4.4\nabove, if one were worried about the “YES/NO” nature of\n“The Turing Test”, then one could always get the judges to\nproduce probabilistic verdicts instead. This change preserves the\ncharacter of The Turing Test, but gives it scope for greater\nstatistical sophistication. \n\nWhile there are (many) other criticisms that can be made of the\nclaims defended by Hayes and Ford (1995), it should be acknowledged\nthat they are right to worry about the suggestion that The Turing Test\nprovides the defining goal for research in AI. There are various\nreasons why one should be loathe to accept the proposition that the one\ncentral ambition of AI research is to produce artificial people.\nHowever it is worth pointing out that there is no reason to think that\nTuring supposed that The Turing Test defined the field of AI research\n(and there is not much evidence that any other serious thinkers have\nthought so either). Turing himself was well aware that there might be\nnon-human forms of intelligence—cf. (j) above. However, all of this\nremains consistent with the suggestion that it is quite appropriate to\nsuppose that The Turing Test sets one long term goal for AI\nresearch: one thing that we might well aim to do eventually is\nto produce artificial people. If—as Hayes and Ford claim—that\ntask is almost impossibly difficult, then there is no harm in supposing\nthat the goal is merely an ambit goal to which few resources\nshould be committed; but we might still have good reason to allow that\nit is a goal. \n\nThere are many different objections to The Turing Test which have\nsurfaced in the literature during the past fifty years, but which we\nhave not yet discussed. We cannot hope to canvass all of these\nobjections here. However, there is one argument—Searle’s\n“Chinese Room” argument—that is mentioned so often in\nconnection with the Turing Test that we feel obliged to end with some\ndiscussion of it.  \n\nIn Minds, Brains and Programs and elsewhere, John Searle\nargues against the claim that “appropriately programmed computers\nliterally have cognitive states” (64). Clearly enough, Searle is\nhere disagreeing with Turing’s claim that an appropriately programmed\ncomputer could think. There is much that is controversial about\nSearle’s argument; we shall just consider one way of\nunderstanding what it is that he is arguing for. \n\nThe basic structure of Searle’s argument is very well known. We can\nimagine a “hand simulation” of an intelligent agent—in\nthe case described, a speaker of a Chinese language—in circumstances\nin which we might well be very reluctant to allow that there is any\nappropriate intelligence lying behind the simulated behavior. (Thus,\nwhat we are invited to suppose is a logical possibility is not so very\ndifferent from what Block invites us to suppose is a logical\npossibility. However, the argument that Searle goes on to develop is\nrather different from the argument that Block defends.)\nMoreover—and this is really the key point for Searle’s\nargument—the “hand simulation” in question is, in all\nrelevant respects, simply a special kind of digital computation. So,\nthere is a possible world—doubtless one quite remote from the actual\nworld—in which a digital computer simulates intelligence but in\nwhich the digital computer does not itself possess intelligence. But,\nif we consider any digital computer in the actual world, it will not\ndiffer from the computer in that remote possible world in any way which\ncould make it the case that the computer in the actual world is more\nintelligent than the computer in that remote possible world. Given that\nwe agree that the “hand simulating” computer in the Chinese\nRoom is not intelligent, we have no option but to conclude that digital\ncomputers are simply not the kinds of things that can be\nintelligent. \n\nSo far, the argument that we have described arrives at the\nconclusion that no appropriately programmed computer can think. While\nthis conclusion is not one that Turing accepted, it is important to\nnote that it is compatible with the claim that The Turing Test is a\ngood test for intelligence. This is because, for all that has been\nargued, it may be that it is not nomically possible to provide\nany “hand simulation” of intelligence (and, in particular,\nthat it is not possible to simulate intelligence using any kind of\ncomputer). In order to turn Searle’s argument—at least in the way in\nwhich we have developed it—into an objection to The Turing Test, we\nneed to have some reason for thinking that it is at least\nnomically possible to simulate intelligence using computers.\n(If it is nomically impossible to simulate intelligence using\ncomputers, then the alleged fact that digital computers cannot\ngenuinely possess intelligence casts no doubt at all on the usefulness\nof the Turing Test, since digital computers are nomically disqualified\nfrom the range of cases in which there is mere simulation of\nintelligence.) In the absence of reason to believe this, the most that\nSearle’s argument yields is an objection to Turing’s confidently held\nbelief that digital computing machines will one day pass The Turing\nTest. (Here, as elsewhere, we are supposing that, for any kind of\ncreature C, there is a version of The Turing Test in which C takes the\nrole of the machine in the specific test that Turing describes. This\ngeneral format for testing for the presence of intelligence would not\nnecessarily be undermined by the success of Searle’s Chinese Room\nargument.) \n\nThere are various responses that might be made to the argument that\nwe have attributed to Searle. One kind of response is to dispute the\nclaim that there is no intelligence present in the case of the Chinese\nRoom. (Suppose that the “hand simulation” is embedded in a\nrobot that is equipped with appropriate sensors, etc. Suppose, further,\nthat the “hand simulation” involves updating the process of\n“hand simulation,” etc. If enough details of this kind are\nadded, then it becomes quite unclear whether we do want to say that we\nstill haven’t described an intelligent system.) Another kind of\nresponse is to dispute the claim that digital computers in the actual\nworld could not be relevantly different from the system that operates\nin the Chinese Room in that remote possible world. (If we suppose that\nthe core of the Chinese Room is a kind of giant look-up table, then it\nmay well be important to note that digital computers in the actual\nworld do not work with look-up tables in that kind of way.) Doubtless\nthere are other possible lines of response as well. However, it would\ntake us out of our way to try to take this discussion further. (One\ngood place to look for further discussion of these matters is\nBraddon-Mitchell and Jackson (1996).) \n\nThere are radically different views about the measurement of\nintelligence that have not been canvassed in this article. Our concern\nhas been to discuss Turing (1950) and its legacy. But, of course, a\nmore wide-ranging discussion would also consider, for example,\nresearch on the measurement of intelligence using the mathematical and\ncomputational resources of Algorithmic Information Theory, Kolmogorov\nComplexity Theory, Minimum Message Length (MML) Theory, and so forth. (For\nan introduction to this literature, see Hernandez-Orallo and Dowe\n(2010), and the list of references contained therein. For a more general introduction to research into AI, see Marquis et al. (2020).) ","contact.mail":"David.Dowe@monash.edu","contact.domain":"monash.edu"}]
