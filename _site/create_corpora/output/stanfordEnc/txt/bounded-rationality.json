[{"date.published":"2018-11-30","url":"https://plato.stanford.edu/entries/bounded-rationality/","author1":"Gregory Wheeler","author1.info":"http://gregorywheeler.org","entry":"bounded-rationality","body.text":"\n\n\nHerbert Simon introduced the term ‘bounded rationality’\n(Simon 1957b: 198; see also Klaes & Sent 2005) as a shorthand for\nhis brief against neoclassical economics and his call to replace the\nperfect rationality assumptions of homo economicus with a\nconception of rationality tailored to cognitively limited agents.\n\n\n\n\nBroadly stated, the task is to replace the global rationality of\neconomic man with the kind of rational behavior that is compatible\nwith the access to information and the computational capacities that\nare actually possessed by organisms, including man, in the kinds of\nenvironments in which such organisms exist. (Simon 1955a: 99)\n\n\n\n‘Bounded rationality’ has since come to refer to a wide\nrange of descriptive, normative, and prescriptive accounts of\neffective behavior which depart from the assumptions of perfect\nrationality. This entry aims to highlight key contributions—from\nthe decision sciences, economics, cognitive- and neuropsychology,\nbiology, computer science, and philosophy—to our current\nunderstanding of bounded rationality.\n\nBounded rationality has come to broadly encompass models of effective\nbehavior that weaken, or reject altogether, the idealized conditions\nof perfect rationality assumed by models of economic man. In this\nsection we state what models of economic man are committed to and\ntheir relationship to expected utility theory. In later sections we\nreview proposals for departing from expected utility theory. \nThe perfect rationality of homo economicus imagines a\nhypothetical agent who has complete information about the options\navailable for choice, perfect foresight of the consequences from\nchoosing those options, and the wherewithal to solve an optimization\nproblem (typically of considerable complexity) that identifies an\noption which maximizes the agent’s personal utility. The meaning\nof ‘economic man’ has evolved from John Stuart\nMill’s description of a hypothetical, self-interested individual\nwho seeks to maximize his personal utility (1844); to Jevon’s\nmathematization of marginal utility to model an economic consumer\n(1871); to Frank Knight’s portrayal of the slot-machine\nman of neo-classical economics (1921), which is Jevon’s\ncalculator man augmented with perfect foresight and\ndeterminately specified risk; to the modern conception of an\neconomically rational economic agent conceived in terms of Paul\nSamuelson’s revealed preference formulation of utility\n(1947) which, together with von Neumann and Morgenstern’s\naxiomatization (1944), changed the focus of economic modeling from\nreasoning behavior to choice behavior. \nModern economic theory begins with the observation that human beings\nlike some consequences better than others, even if they only assess\nthose consequences hypothetically. A perfectly rational person,\naccording to the canonical paradigm of synchronic decision making\nunder risk, is one whose comparative assessments of a set of\nconsequences satisfies the recommendation to maximize expected\nutility. Yet, this recommendation to maximize expected utility\npresupposes that qualitative comparative judgments of those\nconsequences (i.e., preferences) are structured in such a way (i.e.,\nsatisfy specific axioms) so as to admit a mathematical representation\nthat places those objects of comparison on the real number line (i.e.,\nas inequalities of mathematical expectations), ordered from worst to\nbest. This structuring of preference through axioms to admit a\nnumerical representation is the subject of expected utility\ntheory. \nWe present here one such axiom system to derive expected utility\ntheory, a simple set of axioms for the binary relation \\(\\succeq\\),\nwhich represents the relation “is weakly preferred to”.\nThe objects of comparison for this axiomatization are\nprospects, which associate probabilities to a fixed set of\nconsequences, where both probabilities and consequences are known to\nthe agent. To illustrate, the prospect (−€10, ½;\n€20, ½) concerns two consequences, losing 10\nEuros and winning 20 Euros, each assigned the\nprobability one-half. A rational agent will prefer this prospect to\nanother with the same consequences but greater chance of losing than\nwinning, such as (\\(-\\)€10, ⅔; €20, ⅓), assuming\nhis aim is to maximize his financial welfare. More generally, suppose\nthat \\(X = \\{x_1, x_2, \\ldots, x_n\\}\\) is a mutually exclusive and\nexhaustive set of consequences and that \\(p_i\\) denotes the\nprobability of \\(x_i\\), where each \\(p_i \\geq 0\\) and \\(\\sum_{i}^{n}\np_i = 1\\). A prospect P is simply the set of\nconsequence-probability pairs, \\(P = (x_1, p_1; \\ x_2, p_2; \\ldots; \\\nx_n, p_n)\\). By convention, a prospect’s consequence-probability\npairs are ordered by the value of each consequence, from least\nfavorable to most. When prospects P, Q, R are comparable under a\nspecific preference relation, \\(\\succeq\\), and the (ordered) set of\nconsequences X is fixed, then prospects may be simply\nrepresented by a vector of probabilities. \nThe expected utility hypothesis Bernoulli (1738) states that\nrational agents ought to maximize expected utility. If your\nqualitative preferences \\(\\succeq\\) over prospects satisfy the\nfollowing three constraints, ordering, continuity,\nand independence, then your preferences will maximize\nexpected utility (Neumann & Morgenstern 1944). \nSpecifically, if A1, A2, and A3 hold, then there is a real-valued\nfunction \\(V(\\cdot)\\) of the form \nwhere P is any prospect and \\(u(\\cdot)\\) is a von Neumann and\nMorgenstern utility function defined on the set of consequences\nX, such that \\(P \\succeq Q\\) if and only if \\(V(P) \\geq V(Q)\\).\nIn other words, if your qualitative comparative judgments of prospects\nat a given time satisfy A1, A2, and A3, then those qualitative\njudgments are representable numerically by inequalities of functions\nof the form \\(V(\\cdot)\\), yielding a logical calculus on an interval\nscale for determining the consequences of your qualitative comparative\njudgments at that time. \nIt is commonplace to explore alternatives to an axiomatic system and\nexpected utility theory is no exception. To be clear, not all\ndepartures from expected utility theory are candidates for modeling\nbounded rationality. Nevertheless, some confusion and misguided\nrhetoric over how to approach the problem of modeling bounded\nrationality stems from unfamiliarity with the breadth of contemporary\nstatistical decision theory. Here we highlight some axiomatic\ndepartures from expected utility theory that are motivated by bounded\nrationality considerations, all framed in terms of our particular\naxiomatization from\n section 1.1. \nWeakening the ordering axiom introduces the possibility for an agent\nto forgo comparing a pair of alternatives, an idea both Keynes and\nKnight advocated (Keynes 1921; Knight 1921). Specifically, dropping\nthe completeness axiom allows an agent to be in a position to neither\nprefer one option to another nor be indifferent between the two\n(Koopman 1940; Aumann 1962; Fishburn 1982). Decisiveness, which the\ncompleteness axiom encodes, is more mathematical convenience than\nprinciple of rationality. The question, which is the question that\nevery proposed axiomatic system faces, is what logically follows from\na system which allows for incomplete preferences. Led by Aumann\n(1962), early axiomatizations of rational incomplete preferences were\nsuggested by Giles (1976) and Giron & Rios (1980), and later\nstudied by Karni (1985), Bewley (2002), Walley (1991), Seidenfeld,\nSchervish, & Kadane (1995), Ok (2002), Nau (2006),\nGalaabaatar & Karni (2013) and Zaffalon & Miranda (2017). In\naddition to accommodating indecision, such systems also allow for you\nto reason about someone else’s (possibly) complete preferences\nwhen your information about that other agent’s preferences is\nincomplete. \nDropping transitivity limits extendability of elicited preferences\n(Luce & Raiffa 1957), since the omission of transitivity as an\naxiomatic constraint allows for cycles and preference reversals.\nAlthough violations of transitivity have been long considered both\ncommonplace and a sign of human irrationality (May 1954; Tversky\n1969), reassessments of the experimental evidence challenge this\nreceived view (Mongin 2000; Regenwetter, Dana, & Davis-Stober\n2011). The axioms impose synchronic consistency constraints on\npreferences, whereas the experimental evidence for violations of\ntransitivity commonly conflate dynamic and synchronic consistency\n(Regenwetter et al. 2011). Specifically, a person’s preferences\nat one moment in time that are inconsistent with his preferences at\nanother time is no evidence for that person holding logically\ninconsistent preferences at a single moment in time. Arguments to\nlimit the scope of transitivity in normative accounts of rational\npreference similarly point to diachronic or group preferences, which\nlikewise do not contradict the axioms (Kyburg 1978; Anand 1987;\nBar-Hillel & Margalit 1988; Schick 1986). Arguments that point to\npsychological processes or algorithms that admit cycles or reversals\nof preference over time also point to a misapplication of, rather than\na counter-example to, the ordering condition. Finally, for decisions\nthat involve explicit comparisons of options over time, violating\ntransitivity may be rational. For example, given the goal of\nmaximizing the rate of food gain, an organism’s current food\noptions may reveal information about food availability in the near\nfuture by indicating that a current option may soon disappear or that\na better option may soon reappear. Information about availability of\noptions over time can, and sometimes does, warrant non-transitive\nchoice behavior over time that maximizes food gain\n(McNamara, Trimmer, & Houston 2014). \nDropping the Archimedean axiom allows for an agent to have\nlexicographic preferences (Blume, Brandenburger, & Dekel\n1991); that is, the omission of\n A2\n allows the possibility for an agent to prefer one option infinitely\nmore than another. One motivation for developing a non-Archimedean\nversion of expected utility theory is to address a gap in the\nfoundations of the standard subjective utility framework that prevents\na full reconciliation of admissibility (i.e., the principle\nthat one ought not select a weakly dominated option for choice) with\nfull conditional preferences (i.e., that for any event, there\nis a well-defined conditional probability to represent the\nagent’s conditional preferences; Pedersen 2014). Specifically,\nthe standard subjective expected utility account cannot accommodate\nconditioning on zero-probability events, which is of particular\nimportance to game theory (P. Hammond 1994). Non-Archimedean variants\nof expected utility theory turn to techniques from nonstandard\nanalysis (Goldblatt 1998), full conditional probabilities\n(Rényi 1955; Coletii & Scozzafava 2002; Dubins 1975; Popper\n1959), and lexicographic probabilities (Halpern 2010; Brickhill &\nHorsten 2016\n [Other Internet Resources]),\n and are all linked to imprecise probability theory. \nNon-compensatory single-cue decision models, such as the Take-the-Best\nheuristic\n (section 7.2),\n appeal to lexicographically ordered cues, and admit a numerical\nrepresentation in terms of non-Archimedean expectations\n(Arló-Costa & Pedersen 2011). \n\n A1\n and\n A2\n together entail that \\(V(\\cdot)\\) assigns a real-valued index to\nprospects such that \\(P \\succeq Q\\) if and only if \\(V(P) \\geq V(Q)\\).\nThe independence axiom,\n A3,\n encodes a separability property for choice, one that ensures that\nexpected utilities are linear in probabilities. Motivations for\ndropping the independence axiom stem from difficulties in applying\nexpected utility theory to describe choice behavior, including an\nearly observation that humans evaluate possible losses and possible\ngains differently. Although expected utility theory can represent a\nperson who either gambles or purchases insurance, Friedman and Savage\nremarked in their early critique of von Neumann and Morgenstern’s\naxiomatization, it cannot simultaneously do both (M. Friedman &\nSavage 1948). \nThe principle of loss aversion (Kahneman & Tversky 1979; Rabin\n2000) suggests that the subjective weight that we assign to potential\nlosses is larger than those we assign to potential gains. For example,\nthe endowment effect (Thaler 1980)—the observation that\npeople tend to view the value of a good higher when viewed as a\npotential loss than when viewed as a potential gain—is supported\nby neurological evidence for gains and losses being processed by\ndifferent regions of the brain (Rick 2011). However, even granting the\naffective differences in how we process losses and gains, those\ndifferences do not necessarily translate to a general\n“negativity bias” (Baumeister, Bratslavsky, &\nFinkenauer 2001) in choice behavior (Hochman & Yechiam 2011;\nYechiam & Hochman 2014). Yechiam and colleagues report experiments\nin which participants do not exhibit loss aversion in their choices,\nsuch as cases in which participants respond to repetitive situations\nthat issue losses and gains and single-case decisions involving small\nstakes. That said, observations of risk aversion (Allais 1953) and\nambiguity aversion (Ellsberg 1961) have led to alternatives to\nexpected utility theory, all of which abandon\n A3.\n Those alternative approaches include prospect theory\n (section 2.4),\n regret theory (Bell 1982; Loomes & Sugden 1982), and\nrank-dependent expected utility (Quiggin 1982). \nMost models of bounded rationality do not even fit into this broad\naxiomatic family just outlined. One reason is that bounded rationality\nhas historically emphasized the procedures, algorithms, or\npsychological processes involved in making a decision, rendering a\njudgment, or securing a goal\n (section 2).\n Samuelson’s shift from reasoning behavior to choice behavior\nabstracted away precisely these details, however, treating them as\noutside the scope of rational choice theory. For Simon, that was\nprecisely the problem. A second reason is that bounded rationality\noften focuses on adaptive behavior suited to an organism’s\nenvironment\n (section 3).\n Since ecological modeling involves goal-directed behavior mitigated\nby the constitution of the organism and stable features of its\nenvironment, focusing on (synchronically) coherent comparative\njudgments is often not, directly at least, the best way to frame the\nproblem. \nThat said, one should be cautious about generalizations sometimes made\nabout the limited role of decision theoretic tools in the study of\nbounded rationality. Decision theory—broadly construed to\ninclude statistical decision theory (Berger 1980)—offers a\npowerful mathematical toolbox even though historically, particularly\nin its canonical form, it has traded in psychological myths such as\n“degrees of belief“ and logical omniscience\n (section 1.3).\n One benefit of studying axiomatic departures from expected utility\ntheory is to loosen the grip of Bayesian dogma to expand the range of\npossibilities for applying a growing body of practical and powerful\nmathematical methods. \nMost formal models of judgment and decision making entail logical\nomniscience—complete knowledge of all that logically\nfollows from one’s current commitments combined with any set of\noptions considered for choice—which is as psychologically\nunrealistic as it is difficult, technically, to avoid (Stalnaker\n1991). A descriptive theory that presumes or a prescriptive theory\nthat recommends to disbelieve a claim when the evidence is logically\ninconsistent, for example, will be unworkable when the belief in\nquestion is sufficiently complicated for all but logically omniscient\nagents, even for non-omniscient agents that nevertheless have access\nto unlimited computational resources (Kelly & Schulte 1995). \nThe problem of logical omniscience is particularly acute for expected\nutility theory in general, and the theory of subjective probability in\nparticular. For the postulates of subjective probability imply that an\nagent knows all the logical consequences of her commitments, thereby\nmandating logical omniscience. This limits the applicability of the\ntheory, however. For example, it prohibits having uncertain judgments\nabout mathematical and logical statements. In an article from 1967,\n“Difficulties in the theory of personal probability”,\nreported in Hacking 1967 and Seidenfeld, Schervish, & Kadane 2012\nbut misprinted in Savage 1967, Savage raises the problem of logical\nomniscience for the subjective theory of probability: \nThe analysis should be careful not to prove too much; for some\ndepartures from theory are inevitable, and some even laudable. For\nexample, a person required to risk money on a remote digit of \\(\\pi\\)\nwould, in order to comply fully with the theory, have to compute that\ndigit, though this would really be wasteful if the cost of computation\nwere more than the prize involved. For the postulates of the theory\nimply that you should behave in accordance with the logical\nimplication of all that you know. Is it possible to improve the theory\nin this respect, making allowances within it for the cost of thinking,\nor would that entail paradox, as I am inclined to believe but unable\nto demonstrate? (Savage 1967 excerpted from Savage’s\nprepublished draft; see notes in Seidenfeld et al. 2012) \nResponses to Savage’s problem include a game-theoretic treatment\nproposed by I.J. Good (1983), which swaps the extensional variable\nthat is necessarily true for an intensional variable representing an\naccomplice who knows the necessary truth but withholds enough\ninformation from you for you to be (coherently) uncertain about what\nhe knows. This trick changes the subject of your uncertainty, from a\nnecessarily true proposition that you cannot coherently doubt to a\ncoherent guessing game about that truth facilitated by your\naccomplice’s incomplete description. Another response sticks to\nthe classical line that failures of logical omniscience are deviations\nfrom the normative standard of perfect rationality but introduces an\nindex for incoherence to accommodate reasoning with incoherent\nprobability assessments (Schervish, Seidenfeld, & Kadane 2012). A\nthird approach, suggested by de Finetti (1970), is to restrict\npossible states of affairs to observable states with a finite\nverifiable procedure—which may rule out theoretical states or\nany other that does not admit a verification protocol. Originally,\nwhat de Finetti was after was a principled way to construct a\npartition over possible outcomes to distinguish serious possible\noutcomes of an experiment from wildly implausible but logically\npossible outcomes, yielding a method for distinguishing between\ngenuine doubt and mere “paper doubts” (Peirce 1955). Other\nproposals follow de Finetti’s line by tightening the\nadmissibility criteria and include epistemically possible\nevents, which are events that are logically consistent with the\nagent’s available information; apparently possible\nevents, which include any event by default unless the agent has\ndetermined that it is inconsistent with his information; and\npragmatically possible events, which only includes events\nthat are judged sufficiently important (Walley 1991: 2.1). \nThe notion of apparently possible refers to a procedure for\ndetermining inconsistency, which is a form of bounded procedural\nrationality\n (section 2).\n The challenges of avoiding paradox, which Savage alludes to, are\nformidable. However, work on bounded fragments of Peano arithmetic\n(Parikh 1971) provide coherent foundations for exploring these ideas,\nwhich have been taken up specifically to formulate bounded-extensions\nof default logic for apparent possibility (Wheeler 2004) and\nmore generally in models of computational rationality (Lewis,\nHowes, & Singh 2014). \nIt is commonplace to contrast how people render judgments, or make\ndecisions, from how they ought to do so. However, interest in\ncognitive processes, mechanisms, and algorithms of boundedly rational\njudgment and decision making suggests that we instead distinguish\namong three aims of inquiry rather than these two. Briefly, a\ndescriptive theory aims to explain or predict what judgments\nor decisions people in fact make; a prescriptive theory aims\nto explain or recommend what judgments or decisions people ought to\nmake; a normative theory aims to specify a normative standard\nto use in evaluating a judgment or decision. \nTo illustrate each type, consider a domain where differences between\nthese three lines of inquiry are especially clear: arithmetic. A\ndescriptive theory of arithmetic might concern the psychology of\narithmetical reasoning, a model of approximate numeracy in animals, or\nan algorithm for implementing arbitrary-precision arithmetic on a\ndigital computer. The normative standard of full arithmetic is\nPeano’s axiomatization of arithmetic, which distills natural\nnumber arithmetic down to a function for one number succeeding another\nand mathematical induction. But one might also consider\nRobinson’s induction-free fragment of Peano arithmetic (Tarski,\nMostowski, & Robinson 1953) or axioms for some system of cardinal\narithmetic in the hierarchy for large cardinals. A prescriptive theory\nfor arithmetic will reference both a fixed normative standard and\nrelevant facts about the arithmetical capabilities of the organism or\nmachine performing arithmetic. A curriculum for improving the\narithmetical performance of elementary school children will differ\nfrom one designed to improve the performance of adults. Even though\nthe normative standard of Peano arithmetic is the same for both\nchildren and adults, stable psychological differences in these two\npopulations may warrant prescribing different approaches for improving\ntheir arithmetic. Continuing, even though Peano’s axioms are the\nnormative standard for full arithmetic, nobody would prescribe\nPeano’s axioms for the purpose of improving anyone’s sums.\nThere is no mistaking Peano’s axioms for a descriptive theory of\narithmetical reasoning, either. Even so, a descriptive theory of\narithmetic will presuppose the Peano axioms as the normative standard\nfor full arithmetic, even if only implicitly. In describing how people\nsum two numbers, after all, one presumes that they are attempting to\nsum two numbers rather than concatenate them, count out in sequence,\nor send a message in code. \nFinally, imagine an effective pedagogy for teaching arithmetic to\nchildren is known and we wish to introduce children to cardinal\narithmetic. A reasonable start on a prescriptive theory for cardinal\narithmetic for children might be to adapt as much of the successful\npedagogy for full arithmetic as possible while anticipating that some\nof those methods will not survive the change in normative standards\nfrom Peano to (say) ZFC+. Some of those differences can be seen as a\ndirect consequence of the change from one standard to another, while\nother differences may arise unexpectedly from the observed interplay\nbetween the change in task, that is, from performing full arithmetic\nto performing cardinal arithmetic, and the psychological capabilities\nof children to perform each task. \nTo be sure, there are important differences between arithmetic and\nrational behavior. The objects of arithmetic, numerals and the numbers\nthey refer to, are relatively clear cut, whereas the objects of\nrational behavior vary even when the same theoretical machinery is\nused. Return to expected utility theory as an example. An agent may be\nviewed as deliberating over options with the aim to choose one that\nmaximizes his personal welfare, or viewed to act as if he deliberately\ndoes so without actually doing so, or understood to do nothing of the\nkind but to instead be a bit part player in the population fitness of\nhis kind. \nSeparating the question of how to choose a normative standard from\nquestions about how to evaluate or describe behavior is an important\ntool to reduce misunderstandings that arise in discussions of bounded\nrationality. Even though Peano’s axioms would never be\nprescribed to improve, nor proposed to describe, arithmetical\nreasoning, it does not follow that the Peano axioms of arithmetic are\nirrelevant to descriptive and prescriptive theories of arithmetic.\nWhile it remains an open question whether the normative standards for\nhuman rational behavior admit axiomatization, there should be little\ndoubt over the positive role that clear normative standards play in\nadvancing our understanding of how people render judgments, or make\ndecisions, and how they ought to do so. \nSimon thought the shift in focus from reasoning behavior to choice\nbehavior was a mistake. Since, in the 1950s, little was known about\nthe processes involved in making judgments or reaching decisions, we\nwere not in the position to freely abstract away all of those features\nfrom our mathematical models. Yet, this ignorance of the psychology of\ndecision-making also raised the question of how to proceed. The answer\nwas to attend to the costs in effort from operating a procedure for\nmaking decisions and comparing those costs to the resources available\nto the organism using the procedure and, conversely, to compare how\nwell an organism performs in terms of accuracy\n(section 8.2) with its limited\ncognitive resources in order to investigate models with comparable\nlevels of accuracy within those resource bounds.  Effectively managing\nthe trade-off between the costs and quality of a decision involves\nanother type of rationality, which Simon later called procedural\nrationality (Simon 1976: 69). \nIn this section we highlight early, key contributions to modeling\nprocedures for boundedly rational judgment and decision-making,\nincluding the origins of the accuracy-effort trade-off,\nSimon’s satisficing strategy, improper linear\nmodels, and the earliest effort to systematize several features\nof high-level, cognitive judgment and decision-making: cumulative\nprospect theory. \nHerbert Simon and I.J. Good were each among the first to call\nattention to the cognitive demands of subjective expected utility\ntheory, although neither one in his early writings abandoned the\nprinciple of expected utility as the normative standard for rational\nchoice. Good, for instance, referred to the recommendation to maximize\nexpected utility as the ordinary principle of rationality,\nwhereas Simon called the principle objective rationality and\nconsidered it the central tenet of global rationality. The\nrules of rational behavior are costly to operate in both time and\neffort, Good observed, so real agents have an interest in minimizing\nthose costs (Good 1952: 7(i)). Efficiency dictates that one choose\nfrom available alternatives an option that yields the largest result\ngiven the resources available, which Simon emphasized is not\nnecessarily an option that yields the largest result overall (Simon\n1947: 79). So reasoning judged deficient without considering the\nassociated costs may be found meritorious once all those costs are\naccounted for—a conclusion that a range of authors soon came to\nendorse, including Amos Tversky: \nIt seems impossible to reach any definitive conclusions concerning\nhuman rationality in the absence of a detailed analysis of the\nsensitivity of the criterion and the cost involved in evaluating the\nalternatives. When the difficulty (or the costs) of the evaluations\nand the consistency (or the error) of the judgments are taken into\naccount, a [transitivity-violating method] may prove superior.\n(Tversky 1969) \nBalancing the quality of a decision against its costs soon became a\npopular conception of bounded rationality, particularly in economics\n(Stigler 1961), where it remains commonplace to formulate boundedly\nrational decision-making as a constrained optimization problem. On\nthis view boundedly rational agents are utility maximizers after all,\nonce all the constraints are made clear (Arrow 2004). Another reason\nfor the popularity of this conception of bounded rationality is its\ncompatibility with Milton Friedman’s as if methodology\n(M. Friedman 1953), which licenses models of behavior that ignore the\ncausal factors underpinning judgment and decision making. To say that\nan agent behaves as if he is a utility maximizer is at once\nto concede that he is not but that his behavior proceeds as if he\nwere. Similarly, to say that an agent behaves as if he is a utility\nmaximizer under certain constraints is to concede that he does not\nsolve constrained optimization problems but nevertheless behaves as if\nhe did. \nSimon’s focus on computationally efficient methods that yield\nsolutions that are good enough contrasts with Friedman’s as if\nmethodology, since evaluating whether a solution is “good\nenough”, in Simon’s terms, involves search procedures,\nstopping criteria, and how information is integrated in the course of\nmaking a decision. Simon offers several examples to motivate inquiry\ninto computationally efficient methods. Here is one. Applying the\ngame-theoretic minimax algorithm to the game of chess calls for\nevaluating more chess positions than the number of molecules in the\nuniverse (Simon 1957a: 6). Yet if the game of chess is beyond the reach\nof exact computation, why should we expect everyday problems to be any\nmore tractable? Simon’s question is to explain how human beings\nmanage to solve complicated problems in an uncertain world given their\nmeager resources. Answering Simon’s question, as opposed to\napplying Friedman’s method to fit a constrained optimization\nmodel to observed behavior, is to demand a model with better\npredictive power concerning boundedly rational judgment and decision\nmaking. In pressing this question of how human beings solve uncertain\ninference problems, Simon opened two lines of inquiry that continue to\ntoday, namely: \nHow do human beings actually make decisions “in the\nwild”? \nHow can the standard theories of global rationality be simplified to\nrender them more tractable? \nSimon’s earliest efforts aimed to answer the second question\nwith, owing to the dearth of psychological knowledge at the time about\nhow people actually make decisions, only a layman’s\n“acquaintance with the gross characteristics of human\nchoice” (Simon 1955a: 100). His proposal was to replace the\noptimization problem of maximizing expected utility with a simpler\ndecision criterion he called satisficing, and by models with\nbetter predictive power more generally. \nSatisficing is the strategy of considering the options available to\nyou for choice until you find one that meets or exceeds a predefined\nthreshold—your aspiration level—for a minimally acceptable\noutcome. Although Simon originally thought of procedural rationality\nas a poor approximation of global rationality, and thus viewed the\nstudy of bounded rationality to concern “the behavior of human\nbeings who satisfice because they have not the wits to\nmaximize” (Simon 1957a: xxiv), there are a range of\napplications of satisficing models to sequential choice problems,\naggregation problems, and high-dimensional optimization problems,\nwhich are increasingly common in machine learning. \nGiven a specification of what will count as a good-enough outcome,\nsatisficing replaces the optimization objective from expected utility\ntheory of selecting an undominated outcome with the objective of\npicking an option that meets your aspirations. The model has since\nbeen applied to business (Bazerman & Moore 2008; Puranam,\nStieglitz, Osman, & Pillutla 2015), mate selection (Todd &\nMiller 1999) and other practical sequential-choice problems, like\nselecting a parking spot (Hutchinson, Fanselow, et al. 2012). Ignoring\nthe procedural aspects of Simon’s original formulation of\nsatisficing, if one has a fixed aspirational level for a given\ndecision problem, then admissible choices from satisficing can be\ncaptured by so-called \\(\\epsilon\\)-efficiency methods (Loridan 1984;\nWhite 1986). \nHybrid optimization-satisficing techniques are used in machine\nlearning when many metrics are available but no sound or practical\nmethod is available for combining them into a single value. Instead,\nhybrid optimization-satisficing methods select one metric to optimize\nand satisfice the remainder. For example, a machine learning\nclassifier might optimize accuracy (i.e., maximize the proportion of\nexamples for which the model yields the correct output; see\n section 8.2)\n but set aspiration levels for the false positive rate, coverage, and\nruntime. \nSelten’s aspiration adaption theory models decision\ntasks as problems with multiple incomparable goals that resist\naggregation into a complete preference order over all alternatives\n(Selten 1998). Instead, the decision-maker will have a vector of goal\nvariables, where those vectors are comparable by weak dominance. If\nvector A and vector B are possible assignments for my goals, then A\ndominates vector B if there is no goal in the sequence in which B\nassigns a value that is strictly less than A, and there is some goal\nfor which A assigns a value strictly greater than B. Selten’s\nmodel imagines an aspiration level for each goal, which itself can be\nadjusted upward or downwards depending on the set of feasible\n(admissible) options. Aspiration adaption theory is a highly\nprocedural and local account in the tradition of Newell and\nSimon’s approach to human problem solving (Newell & Simon\n1972), although it was not initially offered as a psychological\nprocess model. Analogous approaches have been explored in the AI\nplanning literature (Bonet & Geffner 2001; Ghallab, Nau, &\nTraverso 2016). \nProper linear models represent another important class of optimization\nmodels. A proper linear model is one where predictor variables are\nassigned weights, which are selected so that the linear combination of\nthose weighted predictor variables optimally predicts a target\nvariable of interest. For example, linear regression is a proper\nlinear model that selects weights such that the squared\n“distance” between the model’s predicted value of\nthe target variable and the actual value (given in the data set) is\nminimized. \nPaul Meehl’s review in the 1950s of psychological studies using\nstatistical methods versus clinical judgment cemented the statistical\nturn in psychology (Meehl 1954). Meehl’s review found that\nstudies involving the prediction of a numerical target variable from\nnumerical predictors is better done by a proper linear model than by\nthe intuitive judgment of clinicians. Concurrently, the psychologist\nKenneth Hammond formulated Brunswik’s lens model\n (section 3.2)\n as a composition of proper linear models to model the differences\nbetween clinical versus statistical predictions (K. Hammond 1955).\nProper linear models have since become a workhorse in cognitive\npsychology in areas that include decision analysis (Keeney &\nRaiffa 1976; Kaufmann & Wittmann 2016), causal inference\n(Waldmann, Holyoak, & Fratianne 1995; Spirtes 2010), and\nresponse-times to choice (Brown & Heathcote 2008; Turner,\nRodriguez, et al. 2016). \nRobin Dawes, returning to Meehl’s question about statistical\nversus clinical predictions, found that even improper linear models\nperform better than clinical intuition (Dawes 1979). The\ndistinguishing feature of improper linear models is that the weights\nof a linear model are selected by some non-optimal method. For\ninstance, equal weights might be assigned to the predictor variables\nto afford each equal weight or a unit-weight, such as 1 or −1, to tally features supporting a positive or negative prediction,\nrespectively. As an example, Dawes proposed an improper model to\npredict subjective ratings of marital happiness by couples based on\nthe difference between their rates of lovemaking and fighting. The\nresults? Among the thirty happily married couples, two argued more\nthan they had intercourse. Yet all twelve unhappy couples fought more\nfrequently. And those results replicated in other laboratories\nstudying human sexuality in the 1970s. Both equal-weight regression\nand unit-weight tallying have since been found to commonly\noutperform proper linear models on small data sets. Although no simple\nimproper linear model performs well across all common benchmark\ndatasets, for almost every data set in the benchmark there is some\nsimple improper model that performs well in predictive accuracy\n(Lichtenberg & Simsek 2016). This observation, and many others in\nthe heuristics literature, points to biases of simplified models that\ncan lead to better predictions when used in the right circumstances\n (section 4). \nDawes’s original point was not that improper linear models\noutperform proper linear models in terms of accuracy, but rather that\nthey are more efficient and (often) close approximations of proper\nlinear models. “The statistical model may integrate the\ninformation in an optimal manner”, Dawes observed, “but it\nis always the individual …who chooses variables” (Dawes\n1979: 573). Moreover, Dawes argued that it takes human judgment to\nknow the direction of influence between predictor variables and target\nvariables, which includes the knowledge of how to numerically code\nthose variables to make this direction clear. Recent advances in\nmachine learning chip away at Dawes’s claims about the unique\nrole of human judgment, and results from Gigerenzer’s ABC Group\nabout unit-weight tallying outperforming linear regression in\nout-of-sample prediction tasks with small samples is an instance of\nimproper linear models outperforming proper linear models (Czerlinski,\nGigerenzer, & Goldstein 1999). Nevertheless, Dawes’s general\nobservation about the relative importance of variable selection over\nvariable weighting stands (Katsikopoulos, Schooler, & Hertwig\n2010). \nIf both satisficing and improper linear models are examples addressing\nSimon’s second question at the start of this\nsection—namely, how to simplify existing models to render them\nboth tractable and effective—then Daniel Kahneman and Amos\nTversky’s cumulative prospect theory is among the first\nmodels to directly incorporate knowledge about how humans actually\nmake decisions. \nIn our discussion in\n section 1.1\n about alternatives to the Independence Axiom,\n (A3),\n we mentioned several observed features of human choice behavior that\nstand at odds with the prescriptions of expected utility theory.\nKahneman and Tversky developed prospect theory around four of those\nobservations about human decision-making (Kahneman & Tversky 1979;\nWakker 2010). \nReference Dependence. Rather than make decisions by\ncomparing the absolute magnitudes of welfare, as prescribed by\nexpected utility theory, people instead tend to value prospects by\ntheir change in welfare with respect to a reference point. This\nreference point can be a person’s current state of wealth, an\naspiration level, or a hypothetical point of reference from which to\nevaluate options. The intuition behind reference dependence is that\nour sensory organs have evolved to detect changes in sensory stimuli\nrather than store and compare absolute values of stimuli. Therefore,\nthe argument goes, we should expect to see the cognitive mechanisms\ninvolved in decision-making to inherit this sensitivity to changes in\nperceptual attributes values. \nIn prospect theory, reference dependence is reflected by utility\nchanging sign at the origin of the valuation curve \\(v(\\cdot)\\) in\n Figure 1(a).\n The x-axis represents gains (right side) and losses (left\nside) in euros, and y-axis plots the value placed on relative\ngains and losses by a valuation function \\(v(\\cdot)\\), which is fit to\nexperimental data on people’s choice behavior. \nLoss Aversion. People are more sensitive to losses\nthan gains of the same magnitude; the thrill of victory does not\nmeasure up to the agony of defeat. So, Kahneman and Tversky\nmaintained, people will prefer an option that does not incur a loss to\nan alternative option that yields an equivalent gain. The disparity in\nhow potential gains and losses are evaluated also accounts for the\nendowment effect, which is the tendency for people to value a good\nthat they own more than a comparatively valued substitute (Thaler\n1980). \nIn prospect theory, loss aversion appears in\n Figure 1(a)\n in the (roughly) steeper slope of \\(v(\\cdot)\\) to the left of the\norigin, representing losses relative to the subject’s reference\npoint, than the slope of \\(v(\\cdot)\\) for gains on the right side of\nthe reference point. Thus, for the same magnitude of change in reward\nx from the reference point, the magnitude of the consequence\nof gaining x is less than the magnitude of losing x. \nNote that differences in affective attitudes toward, and the\nneurological processes responsible for processing, losses and gains do\nnot necessarily translate to differences in people’s choice\nbehavior (Yechiam & Hochman 2014). The role and scope that loss\naversion plays in judgment and decision making is less clear than was\ninitially assumed\n (section 1.2). \nDiminishing Returns for both Gains and Losses. Given\na fixed reference point, people’s sensitivity to changes in\nasset values (x in\n Figure 1a)\n diminish the further one moves from that reference point, both in the\ndomain of losses and the domain of gains. This is inconsistent with\nexpected utility theory, even when the theory is modified to\naccommodate diminishing marginal utility (M. Friedman & Savage\n1948). \nIn prospect theory, the valuation function \\(v(\\cdot)\\) is concave for\ngains and convex for losses, representing a diminishing sensitivity to\nboth gains and losses. Expected utility theory can be made to\naccommodate sensitivity effects, but the utility function is typically\neither strictly concave or strictly convex, not both. \nProbability Weighting. Finally, for known exogenous\nprobabilities, people do not calibrate their subjective probabilities\nby direct inference (Levi 1977), but instead systematically\nunderweight high-probability events and overweight low-probability\nevents, with a cross-over point of approximately one-third\n (Figure 1b).\n Thus, changes in very small or very large probabilities have greater\nimpact on the evaluation of prospects than they would under expected\nutility theory. People are willing to pay more to reduce the number of\nbullets in the chamber of a gun from 1 to 0 than from 4 bullets to 3\nin a hypothetical game of Russian roulette. \n\n Figure 1(b)\n plots the median values for the probability weighting function\n\\(w(\\cdot)\\) that takes the exogenous probability p associated\nwith prospects, as reported in Tversky & Kahneman 1992. Roughly,\nbelow probability values of one-third people overestimate the\nprobability of an outcome (consequence), and above probability\none-third people tend to underestimate the probability of an outcome\noccurring. Traditionally, overweighting is thought to concern the\nsystematic miscalibration of people’s subjective estimates of\noutcomes against a known exogenous probability, p, serving as\nthe reference standard. In support of this view, miscalibration\nappears to disappear when people learn a distribution through sampling\ninstead of learning identical statistics by description (Hertwig,\nBarron, Weber, & Erev 2004). Miscalibration in this context ought\nto be distinguished from overestimating or underestimating subjective\nprobabilities when the relevant statistics are not supplied as part of\nthe decision task. For example, televised images of the aftermath of\nairplane crashes lead to an overestimation of the low-probability\nevent of commercial airplanes crashing. Even though a person’s\nsubjective probability of the risk of a commercial airline crash would\nbe too high given the statistics, the mechanism responsible is\ndifferent: here the recency or availability of\nimages from the evening news is to blame for scaring him out of his\nwits, not the sober fumbling of a statistics table. An alternative\nview maintains that people understand that their weighted\nprobabilities are different than the exogenous probability but\nnevertheless prefer to act as if the exogenous probability were so\nweighted (Wakker 2010). On this view, probability weighting is not a\n(mistaken) belief but a preference. \n\n Figure 1:\n (a) plots the value function \\(v(\\cdot)\\) applied to consequences of\na prospect; (b) plots the median value of the probability weighting\nfunction \\(w(\\cdot)\\) applied to positive prospects of the form \\((x,\np; 0, 1-p)\\) with probability p.\n[An extended description of this figure is in the supplement.]\n \nProspect theory incorporates these components into models of human\nchoice under risk by first identifying a reference point that either\nrefers to the status quo or some other aspiration level. The\nconsequences of the options under consideration then are framed in\nterms of deviations from this reference point. Extreme probabilities\nare simplified by rounding off, which yields miscalibration of the\ngiven, exogenous probabilities. Dominance reasoning is then applied,\nwhere dominated alternatives are eliminated from choice, along with\nadditional steps to separate options without risk, probabilities\nassociated with a specific outcome are combined, and a version of\neliminating irrelevant alternatives is applied (Kahneman & Tversky\n1979: 284–285). \nNevertheless, prospect theory comes with problems. For example, a\nshift of probability from less favorable outcomes to more favorable\noutcomes ought to yield a better prospect, all things considered, but\nthe original prospect theory violates this principle of stochastic\ndominance. Cumulative prospect theory satisfies stochastic\ndominance, however, by appealing to a rank-dependent method for\ntransforming probabilities (Quiggin 1982). For a review of the\ndifferences between prospect theory and cumulative prospect theory,\nalong with an axiomatization of cumulative prospect theory, see\nFennema & Wakker 1997. \nImagine a meadow whose plants are loaded with insects but few are in\nflight. Then, this meadow is a more favorable environment for a bird\nthat gleans rather than hawks. In a similar fashion, a decision-making\nenvironment might be more favorable for one decision-making strategy\nthan for another. Just as it would be “irrational” for a\nbird to hawk rather than glean, given the choice for this meadow, so\ntoo what may be an irrational decision strategy in one environment may\nbe entirely rational in another. \nIf procedural rationality attaches a cost to the making of a decision,\nthen ecological rationality locates that procedure in the world. The\nquestions ecological rationality ask are what features of an\nenvironment can help or hinder decision making and how should we model\njudgment or decision-making ecologies. For example, people make causal\ninferences about patterns of covariation they observe—especially\nchildren, who then perform experiments testing their causal hypotheses\n(Glymour 2001). Unsurprisingly, people who draw the correct inferences\nabout the true causal model do better than those who infer the wrong\ncausal model (Meder, Mayrhofer, & Waldmann 2014). More surprising,\nMeder and his colleagues found that those making correct causal\njudgments do better than subjects who make no causal judgments at all.\nAnd perhaps most surprising of all is that those with true causal\nknowledge also beat the benchmark standards in the literature which\nignore causal structure entirely; the benchmarks encode, spuriously,\nthe assumption that the best we can do is to make no causal judgments\nat all. \nIn this section and the next we will cover five important\ncontributions to the emergence of ecological rationality. In this\nsection, after reviewing Simon’s proposal for distinguishing\nbetween behavioral constraints and environmental\nstructure, we turn to three historically important contributions:\nthe lens model, rational analysis, and cultural\nadaptation. Finally, in\n section 4,\n we review the bias-variance decomposition, which has figured\nin the Fast and Frugal Heuristics literature\n (section 7.2). \nSimon thought that both behavioral constraints and environmental\nstructure ought to figure in a theory of bounded rationality, yet he\ncautioned against identifying behavioral and environmental properties\nwith features of an organism and features of its physical environment,\nrespectively: \nwe must be prepared to accept the possibility that what we call\n“the environment” may lie, in part, within the skin of the\nbiological organisms. That is, some of the constraints that must be\ntaken as givens in an optimization problem may be physiological and\npsychological limitations of the organism (biologically defined)\nitself. For example, the maximum speed at which an organism can move\nestablishes a boundary on the set of its available behavior\nalternatives. Similarly, limits on computational capacity may be\nimportant constraints entering into the definition of rational choice\nunder particular circumstances. (Simon 1955a: 101) \nThat said, what is classified as a behavioral constraint rather than\nan environmental affordance varies across disciplines and the\ntheoretical tools pressed into service. For example, one computational\napproach to bounded rationality, computational rationality\ntheory (Lewis et al. 2014), classifies the cost to an organism of\nexecuting an optimal program as a behavioral constraint, classifies\nlimits on memory as an environmental constraint, and treats the costs\nassociated with searching for an optimal program to execute as\nexogenous. Anderson and Schooler’s study and computational\nmodeling of human memory (Anderson & Schooler 1991) within the\nACT-R framework, on the other hand, views the limits on memory and\nsearch-costs as behavioral constraints which are adaptive responses to\nthe structure of the environment. Still another broad class of\ncomputational approaches are found in statistical signal\nprocessing, such as adaptive filters (Haykin 2013), which are\ncommonplace in engineering and vision (Marr 1982; Ballard & Brown\n1982). Signal processing methods typically presume the sharp\ndistinction between device and world that Simon cautioned against,\nhowever. Still others have challenged the distinction between\nbehavioral constraints and environmental structure by arguing that\nthere is no clear way to separate organisms from the environments they\ninhabit (Gibson 1979), or by arguing that features of cognition which\nappear body-bound may not be necessarily so (Clark & Chalmers\n1998). \nBearing in mind the different ways the distinction between behavior\nand environment have been drawn, and challenges to what precisely\nfollows from drawing such a distinction, ecological approaches to\nrationality all endorse the thesis that the ways in which an organism\nmanages structural features of its environment are essential to\nunderstanding how deliberation occurs and effective behavior arises.\nIn doing so theories of bounded rationality have traditionally focused\non at least some of the following features, under this rough\nclassification: \nBehavioral Constraints—may refer to bounds on\ncomputation, such as the cost of searching the best algorithm\nto run, an appropriate rule to apply, or a satisficing option to\nchoose; the cost of executing an optimal algorithm,\nappropriate rule, or satisficing choice; and costs of storing\nthe data structure of an algorithm, the constitutive elements of a\nrule, or the objects of a decision problem. \nEcological Structure—may refer to\nstatistical, topological, or other perceptible\ninvariances of the task environment that an organism is adapted\nto; or to architectural features or biological\nfeatures of the computational processes or cognitive mechanisms\nresponsible for effective behavior, respectively. \nEgon Brunswik was among the first to apply probability and statistics\nto the study of human perception, and was ahead of his time in\nemphasizing the role ecology plays in the generalizability of\npsychological findings. Brunswik thought psychology ought to aim for\nstatistical descriptions of adaptive behavior (Brunswik 1943). Instead\nof isolating a small number of independent variables to manipulate\nsystematically to observe the effects on a dependent\nvariable, psychological experiments ought instead to assess how an\norganism adapts to its environment. So, not only should experimental\nsubjects be representative of the population, as one would presume,\nbut the experimental situations they are subjected to ought to be\nrepresentative of the environment that the subjects inhabit (Brunswik\n1955). Thus, Brunswik maintained, psychological experiments ought to\nemploy a representative design to preserve the causal\nstructure of an organism’s natural environment. For a review of\nthe development of representative design and its use in the study of\njudgment and decision-making, see Dhami, Hertwig, & Hoffrage\n2004. \nBrunswik’s lens model is formulated around his ideas\nabout how behavioral and environmental conditions bear on organisms\nperceiving proximal cues to draw inferences about some distal feature\nof its “natural-cultural habitat” (Brunswik 1955: 198). To\nillustrate, an organism may detect the color markings (distal object)\nof a potential mate through contrasts in light frequencies reflecting\nacross its retina (proximal cues). Some proximal cues will be more\ninformative about the distal objects of interest than others, which\nBrunswik understood as a difference in the “objective”\ncorrelations between proximal cues and the target distal objective.\nThe ecological validity of proximal cues thus refers to their\ncapacity for providing the organism useful information about some\ndistal object within a particular environment. Assessments of\nperformance for an organism then amount to a comparison of the\norganism’s actual use of cue information to the cue’s\ninformation capacity. \nKenneth Hammond and colleagues (K. Hammond, Hursch, & Todd 1964)\nformulated Brunswik’s lens model as a system of linear bivariate\ncorrelations, as depicted in\n Figure 2\n (Hogarth & Karelaia 2007). Informally,\n Figure 2\n says that the accuracy of a subject’s judgment (response),\n\\(Y_s\\), about a numerical target criterion, \\(Y_e\\), given some\ninformative cues (features) \\(X_1, \\ldots, X_n\\), is determined by the\ncorrelation between the subject’s response and the target. More\nspecifically, the linear lens model imagines two large linear systems,\none for the environment, e, and another for the subject,\ns, which both share a set of cues, \\(X_1, \\ldots, X_n\\). Note\nthat cues may be associated with one another, i.e., it is possible\nthat \\(\\rho(X_i,X_j) \\neq 0\\) for indices \\(i\\neq j\\) from 1 to\nn. \nThe accuracy of the subject’s judgment \\(Y_s\\) about the target\ncriterion value \\(Y_e\\) is measured by an achievement index,\n\\(r_a\\), which is computed by Pearson’s correlation coefficient\n\\(\\rho\\) of \\(Y_e\\) and \\(Y_s\\). The subject’s predicted\nresponse \\(\\hat{Y}_s\\) to the cues is determined by the weights\n\\(\\beta_{s_i}\\) the subject assigns to each cue \\(X_i\\), and the\nlinearity of the subject’s response, \\(R_s\\), measures the noise\nin the system, \\(\\epsilon_s\\). Thus, the subject’s response is\nconceived to be a weighted linear sum of subject-weighted cues plus\nnoise. The analogue of response linearity in the environment\nis environmental predictability, \\(R_e\\). The environment, on\nthis model, is thought to be probabilistic—or\n“chancy” as some say. Finally, the environment-weighted\nsum of cues, \\(\\hat{Y}_e\\), is compared to the subject-weighted sum of\ncues, \\(\\hat{Y}_s\\), by a matching index, G. \nFigure 2:\n Brunswik’s Lens Model\n[An extended description of this figure is in the supplement.]\n \nIn light of this formulation of the lens model, return to\nSimon’s remarks concerning the classification of environmental\naffordance versus behavioral constraint. The conception of the lens\nmodel as a linear model is indebted to signal detection theory, which\nwas developed to improve the accuracy of early radar systems. Thus,\nthe model inherits from engineering a clean division between subject\nand environment. However, suppose for a moment that both the\nenvironmental mechanism producing the criterion value and the\nsubject’s predicted response are linear. Now consider the\nerror-term, \\(\\epsilon_s\\). That term may refer to biological\nconstraints that are responses to adaptive pressures on the whole\norganism. If so, ought \\(\\epsilon_s\\) be classified as an\nenvironmental constraint rather than a behavioral constraint? The\nanswer will depend on what follows from the reclassification, which\nwill depend on the model and the goal of inquiry\n (section 8).\n If we were using the lens model to understand the ecological validity\nof an organism’s judgment, then reclassifying \\(\\epsilon_s\\) as\nan environmental constraint would only introduce confusion; If instead\nour focus was to distinguish between behavior that is subject to\nchoice and behavior that is precluded from choice, then the proposed\nreclassification may herald clarity—but then we would surely\nabandon the lens model for something else, or in any case would no\nlonger be referring to the parameter \\(\\epsilon_s\\) in\n Figure 2. \nFinally, it should be noted that the lens model, like nearly all\nlinear models used to represent human judgment and decision-making,\ndoes not scale well as a descriptive model. In multi-cue\ndecision-making tasks involving more than three cues, people often\nturn to simplifying heuristics due to the complications involved in\nperforming the necessary calculations\n (section 2.1;\n see also\n section 4).\n More generally, as we remarked in\n section 2.3,\n linear models involve calculating trade-offs that are difficult for\npeople to perform. Lastly, the supposition that the environment is\nlinear is a strong modeling assumption. Quite apart from the\ndifficulties that arise for humans to execute the necessary\ncomputations, it becomes theoretically more difficult to justify model\nselection decisions as the number of features increases. The matching\nindex G is a goodness-of-fit measure, but goodness-of-fit tests\nand residual analysis begin to lead to misleading conclusions for\nmodels with as five or more dimensions. Modern machine learning\ntechniques for supervised learning get around this limitation by\nfocusing on analogues of the achievement index, construct predictive\nhypotheses purely instrumentally, and dispense with matching\naltogether (Wheeler 2017). \nRational analysis is a methodology applied in cognitive science and\nbiology to explain why a cognitive system or organism engages in a\nparticular behavior by appealing to the presumed goals of the\norganism, the adaptive pressures of its environment, and the\norganism’s computational limitations. Once an organism’s\ngoals are identified, the adaptive pressures of its environment\nspecified, and the computational limitations are accounted for, an\noptimal solution under those conditions is derived to explain why a\nbehavior that is otherwise ineffective may nevertheless be effective\nin achieving that goal under those conditions (Marr 1982; Anderson\n1991; Oaksford & Chater 1994; Palmer 1999). Rational analyses are\ntypically formulated independently of the cognitive processes or\nbiological mechanisms that explain how an organism realizes a\nbehavior. \nOne theme to emerge from the rational analysis literature that has\ninfluenced bounded rationality is the study of memory (Anderson &\nSchooler 1991). For instance, given the statistical features of our\nenvironment, and the sorts of goals we typically pursue, forgetting is\nan advantage rather than a liability (Schooler & Hertwig 2005).\nMemory traces vary in their likelihood of being used, so the memory\nsystem will try to make readily available those memories which are\nmost likely to be useful. This is a rational analysis style argument,\nwhich is a common feature of the Bayesian turn in cognitive psychology\n(Oaksford & Chater 2007; Friston 2010). More generally, spacial\narrangements of objects in the environment can simplify perception,\nchoice, and the internal computation necessary for producing an\neffective solution (Kirsch 1995). Compare this view to the discussion\nof recency or availability effects distorting subjective probability\nestimates in\n section 2.4. \nRational analyses separate the goal of behavior from the mechanisms\nthat cause behavior. Thus, when an organism’s observed behavior\nin an environment does not agree with the behavior prescribed by a\nrational analysis for that environment, there are traditionally three\nresponses. One strategy is to change the specifications of the\nproblem, by introducing an intermediate step or changing the goal\naltogether, or altering the environmental constraints, et cetera\n(Anderson & Schooler 1991; Oaksford & Chater 1994). Another\nstrategy is to argue that mechanisms matter after all, so details of\nhuman psychology are taken into an alternative account (Newell &\nSimon 1972; Gigerenzer, Todd, et al. 1999; Todd, Gigerenzer, et al.\n2012). A third option is to enrich rational analysis by incorporating\ncomputational mechanisms directly into the model (Russell &\nSubramanian 1995; Chater 2014). Lewis, Howes, and Singh, for instance,\npropose to construct theories of rationality from (i) structural\nfeatures of the task environment; (ii) the bounded machine the\ndecision-process will run on, about which they consider four different\nclasses of computational resources that may be available to an agent;\nand (iii) a utility function to specify the goal, numerically, so as\nto supply an objective function against which to score outcomes (Lewis\net al. 2014). \nSo far we have considered theories and models which emphasize an\nindividual organism and its surrounding environment, which is\ntypically understood to be either the physical environment or, if\nsocial, modeled as if it were the physical environment. And we\nconsidered whether some features commonly understood to be behavioral\nconstraints ought to be instead classified as environmental\naffordances. \nYet people and their responses to the world are also part of each\nperson’s environment. Boyd and Richardson argue that human\nsocieties ought to be viewed as an adaptive environment, which in turn\nhas consequences for how individual behavior is evaluated. Human\nsocieties contain a large reservoir of information that is preserved\nthrough generations and expanded upon, despite limited, imperfect\nlearning by the members of human societies. Imitation, which\nis a common strategy in humans, including pre-verbal infants (Gergely,\nBekkering, & Király 2002), is central to cultural\ntransmission (Boyd & Richerson 2005) and the emergence of social\nnorms (Bicchieri & Muldoon 2014). In our environment, only a few\nindividuals with an interest in improving on the folk lore are\nnecessary to nudge the culture to be adaptive. The main advantage that\nhuman societies have over other groups of social animals, this\nargument runs, is that cultural adaptation is much faster than genetic\nadaptation (Bowles & Gintis 2011). On this view, human psychology\nevolved to facilitate speedy adaptation. Natural selection did not\nequip our large-brained ancestors with rigid behavior, but instead\nselected for brains that allowed then to modify their behavior\nadaptively in response to their environment (Barkow, Cosmides, &\nTooby 1992). \nBut if human psychology evolved to facility fast social learning, it\ncomes at the cost of human credulity. To have speedy adaptation\nthrough imitation of social norms and human behavior, the risk is the\nadoption of maladaptive norms or stupid behavior. \nThe bias-variance trade-off refers to a particular\ndecomposition of overall prediction error for an estimator into its\ncentral tendency (bias) and dispersion (variance). Sometimes overall\nerror can be reduced by increasing bias in order to reduce variance,\nor vice versa, effectively trading an increase in one type of error to\nafford a comparatively larger reduction in the other. To give an\nintuitive example, suppose your goal is to minimize your score with\nrespect to the following targets. \n\nFigure 3\n\n[An extended description of this figure is in the supplement.]\n  \nIdeally, you would prefer a procedure for delivering your\n“shots” that had both a low bias and low variance. Absent\nthat, and given the choice between a low bias and high variance\nprocedure versus a high bias and low variance procedure, you would\npresumably prefer the latter procedure if it returned a lower overall\nscore than the former, which is true of the corresponding figures\nabove. Although a decision maker’s learning algorithm ideally\nshould have low bias and low variance, in practice it is common that the\nreduction in one type of error yields some increase in the other. In\nthis section we explain the conditions under which the relationship\nbetween expected squared loss of an estimator and its bias and\nvariance holds and then remark on the role that the bias-variance\ntrade-off plays in research on bounded rationality. \nPredicting the exact volume of gelato to be consumed in Rome next\nsummer is more difficult than predicting that more gelato will be\nconsumed next summer than next winter. For although it is a foregone\nconclusion that higher temperatures beget higher demand for gelato,\nthe precise relationship between daily temperatures in Rome and\nconsumo di gelato is far from certain. Modeling quantitative,\npredictive relationships between random variables, such as the\nrelationship between the temperature in Rome, X, and volume of\nRoman gelato consumption, Y, is the subject of regression\nanalysis. \nSuppose we predict that the value of Y is h. How should\nwe evaluate whether this prediction is any good? Intuitively, the best\nwe can do is to pick an h that is as close to Y as we\ncan make it, one that would minimize the difference \\(Y - h\\). If we\nare indifferent to the direction of our errors, viewing positive\nerrors of a particular magnitude to be no worse than negative errors\nof the same magnitude, and vice versa, then a common practice is to\nmeasure the performance of h by its squared difference from\nY, \\((Y - h)^2\\). (We are not always indifferent; consider the\nplight of William Tell aiming at that apple.) Finally, since the\nvalues of Y vary, we might be interested in the average value\nof \\((Y - h)^2\\) by computing its expectation, \\(\\mathbb{E} \\left[ (Y\n- h)^2 \\right]\\). This quantity is the mean squared error of\nh,  \nNow imagine our prediction of Y is based on some data\n\\(\\mathcal{D}\\) about the relationship between X and Y,\nsuch as last year’s daily temperatures and daily total sales of\ngelato in Rome. The role that this particular dataset \\(\\mathcal{D}\\)\nplays as opposed to some other possible data set is a detail that will\nfigure later. For now, view our prediction of Y as some\nfunction of X, written \\(h(X)\\). Here again we wish to pick an\n\\(h(\\cdot)\\) to minimize \\(\\mathbb{E} \\left[ (Y - h(X))^2 \\right]\\),\nbut how close \\(h(\\cdot)\\) is to Y will depend on the possible\nvalues of X, which we can represent by the conditional\nexpectation  \nHow then should we evaluate this conditional prediction? The same as\nbefore, only now accounting for X. For each possible value\nx of X, the best prediction of Y is the\nconditional mean, \\(\\mathbb{E}\\left[ Y \\mid X = x\\right]\\). The\nregression function of Y on X, \\(r(x)\\), gives\nthe optimal value of Y for each value \\(x \\in X\\):  \nAlthough the regression function represents the true population value\nof Y given X, this function is usually unknown,\ntypically complicated, therefore often approximated by a simplified\nmodel or learning algorithm, \\(h(\\cdot)\\). \nWe might restrict candidates for \\(h(X)\\) to linear (or affine)\nfunctions of X, for instance. Yet making predictions about the\nvalue of Y with a simplified linear model, or some other\nsimplified model, can introduce a systematic prediction error called\nbias. Bias results from a difference between the central\ntendency of data generated by the true model, \\(r(X)\\) (for all \\(x\n\\in X\\)), and the central tendency of our estimator,\n\\(\\mathbb{E}\\left[h(X)\\right]\\), written  \nwhere any non-zero difference between the pair is interpreted as a\nsystematically positive or systematically negative error of the\nestimator, \\(h(X)\\). \nVariance measures the average deviation of a random variable\nfrom its expected value. In the current setting we are comparing the\npredicted value \\(h(X)\\) of Y, with respect to some data\n\\(\\mathcal{D}\\) about the relationship between X and Y,\nand the average value of \\(h(X)\\), \\(\\mathbb{E}\\left[ h(X) \\right]\\),\nwhich we will write  \nThe bias-variance decomposition of mean squared error is rooted in\nfrequentist statistics, where the objective is to compute an estimate\n\\(h(X)\\) of the true parameter \\(r(X)\\) with respect to data\n\\(\\mathcal{D}\\) about the relationship between X and Y.\nHere the parameter \\(r(X)\\) characterizing the truth about Y is\nassumed to be fixed and the data \\(\\mathcal{D}\\) is treated as a\nrandom quantity, which is exactly the reverse of Bayesian statistics.\nWhat this means is that the data set \\(\\mathcal{D}\\) is interpreted to\nbe one among many possible data sets of the same dimension generated\nby the true model, the deterministic process \\(r(X)\\). \nFollowing Christopher M. Bishop (2006), we may derive the\nbias-variance decomposition of mean squared error of h as\nfollows. Let h refer to our estimate \\(h(X)\\) of Y,\nr refer to the true value of Y, and \\(\\mathbb{E}\\left[ h\n\\right]\\) the expected value of the estimate h. Then,  \nwhere the term \\(2 \\mathbb{E}\\left[ \\left( \\mathbb{E}\\left[ h \\right]\n- h \\right) \\cdot \\left( r - \\mathbb{E}\\left[ h \\right] \\right)\n\\right]\\) is zero, since  \nNote that the frequentist assumption that r is a deterministic\nprocess is necessary for the derivation to go through; for if r\nwere a random quantity, the reduction of \\(\\mathbb{E} \\left[ r \\cdot\n\\mathbb{E} \\left[ h \\right] \\right]\\) to \\( r \\cdot \\mathbb{E} \\left[\nh \\right]\\) in line (2) would be invalid. \nOne last detail that we have skipped over is the prediction error of\n\\(h(X)\\) due to noise, N, which occurs independent of the\nmodel/learning algorithm used. Thus, the full bias-variance\ndecomposition of the mean-squared error of an estimate h is the\nsum of the bias (squared), variance, and irreducible error:  \nIntuitively, the bias-variance decomposition brings to light a\ntrade-off between two extreme approaches to making a prediction. At\none extreme, you might adopt as an estimator a constant function which\nproduces the same answer no matter what data you see. Suppose 7 is\nyour lucky number and your estimator’s prediction, \\(h(X) = 7\\).\nThen the variance of \\(h(\\cdot)\\) would be zero, since its prediction\nis always the same. The bias of your estimator, however, will be very\nlarge. In other words, your lucky number 7 model will massively\nunder fit your data. \nAt the other extreme, suppose you aim to make your bias error zero.\nThis occurs just when the predicted value of Y and the actual\nvalue of Y are identical, that is, \\(h(x_i) = y_i\\), for every\n\\((x_i, y_i)\\). Since you are presumed to not know the true function\n\\(r(X)\\) but instead only see a sample of data from the true model,\n\\(\\mathcal{D}\\), it is from this sample that you will aspire to\nconstruct an estimator that generalizes to accurately predict examples\noutside your training data \\(\\mathcal{D}\\). Yet if you were to fit\n\\(h_{\\mathcal{D}}(X)\\) perfectly to \\(\\mathcal{D}\\), then the variance\nof your estimator will be very high, since a different data set\n\\(\\mathcal{D}'\\) from the true model is not, by definition, identical\nto \\(\\mathcal{D}\\). How different is \\(\\mathcal{D}'\\) to\n\\(\\mathcal{D}\\)? The variation from one data set to another among all\nthe possible data sets is the variance or irreducible noise of the\ndata generated by the true model, which may be considerable.\nTherefore, in this zero-bias case your model will massively\noverfit your data. \nThe bias-variance trade-off therefore concerns the question of how\ncomplex a model ought to be to make reasonably accurate predictions on\nunseen or out-of-sample examples. The problem is to strike a balance\nbetween an under-fitting model, which erroneously ignores available\ninformation about the true function r, and an overfitting\nmodel, which erroneously includes information that is noise and\nthereby gives misleading information about the true function\nr. \nOne thing that human cognitive systems do very well is to generalize\nfrom a limited number of examples. The difference between humans and\nmachines is particularly striking when we compare how humans learn a\ncomplicated skill, such as driving a car, from how a machine learning\nsystem learns the same task. As harrowing an experience it is to teach\na teenager how to drive a car, they do not need to crash into a\nutility pole 10,000 times to learn that utility poles are not\ntraversable. What teenagers learn as children about the world through\nplay and observing other people drive lends to them an understanding\nthat utility poles are to be steered around, a piece of\ncommonsense that our current machine learning systems do not have but\nmust learn from scratch on a case-by-case basis. We, unlike our\nmachines, have a remarkable capacity to transfer what we learn from\none domain to another domain, a capacity fueled in part by our\ncuriosity (Kidd & Hayden 2015). \nViewed from the perspective of the bias-variance trade-off, the\nability to make accurate predictions from sparse data suggests that\nvariance is the dominant source of error but that our cognitive system\noften manages to keep these errors within reasonable limits\n(Gigerenzer & Brighton 2009). Indeed, Gigerenzer and Brighton make\na stronger argument, stating that “the bias-variance dilemma\nshows formally why a mind can be better off with an adaptive toolbox\nof biased, specialized heuristics” (Gigerenzer & Brighton\n2009: 120); see also\n section 7.2.\n However, the bias-variance decomposition is a decomposition of\nsquared loss, which means that the decomposition above depends on how\ntotal error (loss) is measured. There are many loss functions,\nhowever, depending on the type of inference one is making along with\nthe stakes in making it. If one were to use a 0-1 loss function, for\nexample, where all non-zero errors are treated equally—meaning\nthat “a miss as good as a mile”—the decomposition\nabove breaks down. In fact, for 0-1 loss, bias and variance combine\nmultiplicatively (J. Friedman 1997)! A generalization of the\nbias-variance decomposition that applies to a variety of loss\nfunctions \\(\\mathrm{L}(\\cdot)\\), including 0-1 loss, has been offered\nby (Domingos 2000),  \nwhere the original bias-variance decomposition,\n Equation 4,\n appears as a special case, namely when \\(\\mathrm{L}(h) =\n\\textrm{MSE}(h)\\) and \\(\\beta_1 = \\beta_2 = 1\\). \nOur discussion of improper linear models\n (section 2.3)\n mentioned a model that often comes surprisingly close to\napproximating a proper linear model, and our discussion of the\nbias-variance decomposition\n (section 4.2)\n referred to conjectures about how cognitive systems might manage to\nmake accurate predictions with very little data . In this section we\nreview examples of models which deviate from the normative standards\nof global rationality yet yield markedly improved\noutcomes—sometimes even yielding results which are impossible\nunder the conditions of global rationality. Thus, in this section we\nwill survey examples from the statistics of small samples and\ngame theory which point to demonstrable advantages to\ndeviating from global rationality. \nIn a review of experimental results assessing human statistical\nreasoning published in the late 1960s that took stock of research\nconducted after psychology’s full embrace of statistical\nresearch methods\n (section 2.3),\n Petersen and Beach argued that the normative standard of probability\ntheory and statistical optimization methods were “a good first\napproximation for a psychological theory of inference” (Peterson\n& Beach 1967: 42). Petersen and Beach’s view that humans\nwere intuitive statisticians that closely approximate the\nideal standards of homo statisticus fit into a broader\nconsensus at that time about the close fit between the normative\nstandards of logic and intelligent behavior (Newell & Simon 1956,\n1976). The assumption that human judgment and decision-making closely\napproximates normative theories of probability and logic would later\nbe challenged by experimental results by Kahneman and Tversky, and the\nbiases and heuristics program more generally\n (section 7.1). \nAmong Kahneman and Tversky’s earliest findings was that people\ntend to make statistical inferences from samples that are too small,\neven when given the opportunity to control the sampling procedure.\nKahneman and Tversky attributed this effect to a systematic failure of\npeople to appreciate the biases that attend small samples, although\nHertwig and others have offered evidence that samples drawn from a\nsingle population are close to the known limits to working memory\n(Hertwig, Barron et al. 2004). \nOverconfidence can be understood as an artifact of small samples. The\nNaïve Sampling Model (Juslin, Winman, & Hansson\n2007) assumes that agents base judgments on a small sample retrieved\nfrom long-term memory at the moment a judgment is called for, even\nwhen there are a variety of other methods available to the agent. This\nmodel presumes that people are naïve statisticians (Fiedler &\nJuslin 2006) who assume, sometimes falsely, that samples are\nrepresentative of the target population of interest and that sample\nproperties can be used directly to yield accurate estimates of a\npopulation. The idea is that when sample properties are uncritically\ntaken as estimators of population parameters a reasonably accurate\nprobability judgment can be made with overconfidence, even if the\nsamples are unbiased, accurately represented, and correctly processed\nby the cognitive mechanisms of the agent. When sample sizes are\nrestricted, these effects are amplified. \nHowever, sometimes effective behavior is aided by inaccurate judgments\nor cognitively adaptive illusions (Howe 2011). The statistical\nproperties of small samples are a case in point. One feature of small\nsamples is that correlations are amplified, making them easier to\ndetect (Kareev 1995). This fact about small samples, when combined\nwith the known limits to human short-term memory, suggests that our\nworking-memory limits may be an adaptive response to our environment\nthat we exploit at different stages in our lives. Adult short-term\nworking memory is limited to seven items, plus or minus two. For\ncorrelations of 0.5 and higher, Kareev demonstrates that sample sizes\nbetween five and nine are most likely to yield a sample correlation\nthat is greater than the true correlation in the population (Kareev\n2000), making those correlations nevertheless easier to detect.\nFurthermore, children’s short-term memories are even more\nrestricted than adults, thus making correlations in the environment\nthat much easier to detect. Of course, there is no free lunch: this\nsmall-sample effect comes at the cost of inflating estimates of the\ntrue correlation coefficients and admitting a higher rate of false\npositives (Juslin & Olsson 2005). However, in many contexts,\nincluding child development, the cost of error arising from\nunder-sampling may be more than compensated by the benefits from\nsimplifying choice (Hertwig & Pleskac 2008) and accelerating\nlearning. In the spirit of Brunswik’s argument for\nrepresentative experimental design\n (section 3.2),\n a growing body of literature cautions that the bulk of experiments on\nadaptive decision-making are performed in highly simplified\nenvironments that differ in important respects from the natural world\nin which human beings make decisions (Fawcett et al. 2014). In\nresponse, Houston, MacNamara and colleagues argue, we should\nincorporate more environmental complexity in our models. \nPro-social behavior, such as cooperation, is challenging to explain.\nEvolutionary game theory predicts that individuals will forgo a public\ngood and that individual utility maximization will win over collective\ncooperation. Even though this outcome is often seen in economic\nexperiments, in broader society cooperative behavior is pervasive\n(Bowles & Gintis 2011). Why? The traditional evolutionary\nexplanations of human cooperation in terms of reputation,\nreciprocation, and retribution (Trivers 1971; R.\nAlexander 1987), are unsatisfactory because they do not uniquely\nexplain why cooperation is a stable behavior. If a group punishes\nindividuals for failing to perform a behavior, and the punishment\ncosts exceed the benefit of doing that behavior, then this behavior\nwill become stable regardless of its social benefits. Anti-social\nnorms arguably take root by precisely the same mechanisms (Bicchieri\n& Muldoon 2014). Although reputation, reciprocation, and\nretribution may explain how large-scale cooperation is sustained in\nhuman societies, it does not explain how the behavior emerged (Boyd\n& Richerson 2005). Furthermore, cooperation is observed in\nmicroorganisms (Damore & Gore 2012), which suggests that much\nsimpler mechanisms are sufficient for the emergence of cooperative\nbehavior. \nWhereas the 1970s saw a broader realization of the advantages of\nimproper models to yield results that were often good enough\n (section 2.3),\n the 1980s and 1990s witnessed a series of results involving improper\nmodels yielding results that were strictly better than what was\nprescribed by the corresponding proper model. In the early 1980s\nRobert Axelrod held a tournament to empirically test which among a\ncollection of strategies for playing iterations of the\nprisoner’s dilemma performed best in a round-robin competition.\nThe winner was a simple reciprocal altruism strategy called\ntit-for-tat (Rapoport & Chammah 1965), which simply\nstarts off each game cooperating then, on each successive round,\ncopies the strategy the opposing player played in the previous round.\nSo, if your opponent cooperated in this round, then you will cooperate\non the next round; and if your opponent defected this round, then you\nwill defect the next. Subsequent tournaments have shown that\ntit-for-tat is remarkably robust against much more sophisticated\nalternatives (Axelrod 1984). For example, even a rational utility\nmaximizing player playing against an opponent who only plays\ntit-for-tat (i.e., will play tit-for-tat no matter whom he faces) must\nadapt and play tit-for-tat—or a strategy very close to it\n(Kreps, Milgrom, et al. 1982). \nSince tit-for-tat is a very simple strategy, computationally, one can\nbegin to explore a notion of rationality that emerges in a group of\nboundedly rational agents and even see evidence of those bounds\ncontributing to the emergence of pro-social norms. Rubinstein\n(Rubinstein 1986) studied finite automata which play repeated\nprisoner’s dilemmas and whose aims are to maximize average\npayoff while minimizing the number of states of a machine. Finite\nautomata capture regular languages, the lowest-level of the\nChomsky-hierarchy, thus model a type of boundedly rational agents.\nSolutions are a pair of machines in which the choice of the machine is\noptimal for each player at every stage of the game. In an evolutionary\ninterpretation of repeated games, each iteration of Rubinstein’s\ncan be seen as successive generations of agents. This approach is in\ncontrast to Neyman’s study of players of repeated games who can\nonly play mixtures of pure strategies that can be programmed on finite\nautomata, where the number of states that are available is an\nexogenous variable whose value is fixed by the modeler. In\nNeyman’s model, each generation plays the entire game and thus\ntraits connected to reputation can arise (Neyman 1985). More\ngenerally, although cooperation is impossible for infinitely repeated\nprisoner’s dilemmas, for finitely repeated prisoner’s\ndilemmas, a cooperative equilibrium exists for finite automata players\nwhose number of states is less than exponential in the number of\nrounds of the game (Papadimitriou & Yannakakis 1994; Ho 1996). The\ndemands on memory may exceed the psychological capacities of people,\nhowever, even for simple strategies like tit-for-tat played by a\nmoderately sized group of players (Stevens, Volstorf, et al. 2011).\nThese theoretical models showing a number of simple paths to\npro-social behavior may not, on their own, be simple enough to offer\nplausible process models for cooperation. \nOn the heels of work on the effects of time (finite iteration versus\ninfinite iteration) and memory/cognitive ability (finite state\nautomata versus Turing machines), attention soon turned to\nenvironmental constraints. Nowak and May looked at the spatial\ndistribution on a two-dimensional grid of ‘cooperators’\nand ‘defectors’ in iterated prisoner’s dilemmas and\nfound cooperation to emerge among players without memories or\nstrategic foresight (Nowak & May 1992). This work led to the study\nof network topology as a factor in social behavior (Jackson\n2010), including social norms (Bicchieri 2005; J. Alexander 2007),\nsignaling (Skyrms 2003), and wisdom of crowd effects (Golub &\nJackson 2010). When social ties in a network follow a scale-free\ndistribution, the resulting diversity in the number and size of\npublic-goods games is found to promote cooperation, which contributes\nto explaining the emergence of cooperation in communities without\nmechanisms for reputation and punishment (F. Santos, M. Santos, &\nPacheco 2008). \nBut, perhaps the simplest case for bounded rationality are examples of\nagents achieving a desirable goal without any deliberation at all.\nInsects, flowers, and even bacteria exhibit evolutionary stable\nstrategies (Maynard Smith 1982), effectively arriving at Nash\nequilibria in strategic normal form games. If we imagine two species\ninteracting with one another, say honey bees (Apis mellifera)\nand a species of flower, each interaction between a bee and a flower\nhas some bearing on the fitness of each species, where fitness is\ndefined as the expected number of offspring. There is an incremental\npayoff to bees and flowers, possibly negative, after each interaction,\nand the payoffs are determined by the genetic endowments of bees and\nflowers each. The point is that there is no choice exhibited by these\norganisms nor in the models; the process itself selects the traits.\nThe agents have no foresight. There are no strategies that the players\nthemselves choose. The process is entirely mechanical. What emerges in\nthis setting are evolutionary dynamics, a form of bounded\nrationality without foresight. \nOf course, any improper model can misfire. A rule of thumb shared by\npeople the world-over is to not let other people take advantage of\nthem. While this rule works most of the time, it misfires in the\nultimatum game (Güth, Schmittberger, & Schwarze\n1982). The ultimatum game is a two-player game in which one player,\nendowed with a sum of money, is given the task of splitting the sum\nwith another player who may either accept the offer—in which\ncase the pot is accordingly split between the two players—or\nreject, in which case both players receive nothing. People\nreceiving offers of 30 percent or less of the pot are often observed\nto reject the offer, even when players are anonymous and therefore\nwould not suffer the consequences of a negative reputation signal\nassociated with accepting a very low offer. In such cases, one might\nreasonably argue that no proposed split is worse than the status quo\nof zero, so people ought to accept whatever they are offered. \nSimon’s remark that people satisfice when they haven’t the\nwits to maximize (Simon 1957a: xxiv) points to a common assumption,\nthat there is a trade-off between effort and accuracy\n (section 2.1).\n Because the rules of global rationality are expensive to operate\n(Good 1952: 7(i)), people will trade a loss in accuracy for gains in\ncognitive efficiency (Payne, Bettman, & Johnson 1988). The\nmethodology of rational analysis\n (section 3.3)\n likewise appeals to this trade-off. \nThe results surveyed in\n Section 5.2\n caution against blindly endorsing the accuracy-effort trade-off as\nuniversal, a point that has been pressed in the defense of heuristics\nas reasonable models for decision-making (Katsikopoulos 2010; Hogarth\n2012). \nSimple heuristics like Tallying, which is a type of improper\nlinear model\n (section 2.3),\n and Take-the-best\n (section 7.2),\n when tested against linear regression on many data sets, have been\nboth found to outperform linear regression on out-of-sample prediction\ntasks, particularly when the training-sample size is low (Czerlinski\net al. 1999; Rieskamp & Dieckmann 2012). \nAumann advanced five arguments for bounded rationality, which we\nparaphrase here (1997). \nEven in very simple decision problems, most economic agents are not\n(deliberate) maximizers. People do not scan the choice set and\nconsciously pick a maximal element from it. \nEven if economic agents aspired to pick a maximal element from a\nchoice set, performing such maximizations are typically difficult and\nmost people are unable to do so in practice. \nExperiments indicate that people fail to satisfy the basic assumptions\nof rational decision theory. \nExperiments indicate that the conclusions of rational analysis\n(broadly construed to include rational decision theory) do not match\nobserved behavior. \nSome conclusions of rational analysis appear normatively\nunreasonable. \nIn the previous sections we covered the origins of each of\nAumann’s arguments. Here we briefly review each, highlighting\nmaterial in other sections under this context. \nThe first argument, that people are not deliberate maximizers, was a\nworking hypothesis of Simon’s, who maintained that people tend\nto satisfice rather than maximize\n (section 2.2).\n Kahneman and Tversky gathered evidence for the reflection effect in\nestimating the value of options, which is the reason for reference\npoints in prospect theory\n (section 2.4)\n and analogous properties within rank-dependent utility theory more\ngenerally (sections\n 1.2\n and\n 2.4).\n Gigerenzer’s and Hertwig’s groups at the Max Planck\nInstitute for Human Development both study the algorithmic structure\nof simple heuristics and the adaptive psychological mechanisms which\nexplain their adoption and effectiveness; both of their research\nprograms start from the assumption that expected utility theory is not\nthe right basis for a descriptive theory of judgment and\ndecision-making (sections\n 3,\n 5.3, and\n 7.2). \nThe second argument, that people are often unable to maximize even if\nthey aspire to, was made by Simon and Good, among others, and later by\nKahneman and Tversky. Simon’s remarks about the complexity of\n\\(\\Gamma\\)-maxmin reasoning in working out the end-game moves in chess\n (section 2.2)\n is one of many examples he used over the span of his career, starting\nbefore his seminal papers on bounded rationality in the 1950s. The\nbiases and heuristics program spurred by Tversky and\nKahneman’s work in the late 1960s and 1970s\n (section 7.1)\n launched the systematic study of when and why people’s\njudgments deviate from the normative standards of expected utility\ntheory and logical consistency. \nThe third argument, that experiments indicate that people fail to\nsatisfy the basic assumptions of expected utility theory, was known\nfrom early on and emphasized by the very authors who formulated and\nrefined the homo economicus hypothesis\n (section 1)\n and whose names are associated with the mathematical foundations. We\nhighlighted an extended quote from Savage in\n section 1.3,\n but could mention as well a discussion of the theory’s\nlimitations by de Finetti and Savage (1962), and even a closer reading\nof the canonical monographs of each, namely Savage 1954 and de Finetti\n1970. A further consideration, which we discussed in\n section 1.3\n is the demand of logical omniscience in expected utility\ntheory and nearly all axiomatic variants. \nThe fourth argument, regarding the differences between the predictions\nof rational analysis and observed behavior, we addressed in\ndiscussions of Brunswik’s notion of ecological validity\n (section 3.2)\n and the traditional responses to these observations by rational\nanalysis\n (section 3.3).\n The fifth argument, that some of the conclusions of rational analysis\ndo not agree with a reasonable normative standard, was touched on in\nsections\n 1.2,\n 1.3, and the subject of\n section 5. \nImplicit in Aumann’s first four arguments is the notion that\nglobal rationality\n (section 2)\n is a reasonable normative standard but problematic for descriptive\ntheories of human judgment and decision-making\n (section 8).\n Even the literature standing behind Aumann’s fifth argument,\nnamely that there are problems with expected utility theory as a\nnormative standard, nevertheless typically address those shortcomings\nthrough modifications to, or extensions of, the underlying\nmathematical theory\n (section 1.2).\n This broad commitment to optimization methods, dominance reasoning,\nand logical consistency as bedrock normative principles is behind\napproaches that view bounded rationality as optimization under\nconstraints: \nBoundedly rational procedures are in fact fully optimal procedures\nwhen one takes account of the cost of computation in addition to the\nbenefits and costs inherent in the problem as originally posed (Arrow\n2004). \nFor a majority of researchers across disciplines, bounded rationality\nis identified with some form of optimization problem under\nconstraints. \nGerd Gigerenzer is among the most prominent and vocal critics of the\nrole that optimization methods and logical consistency plays in\ncommonplace normative standards for human rationality (Gigerenzer\n& Brighton 2009), especially the role those standards play in\nKahneman and Tversky’s biases and heuristics program (Kahneman\n& Tversky 1996; Gigerenzer 1996). We turn to this debate next, in\n section 7. \nHeuristics are simple rules of thumb for rendering a judgment or\nmaking a decision. Some examples that we have seen thus far include\nSimon’s satisficing, Dawes’s improper linear models,\nRapoport’s tit-for-tat, imitation, and several effects observed\nby Kahneman and Tversky in our discussion of prospect theory. \nThere are nevertheless two views on heuristics that are roughly\nidentified with the research traditions associated with Kahneman and\nTversky’s biases and heuristics program and\nGigerenzer’s fast and frugal heuristics program,\nrespectively. A central dispute between these two research programs is\nthe appropriate normative standard for judging human behavior (Vranas\n2000). According to Gigerenzer, the biases and heuristics program\nmistakenly classifies all biases as errors (Gigerenzer, Todd, et al.\n1999; Gigerenzer & Brighton 2009) despite evidence pointing to\nsome biases in human psychology being adaptive. In contrast, in a rare\nexchange with a critic, Kahneman and Tversky maintain that the dispute\nis merely terminological (Kahneman & Tversky 1996; Gigerenzer\n1996). \nIn this section, we briefly survey each of these two schools. Our aim is\nto give a characterization of each research program rather than an\nexhaustive overview. \nBeginning in the 1970s, Kahneman and Tversky conducted a series of\nexperiments showing various ways that human participants’\nresponses to decision tasks deviate from answers purportedly derived\nfrom the appropriate normative standards (sections\n 2.4\n and\n 5.1).\n These deviations were given names, such as availability\n(Tversky & Kahneman 1973), representativeness, and\nanchoring (Tversky & Kahneman 1974). The set of cognitive\nbiases now numbers into the hundreds, although some are minor variants\nof other well-known effects, such as “The IKEA effect”\n(Norton, Mochon, & Ariely 2012) being a version of the well-known\nendowment effect\n (section 1.2).\n Nevertheless, core effects studied by the biases and heuristics\nprogram, particularly those underpinning prospect theory\n (section 2.4),\n are entrenched in cognitive psychology (Kahneman, Slovic, &\nTversky 1982). \nAn example of a probability judgment task is Kahneman and\nTversky’s Taxi-cab problem, which purports to show that subjects\nneglect base rates. \nA cab was involved in a hit and run accident at night. Two cab\ncompanies, the Green and the Blue, operate in the city. You are given\nthe following data: \n85% of the cabs in the city are Green and 15% are Blue. \nA witness identified the cab as a Blue cab. The court tested his\nability to identify cabs under the appropriate visibility conditions.\nWhen presented with a sample of cabs (half of which were Blue and half\nof which were Green) the witness made correct identifications in 80%\nof the cases and erred in 20% of the cases. \nQuestion: What is the probability that the cab involved in the\naccident was Blue rather than Green? (Tversky & Kahneman 1977:\n3–3). \nContinuing, Kahneman and Tversky report that several hundred subjects\nhave been given slight variations of this question and for all\nversions the modal and median responses was 0.8, instead of the\ncorrect answer of \\(\\bfrac{12}{29}\\) (\\(\\approx 0.41\\)).  \nThus, the intuitive judgment of probability coincides with the\ncredibility of the witness and ignores the relevant base-rate, i.e.,\nthe relative frequency of Green and Blue cabs. (Tversky & Kahneman\n1977: 3–3) \nCritical responses to results of this kind fall into three broad\ncategories. The first types of reply is to argue that the\nexperimenters, rather than the subjects, are in error (Cohen 1981). In\nthe Taxi-cab problem, arguably Bayes sides with the folk (Levi 1983)\nor, alternatively, is inconclusive because the normative standard of\nthe experimenter and the presumed normative standard of the subject\nrequires a theory of witness testimony, neither of which is specified\n(Birnbaum 1979). Other cognitive biases have been ensnared in the\nreplication crises, such as implicit bias (Oswald, Mitchell,\net al. 2013; Forscher, Lai et al. 2017) and social priming\n(Doyen, Klein, et al. 2012; Kahneman 2017\n [Other Internet Resources]). \nThe second response is to argue that there is an important difference\nbetween identifying a normative standard for combining probabilistic\ninformation and applying it across a range of cases\n (section 8.2),\n and it is difficult in practice to determine that a decision-maker is\nrepresenting the task in the manner that the experimenters intend\n(Koehler 1996). Observed behavior that appears to be boundedly\nrational or even irrational may result from a difference between the\nintended specification of a problem and the actual problem subjects\nface. \nFor example, consider the systematic biases in people’s\nperception of randomness reported in some of Kahneman and\nTversky’s earliest work (Kahneman & Tversky 1972). For\nsequences of flips of a fair coin, people expect to see, even for\nsmall samples, a roughly-equal number heads and tails and alternation\nrates between heads and tails that are slightly higher than long-run\naverages (Bar-Hillel & Wagenaar 1991). This effect is thought to\nexplain the gambler’s fallacy, the false belief that a\nrun of heads from an i.i.d. sequence of fair coin tosses will make the\nnext flip more likely to land tails. Hahn and Warren argue that the\nlimited nature of people’s experiences with random sequences is\na better explanation than to view them as cognitive deficiencies.\nSpecifically, people only ever experience finite sequence of outputs\nfrom a randomizer, such as a sequence of fair coin tosses, and the\nlimits to their memory\n (section 5.1)\n of past outcomes in a sequence will mean that not all possible\nsequences of a given length with appear to them with equal\nprobability. Therefore, there is a psychologically plausible\ninterpretation of the question, “is it more likely to see\nHHHT than HHHH from flips of a fair coin?”,\nfor which the correct answer is, “Yes” (Hahn & Warren\n2009). If the gambler’s fallacy boils down to a failure to\ndistinguish between sampling with and without replacement, Hahn and\nWarren’s point is that our intuitive statistical abilities\nacquired through experience alone is unable to make the distinction\nbetween these two sampling methods. Analytical reasoning is\nnecessary. \nConsider also the risky-choice framing effect that was mentioned\nbriefly in\n section 2.4.\n An example is the Asian disease example, \nIf program A is adopted, 200 people will be saved. \nIf program B is adopted, there is a ⅓ probability that\n600 people will be saved, and a ⅔ probability that no people\nwill be saved (Tversky & Kahneman 1981: 453). \nTversky and Kahneman report that a majority of respondents (72\npercent) chose option (a), whereas a majority of respondents (78\npercent) shown an equivalent reformulation of the problem in terms of\nthe number of people who would die rather than survive chose (b). A\nmeta-analysis of subsequent experiments has shown that the framing\ncondition accounts for most of the variance, but it also reveals no\nlinear combination of formally specified predictors that are used in\nprospect theory, cumulative prospect theory, and Markowitz’s\nutility theory, suffices to capture this framing effect\n(Kühberger, Schulte-Mecklenbeck, & Perner 1999). \nThe point to this second line of criticism is not that people’s\nresponses are at variance with the correct normative standard but\nrather that the explanation for why they are at variance will matter\nnot only for assessing the rationality of people but what prescriptive\ninterventions ought to be taken to counter the error. It is rash to\nconclude that people, rather than the peculiarities of the task or the\ntheoretical tools available to us at the moment, are in error. \nLastly, the third type of response is to accept the experimental\nresults but challenge the claim that they are generalizable. In a\ncontrolled replication of Kahneman and Tversky’s lawyer-engineer\nexample (Tversky & Kahneman 1977), for example, a crucial\nassumption is whether the descriptions of the individuals were drawn\nat random, which was tested by having subjects draw blindly from an\nurn (Gigerenzer, Hell, & Blank 1988). Under these conditions,\nbase-rate neglect disappeared. In response to the Linda example\n(Tversky & Kahneman 1983), rephrasing the example in terms of\nwhich alternative is more frequent rather than which\nalternative is more probable reduces occurrences of the\nconjunction fallacy among subjects from 77% to 27% (Fiedler 1988).\nMore generally, a majority of people presented with the Linda example\nappear to interpret ‘probability’ non-mathematically but\nswitch to a mathematical interpretation when asked for frequency\njudgments (Hertwig & Gigerenzer 1999). Ralph Hertwig and\ncolleagues have since noted a variety of other effects involving\nprobability judgments to diminish or disappear when subjects are\npermitted to learn the probabilities through sampling, suggesting that\npeople are better adapted to making a decision by experience\nof the relevant probabilities as opposed to making a decision by their\ndescription (Hertwig, Barron et al. 2004). \nThe Fast and Frugal school and the Biases and Heuristics school both\nagree that heuristics are biased. Where they disagree, and disagree\nsharply, is whether those biases are necessarily a sign of\nirrationality. For the Fast and Frugal program the question is under\nwhat environmental conditions, if any, does a particular heuristic\nperform effectively. If the heuristic’s structural bias is\nwell-suited to the task environment, then the bias of that heuristic\nmay be an advantage for making accurate judgments rather than a\nliability\n (section 4).\n We saw this adaptive strategy before in our discussion of\nBrunswik’s lens model\n (section 3.2),\n although there the bias in the model was to assume that both the\nenvironment and the subject’s responses were linear. The aim of\nthe Fast and Frugal program is to adapt this Brunswikian strategy to a\nvariety of improper models. \nThis general goal of the Fast and Frugal program leads to a second\ndifference between the two schools. Because the Fast and Frugal\nprogram aims to specify the conditions under which a heuristic will\nlead to better outcomes than competing models, heuristics are treated\nas algorithmic models of decision-making rather than descriptions of\nerrant effects; heuristics are themselves objects of study. To that\nend, all heuristics in the fast and frugal tradition are conceived to\nhave three components: (i) a search rule, (ii) a stopping rule, and\n(iii) a decision rule. For example, Take-the-Best (Gigerenzer\n& Goldstein 1996), is a heuristic applied to binary, forced-choice\nproblems. Specifically, the task is to pick the correct option\naccording to an external criterion, such as correctly picking which of\na pair of cities has a larger population, based on cue information\nthat is available to the decision-maker, such as whether she has heard\nof one city but not the other, whether one city is known to have a\nfootball franchise in the professional league, et cetera. Based on\ndata sets, one can compute the predictive validity of different cues,\nand thus derive their weights. Take-the-Best then has the following\nstructure: Search rule: Look up the cue with the highest\ncue-validity; Stopping rule: If the pair of objects have\ndifferent cue values, that is, one is positive and the other negative,\nstop the search. If the cue values are the same, continue searching\ndown the cue-order; Decision rule: Predict that the\nalternative with the positive cue value has the higher\ntarget-criterion value. If all cues fail to discriminate, that is, if\nall cue values are the same, then predict the alternative randomly by\na coin flip. The bias of Take-the-Best is that it ignores relevant\ncues. Another example is tallying, which is a type of\nimproper linear model\n (section 2.3).\n Tallying has the following structure for a binary, forced-choice\ntask: Search rule: Look up cues in a random order;\nStopping rule: After some exogenously determined m\n\\((1 < m \\leq N)\\) of the N available cues are evaluated,\nstop the search; Decision rule: Predict that the alternative\nwith the higher number of positive cue values has the higher\ntarget-criterion value. The bias in tallying is that it ignores cue\nweights. One can see then how models are compared to one another by\nhow they process cues and their performance is evaluated with respect\nto a specified criterion for success, such as the number of correct\nanswers to the city population task. \nBecause Fast and Frugal heuristics are computational models, this\nleads to a third difference between the two schools. Kahneman endorses\nthe System I and System II theory of cognition (Stanovich & West\n2000). Furthermore, Kahneman classifies heuristics as fast, intuitive,\nand non-deliberative System I thinking. Gigerenzer, by contrast, does\nnot endorse the System I and System II hypothesis, thus rejects\nclassifying heuristics as, necessarily, non-deliberative cognitive\nprocesses. Because heuristics are computational models in the Fast and\nFrugal program, in principle each may be used deliberatively by a\ndecision-maker or used by a decision-modeler to explain or predict a\ndecision-maker’s non-deliberative behavior. The Linear Optical\nTrajectory (LOT) heuristic (McBeath, Shaffer, & Kaiser 1995) that\nbaseball players use intuitively, without deliberation, to catch fly\nballs, and which some animals appear to use to intercept prey, is the\nsame heuristic that the “Miracle on the Hudson” airline\npilots used deliberatively to infer that they could not reach an\nairport runway and decided instead to land their crippled plane in the\nHudson river. \nHere are a list of heuristics studied in the Fast and Frugal program\n(Gigerenzer, Hertwig, & Pachur 2011), along with an informal\ndescription for each along with historical and selected contemporary\nreferences. \nImitation. People have a strong tendency to\nimitate the successful members of their communities (Henrich\n& Gil-White 2001).  \nIf some one man in a tribe …invented a new snare or weapon, or\nother means of attack or defense, the plainest self-interest, without\nthe assistance of much reasoning power, would prompt other members to\nimitate him. (Darwin 1871, 155)\n \nImitation is presumed to be fundamental to the speed of cultural\nadaptation including the adoption of social norms\n (section 3.4). \nPreferential Attachment. When given the choice to\nform a new connection to someone, pick the individual with the most\nconnections to others (Yule 1925; Barabási & Albert\n1999; Simon 1955b). \nDefault rules. If there is an applicable default\nrule, and no apparent reason for you to do otherwise, follow the\nrule. (Fisher 1936; Reiter 1980; Thaler & Sustein 2008;\nWheeler 2004). \nSatisficing. Search available options and choose\nthe first one that exceeds your aspiration level. (Simon 1955a;\nHutchinson et al. 2012). \nTallying. To estimate a target criterion, rather\nthan estimate the weights of available cues, instead count the number\nof positive instances (Dawes 1979; Dana & Dawes\n2004). \nOne-bounce Rule (Hey’s Rule B). Have at\nleast two searches for an option. Stop if a price quote is larger than\nthe previous quote. The one-bounce rule plays\n“winning-streaks” by continuing search while you keep\nreceiving a series of lower and lower quotes, but stops as soon as\nyour luck runs out (Hey 1982; Charness & Kuhn 2011). \nTit-for-tat. Begin by cooperating, then respond\nin kind to your opponent; If your opponent cooperates, then cooperate;\nif your opponent defects, then defect (Axelrod 1984; Rapaport,\nSeale, & Colman 2015). \nLinear Optical Trajectory (LOT). To intersect\nwith another moving object, adjust your speed so that your angle of\ngaze remains constant. (McBeath et al. 1995; Gigerenzer\n2007). \nTake-the-best. To decide which of two\nalternatives has a higher value on a specific criterion, (i) first\nsearch the cues in order of their predictive validity; (ii) next, stop\nsearch when a cue is found which discriminates between the\nalternatives; (iii) then, choose the alternative selected by the\ndiscriminating cue. (iv) If all cues fail to discriminate between the\ntwo alternatives, then choose an alternative by chance (Einhorn\n1970; Gigerenzer & Goldstein 1996). \nRecognition: To decide which of two alternatives\nhas a higher value on a specific criterion and one of the two\nalternatives is recognized, choose the alternative that is\nrecognized (Goldstein & Gigerenzer 2002; Davis-Stober, Dana,\n& Budescu 2010; Pachur, Todd, et al. 2012). \nFluency: To decide which of two alternatives has\na higher value on a specific criterion, if both alternatives are\nrecognized but one is recognized faster, choose the alternative that\nis recognized faster (Schooler & Hertwig 2005; Herzog &\nHertwig 2013). \n\\(\\frac{1}{N}\\) Rule: For N feasible\noptions, invest resources equally across all N options\n(Hertwig, Davis, & Sulloway 2002; DeMiguel, Garlappi, & Uppal\n2009). \nThere are three lines of responses to the Fast and Frugal program to\nmention. Take-the-Best is an example of a non-compensatory\ndecision rule, which means that the first discriminating cue cannot be\n“compensated” by the cue-information remaining down the\norder. This condition, when it holds, is thought to warrant taking a\ndecision on the first discriminating cue and ignoring the remaining\ncue-information. The computational efficiency of Take-the-Best is\nsupposed to come from only evaluating a few cues, which number less\nthan 3 on average in benchmarks tests (Czerlinski et al. 1999).\nHowever, all of the cue validities need to be known by the\ndecision-maker and sorted before initiating the search. So,\nTake-the-Best by design treats a portion of the necessary\ncomputational costs to execute the heuristic as exogenous. Although\nthe lower-bound for sorting cues by comparison is \\(O(n \\log n)\\),\nthere is little evidence to suggest that humans sort cues by the most\nefficient sorting algorithms in this class. On the contrary, such\noperations are precisely of the kind that qualitative probability\njudgments demand\n (section 1.2).\n Furthermore, in addition to the costs of ranking cue validities,\nthere is the cost of acquisition and the determination that the\nagent’s estimates are non-compensatory. Although the exact\naccounting of the cognitive effort presupposed is unknown, and argued\nto be lower than critics suggest (Katsikopoulos et al. 2010),\nnevertheless these necessary steps threaten to render Take-the-Best\nnon-compensatory in execution but not in what is necessary prior to\nsetting up the model to execute. \nA second line of criticism concerns the cognitive plausibility of Take\nthe Best (Chater, Oaksford, Nakisa, & Redington 2003). Nearly all\nof the empirical data on the performance characteristics of\nTake-the-Best are by computer simulations, and those original\ncompetitions pitted Take the Best against standard statistical models\n(Czerlinski et al. 1999) but omitted standard machine learning\nalgorithms that Chater, Oaksford and colleagues found performed just\nas well as Take the Best. Since these initial studies, the focus has\nshifted to machine learning, and includes variants of Take-the-Best,\nsuch as “greedy cue permutation” that performs provably\nbetter than the original and is guaranteed to always find accurate\nsolutions when they exist (Schmitt & Martignon 2006). Setting\naside criticisms targeting the comparative performance advantages of\nTake the Best qua decision model, others have questioned the\nplausibility of using Take-the-Best as a cognitive model. For example,\nTake-the-Best presumes that cue-information is processed serially, but\nthe speed advantages of the model translate to an advantage in human\ndecision-making derives only if humans process cue information on a\nserial architecture. If instead people process cue information on a\nparallel cognitive architecture, then the comparative speed advantages\nof Take-the-Best would become moot (Chater et al. 2003). \nThe third line of criticism concerns whether the Fast-and-Frugal\nprogram truly mounts a challenge to the normative standards of\noptimization, dominance-reasoning, and consistency, as advertised.\nTake-the-Best is an algorithm for decision-making that does not\ncomport with the axioms of expected utility theory. For one thing, its\nlexicographic structure violates the Archimedean axiom\n (section 1.1, A2).\n For another, it is presumed to violate the transitivity condition of\nthe Ordering axiom\n (A1).\n Further still, the “less-is-more” effects appear to\nviolate Good’s principle (Good 1967), a central pillar of\nBayesian decision theory, which recommends to delay making a terminal\ndecision between alternative options if the opportunity arises to\nacquire free information. In other words, according canonical\nBayesianism, free advice is a bore but no one ought to turn down free\ninformation (Pedersen & Wheeler 2014). If noncompensatory decision\nrules like Take-the-Best violate Good’s principle, then perhaps\nthe whole Bayesian machinery ought to go (Gigerenzer & Brighton\n2009). \nBut these points merely tell us that attempts to formulate\nTake-the-Best in terms of an ordering of prospects on a real-valued\nindex won’t do, not that ordering and numerical indices have all\ngot to go. As we saw in\n section 1.1,\n there is a long and sizable literature on lexicographic probabilities\nand non-standard analysis, including early work specifically\naddressing non-compensatory nonlinear models (Einhorn 1970). Second,\nGigerenzer argues that “cognitive algorithms…need to meet\nmore important constraints than internal consistency”\n(Gigerenzer & Goldstein 1996), which includes transitivity, and\nelsewhere advocates abandoning coherence as a normative standard\n(Arkes, Gigerenzer, & Hertwig 2016). However, Take-the-Best\npresupposes that cues are ordered by cue validity, which naturally\nentails transitivity, otherwise Take-The-Best could neither be\ncoherently specified nor effectively executed. More generally, the\nFast and Frugal school’s commitment to formulating heuristics\nalgorithmically and implementing them as computational models commits\nthem to the normative standards of optimization, dominance reasoning,\nand logical consistency. \nFinally, Good’s principle states that a decision-maker facing a\nsingle-person decision-problem cannot be worse (in expectation) from\nreceiving free information. Exceptions are known in game theory\n(Osborne 2003: 283), however, that involve asymmetric information\namong two or more decision-makers. But there is also an exception for\nsingle-person decision-problems involving indeterminate or imprecise\nprobabilities (Pedersen & Wheeler 2015). The point is that\nGood’s principle is not a fundamental principle of probabilistic\nmethods, but instead is a specific result that holds for the canonical\ntheory of single-person decision-making with determinate\nprobabilities. \nThe rules of logic, the axioms of probability, the principles of\nutility theory—humans flout them all, and do so as a matter of\ncourse. But are we irrational to do so? That depends on what being\nrational amounts to. For a Bayesian, any qualitative comparative\njudgment that does not abide by the axioms of probability is, by\ndefinition, irrational. For a baker, any recipe for bread that is\nequal parts salt and flour is irrational, even if coherent. Yet\nBayesians do not war with bakers. Why? Because bakers are satisfied\nwith the term ‘inedible’ and do not aspire to commandeer\n‘irrational’. \nThe two schools of heuristics\n (section 7)\n reach sharply different conclusions about human rationality. Yet,\nunlike bakers, their disagreement involves the meaning of\n‘rationality’ and how we ought to appraise human judgment\nand decision making. The “rationality wars” are not the\nresult of “rhetorical flourishes” concealing a broad\nconsensus (Samuels, Stich, & Bishop 2002), but substantive\ndisagreements\n (section 7.2)\n that are obscured by ambiguous use of terms like\n‘rationality’. \nIn this section we first distinguish seven different notions of\nrationality, highlighting the differences in aim, scope, standards of\nassessment, and differences in the objects of evaluation. We then turn\nto consider two importantly different normative standards\nused in bounded rationality, followed by an example, the\nperception-cognition gap, illustrating how slight variations\nof classical experimental designs in the biases and heuristics\nliterature change both the results and the normative standards used to\nevaluate those results. \nWhile Aristotle is credited with saying that humans are rational,\nBertrand Russell later confessed to searching a lifetime in vain for\nevidence in Aristotle’s favor. Yet ‘rationality’ is\nwhat Marvin Minsky called a suitcase word, a term that needs to be\nunpacked before getting anywhere. \nOne meaning, central to decision theory, is coherence, which\nis merely the requirement that your commitments not be self-defeating.\nThe subjective Bayesian representation of rational preference over\noptions as inequalities in subjective expected utility delivers\ncoherence by applying a dominance principle to (suitably structured)\npreferences. A closely related application of dominance reasoning is\nthe minimization of expected loss (or maximization of expected gain in\neconomics) according to a suitable loss function, which may even be\nasymmetric (Elliott, Komunjer, & Timmermann 2005) or applied to\nradically restricted agents, such as finite automata (Rubinstein\n1986). Coherence and dominance reasoning underpin expected utility\ntheory\n (section 1.1),\n too. \nA second meaning of rationality refers to an interpretive stance or\ndisposition that we take to understand the beliefs, desires, and\nactions of another person (Dennett 1971) or to understand anything\nthey might say in a shared language (Davidson 1974). On this view,\nrationality refers to a bundle of assumptions we grant to another\nperson in order to understand their behavior, including speech. When\nwe offer a reason-giving explanation for another person’s\nbehavior, we take such a stance. If I say “the driver\nlaughed because she made a joke” you would not get far in\nunderstanding me without granting to me, and even this imaginary\ndriver and woman, a lot. So, in contrast to the lofty normative\nstandards of coherence that few if any mortals meet, the standards of\nrationality associated with an interpretive stance are met by\npractically everyone. \nA third meaning of rationality, due to Hume (1738), applies to your\nbeliefs, appraising them in how well they are calibrated with your\nexperience. If in your experience the existence of one thing is\ninvariably followed by an experience of another, then believing that\nthe latter follows the former is rational. We might even go so far as\nto say that your expectation of the latter given your experience of\nthe former is rational. This view of rationality is an evaluation of a\nperson’s commitments, like coherence standards; but unlike\ncoherence, Hume’s notion of rationality seeks to tie the\nrational standing of a belief directly to evidence from the world.\nMuch of contemporary epistemology endorses this concept of rationality\nwhile attempting to specify the conditions under which we can\ncorrectly attribute knowledge to someone. \nA fourth meaning of rationality, called substantive\nrationality by Max Weber (1905), applies to the evaluation of\nyour aims of inquiry. Substantive rationality invokes a Kantian\ndistinction between the worthiness of a goal, on the one hand, and how\nwell you perform instrumentally in achieving that goal, on the other.\nAiming to count the blades of grass in your lawn is arguably not a\nrational end to pursue, even if you were to use the instruments of\nrationality flawlessly to arrive at the correct count. \nA fifth meaning of rationality, due to Peirce (1955) and taken up by\nthe American pragmatists, applies to the process of changing a belief\nrather than the Humean appraisal of a currently held belief. On\nPeirce’s view, people are plagued by doubt not by belief; we\ndon’t expend effort testing the sturdiness of our beliefs, but\nrather focus on those that come into doubt. Since inquiry is pursued\nto remove the doubts we have, not certify the stable beliefs we\nalready possess, principles of rationality ought to apply to the\nmethods for removing doubt (Dewey 1960). On this view, questions of\nwhat is or is not substantively rational will be answered by the\ninquirer: for an agronomist interested in grass cover sufficient to\ncrowd out an invasive weed, obtaining the grass-blade count of a lawn\nmight be a substantively rational aim to pursue. \nA sixth meaning of rationality appeals to an organism’s\ncapacities to assimilate and exploit complex information and revise or\nmodify it when it is no longer suited to task. The object of\nrationality according to this notion is effective behavior.\nJonathan Bennett discusses this notion of rationality in his case\nstudy of bees: \nAll our prima facie cases of rationality or intelligence were\nbased on the observation that some creature’s behaviour was in\ncertain dependable ways successful or appropriate or apt, relative to\nits presumed wants or needs. …There are canons of\nappropriateness whereby we can ask whether an apian act is appropriate\nnot to that which is particular and present to the bee but rather to\nthat which is particular and past or to that which is not particular\nat all but universal. (Bennett 1964: 85) \nLike Hume’s conception, Bennett’s view ties rationality to\nsuccessful interactions with the world. Further, like the pragmatists,\nBennett includes for appraisal the dynamic process rather than simply\nthe synchronic state of one’s commitments or the current merits\nof a goal. But unlike the pragmatists, Bennett conceives of\nrationality to apply to a wider range of behavior than the logic of\ndeliberation, inquiry, and belief change. \nA seventh meaning of rationality resembles the notion of coherence by\ndefining rationality as the absence of a defect. For Bayesians,\nsure-loss is the epitome of irrationality and coherence is\nsimply its absence. Sorensen has suggested a generalization of this\nstrategy, one where rationality is conceived as the absence of\nirrationality tout court, just as cleanliness is the absence\nof dirt. Yet, owing to the long and varied ways that irrationality can\narise, a consequence of this view is that there then would be no\nunified notion of rationality to capture the idea of thinking as one\nought to think (Sorensen 1991). \nThese seven accounts of rationality are neither exhaustive nor\ncomplete. But they suffice to illustrate the range of differences\namong rationality concepts, from the objects of evaluation and the\nstandards of assessment, to the roles, if any at all, that rationality\nis conceived to play in reasoning, planning, deliberation,\nexplanation, prediction, signaling, and interpretation. One\nconsequence of this hodgepodge of rationality concepts is a pliancy in\nthe attribution of irrationality that resembles Victorian methods for\ndiagnosing the vapors. The time may have come to retire talk of\nrationality altogether, or to demand a specification of the objects of\nevaluation, the normative standards to be used for assessment, and\nrequire ample attention to the implications that follow from those\ncommitments. \nWhat are the standards against which our judgments and decisions ought\nto be evaluated? A property like systematic bias may be viewed as a\nfault or an advantage depending on how outcomes are scored\n (section 4).\n A full reckoning of the costs of operating a decision procedure may\ntip the balance in favor of a model that is sub-optimal when costs are\nnot considered, even when there is agreement of how outcomes are to be\nscored\n (sections 2.1).\n Desirable behavior, such as prosocial norms, may be impossible within\nan idealized model but commonplace in several different types of\nnon-idealized models\n (section 5.2). \nAccounts of bounded rationality typically invoke one of two types of\nnormative standards, a coherence standard or an accuracy standard.\nAmong the most important insights from the study of boundedly rational\njudgment and decision making is that, not only is it possible to meet\none standard without meeting the other, but meeting one standard may\ninhibit meeting the other. \nCoherence standards in bounded rationality typically appeal to\nprobability, statistical decision theory, or propositional logic. The\n“standard picture” of rational reasoning, according to\nEdward Stein, \nis to reason in accordance with principles of reasoning that are based\non rules of logic, probability theory, and so forth. If the standard\npicture of reasoning is right, principles of reasoning that are based\non such rules are normative principles of reasoning, namely\nthey are principles we ought to reason in accordance with.\n(Stein 1996: 1.2) \nThe coherence standards of logic and probability are usually invoked when\nthere are experimental results purporting to violate those\nstandards, particularly in the biases and heuristics literature\n (section 7.1).\n However, little is said about how and when our reasoning ought to be\nin accordance with these standards or even what, precisely, the applicable \nnormative standards of logic and probability are. Stein\ndiscusses the logical rule of And-Elimination and a normative\nprinciple for belief that it supports, one where believing the\nconjunction birds sing and bees waggle commits you rationally\nto believing each conjunct. Yet Stein switches to probability to\ndiscuss what principle ought to govern conjoining two beliefs.\nWhy? \nPropositional logic and probability are very different formalisms \n(Haenni, Romeijn, Wheeler, & Williamson 2011). For one thing, the\ntruth-functional semantics of logic is compositional whereas\nprobability is not compositional, except when events are\nprobabilistically independent. Why then is the elimination rule from\nlogic and the introduction rule from probability the standard rather\nthan the elimination rule from probability (i.e., marginalization) and the\nintroduction rule from logic (i.e., adjunction)? Answering this question\nrequires a positive account of what “based on”,\n“anchored in”, or other metaphorical relationships amount\nto. But, in appeals to principles of reasoning, typically there is no analog to the\nrepresentation theorems of expected utility theory \n (section 1.1)\n specifying the relationship between qualitative judgments and their\nlogical or numerical representation, and no account of the conditions\nunder which such relationships hold. \nThe second type of normative standard assesses the accuracy of a\njudgment or decision making process, where the focus is getting the\ncorrect answer. Consider the accuracy of a categorical judgment, such\nas predicting whether a credit-card transaction is fraudulent (\\(Y =\n1\\)) or legitimate (\\(Y = 0\\)). Classification accuracy is\nthe number of correct predictions from all predictions made, which is\noften expressed as a ratio. But classification accuracy can yield a\nmisleading assessment. For example, a method that always reported\ntransactions as legitimate, \\(Y = 0\\), would in fact yield a very high\naccuracy score (>97%) due to the very low rate (<3%) of\nfraudulent credit card transactions. The problem here is that\nclassification accuracy is a poor metric for problems that involve\nimbalanced classes with few positive instances (i.e., few cases where\n\\(Y=1\\)). More generally, a model with no predictive power can have\nhigh accuracy, and a model with comparatively lower accuracy can have\ngreater predictive power. This observation is referred to as the\naccuracy paradox. \nThe accuracy paradox is one motivation for introducing other measures\nof predictive performance. For our fraud detection problem there are\ntwo ways your prediction can be correct and two ways it can be wrong.\nA prediction can be correct by predicting that \\(Y=1\\) when in fact a\ntransaction is fraudulent (a true positive) or predicting\n\\(Y=0\\) when in fact a transaction is legitimate (a true\nnegative). Correspondingly, one may err by either predicting\n\\(Y=1\\) when in fact \\(Y=0\\) (a false positive) or predicting\n\\(Y=0\\) when in fact a transaction is fraudulent (a false\nnegative). These four possibilities are presented in the\nfollowing two-by-two contingency table, which is sometimes referred to\nas a confusion matrix: \nFor a binary classification problem involving N examples, each\nprediction will fall into one of these four categories. The\nperformance of your classifier with respect to those N examples\ncan then be assessed. A perfectly inaccurate classifier will have all\nzeros in the diagonal; a perfectly accurate classifier will have all\nzeros in the counterdiagonal. The precision of your\nclassifier is the ratio of true positives to all positive predictions,\nthat is true positives / (true positives + false\npositives). The recall of your classifier is the ratio\nof true positives to all true predictions, that is true\npositives / (true positives + false\nnegatives). \nThere are two points to notice. The first is that in practice there is\ntypically a trade-off between precision and recall, and the costs to\nyou of each will vary from one problem to another. A trade-off of\nprecision and recall that suits detecting credit card fraud may not\nsuit detecting cancer, even if the frequencies of positive instances\nare identical. The point of training a classifier on known data is to\nmake predictions on out of sample instances. So, tuning your\nclassifier to yield a suitable trade-off between precision and recall\nin your training data is no guarantee that you will see this trade-off\ngeneralize. \nThe moral is that to evaluate the performance of your classifier it is\nnecessary to specify the purpose for making the classification and\neven then good performance on your training data may not generalize.\nNone of this is antithetical to coherence reasoning per se, as we are\nmaking comparative judgments and reasoning by dominance. But putting\nthe argument in terms of coherence changes the objects of\nevaluation, moving from the point of view from the first person \ndecision maker to that of a third person decision modeler. \nDo human beings systematically violate the norms of probability and\nstatistics? Petersen and Beach (1967) thought not. On their view human\nbeings are intuitive statisticians\n (section 5.1),\n so probability theory and statistics are a good, first approximation\nof human judgment and decision making. Yet, just as their optimistic\nreview appeared to cement a consensus view about human rationality,\nAmos Tversky and Daniel Kahneman began their work to undo it. People\nare particularly bad at probability and statistics, the heuristics and\nbiases program\n (section 7.1)\n found, so probability theory, statistics, and even logic do not offer\na good approximation of human decision making. One controversy over\nthese negative findings concerns the causes of those\neffects—whether the observed responses point to minor flaws in\notherwise adaptive human behavior or something much less charitable\nabout our habits and constitution. \nIn contrast to this poor showing on cognitive tasks, people are\ngenerally thought to be optimal or near-optimal in performing\nlow-level motor control and perception tasks. Planning goal-directed\nmovement, like pressing an elevator button with your finger or placing\nyour foot on a slippery river stone, requires your motor control\nsystem to pick one among a dizzying number of possible movement\nstrategies to achieve your goal while minimizing biomechanical costs\n(Trommershäuser, Maloney, & Landy 2003). The loss function\nthat our motor control system appears to use increases approximately\nquadratically with error for small errors but significantly less for\nlarge errors, suggesting that our motor control system is also robust\nto outliers (Körding & Wolpert 2004). What is more, advances\nin machine learning have been guided by treating human performance\nerrors for a range of perception tasks as proxies for Bayes error,\nyielding an observable, near-perfect normative standard. Unlike\ncognitive decisions, there is very little controversy concerning the\noverall optimality of our motor-perceptual decisions. This difference\nbetween high-level and low-level decisions is called the\nperception-cognition gap. \nSome view the perception-cognition gap as evidence for the claim that\npeople use fundamentally different strategies for each type of task\n (section 7.2).\n An approximation of an optimal method is not necessarily an optimal\napproximation of that method, and the study of cognitive judgments and\ndeliberative decision-making is led astray by assuming otherwise\n(Mongin 2000). Another view of the perception-cognition gap is that it\nis largely an artifact of methodological differences across studies\nrather than a robust feature of human behavior. We review evidence for\nthis second argument here. \nClassical studies of decision-making present choice problems to\nsubjects where probabilities are described. For example, you might be\nasked to choose the prospect of winning €300 with probability\n0.25 or the prospect of winning €400 with probability 0.2. Here,\nsubjects are given a numerical description of probabilities, are\ntypically asked to make one-shot decisions without feedback, and their\nresponses are found to deviate from the expected utility hypothesis.\nHowever, in motor control tasks, subjects have to use internal,\nimplicit estimates of probabilities, often learned with feedback, and\nthese internal estimates are near optimal. Are perceptual-motor\ncontrol decisions better because they provide feedback whereas\nclassical decision tasks do not, or are perceptual-motor control\ndecisions better because they are non-cognitive? \nJarvstad et al. (2013) explored the robustness of the\nperception-cognition gap by designing (a) a finger-pointing task that\ninvolved varying target sizes on a touch-screen computer display; (b)\nan arithmetic learning task involving summing four numbers and\naccepting or rejecting a proposed answer with a target tolerance,\nwhere the tolerance range varied from problem to problem, analogous to\nthe width of the target in the motor-control task; and (c) a standard\nclassical probability judgment task that involved computing the\nexpected value of two prospects. The probability information across\nthe tasks was in three formats: low-level, high-level, and classical,\nrespectively. \nOnce confounding factors across the three types of tasks are\ncontrolled for, Jarvstad et al.’s results suggest that (i) the\nperception-cognition gap is largely explained by differences in how\nperformance is assessed; (ii) the decisions by experience vs\ndecisions by description gap (Hertwig, Barron et al. 2004) is\ndue to assuming that exogenous objective probabilities and subjective\nprobabilities match; (iii) people’s ability to make high-level\ndecisions is better than the biases and heuristics literature suggests\n (section 7.1);\n and (iv) differences between subjects are more important for\npredicting performance than differences between the choice tasks\n(Jarvstad et al. 2013). \nThe upshot, then, is that once the methodological differences are\ncontrolled for, the perception-cognition gap appears to be an artifact\nof two different normative standards applied to tasks. If the\nstandards applied to assessing perceptual-motor tasks are applied to\nclassical cognitive decision-making tasks, then both appear to perform\nwell. If instead the standards used for assessing the classical\ncognitive tasks are applied to perceptual-motor tasks, then both will\nappear to perform poorly.","contact.mail":"g.wheeler@fs.de","contact.domain":"fs.de"}]
