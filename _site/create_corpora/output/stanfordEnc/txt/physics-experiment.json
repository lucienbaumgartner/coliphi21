[{"date.published":"1998-10-05","date.changed":"2019-10-16","url":"https://plato.stanford.edu/entries/physics-experiment/","author1":"Allan Franklin","author1.info":"http://spot.colorado.edu/~franklia/","author2.info":"http://slobodanperovic.weebly.com/","entry":"physics-experiment","body.text":"\n\n\n\nPhysics, and natural science in general, is a reasonable enterprise\nbased on valid experimental evidence, criticism, and rational\ndiscussion. It provides us with knowledge of the physical world, and it\nis experiment that provides the evidence that grounds this knowledge.\nExperiment plays many roles in science. One of its important roles is\nto test theories and to provide the basis for scientific\n knowledge.[1]\n It can also call for a new theory, either by\nshowing that an accepted theory is incorrect, or by exhibiting a new\nphenomenon that is in need of explanation. Experiment can provide hints\ntoward the structure or mathematical form of a theory and it can\nprovide evidence for the existence of the entities involved in our\ntheories. Finally, it may also have a life of its own, independent of\ntheory. Scientists may investigate a phenomenon just because it looks\ninteresting. Such experiments may provide evidence for a future theory\nto explain. [Examples of these different roles will be presented\nbelow.] As we shall see below, a single experiment may play several of\nthese roles at once.\n\n\n\nIf experiment is to play these important roles in science then we\nmust have good reasons to believe experimental results, for science is\na fallible enterprise. Theoretical calculations, experimental results,\nor the comparison between experiment and theory may all be wrong.\nScience is more complex than “The scientist proposes, Nature disposes.”\nIt may not always be clear what the scientist is proposing. Theories\noften need to be articulated and clarified. It also may not be clear\nhow Nature is disposing. Experiments may not always give clear-cut\nresults, and may even disagree for a time.\n\n\n\nIn what follows, the reader will find an epistemology of experiment,\na set of strategies that provides reasonable belief in experimental\nresults. Scientific knowledge can then be reasonably based on these\nexperimental results.\n\n\n\nThe 17th century witnessed the first philosophical reflections on the\nnature of experimentation. This should not be surprising given that\nexperiment was emerging as a central scientific tool at the time. The\naim of these reflections was to uncover why nature reveals its hidden\naspects to us when we force experimental methods upon it.  Some natural philosophers believed that scientific knowledge was\nlittle more than the proper application of observational and\nexperimental techniques on natural phenomena. Francis Bacon went so\nfar as to claim that it was possible to perform what he called a\ncrucial experiment (experimentum crucis), an ideal experiment of sorts\nthat can determine alone which of two rival hypotheses is correct. And\neven some of the giants of modern science such as Newton subscribed to\nthe view that scientific theories are directly induced from\nexperimental results and observations without the help of untested\nhypotheses. It is little wonder, then, that many natural philosophers\nthought that experimental techniques and their proper application\nshould be a primary object of philosophical study of science. \n\nThomas Kuhn and Paul Feyerabend vigorously criticized this view. They\nargued that observations and experimental results are already part of\na theoretical framework and thus cannot confirm a theory\nindependently. Nor there is a theory-neutral language for capturing\nobservations. Even a simple reading of a mercury thermometer\ninevitably depends on a theoretically-charged concept of\ntemperature. In short, the evidence is always theory-laden. \n\nYet neither the proponents of logical positivism nor their critics\never attempted to explain the nature of experimentation that produces\nall-important observational statements. And the reason for this was\nvery simple: they didn’t think that there was anything interesting to\nexplain. Their views on the relationship between theory and evidence\nwere diametrically opposed, but they all found only the final product\nof experimentation, namely observational statements, philosophically\ninteresting. As a result, the experimental process itself was set\naside in their philosophical study of science. This has gradually\nchanged only with the advent of New Experimentalism, with Ian\nHacking’s work at its forefront. \nYet not everybody agreed.  Hobbes, for instance pointed out that human\nreason preceded experimental techniques and their application. He\nthought that human reasoning reveals to us the natural law, and\ncriticized Boyle’s optimism regarding experimental method’s ability to\nreveal it (Shapin and Schaffer 1984). Doesn’t human reason guide\nexperimenter’s actions, in the way it leads us to choose data and\nsamples, and the way it allows us to interpret them, after all? If so,\nwe should focus on the philosophical study of reason and theoretical\nscientific reasoning rather than on the study of experimental\ntechniques and their applications. \n  \nThis vigorous early debate in many ways anticipated the main points of\ndisagreement in debates to come. Yet the philosophical interest in\nexperimentation almost completely lost its steam at the end of the\n19th century and did not recover until fairly late in the 20th\ncentury. \n\nDuring that period philosophers turned much of their attention to the\nstudy of the logical structure of scientific theories and its\nconnection to evidence. The tenets of logical positivism influenced\nthis area of investigation — as well as philosophy more\ngenerally — at the time. One of these tenets stated that\nobservational and theoretical propositions in science are\nseparable. My readings of the gradation on the scale of a mercury\nthermometer can be separated from rather complicated theoretical\nstatements concerning heat transfer and the theoretical concept of\ntemperature. \n\nIn fact, not only can one separate theory and observation, but the\nformer is considered justified only in light of its correspondence\nwith the latter. The theory of heat transfer is confirmed by\npropositions originating in the kind of readings I perform on my\nmercury thermometer. Thus, observational propositions are simply a\nresult of an experiment or a set of observations a scientist performs\nin order to confirm or refute a theory. \n\nIt has been almost four decades since Ian Hacking asked, “Do we see\nthrough a microscope?” (Hacking 1981). Hacking’s question really\nasked how do we come to believe in an experimental result obtained\nwith a complex experimental apparatus? How do we distinguish between a\nvalid\n result[2]\n and an artifact created by that apparatus?\nIf experiment is to play all of the important roles in science\nmentioned above and to provide the evidential basis for scientific\nknowledge, then we must have good reasons to believe in those results.\nHacking provided an extended answer in the second half of\nRepresenting and Intervening (1983). He pointed out that even\nthough an experimental apparatus is laden with, at the very least, the\ntheory of the apparatus, observations remain robust despite changes in\nthe theory of the apparatus or in the theory of the phenomenon. His\nillustration was the sustained belief in microscope images despite the\nmajor change in the theory of the microscope when Abbe pointed out the\nimportance of diffraction in its operation. One reason Hacking gave for\nthis is that in making such observations the experimenters\nintervened—they manipulated the object under observation. Thus, in\nlooking at a cell through a microscope, one might inject fluid into the\ncell or stain the specimen. One expects the cell to change shape or\ncolor when this is done. Observing the predicted effect strengthens our\nbelief in both the proper operation of the microscope and in the\nobservation. This is true in general. Observing the predicted effect of\nan intervention strengthens our belief in both the proper operation of\nthe experimental apparatus and in the observations made with it.  \n\nHacking also discussed the strengthening of one’s belief in an\nobservation by independent confirmation. The fact that the same\npattern of dots—dense bodies in cells—is seen with\n“different” microscopes, (e.g. ordinary, polarizing,\nphase-contrast, fluorescence, interference, electron, acoustic etc.)\nargues for the validity of the observation.  One might question\nwhether “different” is a theory-laden term. After all, it\nis our theory of light and of the microscope that allows us to\nconsider these microscopes as different from each other. Nevertheless,\nthe argument holds. Hacking correctly argues that it would be a\npreposterous coincidence if the same pattern of dots were produced in\ntwo totally different kinds of physical systems. Different apparatuses\nhave different backgrounds and systematic errors, making the\ncoincidence, if it is an artifact, most unlikely. If it is a correct\nresult, and the instruments are working properly, the coincidence of\nresults is understandable. \n\nHacking’s answer is correct as far as it goes. It is, however,\nincomplete. What happens when one can perform the experiment with only\none type of apparatus, such as an electron microscope or a radio\ntelescope, or when intervention is either impossible or extremely\ndifficult? Other strategies are needed to validate the\n observation.[3]\n These may include: \n\nThese strategies along with Hacking’s intervention and independent\nconfirmation constitute an epistemology of experiment. They provide us\nwith good reasons for belief in experimental results, They do not,\nhowever, guarantee that the results are correct. There are many\nexperiments in which these strategies are applied, but whose results\nare later shown to be incorrect (examples will be presented below).\nExperiment is fallible. Neither are these strategies exclusive or\nexhaustive. No single one of them, or fixed combination of them,\nguarantees the validity of an experimental result. Physicists use as\nmany of the strategies as they can conveniently apply in any given\nexperiment.  \n\nIn How Experiments End (1987), Peter Galison extended the\ndiscussion of experiment to more complex situations. In his histories\nof the measurements of the gyromagnetic ratio of the electron, the\ndiscovery of the muon, and the discovery of weak neutral currents, he\nconsidered a series of experiments measuring a single quantity, a set\nof different experiments culminating in a discovery, and two high-\nenergy physics experiments performed by large groups with complex\nexperimental apparatus.  \n\nGalison’s view is that experiments end when the experimenters\nbelieve that they have a result that will stand up in court—a result\nthat I believe includes the use of the epistemological strategies\ndiscussed earlier. Thus, David Cline, one of the weak neutral-current\nexperimenters remarked, “At present I don’t see how to make these\neffects [the weak neutral current event candidates] go away” (Galison,\n1987, p. 235). \n\nGalison emphasizes that, within a large experimental group, different\nmembers of the group may find different pieces of evidence most\nconvincing. Thus, in the Gargamelle weak neutral current experiment,\nseveral group members found the single photograph of a\nneutrino-electron scattering event particularly important, whereas for\nothers the difference in spatial distribution between the observed\nneutral current candidates and the neutron background was decisive.\nGalison attributes this, in large part, to differences in experimental\ntraditions, in which scientists develop skill in using certain types\nof instruments or apparatus. In particle physics, for example, there\nis the tradition of visual detectors, such as the cloud chamber or the\nbubble chamber, in contrast to the electronic tradition of Geiger and\nscintillation counters and spark chambers. Scientists within the\nvisual tradition tend to prefer “golden events” that\nclearly demonstrate the phenomenon in question, whereas those in the\nelectronic tradition tend to find statistical arguments more\npersuasive and important than individual events. (For further\ndiscussion of this issue see Galison (1997)). \n\nGalison points out that major changes in theory and in experimental\npractice and instruments do not necessarily occur at the same time.\nThis persistence of experimental results provides continuity across\nthese conceptual changes. Thus, the experiments on the gyromagnetic\nratio spanned classical electromagnetism, Bohr’s old quantum theory,\nand the new quantum mechanics of Heisenberg and Schrodinger. Robert\nAckermann has offered a similar view in his discussion of scientific\ninstruments. \n\nGalison also discusses other aspects of the interaction between\nexperiment and theory. Theory may influence what is considered to be a\nreal effect, demanding explanation, and what is considered background.\nIn his discussion of the discovery of the muon, he argues that the\ncalculation of Oppenheimer and Carlson, which showed that showers were\nto be expected in the passage of electrons through matter, left the\npenetrating particles, later shown to be muons, as the unexplained\nphenomenon. Prior to their work, physicists thought the showering\nparticles were the problem, whereas the penetrating particles seemed to\nbe understood. \n\nThe role of theory as an “enabling theory,” (i.e., one that allows\ncalculation or estimation of the size of the expected effect and also\nthe size of expected backgrounds) is also discussed by Galison. (See\nalso (Franklin 1995) and the discussion of the Stern-Gerlach\nexperiment below). Such a theory can help to determine whether an\nexperiment is feasible. Galison also emphasizes that elimination of\nbackground that might simulate or mask an effect is central to the\nexperimental enterprise, and not a peripheral activity. In the case of\nthe weak neutral current experiments, the existence of the currents\ndepended crucially on showing that the event candidates could not all\nbe due to neutron\n background.[6] \n\nThere is also a danger that the design of an experiment may preclude\nobservation of a phenomenon. Galison points out that the original\ndesign of one of the neutral current experiments, which included a muon\ntrigger, would not have allowed the observation of neutral currents. In\nits original form the experiment was designed to observe charged\ncurrents, which produce a high energy muon. Neutral currents do not.\nTherefore, having a muon trigger precluded their observation. Only\nafter the theoretical importance of the search for neutral currents was\nemphasized to the experimenters was the trigger changed. Changing the\ndesign did not, of course, guarantee that neutral currents would be\nobserved. \n\nGalison also shows that the theoretical presuppositions of the\nexperimenters may enter into the decision to end an experiment and\nreport the result. Einstein and de Haas ended their search for\nsystematic errors when their value for the gyromagnetic ratio of the\nelectron, \\(g = 1\\), agreed with their theoretical model of\norbiting electrons. This effect of presuppositions might cause one to\nbe skeptical of both experimental results and their role in theory\nevaluation. Galison’s history shows, however, that, in this case, the\nimportance of the measurement led to many repetitions of the\nmeasurement. This resulted in an agreed-upon result that disagreed with\ntheoretical expectations. \n\nRecently, Galison has modified his views. In Image and Logic,\nan extended study of instrumentation in 20th-century high-energy\nphysics, Galison (1997) has extended his argument that there are two\ndistinct experimental traditions within that field—the visual (or\nimage) tradition and the electronic (or logic) tradition.  The image\ntradition uses detectors such as cloud chambers or bubble chambers,\nwhich provide detailed and extensive information about each individual\nevent. The electronic detectors used by the logic tradition, such as\ngeiger counters, scintillation counters, and spark chambers, provide\nless detailed information about individual events, but detect more\nevents. Galison’s view is that experimenters working in these two\ntraditions form distinct epistemic and linguistic groups that rely on\ndifferent forms of argument. The visual tradition emphasizes the\nsingle “golden” event. “On the image side resides a\ndeep-seated commitment to the ‘golden event’: the single\npicture of such clarity and distinctness that it commands\nacceptance.” (Galison, 1997, p. 22) “The golden event was\nthe exemplar of the image tradition: an individual instance so\ncomplete and well defined, so ‘manifestly’ free of\ndistortion and background that no further data had to be\ninvolved” (p. 23). Because the individual events provided in the\nlogic detectors contained less detailed information than the pictures\nof the visual tradition, statistical arguments based on large numbers\nof events were required. \n\nKent Staley (1999) disagrees. He argues that the two traditions are\nnot as distinct as Galison believes: \n\nStaley believes that although there is certainly epistemic\ncontinuity within a given tradition, there is also a continuity between\nthe traditions. This does not, I believe, mean that the shared\ncommitment comprises all of the arguments offered in any particular\ninstance, but rather that the same methods are often used by both\ncommunities. Galison does not deny that statistical methods are used in\nthe image tradition, but he thinks that they are relatively\nunimportant. “While statistics could certainly be used within the image\ntradition, it was by no means necessary for most applications”\n(Galison, 1997, p. 451). In contrast, Galison believes that arguments\nin the logic tradition “were inherently and inalienably statistical.\nEstimation of probable errors and the statistical excess over\nbackground is not a side issue in these detectors—it is central to the\npossibility of any demonstration at all” (p. 451). \n\nAlthough a detailed discussion of the disagreement between Staley\nand Galison would take us too far from the subject of this essay, they\nboth agree that arguments are offered for the correctness of\nexperimental results. Their disagreement concerns the nature of those\narguments. (For further discussion see Franklin, (2002), pp. 9–17). \n\nCollins, Pickering, and others, have raised objections to the view that\nexperimental results are accepted on the basis of epistemological\narguments. They point out that “a sufficiently determined critic can\nalways find a reason to dispute any alleged ‘result’”\n(MacKenzie 1989, p. 412). Harry Collins, for example, is well known for\nhis skepticism concerning both experimental results and evidence. He\ndevelops an argument that he calls the “experimenters’ regress”\n(Collins 1985, chapter 4, pp. 79–111): What scientists take to be a\ncorrect result is one obtained with a good, that is, properly\nfunctioning, experimental apparatus. But a good experimental apparatus\nis simply one that gives correct results. Collins claims that there are\nno formal criteria that one can apply to decide whether or not an\nexperimental apparatus is working properly. In particular, he argues\nthat calibrating an experimental apparatus by using a surrogate signal\ncannot provide an independent reason for considering the apparatus to\nbe reliable.  \n\nIn Collins’ view the regress is eventually broken by negotiation\nwithin the appropriate scientific community, a process driven by\nfactors such as the career, social, and cognitive interests of the\nscientists, and the perceived utility for future work, but one that is\nnot decided by what we might call epistemological criteria, or reasoned\njudgment. Thus, Collins concludes that his regress raises serious\nquestions concerning both experimental evidence and its use in the\nevaluation of scientific hypotheses and theories. Indeed, if no way out\nof the regress can be found, then he has a point. \n\nCollins strongest candidate for an example of the experimenters’\nregress is presented in his history of the early attempts to detect\ngravitational radiation, or gravity waves. (For more detailed\ndiscussion of this episode see (Collins 1985; 1994; Franklin 1994;\n1997a) In this case, the physics community was forced to compare\nWeber’s claims that he had observed gravity waves with the reports from\nsix other experiments that failed to detect them. On the one hand,\nCollins argues that the decision between these conflicting experimental\nresults could not be made on epistemological or methodological\ngrounds—he claims that the six negative experiments could not\nlegitimately be regarded as\n replications[7]\n and hence become less\nimpressive. On the other hand, Weber’s apparatus, precisely because the\nexperiments used a new type of apparatus to try to detect a hitherto\nunobserved\n phenomenon,[8]\n could not be subjected to standard\ncalibration techniques. \n\nThe results presented by Weber’s critics were not only more\nnumerous, but they had also been carefully cross-checked. The groups\nhad exchanged both data and analysis programs and confirmed their\nresults. The critics had also investigated whether or not their\nanalysis procedure, the use of a linear algorithm, could account for\ntheir failure to observe Weber’s reported results. They had used\nWeber’s preferred procedure, a nonlinear algorithm, to analyze their\nown data, and still found no sign of an effect. They had also\ncalibrated their experimental apparatuses by inserting acoustic pulses\nof known energy and finding that they could detect a signal. Weber, on\nthe other hand, as well as his critics using his analysis procedure,\ncould not detect such calibration pulses. \n\nThere were, in addition, several other serious questions raised\nabout Weber’s analysis procedures. These included an admitted\nprogramming error that generated spurious coincidences between Weber’s\ntwo detectors, possible selection bias by Weber, Weber’s report of\ncoincidences between two detectors when the data had been taken four\nhours apart, and whether or not Weber’s experimental apparatus could\nproduce the narrow coincidences claimed. \n\nIt seems clear that the critics’ results were far more credible than\nWeber’s. They had checked their results by independent confirmation,\nwhich included the sharing of data and analysis programs. They had also\neliminated a plausible source of error, that of the pulses being longer\nthan expected, by analyzing their results using the nonlinear algorithm\nand by explicitly searching for such long\n pulses.[9]\n They had also calibrated\ntheir apparatuses by injecting pulses of known energy and observing the\noutput. \n\nContrary to Collins, I believe that the scientific community made a\nreasoned judgment and rejected Weber’s results and accepted those of\nhis critics. Although no formal rules were applied (e.g. if you make\nfour errors, rather than three, your results lack credibility; or if\nthere are five, but not six, conflicting results, your work is still\ncredible) the procedure was reasonable. \n\nPickering has argued that the reasons for accepting results are the\nfuture utility of such results for both theoretical and experimental\npractice and the agreement of such results with the existing community\ncommitments. In discussing the discovery of weak neutral currents,\nPickering states, \n\nScientific communities tend to reject data that conflict with group\ncommitments and, obversely, to adjust their experimental techniques to\ntune in on phenomena consistent with those commitments. (1981, p.\n236) \n\nThe emphasis on future utility and existing commitments is clear. These\ntwo criteria do not necessarily agree. For example, there are episodes\nin the history of science in which more opportunity for future work is\nprovided by the overthrow of existing theory. (See, for example, the\nhistory of the overthrow of parity conservation and of CP symmetry\ndiscussed below and in (Franklin 1986, Ch. 1, 3)).  \n\nPickering has recently offered a different view of experimental\nresults. In his view the material procedure (including the experimental\napparatus itself along with setting it up, running it, and monitoring\nits operation), the theoretical model of that apparatus, and the\ntheoretical model of the phenomena under investigation are all plastic\nresources that the investigator brings into relations of mutual\nsupport. (Pickering 1987; Pickering 1989). He says:  \n\nHe uses Morpurgo’s search for free quarks, or fractional charges of\n\\(\\tfrac{1}{3} e\\) or \\(\\tfrac{2}{3} e\\), where \\(e\\) is the charge of the\nelectron. (See also (Gooding 1992)). Morpurgo used a modern\nMillikan-type apparatus and initially found a continuous distribution\nof charge values. Following some tinkering with the apparatus, Morpurgo\nfound that if he separated the capacitor plates he obtained only\nintegral values of charge. “After some theoretical analysis, Morpurgo\nconcluded that he now had his apparatus working properly, and reported\nhis failure to find any evidence for fractional charges” (Pickering\n1987, p. 197).  \n\nPickering goes on to note that Morpurgo did not tinker with the two\ncompeting theories of the phenomena then on offer, those of integral\nand fractional charge: \n\nThe conclusion of Morpurgo’s first series of experiments, then, and\nthe production of the observation report which they sustained, was\nmarked by bringing into relations of mutual support of the three\nelements I have discussed: the material form of the apparatus and the\ntwo conceptual models, one instrumental and the other phenomenal.\nAchieving such relations of mutual support is, I suggest, the defining\ncharacteristic of the successful experiment. (p. 199) \n\nPickering has made several important and valid points concerning\nexperiment. Most importantly, he has emphasized that an experimental\napparatus is initially rarely capable of producing a valid experimental\nresults and that some adjustment, or tinkering, is required before it\ndoes. He has also recognized that both the theory of the apparatus and\nthe theory of the phenomena can enter into the production of a valid\nexperimental result. What one may question, however, is the emphasis\nhe places on these theoretical components. From Millikan onwards,\nexperiments had strongly supported the existence of a fundamental unit\nof charge and charge quantization. The failure of Morpurgo’s apparatus to\nproduce measurements of integral charge indicated that it was not\noperating properly and that his theoretical understanding of it was\nfaulty. It was the failure to produce measurements in agreement with\nwhat was already known (i.e., the failure of an important experimental\ncheck) that caused doubts about Morpurgo’s measurements. This was true\nregardless of the theoretical models available, or those that Morpurgo\nwas willing to accept. It was only when Morpurgo’s apparatus could\nreproduce known measurements that it could be trusted and used to\nsearch for fractional charge. To be sure, Pickering has allowed a role\nfor the natural world in the production of the experimental result, but\nit does not seem to be decisive. \n\nAckermann has offered a modification of Pickering’s view. He suggests\nthat the experimental apparatus itself is a less plastic resource then\neither the theoretical model of the apparatus or that of the\nphenomenon.  \n\nHacking (1992) has also offered a more complex version of\nPickering’s later view. He suggests that the results of mature\nlaboratory science achieve stability and are self-vindicating when the\nelements of laboratory science are brought into mutual consistency and\nsupport. These are (1) ideas: questions, background knowledge,\nsystematic theory, topical hypotheses, and modeling of the apparatus;\n(2) things: target, source of modification, detectors, tools, and data\ngenerators; and (3) marks and the manipulation of marks: data, data\nassessment, data reduction, data analysis, and interpretation. \n\nWe invent devices that produce data and isolate or create phenomena,\nand a network of different levels of theory is true to these phenomena.\nConversely we may in the end count them only as phenomena only when the\ndata can be interpreted by theory. (pp. 57–8) \n\nOne might ask whether such mutual adjustment between theory and\nexperimental results can always be achieved? What happens when an\nexperimental result is produced by an apparatus on which several of the\nepistemological strategies, discussed earlier, have been successfully\napplied, and the result is in disagreement with our theory of the\nphenomenon? Accepted theories can be refuted. Several examples will be\npresented below.  \n\nHacking himself worries about what happens when a laboratory science\nthat is true to the phenomena generated in the laboratory, thanks to\nmutual adjustment and self-vindication, is successfully applied to the\nworld outside the laboratory. Does this argue for the truth of the\nscience. In Hacking’s view it does not. If laboratory science does\nproduce happy effects in the “untamed world,… it is not the\ntruth of anything that causes or explains the happy effects” (1992, p.\n60). \n\nRecently Pickering has offered a somewhat revised account of science.\n“My basic image of science is a performative one, in which the\nperformances the doings of human and material agency come to the fore.\nScientists are human agents in a field of material agency which they\nstruggle to capture in machines (Pickering, 1995, p. 21).” He then\ndiscusses the complex interaction between human and material agency,\nwhich I interpret as the interaction between experimenters, their\napparatus, and the natural world.  \n\nPickering’s idea of resistance is illustrated by Morpurgo’s\nobservation of continuous, rather than integral or fractional,\nelectrical charge, which did not agree with his expectations.\nMorpurgo’s accommodation consisted of changing his experimental\napparatus by using a larger separation between his plates, and also by\nmodifying his theoretical account of the apparatus. That being done,\nintegral charges were observed and the result stabilized by the mutual\nagreement of the apparatus, the theory of the apparatus, and the\ntheory of the phenomenon. Pickering notes that ”the outcomes\ndepend on how the world is (p. 182).“ ”In this way,\nthen, how the material world is leaks into and infects our\nrepresentations of it in a nontrivial and consequential fashion. My\nanalysis thus displays an intimate and responsive engagement between\nscientific knowledge and the material world that is integral to\nscientific practice (p. 183).“ \n\nNevertheless there is something confusing about Pickering’s invocation\nof the natural world. Although Pickering acknowledges the importance\nof the natural world, his use of the term ”infects“ seems\nto indicate that he isn’t entirely happy with this. Nor does the\nnatural world seem to have much efficacy. It never seems to be\ndecisive in any of Pickering’s case studies. Recall that he argued\nthat physicists accepted the existence of weak neutral currents\nbecause ”they could ply their trade more profitably in a world\nin which the neutral current was real.“ In his account,\nMorpurgo’s observation of continuous charge is important only because\nit disagrees with his theoretical models of the phenomenon. The fact\nthat it disagreed with numerous previous observations of integral\ncharge doesn’t seem to matter. This is further illustrated by\nPickering’s discussion of the conflict between Morpurgo and\nFairbank. As we have seen, Morpurgo reported that he did not observe\nfractional electrical charges. On the other hand, in the late 1970s\nand early 1980s, Fairbank and his collaborators published a series of\npapers in which they claimed to have observed fractional charges (See,\nfor example, LaRue, Phillips et al. 1981 ). Faced with this discord\nPickering concludes, \n\nThe natural world seems to have disappeared from Pickering’s\naccount. There is a real question here as to whether or not fractional\ncharges exist in nature. The conclusions reached by Fairbank and by\nMorpurgo about their existence cannot both be correct. It seems\ninsufficient to merely state, as Pickering does, that Fairbank and\nMorpurgo achieved their individual stabilizations and to leave the\nconflict unresolved. (Pickering does comment that one could follow the\nsubsequent history and see how the conflict was resolved, and he does\ngive some brief statements about it, but its resolution is not\nimportant for him). At the very least one should consider the actions\nof the scientific community. Scientific knowledge is not determined\nindividually, but communally. Pickering seems to acknowledge\nthis. ”One might, therefore, want to set up a metric and say\nthat items of scientific knowledge are more or less objective\ndepending on the extent to which they are threaded into the rest of\nscientific culture, socially stabilized over time, and so on. I can\nsee nothing wrong with thinking this way…. (p. 196).“ The\nfact that Fairbank believed in the existence of fractional electrical\ncharges, or that Weber strongly believed that he had observed gravity\nwaves, does not make them right. These are questions about the natural\nworld that can be resolved. Either fractional charges and gravity\nwaves exist or they don’t, or to be more cautious we might say that we\nhave good reasons to support our claims about their existence, or we\ndo not. \n\nAnother issue neglected by Pickering is the question of whether a\nparticular mutual adjustment of theory, of the apparatus or the\nphenomenon, and the experimental apparatus and evidence is justified.\nPickering seems to believe that any such adjustment that provides\nstabilization, either for an individual or for the community, is\nacceptable. Others disagree. They note that experimenters sometimes\nexclude data and engage in selective analysis procedures in producing\nexperimental results.  These practices are, at the very least,\nquestionable as is the use of the results produced by such practices\nin science. There are, in fact, procedures in the normal practice of\nscience that provide safeguards against them. (For details see\nFranklin, 2002, Section 1). \n\nThe difference in attitudes toward the resolution of discord is one of\nthe important distinctions between Pickering’s and Franklin’s view of\nscience.  Franklin remarks that it is insufficient simply to say that\nthe resolution is socially stabilized. The important question is how\nthat resolution was achieved and what were the reasons offered for\nthat resolution. If we are faced with discordant experimental results\nand both experimenters have offered reasonable arguments for their\ncorrectness, then clearly more work is needed. It seems reasonable, in\nsuch cases, for the physics community to search for an error in one,\nor both, of the experiments. \n\nPickering discusses yet another difference between his view and that\nof Franklin. Pickering sees traditional philosophy of science as\nregarding objectivity ”as stemming from a peculiar kind of\nmental hygiene or policing of thought.  This police function relates\nspecifically to theory choice in science, which,… is usually\ndiscussed in terms of the rational rules or methods responsible for\nclosure in theoretical debate (p. 197).“ He goes on to remark\nthat, \n\nFor further discussion see (Franklin 1993b)). Although Franklin’s\nepistemology of experiment is designed to offer good reasons for\nbelief in experimental results, they are not a set of rules. Franklin\nregards them as a set of strategies, from which physicists choose, in\norder to argue for the correctness of their results. As noted above,\nthe strategies offered are neither exclusive or exhaustive. \n\nThere is another point of disagreement between Pickering and Franklin.\nPickering claims to be dealing with the practice of science, and yet he\nexcludes certain practices from his discussions. One scientific\npractice is the application of the epistemological strategies \noutlined above to argue for the correctness of an experimental results.\nIn fact, one of the essential features of an experimental paper is the\npresentation of such arguments. Writing such\npapers, a performative act, is also a scientific practice and it would\nseem reasonable to examine both the structure and content of those\npapers. \n\nRecently Ian Hacking (1999, chapter 3) has provided an incisive and\ninteresting discussion of the issues that divide the constructivists\n(Collins, Pickering, etc.) from the rationalists (Stuewer, Franklin,\nBuchwald, etc.). He sets out three sticking points between the two\nviews: 1) contingency, 2) nominalism, and 3) external explanations of\nstability.  \n\nContingency is the idea that science is not predetermined, that it\ncould have developed in any one of several successful ways. This is the\nview adopted by constructivists. Hacking illustrates this with\nPickering’s account of high-energy physics during the 1970s during\nwhich the quark model came to dominate. (See Pickering 1984a). \n\nTo sum up Pickering’s doctrine: there could have been a research\nprogram as successful (“progressive”) as that of\nhigh-energy physics in the 1970s, but with different theories,\nphenomenology, schematic descriptions of apparatus, and apparatus, and\nwith a different, and progressive, series of robust fits between these\ningredients. Moreover and this is something badly in need of\nclarification the “different” physics would not have been\nequivalent to present physics. Not logically incompatible with, just\ndifferent. \n\nThe constructionist about (the idea) of quarks thus claims that the\nupshot of this process of accommodation and resistance is not fully\npredetermined. Laboratory work requires that we get a robust fit\nbetween apparatus, beliefs about the apparatus, interpretations and\nanalyses of data, and theories. Before a robust fit has been\nachieved, it is not determined what that fit will be. Not determined by\nhow the world is, not determined by technology now in existence, not\ndetermined by the social practices of scientists, not determined by\ninterests or networks, not determined by genius, not determined by\nanything (pp. 72–73, emphasis added). \n\nMuch depends here on what Hacking means by “determined.”\nIf he means entailed then one must agree with him. It is doubtful that\nthe world, or more properly, what we can learn about it, entails a\nunique theory. If not, as seems more plausible, he means that the way\nthe world is places no restrictions on that successful science, then\nthe rationalists disagree strongly. They want to argue that the way the\nworld is restricts the kinds of theories that will fit the phenomena,\nthe kinds of apparatus we can build, and the results we can obtain\nwith such apparatuses. To think otherwise seems silly. Consider a\nhomey example.  It seems highly unlikely that someone can come up with\na successful theory in which objects whose density is greater than\nthat of air fall upwards. This is not a caricature of the view Hacking\ndescribes. Describing Pickering’s view, he states, “Physics did\nnot need to take a route that involved Maxwell’s Equations, the Second\nLaw of Thermodynamics, or the present values of the velocity of light\n(p. 70).” Although one may have some sympathy for this view as\nregards Maxwell’s Equations or the Second Law of Thermodynamics, one\nmay not agree about the value of the speed of light. That is\ndetermined by the way the world is. Any successful theory of light\nmust give that value for its speed. \n\nAt the other extreme are the “inevitablists,” among whom\nHacking classifies most scientists. He cites Sheldon Glashow, a Nobel\nPrize winner, “Any intelligent alien anywhere would have come\nupon the same logical system as we have to explain the structure of\nprotons and the nature of supernovae (Glashow 1992, p. 28).” \n\nAnother difference between Pickering and Franklin on contingency\nconcerns the question of not whether an alternative is possible, but\nrather whether there are reasons why that alternative should be\npursued. Pickering seems to identify can with\nought. \n\nIn the late 1970s there was a disagreement between the results of\nlow-energy experiments on atomic parity violation (the violation of\nleft-right symmetry) performed at the University of Washington and at\nOxford University and the result of a high-energy experiment on the\nscattering of polarized electrons from deuterium (the SLAC E122\nexperiment). The atomic-parity violation experiments failed to observe\nthe parity-violating effects predicted by the Weinberg- Salam (W-S)\nunified theory of electroweak interactions, whereas the SLAC experiment\nobserved the predicted effect. These early atomic physics\nresults were quite uncertain in themselves and that uncertainty was\nincreased by positive results obtained in similar experiments at\nBerkeley and Novosibirsk. At the time the theory had other evidential\nsupport, but was not universally accepted. Pickering and Franklin are in\nagreement that the W-S theory was accepted on the basis of the SLAC\nE122 result. They differ dramatically in their discussions of the\nexperiments. Their difference on contingency concerns a particular\ntheoretical alternative that was proposed at the time to explain the\ndiscrepancy between the experimental results. \n\nPickering asked why a theorist might not have attempted to find a\nvariant of electroweak gauge theory that might have reconciled the\nWashington-Oxford atomic parity results with the positive E122 result.\n(What such a theorist was supposed to do with the supportive atomic\nparity results later provided by experiments at Berkeley and at\nNovosibirsk is never mentioned). “But though it is true that\nE122 analysed their data in a way that displayed the improbability\n[the probability of the fit to the hybrid model was 6 ×\n10−4] of a particular class of variant gauge\ntheories, the so-called ‘hybrid models,’ I do not believe\nthat it would have been impossible to devise yet more variants”\n(Pickering 1991, p. 462). Pickering notes that open-ended recipes for\nconstructing such variants had been written down as early as 1972\n(p. 467). It would have been possible to do so, but one\nmay ask whether or not a scientist might have wished to do so. If the\nscientist agreed with Franklin’s view that the SLAC E122 experiment provided\nconsiderable evidential weight in support of the W-S theory and that a\nset of conflicting and uncertain results from atomic parity-violation\nexperiments gave an equivocal answer on that support, what reason\nwould they have had to invent an alternative? \n\nThis is not to suggest that scientists do not, or should not, engage\nin speculation, but rather that there was no necessity to do so in this\ncase. Theorists often do propose alternatives to existing,\nwell-confirmed theories. \n\nConstructivist case studies always seem to result in the support of\nexisting, accepted theory (Pickering 1984a; 1984b; 1991; Collins 1985;\nCollins and Pinch 1993). One criticism implied in such cases is that\nalternatives are not considered, that the hypothesis space of\nacceptable alternatives is either very small or empty. One may\nseriously question this. Thus, when the experiment of\nChristenson et al. (1964) detected \\(\\ce{K2^0}\\)\ndecay into two pions, which seemed to show that CP symmetry (combined\nparticle-antiparticle and space inversion symmetry) was violated, no\nfewer than 10 alternatives were offered. These included (1) the\ncosmological model resulting from the local dysymmetry of matter and\nantimatter, (2) external fields, (3) the decay of the\n\\(\\ce{K2^0}\\) into a \\(\\ce{K1^0}\\) with the\nsubsequent decay of the \\(\\ce{K1^0}\\)into two pions,\nwhich was allowed by the symmetry, (4) the emission of another neutral\nparticle, “the paritino,” in the \\(\\ce{K2^0}\\)\ndecay, similar to the emission of the neutrino in beta decay, (5) that\none of the pions emitted in the decay was in fact a\n“spion,” a pion with spin one rather than zero, (6) that\nthe decay was due to another neutral particle, the L, produced\ncoherently with the \\(\\ce{K^0}\\), (7) the existence of a\n“shadow” universe, which interacted with out universe only\nthrough the weak interactions, and that the decay seen was the decay\nof the “shadow \\(\\ce{K2^0}\\),” (8) the failure\nof the exponential decay law, 9) the failure of the principle of\nsuperposition in quantum mechanics, and 10) that the decay pions were\nnot bosons. \n\nAs one can see, the limits placed on alternatives were not very\nstringent. By the end of 1967, all of the alternatives had been tested\nand found wanting, leaving CP symmetry unprotected. Here the differing\njudgments of the scientific community about what was worth proposing\nand pursuing led to a wide variety of alternatives being tested. \n\nHacking’s second sticking point is nominalism, or name-ism. He notes\nthat in its most extreme form nominalism denies that there is anything\nin common or peculiar to objects selected by a name, such as “Douglas\nfir” other than that they are called Douglas fir. Opponents contend\nthat good names, or good accounts of nature, tell us something correct\nabout the world. This is related to the realism-antirealism debate\nconcerning the status of unobservable entities that has plagued\nphilosophers for millennia. For example Bas van Fraassen (1980), an\nantirealist, holds that we have no grounds for belief in unobservable\nentities such as the electron and that accepting theories about the\nelectron means only that we believe that the things the theory says\nabout observables is true. A realist claims that electrons really exist\nand that as, for example, Wilfred Sellars remarked, “to have good\nreason for holding a theory is ipso facto to have good reason\nfor holding that the entities postulated by the theory exist (Sellars\n1962, p. 97).” In Hacking’s view a scientific nominalist is more\nradical than an antirealist and is just as skeptical about fir trees as\nthey are about electrons. A nominalist further believes that the\nstructures we conceive of are properties of our representations of the\nworld and not of the world itself. Hacking refers to opponents of that\nview as inherent structuralists. \n\nHacking also remarks that this point is related to the question of\n“scientific facts.” Thus, constructivists Latour and\nWoolgar originally entitled their book Laboratory Life: The Social\nConstruction of Scientific Facts (1979). Andrew Pickering\nentitled his history of the quark model Constructing Quarks\n(Pickering 1984a).  Physicists argue that this demeans their\nwork. Steven Weinberg, a realist and a physicist, criticized\nPickering’s title by noting that no mountaineer would ever name a\nbook Constructing Everest. For Weinberg, quarks and Mount\nEverest have the same ontological status.  They are both facts about\nthe world. Hacking argues that constructivists do not, despite\nappearances, believe that facts do not exist, or that there is no such\nthing as reality. He cites Latour and Woolgar “that\n‘out-there-ness’ is a consequence of scientific work\nrather than its cause (Latour and Woolgar 1986, p. 180).”\nHacking reasonably concludes that, \n\nOne might add, however, that the reasons Hacking cites as supporting\nthat belief are given to us by valid experimental evidence and not by\nthe social and personal interests of scientists. \nLatour and Woolgar might not agree. Franklin argues that we have good\nreasons to believe in facts, and in the entities involved in our\ntheories, always remembering, of course, that science is fallible. \n\nHacking’s third sticking point is the external explanations of\nstability. \n\nRationalists think that most science proceeds as it does in the\nlight of good reasons produced by research. Some bodies of knowledge\nbecome stable because of the wealth of good theoretical and\nexperimental reasons that can be adduced for them. Constructivists\nthink that the reasons are not decisive for the course of science.\nNelson (1994) concludes that this issue will never be decided.\nRationalists, at least retrospectively, can always adduce reasons that\nsatisfy them. Constructivists, with equal ingenuity, can always find to\ntheir own satisfaction an openness where the upshot of research is\nsettled by something other than reason. Something external. That is one\nway of saying we have found an irresoluble “sticking point” (pp.\n91–92) \n\nThus, there is a rather severe disagreement on the reasons for the\nacceptance of experimental results. For some, like Staley, Galison and\nFranklin, it is because of epistemological arguments. For others, like\nPickering, the reasons are utility for future practice and agreement\nwith existing theoretical commitments. Although the history of science\nshows that the overthrow of a well-accepted theory leads to an enormous\namount of theoretical and experimental work, proponents of this view\nseem to accept it as unproblematical that it is always agreement with\nexisting theory that has more future utility. Hacking and Pickering\nalso suggest that experimental results are accepted on the basis of the\nmutual adjustment of elements which includes the theory of the\nphenomenon. \n\nNevertheless, everyone seems to agree that a consensus does arise on\nexperimental results. \n\nAuthors like Thomas Kuhn and Paul Feyerabend put forward the view that\nevidence does not confirm or refute a scientific theory since it is\nladen by it. Evidence is not a set of observational sentences\nautonomous from theoretical ones, as logical positivists\nbelieved. Each new theory or a theoretical paradigm, as Kuhn labeled\nlarger theoretical frameworks, produces, as it were, evidence\nanew.  Thus, theoretical concepts infect the entire experimental process\nfrom the stage of design and preparation to the production and\nanalysis of data. A simple example that is supposed to convincingly\nillustrate this view are measurements of temperature with a mercury\nthermometer one uses in order to test whether objects expand when\ntheir temperature increases. Note that in such a case one tests the\nhypothesis by relying on the very assumption that the expansion of\nmercury indicates increase in temperature.  There may be a fairly simple way out of the vicious circle in\nwhich theory and experiment are caught in this particular case of\ntheory-ladenness. It may suffice to calibrate the mercury thermometer\nwith a constant volume gas thermometer, for example, where its use\ndoes not rely on the tested hypothesis but on the proportionality of\nthe pressure of the gas and its absolute temperature (Franklin et\nal. 1989).  Although most experiments are far more complex than this toy\nexample, one could certainly approach the view that experimental\nresults are theory-laden on a case-by-case basis. Yet there may be a\nmore general problem with the view. \nBogen and Woodward (1988) argued that debate on the relationship\nbetween theory and observation overlooks a key ingredient in the\nproduction of experimental evidence, namely the experimental\nphenomena. The experimentalists distill experimental phenomena from\nraw experimental data (e.g. electronic or digital tracks in particle\ncolliders) using various tools of statistical analysis. Thus,\nidentification of an experimental phenomenon as significant (e.g. a\npeak at a particular energy of colliding beams) is free of the theory\nthat the experiment may be designed to test (e.g. the prediction of a\nparticular particle). Only when significant phenomenon has been\nidentified can a stage of data analysis begin in which the phenomenon\nis deemed to either support or refute a theory. Thus, the\ntheory-ladenness of evidence thesis fails at least in some experiments\nin physics. \n\nThe authors substantiate their argument in part through an analysis of\nexperiments that led to a breakthrough discovery of weak neutral\ncurrents. It is a type of force produced by so-called bosons —\nshort-lived particles responsible for energy transfer between other\nparticles such as hadrons and leptons. The relevant peaks were\nrecognized as significant via statistical analysis of data, and later\non interpreted as evidence for the existence of the bosons. \nThis view and the case study have recently been challenged by\nSchindler (2011). He argues that the tested theory was critical in the\nassessment of the reliability of data in the experiments with weak\nneutral currents. He also points out that, on occasion, experimental\ndata can even be ignored if they are deemed irrelevant from a\ntheoretical perspective that physicists find particularly\ncompelling. This was the case in experiments with so-called zebra\npattern magnetic anomalies on the ocean floor. The readings of new\napparatuses used to scan the ocean floor produced intriguing\nsignals. Yet the researchers could not interpret these signals\nmeaningfully or satisfyingly distinguish them from noise unless they\nrelied on some theoretical account of both the structure of the ocean\nfloor and the earth’s magnetic field. \n\nKaraca (2013) points out that a crude theory-observation distinction\nis particularly unhelpful in understanding high-energy physics\nexperiments. It fails to capture the complexity of relevant\ntheoretical structures and their relation to experimental data.\nTheoretical structures can be composed of background, model, and\nphenomenological theories. Background theories are very general\ntheories (e.g. quantum field theory or quantum electrodynamics) that\ndefine the general properties of physical particles and their\ninteractions. Models are specific instances of background theories\nthat define particular particles and their properties. While\nphenomenological theories develop testable predictions based on these\nmodels. \nNow, each of these theoretical segments stands in a different\nrelationship to experimental data—the experiments can be laden\nby a different segment to a different extent. This requires a nuanced\ncategorization of theory-ladeness, from weak to strong.  Thus, an experimental apparatus can be designed to test a very\nspecific theoretical model. UA1 and UA2 detectors at CERN’s Super\nProton Synchrotron were designed to detect particles only in a very\nspecific energy regime in which W and Z bosons of the Standard Model\nwere expected to exist. \nIn contrast, exploratory experiments approach phenomena without\nrelying on a particular theoretical model. Thus, sometimes a\ntheoretical framework for an experiment consists of phenomenological\ntheory alone. Karaca argues that experiments with deep-inelastic\nelectron-proton scattering in the late 1960s and early 1970s are\nexample of such weakly theory-laden experiments. The application of\nmerely phenomenological parameters in the experiment resulted in the\nvery important discovery of the composite rather than point-like\nstructure of hadrons (protons and neutrons), or the so-called scaling\nlaw. And this eventually led to a successful theoretical model of the\ncomposition of hadrons, namely quantum chromodynamics, or the\nquark-model of strong interactions. \n\nAlthough experiment often takes its importance from its relation to\ntheory, Hacking pointed out that it often has a life of its own,\nindependent of theory. He notes the pristine observations of Carolyn\nHerschel’s discovery of comets, William Herschel’s work on “radiant\nheat,” and Davy’s observation of the gas emitted by algae and the\nflaring of a taper in that gas. In none of these cases did the\nexperimenter have any theory of the phenomenon under investigation. One\nmay also note the nineteenth century measurements of atomic spectra and\nthe work on the masses and properties on elementary particles during\nthe 1960s. Both of these sequences were conducted without any guidance\nfrom theory.  \n\nIn deciding what experimental investigation to pursue, scientists may\nvery well be influenced by the equipment available and their own\nability to use that equipment (McKinney 1992). Thus, when the\nMann-O’Neill collaboration was doing high energy physics experiments\nat the Princeton-Pennsylvania Accelerator during the late 1960s, the\nsequence of experiments was (1) measurement of the \\(\\ce{K+}\\) decay\nrates, (2) measurement of the \\(\\ce{K+_{e 3}}\\) branching\nratio and decay spectrum, (3) measurement of the\n\\(\\ce{K+_{e 2}}\\) branching ratio, and (4) measurement of the\nform factor in \\(\\ce{K+_{e 3}}\\) decay. These experiments\nwere performed with basically the same experimental apparatus, but\nwith relatively minor modifications for each particular experiment. By\nthe end of the sequence the experimenters had become quite expert in\nthe use of the apparatus and knowledgeable about the backgrounds and\nexperimental problems. This allowed the group to successfully perform\nthe technically more difficult experiments later in the sequence. We\nmight refer to this as “instrumental loyalty” and the\n“recycling of expertise” (Franklin 1997b). This meshes\nnicely with Galison’s view of experimental traditions. Scientists,\nboth theorists and experimentalists, tend to pursue experiments and\nproblems in which their training and expertise can be used. \n\nHacking also remarks on the “noteworthy observations” on Iceland\nSpar by Bartholin, on diffraction by Hooke and Grimaldi, and on the\ndispersion of light by Newton. “Now of course Bartholin, Grimaldi,\nHooke, and Newton were not mindless empiricists without an\n‘idea’ in their heads. They saw what they saw because they\nwere curious, inquisitive, reflective people. They were attempting to\nform theories. But in all these cases it is clear that the observations\npreceded any formulation of theory” (Hacking 1983, p. 156). In all of\nthese cases we may say that these were observations waiting for, or\nperhaps even calling for, a theory. The discovery of any unexpected\nphenomenon calls for a theoretical explanation. \n\nNevertheless several of the important roles of experiment involve its\nrelation to theory. Experiment may confirm a theory, refute a theory,\nor give hints to the mathematical structure of a theory.  \n\nLet us consider first an episode in which the relation between theory\nand experiment was clear and straightforward. This was a “crucial”\nexperiment, one that decided unequivocally between two competing\ntheories, or classes of theory. The episode was that of the discovery\nthat parity, mirror-reflection symmetry or left-right symmetry, is not\nconserved in the weak interactions. (For details of this episode see\nFranklin (1986, Ch. 1) and\n Appendix 1).\n Experiments showed that in the beta\ndecay of nuclei the number of electrons emitted in the same direction\nas the nuclear spin was different from the number emitted opposite to\nthe spin direction. This was a clear demonstration of parity violation\nin the weak interactions.  \n\nAfter the discovery of parity and charge conjugation nonconservation,\nand following a suggestion by Landau, physicists considered CP\n(combined parity and particle-antiparticle symmetry), which was still\nconserved in the experiments, as the appropriate symmetry. One\nconsequence of this scheme, if CP were conserved, was that the\n\\(\\ce{K1^0}\\) meson could decay into two pions, whereas the\n\\(\\ce{K2^0}\\) meson could\n not.[10]\n Thus, observation of\nthe decay of \\(\\ce{K2^0}\\) into two pions would indicate CP\nviolation. The decay was observed by a group at Princeton University.\nAlthough several alternative explanations were offered, experiments\neliminated each of the alternatives leaving only CP violation as an\nexplanation of the experimental result. (For details of this episode\nsee Franklin (1986, Ch. 3) and\n Appendix 2.) \n\nIn both of the episodes discussed previously, those of parity\nnonconservation and of CP violation, we saw a decision between two\ncompeting classes of theories. This episode, the discovery of\nBose-Einstein condensation (BEC), illustrates the confirmation of a\nspecific theoretical prediction 70 years after the theoretical\nprediction was first made. Bose (1924) and Einstein (1924; 1925)\npredicted that a gas of noninteracting bosonic atoms will, below a\ncertain temperature, suddenly develop a macroscopic population in the\nlowest energy quantum\n state.[11]\n (For details of this episode see\n Appendix 3.) \n\nIn the three episodes discussed in the previous section, the relation\nbetween experiment and theory was clear. The experiments gave\nunequivocal results and there was no ambiguity about what theory was\npredicting. None of the conclusions reached has since been questioned.\nParity and CP symmetry are violated in the weak interactions and\nBose-Einstein condensation is an accepted phenomenon. In the practice\nof science things are often more complex. Experimental results may be\nin conflict, or may even be incorrect. Theoretical calculations may\nalso be in error or a correct theory may be incorrectly applied. There\nare even cases in which both experiment and theory are wrong. As noted\nearlier, science is fallible. In this section I will discuss\nseveral episodes which illustrate these complexities.  \n\nThe episode of the fifth force is the case of a refutation of an\nhypothesis, but only after a disagreement between experimental results\nwas resolved. The “Fifth Force” was a proposed modification of Newton’s\nLaw of Universal Gravitation. The initial experiments gave conflicting\nresults: one supported the existence of the Fifth Force whereas the\nother argued against it. After numerous repetitions of the experiment,\nthe discord was resolved and a consensus reached that the Fifth Force\ndid not exist. (For details of this episode see\n Appendix 4.) \n\nThe Stern-Gerlach experiment was regarded as crucial at the time it\nwas performed, but, in fact, \n wasn’t.[12]\n In the view of the physics\ncommunity it decided the issue between two theories, refuting one and\nsupporting the other. In the light of later work, however, the\nrefutation stood, but the confirmation was questionable. In fact, the\nexperimental result posed problems for the theory it had seemingly\nconfirmed. A new theory was proposed and although the Stern-Gerlach\nresult initially also posed problems for the new theory, after a\nmodification of that new theory, the result confirmed it. In a sense,\nit was crucial after all. It just took some time. \n\nThe Stern-Gerlach experiment provides evidence for the existence of\nelectron spin. These experimental results were first published in 1922,\nalthough the idea of electron spin wasn’t proposed by Goudsmit and\nUhlenbeck until 1925 (1925; 1926). One might say that electron spin was\ndiscovered before it was invented. (For details of this episode see\n Appendix 5). \n\nIn the last section we saw some of the difficulty inherent in\nexperiment-theory comparison. One is sometimes faced with the question\nof whether the experimental apparatus satisfies the conditions required\nby theory, or conversely, whether the appropriate theory is being\ncompared to the experimental result. A case in point is the history of\nexperiments on the double-scattering of electrons by heavy nuclei (Mott\nscattering) during the 1930s and the relation of these results to\nDirac’s theory of the electron, an episode in which the question of\nwhether or not the experiment satisfied the conditions of the\ntheoretical calculation was central. Initially, experiments disagreed\nwith Mott’s calculation, casting doubt on the underlying Dirac theory.\nAfter more than a decade of work, both experimental and theoretical, it\nwas realized that there was a background effect in the experiments that\nmasked the predicted effect. When the background was eliminated\nexperiment and theory agreed.\n (Appendix 6) \n\nEver vaster amounts of data have been produced by particle colliders\nas they have grown from room-size apparata, to tens of kilometers long\nmega-labs. Vast numbers of background interactions that are well\nunderstood and theoretically uninteresting occur in the\ndetector. These have to be combed in order to identify interactions of\npotential interest. This is especially true of hadron (proton-proton)\ncolliders like the Large Hadron Collider (LHC), where the Higgs boson\nwas discovered. Protons that collide in the LHC and similar hadron\ncolliders are composed of more elementary particles, collectively\nlabeled partons. Partons mutually interact, exponentially increasing\nthe number of background interactions. In fact, a minuscule number of\ninteractions are selected from the overwhelming number that occur in\nthe detector. (In contrast, lepton collisions, such as collisions of\nelectrons and positrons, produce much lower backgrounds, since leptons\nare not composed of more elementary particles.) \nThus, a successful search for new elementary particles critically\ndepends on successfully crafting selection criteria and techniques at\nthe stage of data collection and at the stage of data analysis. But\ngradual development and changes in data selection procedures in the\ncolliders raises an important epistemological concern. The main reason\nfor this concern is nicely anticipated by the following question,\nwhich was posed by one of the most prominent experimentalists in\nparticle physics: “What is the extent to which we are negating the\ndiscovery potential of very-high-energy proton machines by the\nnecessity of rejecting, a priori, the events we cannot afford to\nrecord?” (Panofsky 1994, 133). In other words, how does one decide\nwhich interactions to detect and analyze in a multitude, in order to\nminimize the possibility of throwing out novel and unexplored\nones? \n\nOne way of searching through vast amounts of data that are already in,\ni.e. those that the detector has already delivered, is to look for\noccurrences that remain robust under varying conditions of\ndetection. Physicists employ the technique of data cuts in such\nanalysis. They cut out data that may be unreliable—when, for\ninstance, a data set may be an artefact rather than a genuine particle\ninteraction the experimenters expect. E.g. a colliding beam may\ninteract with the walls of the detector and not with the other\ncolliding beam, while producing a signal identical to the signal the\nexperimenters expected the beam-beam interaction to produce. Thus, if\nunder various data cuts a result remains stable, then it is\nincreasingly likely to be correct and to represent the genuine\nphenomenon the physicists think it represents. The robustness of the\nresult under various data cuts minimizes the possibility that the\ndetected phenomenon only mimics the genuine one (Franklin 2013,\n224–5). \n\nAt the data-acquisition stage, however, this strategy does not seem\napplicable. As Panofsky suggests, one does not know with certainty\nwhich of the vast number of the events in the detector may be of\ninterest. \n\nYet, Karaca\n (2011)[13]\n argues that a form of robustness is in\nplay even at the acquisition stage. This experimental approach\namalgamates theoretical expectations and empirical results, as the\nexample of the hypothesis of specific heavy particles is supposed to\nillustrate. \nAlong with the Standard Model of particle physics, a number of\nalternative models have been proposed. Their predictions of how\nelementary particles should behave often differ substantially. Yet in\ncontrast to the Standard Model, they all share the hypothesis that\nthere exist heavy particles that decay into particles with high\ntransverse momentum. Physicists apply a robustness analysis in testing this hypothesis,\nthe argument goes. First, they check whether the apparatus can detect\nknown particles similar to those predicted. Second, guided by the\nhypothesis, they establish various trigger algorithms. (The trigger\nalgorithms, or “the triggers”, determine at what exact point in time\nand under which conditions a detector should record interactions. They\nare necessary because the frequency and the number of interactions far\nexceed the limited recording capacity.) And, finally, they observe\nwhether any results remain stable across the triggers. \n\nYet even in this theoretical-empirical form of robustness, as Franklin\n(2013, 225) points out, “there is an underlying assumption that any\nnew physics will resemble known physics”—usually a theory of the\nday. And one way around this problem is for physicists to produce as\nmany alternative models as possible, including those that may even\nseem implausible at the time. \nPerovic (2011) suggests that such a potential failure, namely to spot\npotentially relevant events occurring in the detector, may be also a\nconsequence of the gradual automation of the detection process.  \n\nThe early days of experimentation in particle physics, around WWII,\nsaw the direct involvement of the experimenters in the\nprocess. Experimental particle physics was a decentralized discipline\nwhere experimenters running individual labs had full control over the\ntriggers and analysis. The experimenters could also control the goals\nand the design of experiments. Fixed target accelerators, where the\nbeam hits the detector instead of another beam, produced a number of\nparticle interactions that was manageable for such labs. The chance of\nmissing an anomalous event not predicted by the current theory was not\na major concern in such an environment.  \n\nYet such labs could process a comparatively small amount of data. This\nhas gradually become an obstacle, with the advent of hadron\ncolliders. They work at ever-higher energies and produce an\never-vaster number of background interactions. That is why the\nexperimental process has become increasingly automated and much more\nindirect. Trained technicians instead of experimenters themselves at\nsome point started to scan the recordings. Eventually, these human\nscanners were replaced by computers, and a full automation of\ndetection in hadron colliders has enabled the processing of vast\nnumber of interactions. This was the first significant change in the\ntransition from small individual labs to mega-labs. \nThe second significant change concerned the organization and goals of\nthe labs. The mega-detectors and the amounts of data they produced\nrequired exponentially more staff and scientists. This in turn led to\neven more centralized and hierarchical labs and even longer periods of\ndesign and performance of the experiments. As a result, focusing on\nconfirming existing dominant hypotheses rather than on exploratory\nparticle searches was the least risky way of achieving results that\nwould justify unprecedented investments.  Now, an indirect detection process combined with mostly\nconfirmatory goals is conducive to overlooking of unexpected\ninteractions. As such, it may impede potentially crucial theoretical\nadvances stemming from missed interactions.  \n\nThis possibility that physicists such as Panofsky have acknowledged is\nnot a mere speculation. In fact, the use of semi-automated, rather\nthan fully-automated regimes of detection turned out to be essential\nfor a number of surprising discoveries that led to theoretical\nbreakthroughs. \nPerovic analyzes several such cases, most notably the discovery of the\nJ/psi particle that provided the first substantial piece of evidence\nfor the existence of the charmed quark. In the experiments, physicists\nwere able to perform exploratory detection and visual analysis of\npractically individual interactions due to low number of background\ninteractions in the linear electron-positron collider. And they could\nafford to do this in an energy range that the existing theory did not\nrecognize as significant, which led to them making the discovery. None\nof this could have been done in the fully automated detecting regime\nof hadron colliders that are indispensable when dealing with an\nenvironment that contains huge numbers of background\ninteractions. \nAnd in some cases, such as the Fermilab experiments that aimed to\ndiscover weak neutral currents, an automated and confirmatory regime\nof data analysis contributed to the failure to detect particles that\nwere readily produced in the apparatus. \nThe complexity of the discovery process in particle physics does not\nend with concerns about what exact data should be chosen out of the\nsea of interactions. The so-called look-elsewhere effect results in a\ntantalizing dilemma at the stage of data analysis. \nSuppose that our theory tells us that we will find a particle in an\nenergy range. And suppose we find a significant signal in a section of\nthat very range. Perhaps we should keep looking elsewhere within the\nrange to make sure it is not another particle altogether we have\ndiscovered. It may be a particle that left other undetected traces in\nthe range that our theory does not predict, along with the trace we\nfound. The question is to what extent we should look elsewhere before\nwe reach a satisfying level of certainty that it is the predicted\nparticle we have discovered. \n\nPhysicists faced such a dilemma during the search for the Higgs boson\nat the Large Hadron Collider at CERN (Dawid 2015). \n\nThe Higgs boson is a particle responsible for the mass of other\nparticles. It is a scalar field that “pulls back” moving and\ninteracting particles. This pull, which we call mass, is different for\ndifferent particles. It is predicted by the Standard Model, whereas\nalternative models predict somewhat similar Higgs-like particles.  \n\nA prediction based on the Standard Model tells us with high\nprobability that we will find the Higgs particle in a particular\nrange. Yet a simple and an inevitable fact of finding it in a\nparticular section of that range may prompt us to doubt whether we\nhave truly found the exact particle our theory predicted. Our initial\nexcitement may vanish when we realize that we are much more likely to\nfind a particle of any sort—not just the predicted\nparticle—within the entire range than in a particular section of\nthat range. Thus, the probability of finding the Higgs anywhere within\na given energy range (consisting of eighty energy ‘bins’) is much\nhigher than the probability of finding it at a particular energy scale\nwithin that range (i.e. in any individual bin). In fact, the\nlikelihood of us finding it in a particular bin of the range is about\nhundred times lower. \nIn other words, the fact that we will inevitably find the particle in\na particular bin, not only in a particular range, decreases the\ncertainty that it was the Higgs we found. Given this fact alone we\nshould keep looking elsewhere for other possible traces in the range\nonce we find a significant signal in a bin. We should not proclaim the\ndiscovery of a particle predicted by the Standard Model (or any model\nfor that matter) too soon. But for how long should we keep looking\nelsewhere? And what level of certainty do we need to achieve before we\nproclaim discovery? \n  \nThe answer boils down to the weight one gives the theory and its\npredictions. This is the reason the experimentalists and theoreticians\nhad divergent views on the criterion for determining the precise point\nat which they could justifiably state ‘Our data indicate that we have\ndiscovered the Higgs boson’. Theoreticians were confident that a\nfinding within the range (any of eighty bins) that was of standard\nreliability (of three or four sigma), coupled with the theoretical\nexpectations that Higgs would be found, would be sufficient. In\ncontrast, experimentalists argued that at no point of data analysis\nshould the pertinence of the look-elsewhere effect be reduced, and the\nsearch proclaimed successful, with the help of the theoretical\nexpectations concerning Higgs. One needs to be as careful in combing\nthe range as one practically may. As a result, the experimentalists’\npreferred value of sigmas for announcing the discovery was five. This\nis a standard under which very few findings have turned out to be a\nfluctuation in the past. \nDawid argues that a question of an appropriate statistical analysis of\ndata is at the heart of the dispute. The reasoning of the\nexperimentalists relied on a frequentist approach that does not\nspecify the probability of the tested hypothesis. It actually isolates\nstatistical analysis of data from the prior probabilities. The\ntheoreticians, however, relied on Bayesian analysis. It starts with\nprior probabilities of initial assumptions and ends with the\nassessment of the probability of tested hypothesis based on the\ncollected evidence. The question remains whether the experimentalists’\nreasoning was fully justified. The prior expectations that the\ntheoreticians had included in their analysis had already been\nempirically corroborated by previous experiments after all. \n\nExperiment can also provide us with evidence for the existence of the\nentities involved in our theories. J.J. Thomson’s experiments on\ncathode rays provided grounds for belief in the existence of electrons.\n(For details of this episode see\n Appendix 7).\n \n\nExperiment can also help to articulate a theory. Experiments on beta\ndecay during from the 1930s to the 1950s determined the precise\nmathematical form of Fermi’s theory of beta decay. (For details of this\nepisode see\n Appendix 8.)\n \n\nOne comment that has been made concerning the philosophy of\nexperiment is that all of the examples are taken from physics and are\ntherefore limited. In this section arguments will be presented that these\ndiscussions also apply to biology. \n\nAlthough all of the illustrations of the epistemology of experiment\ncome from physics, David Rudge (1998; 2001) has shown that they are\nalso used in biology. His example is Kettlewell’s (1955; 1956;\n1958) evolutionary biology experiments on the Peppered Moth, Biston\nbetularia. The typical form of the moth has a pale\nspeckled appearance and there are two darker forms, f.\ncarbonaria, which is nearly black, and f. insularia,\nwhich is intermediate in color. The typical form of the moth\nwas most prevalent in the British Isles and Europe until the middle of\nthe nineteenth century. At that time things began to change. Increasing\nindustrial pollution had both darkened the surfaces of trees and rocks\nand had also killed the lichen cover of the forests downwind of\npollution sources. Coincident with these changes, naturalists had found\nthat rare, darker forms of several moth species, in particular the\nPeppered Moth, had become common in areas downwind of pollution\nsources. \n\nKettlewell attempted to test a selectionist explanation of this\nphenomenon. E.B. Ford (1937; 1940) had suggested a two-part explanation\nof this effect: 1) darker moths had a superior physiology and 2) the\nspread of the melanic gene was confined to industrial areas because the\ndarker color made carbonaria more conspicuous to avian\npredators in rural areas and less conspicuous in polluted areas.\nKettlewell believed that Ford had established the superior viability of\ndarker moths and he wanted to test the hypothesis that the darker form\nof the moth was less conspicuous to predators in industrial areas. \n\nKettlewell’s investigations consisted of three parts. In the\nfirst part he used human observers to investigate whether his proposed\nscoring method would be accurate in assessing the relative\nconspicuousness of different types of moths against different\nbackgrounds. The tests showed that moths on “correct”\nbackgrounds, typical on lichen covered backgrounds and dark\nmoths on soot-blackened backgrounds were almost always judged\ninconspicuous, whereas moths on “incorrect” backgrounds\nwere judged conspicuous. \n\nThe second step involved releasing birds into a cage containing all\nthree types of moth and both soot-blackened and lichen covered pieces\nof bark as resting places. After some difficulties (see Rudge 1998 for\ndetails), Kettlewell found that birds prey on moths in an order of\nconspicuousness similar to that gauged by human observers. \n\nThe third step was to investigate whether birds preferentially prey\non conspicuous moths in the wild. Kettlewell used a\nmark-release-recapture experiment in both a polluted environment\n(Birmingham) and later in an unpolluted wood. He released 630 marked\nmale moths of all three types in an area near Birmingham, which\ncontained predators and natural boundaries. He then recaptured the\nmoths using two different types of trap, each containing virgin females\nof all three types to guard against the possibility of pheromone\ndifferences. \n\nKettlewell found that carbonaria was twice as likely to\nsurvive in soot-darkened environments (27.5 percent) as was\ntypical (12.7 percent). He worried, however, that his results\nmight be an artifact of his experimental procedures. Perhaps the traps\nused were more attractive to one type of moth, that one form of moth\nwas more likely to migrate, or that one type of moth just lived longer.\nHe eliminated the first alternative by showing that the recapture rates\nwere the same for both types of trap. The use of natural boundaries and\ntraps placed beyond those boundaries eliminated the second, and\nprevious experiments had shown no differences in longevity. Further\nexperiments in polluted environments confirmed that carbonaria\nwas twice as likely to survive as typical. An experiment in an\nunpolluted environment showed that typical was three times as\nlikely to survive as carbonaria. Kettlewell concluded that\nsuch selection was the cause of the prevalence of carbonaria\nin polluted environments. \n\nRudge also demonstrates that the strategies used by Kettlewell are\nthose described above in the epistemology of experiment. His examples\nare given in Table 1. (For more details see Rudge 1998). Table 1. Examples of epistemological strategies used\nby experimentalists in evolutionary biology, from H.B.D.\nKettlewell’s (1955, 1956, 1958) investigations of industrial\nmelanism. (See Rudge 1998). \n\nThe roles that experiment plays in physics are also those it plays\nin biology. In the previous section we have seen that\nKettlewell’s experiments both test and confirm a theory. I\ndiscussed earlier a set of crucial experiments that decided between two\ncompeting classes of theories, those that conserved parity and those\nthat did not. In this section I will discuss an experiment that decided\namong three competing mechanisms for the replication of DNA, the\nmolecule now believed to be responsible for heredity. This is another\ncrucial experiment. It strongly supported one proposed mechanism and\nargued against the other two. (For details of this episode see (Holmes\n2001)). \n\nIn 1953 Francis Crick and James Watson proposed a three-dimensional\nstructure for deoxyribonucleic acid (DNA) (Watson and Crick 1953a).\nTheir proposed structure consisted of two polynucleotide chains\nhelically wound about a common axis. This was the famous “Double\nHelix”. The chains were bound together by combinations of four\nnitrogen bases — adenine, thymine, cytosine, and guanine. Because of\nstructural requirements only the base pairs adenine-thymine and\ncytosine-guanine are allowed. Each chain is thus complementary to the\nother. If there is an adenine base at a location in one chain there is\na thymine base at the same location on the other chain, and vice versa.\nThe same applies to cytosine and guanine. The order of the bases along\na chain is not, however, restricted in any way, and it is the precise\nsequence of bases that carries the genetic information. \n\nThe significance of the proposed structure was not lost on Watson\nand Crick when they made their suggestion. They remarked, “It has\nnot escaped our notice that the specific pairing we have postulated\nimmediately suggests a possible copying mechanism for the genetic\nmaterial.” \n\nIf DNA was to play this crucial role in genetics, then there must be\na mechanism for the replication of the molecule. Within a short period\nof time following the Watson-Crick suggestion, three different\nmechanisms for the replication of the DNA molecule were proposed\n(Delbruck and Stent 1957). These are illustrated in Figure A. The\nfirst, proposed by Gunther Stent and known as conservative replication,\nsuggested that each of the two strands of the parent DNA molecule is\nreplicated in new material. This yields a first generation which\nconsists of the original parent DNA molecule and one newly-synthesized\nDNA molecule. The second generation will consist of the parental DNA\nand three new DNAs. Figure A:\nPossible mechanisms for DNA replication. (Left) Conservative\nreplication. Each of the two strands of the parent DNA is replicated to\nyield the unchanged parent DNA and one newly synthesized DNA. The\nsecond generation consists of one parent DNA and three new DNAs.\n(Center) Semiconservative replication. Each first generation DNA\nmolecule contains one strand of the parent DNA and one newly\nsynthesized strand. The second generation consists of two hybrid DNAs\nand two new DNAs. (Right) Dispersive replication. The parent chains\nbreak at intervals, and the parental segments combine with new segments\nto form the daughter chains. The darker segments are parental DNA and\nthe lighter segments are newly synthesized DNA. From Lehninger\n(1975). \n\nThe second proposed mechanism, known as semiconservative replication\nis when each strand of the parental DNA acts as a template for a second\nnewly-synthesized complementary strand, which then combines with the\noriginal strand to form a DNA molecule. This was proposed by Watson and\nCrick (1953b). The first generation consists of two hybrid molecules,\neach of which contains one strand of parental DNA and one newly\nsynthesized strand. The second generation consists of two hybrid\nmolecules and two totally new DNAs. The third mechanism, proposed by\nMax Delbruck, was dispersive replication, in which the parental DNA\nchains break at intervals and the parental segments combine with new\nsegments to form the daughter strands. \n\nIn this section the experiment performed by Matthew Meselson and\nFranklin Stahl, which has been called “the most beautiful\nexperiment in biology”, and which was designed to answer the\nquestion of the correct DNA replication mechanism will be discussed\n(Meselson and Stahl 1958). Meselson and Stahl described their proposed\nmethod. “We anticipated that a label which imparts to the DNA\nmolecule an increased density might permit an analysis of this\ndistribution by sedimentation techniques. To this end a method was\ndeveloped for the detection of small density differences among\nmacromolecules. By use of this method, we have observed the\ndistribution of the heavy nitrogen isotope \\(\\ce{^{15}N}\\) among\nmolecules of DNA following the transfer of a uniformly\n\\(\\ce{^{15}N}\\)-labeled, exponentially growing bacterial population to\na growth medium containing the ordinary nitrogen isotope\n\\(\\ce{^{14}N}\\)” (Meselson and Stahl 1958, pp. 671–672). Figure B:\nSchematic representation of the Meselson-Stahl experiment. From Watson\n(1965). \n\nThe experiment is described schematically in Figure B. Meselson and\nStahl placed a sample of DNA in a solution of cesium chloride. As the\nsample is rotated at high speed the denser material travels further\naway from the axis of rotation than does the less dense material. This\nresults in a solution of cesium chloride that has increasing density as\none goes further away from the axis of rotation. The DNA reaches\nequilibrium at the position where its density equals that of the\nsolution. Meselson and Stahl grew E. coli bacteria in a medium\nthat contained ammonium chloride \\((\\ce{NH4Cl})\\) as the sole source\nof nitrogen. They did this for media that contained either\n\\(\\ce{^{14}N}\\), ordinary nitrogen, or \\(\\ce{^{15}N}\\), a heavier\nisotope. By destroying the cell membranes they could obtain samples of\nDNA which contained either \n\\(\\ce{^{14}N}\\) or \\(\\ce{^{15}N}\\). They first\nshowed that they could indeed separate the two different mass molecules\nof DNA by centrifugation (Figure C). The separation of the two\ntypes of DNA is clear in both the photograph obtained by absorbing\nultraviolet light and in the graph showing the intensity of the signal,\nobtained with a densitometer. In addition, the separation between the\ntwo peaks suggested that they would be able to distinguish an\nintermediate band composed of hybrid DNA from the heavy and light\nbands. These early results argued both that the experimental apparatus\nwas working properly and that all of the results obtained were correct.\nIt is difficult to imagine either an apparatus malfunction or a source\nof experimental background that could reproduce those results. This is\nsimilar, although certainly not identical, to Galileo’s\nobservation of the moons of Jupiter or to Millikan’s measurement\nof the charge of the electron. In both of those episodes it was the\nresults themselves that argued for their correctness. Figure C:\nThe separation of \\(\\ce{^{14}N}\\) DNA from \\(\\ce{^{15}N}\\) DNA by\ncentrifugation. The band on the left is \\(\\ce{^{14}N}\\) DNA and that on\nthe right is from \\(\\ce{^{15}N}\\) DNA. From Meselson and Stahl (1958).\n \n\nMeselson and Stahl then produced a sample of E coli\nbacteria containing only \\(\\ce{^{15}N}\\) by growing it in a medium\ncontaining only ammonium chloride with \\(\\ce{^{15}N}\\)\n\\((\\ce{^{15}NH4Cl})\\) for fourteen generations. They then\nabruptly changed the medium to \\(\\ce{^{14}N}\\) by adding a tenfold\nexcess of \\(\\ce{^{14}NH_4Cl}\\). Samples were taken just before\nthe addition of \\(\\ce{^{14}N}\\) and at intervals afterward for several\ngenerations. The cell membranes were broken to release the DNA into the\nsolution and the samples were centrifuged and ultraviolet absorption\nphotographs taken. In addition, the photographs were scanned with a\nrecording densitometer. The results are shown in Figure D,\nshowing both the photographs and the densitometer traces. The figure\nshows that one starts only with heavy (fully-labeled) DNA. As time\nproceeds one sees more and more half-labeled DNA, until at one\ngeneration time only half-labeled DNA is present. “Subsequently\nonly half labeled DNA and completely unlabeled DNA are found. When two\ngeneration times have elapsed after the addition of \\(\\ce{^{14}N}\\)\nhalf-labeled and unlabeled DNA are present in equal amounts” (p.\n676). (This is exactly what the semiconservative replication mechanism\npredicts). By four generations the sample consists almost entirely of\nunlabeled DNA. A test of the conclusion that the DNA in the\nintermediate density band was half labeled was provided by examination\nof a sample containing equal amounts of generations 0 and 1.9. If the\nsemiconservative mechanism is correct then Generation 1.9 should have\napproximately equal amounts of unlabeled and half-labeled DNA, whereas\nGeneration 0 contains only fully-labeled DNA. As one can see, there are\nthree clear density bands and Meselson and Stahl found that the\nintermediate band was centered at \\((50 \\pm 2)\\) percent of the difference\nbetween the \\(\\ce{^{14}N}\\) and \\(\\ce{^{15}N}\\) bands, shown in the\nbottom photograph (Generations 0 and 4.1). This is precisely what one\nwould expect if that DNA were half labeled. Figure D:\n(Left) Ultraviolet absorption photographs showing DNA bands from\ncentrifugation of DNA from E. Coli sampled at various times\nafter the addition of an excess of \\(\\ce{^{14}N}\\) substrates to a\ngrowing \\(\\ce{^{15}N}\\) culture. (Right) Densitometer traces of the\nphotographs. The initial sample is all heavy (\\(\\ce{^{15}N}\\) DNA). As\ntime proceeds a second intermediate band begins to appear until at one\ngeneration all of the sample is of intermediate mass (Hybrid DNA). At\nlonger times a band of light DNA appears, until at 4.1 generations the\nsample is almost all lighter DNA. This is exactly what is predicted by\nthe Watson-Crick semiconservative mechanism. From Meselson and Stahl\n(1958) \n\nMeselson and Stahl stated their results as follows, “The\nnitrogen of DNA is divided equally between two subunits which remain\nintact through many generations…. Following replication,\neach daughter molecule has received one parental subunit” (p.\n676). \n\nMeselson and Stahl also noted the implications of their work\nfor deciding among the proposed mechanisms for DNA replication. In a\nsection labeled “The Watson-Crick Model” they noted that,\n“This [the structure of the DNA molecule] suggested to Watson and\nCrick a definite and structurally plausible hypothesis for the\nduplication of the DNA molecule. According to this idea, the two chains\nseparate, exposing the hydrogen-bonding sites of the bases. Then, in\naccord with base-pairing restrictions, each chain serves as a template\nfor the synthesis of its complement. Accordingly, each daughter\nmolecule contains one of the parental chains paired with a newly\nsynthesized chain…. The results of the present experiment are\nin exact accord with the expectations of the Watson-Crick model for DNA\nreplication” (pp. 677–678). \n\nIt also showed that the dispersive replication mechanism proposed by\nDelbruck, which had smaller subunits, was incorrect. “Since the\napparent molecular weight of the subunits so obtained is found to be\nclose to half that of the intact molecule, it may be further concluded\nthat the subunits of the DNA molecule which are conserved at\nduplication are single, continuous structures. The scheme for DNA\nduplication proposed by Delbruck is thereby ruled out” (p. 681).\nLater work by John Cairns and others showed that the subunits of DNA\nwere the entire single polynucleotide chains of the Watson-Crick model\nof DNA structure. \n\nThe Meselson-Stahl experiment is a crucial experiment in biology. It\ndecided between three proposed mechanisms for the replication of DNA.\nIt supported the Watson-Crick semiconservative mechanism and eliminated\nthe conservative and dispersive mechanisms. It played a similar role in\nbiology to that of the experiments that demonstrated the\nnonconservation of parity did in physics. Thus, we have seen evidence\nthat experiment plays similar roles in both biology and physics and\nalso that the same epistemological strategies are used in both\ndisciplines. \n\nOne interesting recent development in science, and thus in the\nphilosophy of science, has been the increasing use of, and importance\nof, computer simulations. In some fields, such as high-energy physics,\nsimulations are an essential part of all experiments. It is fair to\nsay that without computer simulations these experiments would be\nimpossible. There has been a considerable literature in the philosophy\nof science discussing whether computer simulations are experiments,\ntheory, or some new kind of hybrid method of doing science. But, as\nEric Winsberg remarked, “We have in other words, rejected the\noverly conservative intuition that computer simulation is nothing but\nboring and straightforward theory application. But we have avoided\nembracing the opposite, overly grandiose intuition that simulation is\na radically new kind of knowledge production, ”on a par“\nwith experimentation. In fact, we have seen that soberly locating\nsimulation ”on the methodological map“ is not a simple\nmatter (Winsberg 2010, p. 136).”  \n\nGiven the importance of computer simulations in science it is\nessential that we have good reasons to believe their results. Eric\nWinsberg (2010), Wendy Parker (2008) and others have shown that\nscientists use strategies quite similar to those discussed in Section\n1.1.1, to argue for the correctness of computer simulations.\n \nThe distinction between observation and experiment is relatively\nlittle discussed in philosophical literature, despite its continuous\nrelevance to the scientific community and beyond in understanding\nspecific traits and segments of the scientific process and the\nknowledge it produces. \nDaston and her coauthors (Daston 2011; Daston and Lunbeck 2011; Daston\nand Galison 2007) have convincingly demonstrated that the distinction\nhas played a role in delineating various features of scientific\npractice. It has helped scientists articulate their reflections on\ntheir own practice. \nObservation is philosophically a loaded term, yet the epistemic status\nof scientific observation has evolved gradually with the advance of\nscientific techniques of inquiry and the scientific communities\npursuing them. Daston succinctly summarizes this evolution in the\nfollowing passage: \nObservation gradually became juxtaposed to other, more complex modes\nof inquiry such as experiment, “whose meaning shifted from the\nbroad and heterogeneous sense of experimentum as recipe, trial, or\njust common experience to a concertedly artificial manipulation, often\nusing special instruments and designed to probe hidden causes”\n(Daston 2011, 82). \nIn the 17th century, observation and experiment were seen as “an\ninseparable pair” (Daston 2011, 82) and by the 19th century they were\nunderstood to be essentially opposed, with the observer increasingly\nseen as passive and thus epistemically inferior to the\nexperimenter. In fact, already Leibniz anticipated this view stating\nthat “[t]here are certain experiments that would be better called\nobservations, in which one considers rather than produces the work”\n(Daston 2011, 86). This aspect of the distinction has been a mainstay\nof understanding scientific practice ever since. \nApart from this historical analysis, there are currently two prominent\nand opposed views of the experiment-observation distinction. Ian\nHacking has characterized it as well-defined, while avoiding the claim\nthat observation and experiment are opposites (Hacking 1983,\n173). According to him, the notions signify different things in\nscientific practice. The experiment is a thorough manipulation that\ncreates a new phenomenon, and observation of the phenomenon is its\noutcome. If scientists can manipulate a domain of nature to such an\nextent that they can create a new phenomenon in a lab, a phenomenon\nthat normally cannot be observed in nature, then they have truly\nobserved the phenomenon (Hacking 1989, 1992). \nMeanwhile, other authors concur that “the familiar distinction between\nobservation and experiment … [is] an artefact of the\ndisembodied, reconstructed character of retrospective accounts”\n(Gooding 1992, 68). The distinction “collapses” when we are faced with\nactual scientific practice as a process, and “Hacking’s observation\nversus experiment framework does not survive intact when put to the\ntest in a range of cases of scientific experimentation” (Malik 2017,\n85).  First, the uses of the distinction cannot be compared across\nscientific fields. And second, as Gooding (1992) suggests, observation\nis a process too, not simply a static result of manipulation. Thus,\nboth observation and experiment are seen as concurrent processes\nblended together in scientific practice. \nMalik (2017, 86) states that these arguments are the reason why “very\nfew [authors] use Hacking’s nomenclature of observation/experiment”\nand goes so far to conclude that “to (try to) distinguish between\nobservation and experiment is futile.” There is no point delineating\nthe two except perhaps in certain narrow domains; e.g., Hacking’s\nnotion of the experiment based on creating phenomena might be useful\nwithin a narrow domain of particle physics. (See also Chang 2011.) He\nadvocates avoiding the distinction altogether and opting for “the\nterminology [that] underlines this sense of continuousness” (Malik\n2017, 88) instead. If we want to analyze scientific practice, the\nargument goes, we should leave behind the idea of the distinction as\nfundamental and turn to the characterization and analysis of various\n“epistemic activities” instead, e.g., along the lines suggested by\nChang (2011). \nA rather obvious danger of this approach is an over-emphasis on the\ncontinuousness of the notions of observation and experiment that\nresults in inadvertent equivocation. And this, in turn, results in\nsidelining the distinction and its subtleties in the analysis of the\nscientific practice, despite their crucial role in articulating and\ndeveloping that practice since the 17th century. This issue certainly\nrequires further philosophical and historical analysis. \n\nIn this entry varying views on the nature of experimental results have\nbeen presented. Some argue that the acceptance of experimental results\nis based on epistemological arguments, whereas others base acceptance\non future utility, social interests, or agreement with existing\ncommunity commitments. Everyone agrees , however, that for whatever\nreasons, a consensus is reached on experimental results. These results\nthen play many important roles in physics and we have examined several\nof these roles, although certainly not all of them. We have seen\nexperiment deciding between two competing theories, calling for a new\ntheory, confirming a theory, refuting a theory, providing evidence that\ndetermined the mathematical form of a theory, and providing evidence\nfor the existence of an elementary particle involved in an accepted\ntheory. We have also seen that experiment has a life of its own,\nindependent of theory. If, as I believe, epistemological procedures\nprovide grounds for reasonable belief in experimental results, then\nexperiment can legitimately play the roles I have discussed and can\nprovide the basis for scientific knowledge. ","contact.mail":"allan.franklin@colorado.edu","contact.domain":"colorado.edu"},{"date.published":"1998-10-05","date.changed":"2019-10-16","url":"https://plato.stanford.edu/entries/physics-experiment/","author1":"Allan Franklin","author1.info":"http://spot.colorado.edu/~franklia/","author2.info":"http://slobodanperovic.weebly.com/","entry":"physics-experiment","body.text":"\n\n\n\nPhysics, and natural science in general, is a reasonable enterprise\nbased on valid experimental evidence, criticism, and rational\ndiscussion. It provides us with knowledge of the physical world, and it\nis experiment that provides the evidence that grounds this knowledge.\nExperiment plays many roles in science. One of its important roles is\nto test theories and to provide the basis for scientific\n knowledge.[1]\n It can also call for a new theory, either by\nshowing that an accepted theory is incorrect, or by exhibiting a new\nphenomenon that is in need of explanation. Experiment can provide hints\ntoward the structure or mathematical form of a theory and it can\nprovide evidence for the existence of the entities involved in our\ntheories. Finally, it may also have a life of its own, independent of\ntheory. Scientists may investigate a phenomenon just because it looks\ninteresting. Such experiments may provide evidence for a future theory\nto explain. [Examples of these different roles will be presented\nbelow.] As we shall see below, a single experiment may play several of\nthese roles at once.\n\n\n\nIf experiment is to play these important roles in science then we\nmust have good reasons to believe experimental results, for science is\na fallible enterprise. Theoretical calculations, experimental results,\nor the comparison between experiment and theory may all be wrong.\nScience is more complex than “The scientist proposes, Nature disposes.”\nIt may not always be clear what the scientist is proposing. Theories\noften need to be articulated and clarified. It also may not be clear\nhow Nature is disposing. Experiments may not always give clear-cut\nresults, and may even disagree for a time.\n\n\n\nIn what follows, the reader will find an epistemology of experiment,\na set of strategies that provides reasonable belief in experimental\nresults. Scientific knowledge can then be reasonably based on these\nexperimental results.\n\n\n\nThe 17th century witnessed the first philosophical reflections on the\nnature of experimentation. This should not be surprising given that\nexperiment was emerging as a central scientific tool at the time. The\naim of these reflections was to uncover why nature reveals its hidden\naspects to us when we force experimental methods upon it.  Some natural philosophers believed that scientific knowledge was\nlittle more than the proper application of observational and\nexperimental techniques on natural phenomena. Francis Bacon went so\nfar as to claim that it was possible to perform what he called a\ncrucial experiment (experimentum crucis), an ideal experiment of sorts\nthat can determine alone which of two rival hypotheses is correct. And\neven some of the giants of modern science such as Newton subscribed to\nthe view that scientific theories are directly induced from\nexperimental results and observations without the help of untested\nhypotheses. It is little wonder, then, that many natural philosophers\nthought that experimental techniques and their proper application\nshould be a primary object of philosophical study of science. \n\nThomas Kuhn and Paul Feyerabend vigorously criticized this view. They\nargued that observations and experimental results are already part of\na theoretical framework and thus cannot confirm a theory\nindependently. Nor there is a theory-neutral language for capturing\nobservations. Even a simple reading of a mercury thermometer\ninevitably depends on a theoretically-charged concept of\ntemperature. In short, the evidence is always theory-laden. \n\nYet neither the proponents of logical positivism nor their critics\never attempted to explain the nature of experimentation that produces\nall-important observational statements. And the reason for this was\nvery simple: they didn’t think that there was anything interesting to\nexplain. Their views on the relationship between theory and evidence\nwere diametrically opposed, but they all found only the final product\nof experimentation, namely observational statements, philosophically\ninteresting. As a result, the experimental process itself was set\naside in their philosophical study of science. This has gradually\nchanged only with the advent of New Experimentalism, with Ian\nHacking’s work at its forefront. \nYet not everybody agreed.  Hobbes, for instance pointed out that human\nreason preceded experimental techniques and their application. He\nthought that human reasoning reveals to us the natural law, and\ncriticized Boyle’s optimism regarding experimental method’s ability to\nreveal it (Shapin and Schaffer 1984). Doesn’t human reason guide\nexperimenter’s actions, in the way it leads us to choose data and\nsamples, and the way it allows us to interpret them, after all? If so,\nwe should focus on the philosophical study of reason and theoretical\nscientific reasoning rather than on the study of experimental\ntechniques and their applications. \n  \nThis vigorous early debate in many ways anticipated the main points of\ndisagreement in debates to come. Yet the philosophical interest in\nexperimentation almost completely lost its steam at the end of the\n19th century and did not recover until fairly late in the 20th\ncentury. \n\nDuring that period philosophers turned much of their attention to the\nstudy of the logical structure of scientific theories and its\nconnection to evidence. The tenets of logical positivism influenced\nthis area of investigation — as well as philosophy more\ngenerally — at the time. One of these tenets stated that\nobservational and theoretical propositions in science are\nseparable. My readings of the gradation on the scale of a mercury\nthermometer can be separated from rather complicated theoretical\nstatements concerning heat transfer and the theoretical concept of\ntemperature. \n\nIn fact, not only can one separate theory and observation, but the\nformer is considered justified only in light of its correspondence\nwith the latter. The theory of heat transfer is confirmed by\npropositions originating in the kind of readings I perform on my\nmercury thermometer. Thus, observational propositions are simply a\nresult of an experiment or a set of observations a scientist performs\nin order to confirm or refute a theory. \n\nIt has been almost four decades since Ian Hacking asked, “Do we see\nthrough a microscope?” (Hacking 1981). Hacking’s question really\nasked how do we come to believe in an experimental result obtained\nwith a complex experimental apparatus? How do we distinguish between a\nvalid\n result[2]\n and an artifact created by that apparatus?\nIf experiment is to play all of the important roles in science\nmentioned above and to provide the evidential basis for scientific\nknowledge, then we must have good reasons to believe in those results.\nHacking provided an extended answer in the second half of\nRepresenting and Intervening (1983). He pointed out that even\nthough an experimental apparatus is laden with, at the very least, the\ntheory of the apparatus, observations remain robust despite changes in\nthe theory of the apparatus or in the theory of the phenomenon. His\nillustration was the sustained belief in microscope images despite the\nmajor change in the theory of the microscope when Abbe pointed out the\nimportance of diffraction in its operation. One reason Hacking gave for\nthis is that in making such observations the experimenters\nintervened—they manipulated the object under observation. Thus, in\nlooking at a cell through a microscope, one might inject fluid into the\ncell or stain the specimen. One expects the cell to change shape or\ncolor when this is done. Observing the predicted effect strengthens our\nbelief in both the proper operation of the microscope and in the\nobservation. This is true in general. Observing the predicted effect of\nan intervention strengthens our belief in both the proper operation of\nthe experimental apparatus and in the observations made with it.  \n\nHacking also discussed the strengthening of one’s belief in an\nobservation by independent confirmation. The fact that the same\npattern of dots—dense bodies in cells—is seen with\n“different” microscopes, (e.g. ordinary, polarizing,\nphase-contrast, fluorescence, interference, electron, acoustic etc.)\nargues for the validity of the observation.  One might question\nwhether “different” is a theory-laden term. After all, it\nis our theory of light and of the microscope that allows us to\nconsider these microscopes as different from each other. Nevertheless,\nthe argument holds. Hacking correctly argues that it would be a\npreposterous coincidence if the same pattern of dots were produced in\ntwo totally different kinds of physical systems. Different apparatuses\nhave different backgrounds and systematic errors, making the\ncoincidence, if it is an artifact, most unlikely. If it is a correct\nresult, and the instruments are working properly, the coincidence of\nresults is understandable. \n\nHacking’s answer is correct as far as it goes. It is, however,\nincomplete. What happens when one can perform the experiment with only\none type of apparatus, such as an electron microscope or a radio\ntelescope, or when intervention is either impossible or extremely\ndifficult? Other strategies are needed to validate the\n observation.[3]\n These may include: \n\nThese strategies along with Hacking’s intervention and independent\nconfirmation constitute an epistemology of experiment. They provide us\nwith good reasons for belief in experimental results, They do not,\nhowever, guarantee that the results are correct. There are many\nexperiments in which these strategies are applied, but whose results\nare later shown to be incorrect (examples will be presented below).\nExperiment is fallible. Neither are these strategies exclusive or\nexhaustive. No single one of them, or fixed combination of them,\nguarantees the validity of an experimental result. Physicists use as\nmany of the strategies as they can conveniently apply in any given\nexperiment.  \n\nIn How Experiments End (1987), Peter Galison extended the\ndiscussion of experiment to more complex situations. In his histories\nof the measurements of the gyromagnetic ratio of the electron, the\ndiscovery of the muon, and the discovery of weak neutral currents, he\nconsidered a series of experiments measuring a single quantity, a set\nof different experiments culminating in a discovery, and two high-\nenergy physics experiments performed by large groups with complex\nexperimental apparatus.  \n\nGalison’s view is that experiments end when the experimenters\nbelieve that they have a result that will stand up in court—a result\nthat I believe includes the use of the epistemological strategies\ndiscussed earlier. Thus, David Cline, one of the weak neutral-current\nexperimenters remarked, “At present I don’t see how to make these\neffects [the weak neutral current event candidates] go away” (Galison,\n1987, p. 235). \n\nGalison emphasizes that, within a large experimental group, different\nmembers of the group may find different pieces of evidence most\nconvincing. Thus, in the Gargamelle weak neutral current experiment,\nseveral group members found the single photograph of a\nneutrino-electron scattering event particularly important, whereas for\nothers the difference in spatial distribution between the observed\nneutral current candidates and the neutron background was decisive.\nGalison attributes this, in large part, to differences in experimental\ntraditions, in which scientists develop skill in using certain types\nof instruments or apparatus. In particle physics, for example, there\nis the tradition of visual detectors, such as the cloud chamber or the\nbubble chamber, in contrast to the electronic tradition of Geiger and\nscintillation counters and spark chambers. Scientists within the\nvisual tradition tend to prefer “golden events” that\nclearly demonstrate the phenomenon in question, whereas those in the\nelectronic tradition tend to find statistical arguments more\npersuasive and important than individual events. (For further\ndiscussion of this issue see Galison (1997)). \n\nGalison points out that major changes in theory and in experimental\npractice and instruments do not necessarily occur at the same time.\nThis persistence of experimental results provides continuity across\nthese conceptual changes. Thus, the experiments on the gyromagnetic\nratio spanned classical electromagnetism, Bohr’s old quantum theory,\nand the new quantum mechanics of Heisenberg and Schrodinger. Robert\nAckermann has offered a similar view in his discussion of scientific\ninstruments. \n\nGalison also discusses other aspects of the interaction between\nexperiment and theory. Theory may influence what is considered to be a\nreal effect, demanding explanation, and what is considered background.\nIn his discussion of the discovery of the muon, he argues that the\ncalculation of Oppenheimer and Carlson, which showed that showers were\nto be expected in the passage of electrons through matter, left the\npenetrating particles, later shown to be muons, as the unexplained\nphenomenon. Prior to their work, physicists thought the showering\nparticles were the problem, whereas the penetrating particles seemed to\nbe understood. \n\nThe role of theory as an “enabling theory,” (i.e., one that allows\ncalculation or estimation of the size of the expected effect and also\nthe size of expected backgrounds) is also discussed by Galison. (See\nalso (Franklin 1995) and the discussion of the Stern-Gerlach\nexperiment below). Such a theory can help to determine whether an\nexperiment is feasible. Galison also emphasizes that elimination of\nbackground that might simulate or mask an effect is central to the\nexperimental enterprise, and not a peripheral activity. In the case of\nthe weak neutral current experiments, the existence of the currents\ndepended crucially on showing that the event candidates could not all\nbe due to neutron\n background.[6] \n\nThere is also a danger that the design of an experiment may preclude\nobservation of a phenomenon. Galison points out that the original\ndesign of one of the neutral current experiments, which included a muon\ntrigger, would not have allowed the observation of neutral currents. In\nits original form the experiment was designed to observe charged\ncurrents, which produce a high energy muon. Neutral currents do not.\nTherefore, having a muon trigger precluded their observation. Only\nafter the theoretical importance of the search for neutral currents was\nemphasized to the experimenters was the trigger changed. Changing the\ndesign did not, of course, guarantee that neutral currents would be\nobserved. \n\nGalison also shows that the theoretical presuppositions of the\nexperimenters may enter into the decision to end an experiment and\nreport the result. Einstein and de Haas ended their search for\nsystematic errors when their value for the gyromagnetic ratio of the\nelectron, \\(g = 1\\), agreed with their theoretical model of\norbiting electrons. This effect of presuppositions might cause one to\nbe skeptical of both experimental results and their role in theory\nevaluation. Galison’s history shows, however, that, in this case, the\nimportance of the measurement led to many repetitions of the\nmeasurement. This resulted in an agreed-upon result that disagreed with\ntheoretical expectations. \n\nRecently, Galison has modified his views. In Image and Logic,\nan extended study of instrumentation in 20th-century high-energy\nphysics, Galison (1997) has extended his argument that there are two\ndistinct experimental traditions within that field—the visual (or\nimage) tradition and the electronic (or logic) tradition.  The image\ntradition uses detectors such as cloud chambers or bubble chambers,\nwhich provide detailed and extensive information about each individual\nevent. The electronic detectors used by the logic tradition, such as\ngeiger counters, scintillation counters, and spark chambers, provide\nless detailed information about individual events, but detect more\nevents. Galison’s view is that experimenters working in these two\ntraditions form distinct epistemic and linguistic groups that rely on\ndifferent forms of argument. The visual tradition emphasizes the\nsingle “golden” event. “On the image side resides a\ndeep-seated commitment to the ‘golden event’: the single\npicture of such clarity and distinctness that it commands\nacceptance.” (Galison, 1997, p. 22) “The golden event was\nthe exemplar of the image tradition: an individual instance so\ncomplete and well defined, so ‘manifestly’ free of\ndistortion and background that no further data had to be\ninvolved” (p. 23). Because the individual events provided in the\nlogic detectors contained less detailed information than the pictures\nof the visual tradition, statistical arguments based on large numbers\nof events were required. \n\nKent Staley (1999) disagrees. He argues that the two traditions are\nnot as distinct as Galison believes: \n\nStaley believes that although there is certainly epistemic\ncontinuity within a given tradition, there is also a continuity between\nthe traditions. This does not, I believe, mean that the shared\ncommitment comprises all of the arguments offered in any particular\ninstance, but rather that the same methods are often used by both\ncommunities. Galison does not deny that statistical methods are used in\nthe image tradition, but he thinks that they are relatively\nunimportant. “While statistics could certainly be used within the image\ntradition, it was by no means necessary for most applications”\n(Galison, 1997, p. 451). In contrast, Galison believes that arguments\nin the logic tradition “were inherently and inalienably statistical.\nEstimation of probable errors and the statistical excess over\nbackground is not a side issue in these detectors—it is central to the\npossibility of any demonstration at all” (p. 451). \n\nAlthough a detailed discussion of the disagreement between Staley\nand Galison would take us too far from the subject of this essay, they\nboth agree that arguments are offered for the correctness of\nexperimental results. Their disagreement concerns the nature of those\narguments. (For further discussion see Franklin, (2002), pp. 9–17). \n\nCollins, Pickering, and others, have raised objections to the view that\nexperimental results are accepted on the basis of epistemological\narguments. They point out that “a sufficiently determined critic can\nalways find a reason to dispute any alleged ‘result’”\n(MacKenzie 1989, p. 412). Harry Collins, for example, is well known for\nhis skepticism concerning both experimental results and evidence. He\ndevelops an argument that he calls the “experimenters’ regress”\n(Collins 1985, chapter 4, pp. 79–111): What scientists take to be a\ncorrect result is one obtained with a good, that is, properly\nfunctioning, experimental apparatus. But a good experimental apparatus\nis simply one that gives correct results. Collins claims that there are\nno formal criteria that one can apply to decide whether or not an\nexperimental apparatus is working properly. In particular, he argues\nthat calibrating an experimental apparatus by using a surrogate signal\ncannot provide an independent reason for considering the apparatus to\nbe reliable.  \n\nIn Collins’ view the regress is eventually broken by negotiation\nwithin the appropriate scientific community, a process driven by\nfactors such as the career, social, and cognitive interests of the\nscientists, and the perceived utility for future work, but one that is\nnot decided by what we might call epistemological criteria, or reasoned\njudgment. Thus, Collins concludes that his regress raises serious\nquestions concerning both experimental evidence and its use in the\nevaluation of scientific hypotheses and theories. Indeed, if no way out\nof the regress can be found, then he has a point. \n\nCollins strongest candidate for an example of the experimenters’\nregress is presented in his history of the early attempts to detect\ngravitational radiation, or gravity waves. (For more detailed\ndiscussion of this episode see (Collins 1985; 1994; Franklin 1994;\n1997a) In this case, the physics community was forced to compare\nWeber’s claims that he had observed gravity waves with the reports from\nsix other experiments that failed to detect them. On the one hand,\nCollins argues that the decision between these conflicting experimental\nresults could not be made on epistemological or methodological\ngrounds—he claims that the six negative experiments could not\nlegitimately be regarded as\n replications[7]\n and hence become less\nimpressive. On the other hand, Weber’s apparatus, precisely because the\nexperiments used a new type of apparatus to try to detect a hitherto\nunobserved\n phenomenon,[8]\n could not be subjected to standard\ncalibration techniques. \n\nThe results presented by Weber’s critics were not only more\nnumerous, but they had also been carefully cross-checked. The groups\nhad exchanged both data and analysis programs and confirmed their\nresults. The critics had also investigated whether or not their\nanalysis procedure, the use of a linear algorithm, could account for\ntheir failure to observe Weber’s reported results. They had used\nWeber’s preferred procedure, a nonlinear algorithm, to analyze their\nown data, and still found no sign of an effect. They had also\ncalibrated their experimental apparatuses by inserting acoustic pulses\nof known energy and finding that they could detect a signal. Weber, on\nthe other hand, as well as his critics using his analysis procedure,\ncould not detect such calibration pulses. \n\nThere were, in addition, several other serious questions raised\nabout Weber’s analysis procedures. These included an admitted\nprogramming error that generated spurious coincidences between Weber’s\ntwo detectors, possible selection bias by Weber, Weber’s report of\ncoincidences between two detectors when the data had been taken four\nhours apart, and whether or not Weber’s experimental apparatus could\nproduce the narrow coincidences claimed. \n\nIt seems clear that the critics’ results were far more credible than\nWeber’s. They had checked their results by independent confirmation,\nwhich included the sharing of data and analysis programs. They had also\neliminated a plausible source of error, that of the pulses being longer\nthan expected, by analyzing their results using the nonlinear algorithm\nand by explicitly searching for such long\n pulses.[9]\n They had also calibrated\ntheir apparatuses by injecting pulses of known energy and observing the\noutput. \n\nContrary to Collins, I believe that the scientific community made a\nreasoned judgment and rejected Weber’s results and accepted those of\nhis critics. Although no formal rules were applied (e.g. if you make\nfour errors, rather than three, your results lack credibility; or if\nthere are five, but not six, conflicting results, your work is still\ncredible) the procedure was reasonable. \n\nPickering has argued that the reasons for accepting results are the\nfuture utility of such results for both theoretical and experimental\npractice and the agreement of such results with the existing community\ncommitments. In discussing the discovery of weak neutral currents,\nPickering states, \n\nScientific communities tend to reject data that conflict with group\ncommitments and, obversely, to adjust their experimental techniques to\ntune in on phenomena consistent with those commitments. (1981, p.\n236) \n\nThe emphasis on future utility and existing commitments is clear. These\ntwo criteria do not necessarily agree. For example, there are episodes\nin the history of science in which more opportunity for future work is\nprovided by the overthrow of existing theory. (See, for example, the\nhistory of the overthrow of parity conservation and of CP symmetry\ndiscussed below and in (Franklin 1986, Ch. 1, 3)).  \n\nPickering has recently offered a different view of experimental\nresults. In his view the material procedure (including the experimental\napparatus itself along with setting it up, running it, and monitoring\nits operation), the theoretical model of that apparatus, and the\ntheoretical model of the phenomena under investigation are all plastic\nresources that the investigator brings into relations of mutual\nsupport. (Pickering 1987; Pickering 1989). He says:  \n\nHe uses Morpurgo’s search for free quarks, or fractional charges of\n\\(\\tfrac{1}{3} e\\) or \\(\\tfrac{2}{3} e\\), where \\(e\\) is the charge of the\nelectron. (See also (Gooding 1992)). Morpurgo used a modern\nMillikan-type apparatus and initially found a continuous distribution\nof charge values. Following some tinkering with the apparatus, Morpurgo\nfound that if he separated the capacitor plates he obtained only\nintegral values of charge. “After some theoretical analysis, Morpurgo\nconcluded that he now had his apparatus working properly, and reported\nhis failure to find any evidence for fractional charges” (Pickering\n1987, p. 197).  \n\nPickering goes on to note that Morpurgo did not tinker with the two\ncompeting theories of the phenomena then on offer, those of integral\nand fractional charge: \n\nThe conclusion of Morpurgo’s first series of experiments, then, and\nthe production of the observation report which they sustained, was\nmarked by bringing into relations of mutual support of the three\nelements I have discussed: the material form of the apparatus and the\ntwo conceptual models, one instrumental and the other phenomenal.\nAchieving such relations of mutual support is, I suggest, the defining\ncharacteristic of the successful experiment. (p. 199) \n\nPickering has made several important and valid points concerning\nexperiment. Most importantly, he has emphasized that an experimental\napparatus is initially rarely capable of producing a valid experimental\nresults and that some adjustment, or tinkering, is required before it\ndoes. He has also recognized that both the theory of the apparatus and\nthe theory of the phenomena can enter into the production of a valid\nexperimental result. What one may question, however, is the emphasis\nhe places on these theoretical components. From Millikan onwards,\nexperiments had strongly supported the existence of a fundamental unit\nof charge and charge quantization. The failure of Morpurgo’s apparatus to\nproduce measurements of integral charge indicated that it was not\noperating properly and that his theoretical understanding of it was\nfaulty. It was the failure to produce measurements in agreement with\nwhat was already known (i.e., the failure of an important experimental\ncheck) that caused doubts about Morpurgo’s measurements. This was true\nregardless of the theoretical models available, or those that Morpurgo\nwas willing to accept. It was only when Morpurgo’s apparatus could\nreproduce known measurements that it could be trusted and used to\nsearch for fractional charge. To be sure, Pickering has allowed a role\nfor the natural world in the production of the experimental result, but\nit does not seem to be decisive. \n\nAckermann has offered a modification of Pickering’s view. He suggests\nthat the experimental apparatus itself is a less plastic resource then\neither the theoretical model of the apparatus or that of the\nphenomenon.  \n\nHacking (1992) has also offered a more complex version of\nPickering’s later view. He suggests that the results of mature\nlaboratory science achieve stability and are self-vindicating when the\nelements of laboratory science are brought into mutual consistency and\nsupport. These are (1) ideas: questions, background knowledge,\nsystematic theory, topical hypotheses, and modeling of the apparatus;\n(2) things: target, source of modification, detectors, tools, and data\ngenerators; and (3) marks and the manipulation of marks: data, data\nassessment, data reduction, data analysis, and interpretation. \n\nWe invent devices that produce data and isolate or create phenomena,\nand a network of different levels of theory is true to these phenomena.\nConversely we may in the end count them only as phenomena only when the\ndata can be interpreted by theory. (pp. 57–8) \n\nOne might ask whether such mutual adjustment between theory and\nexperimental results can always be achieved? What happens when an\nexperimental result is produced by an apparatus on which several of the\nepistemological strategies, discussed earlier, have been successfully\napplied, and the result is in disagreement with our theory of the\nphenomenon? Accepted theories can be refuted. Several examples will be\npresented below.  \n\nHacking himself worries about what happens when a laboratory science\nthat is true to the phenomena generated in the laboratory, thanks to\nmutual adjustment and self-vindication, is successfully applied to the\nworld outside the laboratory. Does this argue for the truth of the\nscience. In Hacking’s view it does not. If laboratory science does\nproduce happy effects in the “untamed world,… it is not the\ntruth of anything that causes or explains the happy effects” (1992, p.\n60). \n\nRecently Pickering has offered a somewhat revised account of science.\n“My basic image of science is a performative one, in which the\nperformances the doings of human and material agency come to the fore.\nScientists are human agents in a field of material agency which they\nstruggle to capture in machines (Pickering, 1995, p. 21).” He then\ndiscusses the complex interaction between human and material agency,\nwhich I interpret as the interaction between experimenters, their\napparatus, and the natural world.  \n\nPickering’s idea of resistance is illustrated by Morpurgo’s\nobservation of continuous, rather than integral or fractional,\nelectrical charge, which did not agree with his expectations.\nMorpurgo’s accommodation consisted of changing his experimental\napparatus by using a larger separation between his plates, and also by\nmodifying his theoretical account of the apparatus. That being done,\nintegral charges were observed and the result stabilized by the mutual\nagreement of the apparatus, the theory of the apparatus, and the\ntheory of the phenomenon. Pickering notes that ”the outcomes\ndepend on how the world is (p. 182).“ ”In this way,\nthen, how the material world is leaks into and infects our\nrepresentations of it in a nontrivial and consequential fashion. My\nanalysis thus displays an intimate and responsive engagement between\nscientific knowledge and the material world that is integral to\nscientific practice (p. 183).“ \n\nNevertheless there is something confusing about Pickering’s invocation\nof the natural world. Although Pickering acknowledges the importance\nof the natural world, his use of the term ”infects“ seems\nto indicate that he isn’t entirely happy with this. Nor does the\nnatural world seem to have much efficacy. It never seems to be\ndecisive in any of Pickering’s case studies. Recall that he argued\nthat physicists accepted the existence of weak neutral currents\nbecause ”they could ply their trade more profitably in a world\nin which the neutral current was real.“ In his account,\nMorpurgo’s observation of continuous charge is important only because\nit disagrees with his theoretical models of the phenomenon. The fact\nthat it disagreed with numerous previous observations of integral\ncharge doesn’t seem to matter. This is further illustrated by\nPickering’s discussion of the conflict between Morpurgo and\nFairbank. As we have seen, Morpurgo reported that he did not observe\nfractional electrical charges. On the other hand, in the late 1970s\nand early 1980s, Fairbank and his collaborators published a series of\npapers in which they claimed to have observed fractional charges (See,\nfor example, LaRue, Phillips et al. 1981 ). Faced with this discord\nPickering concludes, \n\nThe natural world seems to have disappeared from Pickering’s\naccount. There is a real question here as to whether or not fractional\ncharges exist in nature. The conclusions reached by Fairbank and by\nMorpurgo about their existence cannot both be correct. It seems\ninsufficient to merely state, as Pickering does, that Fairbank and\nMorpurgo achieved their individual stabilizations and to leave the\nconflict unresolved. (Pickering does comment that one could follow the\nsubsequent history and see how the conflict was resolved, and he does\ngive some brief statements about it, but its resolution is not\nimportant for him). At the very least one should consider the actions\nof the scientific community. Scientific knowledge is not determined\nindividually, but communally. Pickering seems to acknowledge\nthis. ”One might, therefore, want to set up a metric and say\nthat items of scientific knowledge are more or less objective\ndepending on the extent to which they are threaded into the rest of\nscientific culture, socially stabilized over time, and so on. I can\nsee nothing wrong with thinking this way…. (p. 196).“ The\nfact that Fairbank believed in the existence of fractional electrical\ncharges, or that Weber strongly believed that he had observed gravity\nwaves, does not make them right. These are questions about the natural\nworld that can be resolved. Either fractional charges and gravity\nwaves exist or they don’t, or to be more cautious we might say that we\nhave good reasons to support our claims about their existence, or we\ndo not. \n\nAnother issue neglected by Pickering is the question of whether a\nparticular mutual adjustment of theory, of the apparatus or the\nphenomenon, and the experimental apparatus and evidence is justified.\nPickering seems to believe that any such adjustment that provides\nstabilization, either for an individual or for the community, is\nacceptable. Others disagree. They note that experimenters sometimes\nexclude data and engage in selective analysis procedures in producing\nexperimental results.  These practices are, at the very least,\nquestionable as is the use of the results produced by such practices\nin science. There are, in fact, procedures in the normal practice of\nscience that provide safeguards against them. (For details see\nFranklin, 2002, Section 1). \n\nThe difference in attitudes toward the resolution of discord is one of\nthe important distinctions between Pickering’s and Franklin’s view of\nscience.  Franklin remarks that it is insufficient simply to say that\nthe resolution is socially stabilized. The important question is how\nthat resolution was achieved and what were the reasons offered for\nthat resolution. If we are faced with discordant experimental results\nand both experimenters have offered reasonable arguments for their\ncorrectness, then clearly more work is needed. It seems reasonable, in\nsuch cases, for the physics community to search for an error in one,\nor both, of the experiments. \n\nPickering discusses yet another difference between his view and that\nof Franklin. Pickering sees traditional philosophy of science as\nregarding objectivity ”as stemming from a peculiar kind of\nmental hygiene or policing of thought.  This police function relates\nspecifically to theory choice in science, which,… is usually\ndiscussed in terms of the rational rules or methods responsible for\nclosure in theoretical debate (p. 197).“ He goes on to remark\nthat, \n\nFor further discussion see (Franklin 1993b)). Although Franklin’s\nepistemology of experiment is designed to offer good reasons for\nbelief in experimental results, they are not a set of rules. Franklin\nregards them as a set of strategies, from which physicists choose, in\norder to argue for the correctness of their results. As noted above,\nthe strategies offered are neither exclusive or exhaustive. \n\nThere is another point of disagreement between Pickering and Franklin.\nPickering claims to be dealing with the practice of science, and yet he\nexcludes certain practices from his discussions. One scientific\npractice is the application of the epistemological strategies \noutlined above to argue for the correctness of an experimental results.\nIn fact, one of the essential features of an experimental paper is the\npresentation of such arguments. Writing such\npapers, a performative act, is also a scientific practice and it would\nseem reasonable to examine both the structure and content of those\npapers. \n\nRecently Ian Hacking (1999, chapter 3) has provided an incisive and\ninteresting discussion of the issues that divide the constructivists\n(Collins, Pickering, etc.) from the rationalists (Stuewer, Franklin,\nBuchwald, etc.). He sets out three sticking points between the two\nviews: 1) contingency, 2) nominalism, and 3) external explanations of\nstability.  \n\nContingency is the idea that science is not predetermined, that it\ncould have developed in any one of several successful ways. This is the\nview adopted by constructivists. Hacking illustrates this with\nPickering’s account of high-energy physics during the 1970s during\nwhich the quark model came to dominate. (See Pickering 1984a). \n\nTo sum up Pickering’s doctrine: there could have been a research\nprogram as successful (“progressive”) as that of\nhigh-energy physics in the 1970s, but with different theories,\nphenomenology, schematic descriptions of apparatus, and apparatus, and\nwith a different, and progressive, series of robust fits between these\ningredients. Moreover and this is something badly in need of\nclarification the “different” physics would not have been\nequivalent to present physics. Not logically incompatible with, just\ndifferent. \n\nThe constructionist about (the idea) of quarks thus claims that the\nupshot of this process of accommodation and resistance is not fully\npredetermined. Laboratory work requires that we get a robust fit\nbetween apparatus, beliefs about the apparatus, interpretations and\nanalyses of data, and theories. Before a robust fit has been\nachieved, it is not determined what that fit will be. Not determined by\nhow the world is, not determined by technology now in existence, not\ndetermined by the social practices of scientists, not determined by\ninterests or networks, not determined by genius, not determined by\nanything (pp. 72–73, emphasis added). \n\nMuch depends here on what Hacking means by “determined.”\nIf he means entailed then one must agree with him. It is doubtful that\nthe world, or more properly, what we can learn about it, entails a\nunique theory. If not, as seems more plausible, he means that the way\nthe world is places no restrictions on that successful science, then\nthe rationalists disagree strongly. They want to argue that the way the\nworld is restricts the kinds of theories that will fit the phenomena,\nthe kinds of apparatus we can build, and the results we can obtain\nwith such apparatuses. To think otherwise seems silly. Consider a\nhomey example.  It seems highly unlikely that someone can come up with\na successful theory in which objects whose density is greater than\nthat of air fall upwards. This is not a caricature of the view Hacking\ndescribes. Describing Pickering’s view, he states, “Physics did\nnot need to take a route that involved Maxwell’s Equations, the Second\nLaw of Thermodynamics, or the present values of the velocity of light\n(p. 70).” Although one may have some sympathy for this view as\nregards Maxwell’s Equations or the Second Law of Thermodynamics, one\nmay not agree about the value of the speed of light. That is\ndetermined by the way the world is. Any successful theory of light\nmust give that value for its speed. \n\nAt the other extreme are the “inevitablists,” among whom\nHacking classifies most scientists. He cites Sheldon Glashow, a Nobel\nPrize winner, “Any intelligent alien anywhere would have come\nupon the same logical system as we have to explain the structure of\nprotons and the nature of supernovae (Glashow 1992, p. 28).” \n\nAnother difference between Pickering and Franklin on contingency\nconcerns the question of not whether an alternative is possible, but\nrather whether there are reasons why that alternative should be\npursued. Pickering seems to identify can with\nought. \n\nIn the late 1970s there was a disagreement between the results of\nlow-energy experiments on atomic parity violation (the violation of\nleft-right symmetry) performed at the University of Washington and at\nOxford University and the result of a high-energy experiment on the\nscattering of polarized electrons from deuterium (the SLAC E122\nexperiment). The atomic-parity violation experiments failed to observe\nthe parity-violating effects predicted by the Weinberg- Salam (W-S)\nunified theory of electroweak interactions, whereas the SLAC experiment\nobserved the predicted effect. These early atomic physics\nresults were quite uncertain in themselves and that uncertainty was\nincreased by positive results obtained in similar experiments at\nBerkeley and Novosibirsk. At the time the theory had other evidential\nsupport, but was not universally accepted. Pickering and Franklin are in\nagreement that the W-S theory was accepted on the basis of the SLAC\nE122 result. They differ dramatically in their discussions of the\nexperiments. Their difference on contingency concerns a particular\ntheoretical alternative that was proposed at the time to explain the\ndiscrepancy between the experimental results. \n\nPickering asked why a theorist might not have attempted to find a\nvariant of electroweak gauge theory that might have reconciled the\nWashington-Oxford atomic parity results with the positive E122 result.\n(What such a theorist was supposed to do with the supportive atomic\nparity results later provided by experiments at Berkeley and at\nNovosibirsk is never mentioned). “But though it is true that\nE122 analysed their data in a way that displayed the improbability\n[the probability of the fit to the hybrid model was 6 ×\n10−4] of a particular class of variant gauge\ntheories, the so-called ‘hybrid models,’ I do not believe\nthat it would have been impossible to devise yet more variants”\n(Pickering 1991, p. 462). Pickering notes that open-ended recipes for\nconstructing such variants had been written down as early as 1972\n(p. 467). It would have been possible to do so, but one\nmay ask whether or not a scientist might have wished to do so. If the\nscientist agreed with Franklin’s view that the SLAC E122 experiment provided\nconsiderable evidential weight in support of the W-S theory and that a\nset of conflicting and uncertain results from atomic parity-violation\nexperiments gave an equivocal answer on that support, what reason\nwould they have had to invent an alternative? \n\nThis is not to suggest that scientists do not, or should not, engage\nin speculation, but rather that there was no necessity to do so in this\ncase. Theorists often do propose alternatives to existing,\nwell-confirmed theories. \n\nConstructivist case studies always seem to result in the support of\nexisting, accepted theory (Pickering 1984a; 1984b; 1991; Collins 1985;\nCollins and Pinch 1993). One criticism implied in such cases is that\nalternatives are not considered, that the hypothesis space of\nacceptable alternatives is either very small or empty. One may\nseriously question this. Thus, when the experiment of\nChristenson et al. (1964) detected \\(\\ce{K2^0}\\)\ndecay into two pions, which seemed to show that CP symmetry (combined\nparticle-antiparticle and space inversion symmetry) was violated, no\nfewer than 10 alternatives were offered. These included (1) the\ncosmological model resulting from the local dysymmetry of matter and\nantimatter, (2) external fields, (3) the decay of the\n\\(\\ce{K2^0}\\) into a \\(\\ce{K1^0}\\) with the\nsubsequent decay of the \\(\\ce{K1^0}\\)into two pions,\nwhich was allowed by the symmetry, (4) the emission of another neutral\nparticle, “the paritino,” in the \\(\\ce{K2^0}\\)\ndecay, similar to the emission of the neutrino in beta decay, (5) that\none of the pions emitted in the decay was in fact a\n“spion,” a pion with spin one rather than zero, (6) that\nthe decay was due to another neutral particle, the L, produced\ncoherently with the \\(\\ce{K^0}\\), (7) the existence of a\n“shadow” universe, which interacted with out universe only\nthrough the weak interactions, and that the decay seen was the decay\nof the “shadow \\(\\ce{K2^0}\\),” (8) the failure\nof the exponential decay law, 9) the failure of the principle of\nsuperposition in quantum mechanics, and 10) that the decay pions were\nnot bosons. \n\nAs one can see, the limits placed on alternatives were not very\nstringent. By the end of 1967, all of the alternatives had been tested\nand found wanting, leaving CP symmetry unprotected. Here the differing\njudgments of the scientific community about what was worth proposing\nand pursuing led to a wide variety of alternatives being tested. \n\nHacking’s second sticking point is nominalism, or name-ism. He notes\nthat in its most extreme form nominalism denies that there is anything\nin common or peculiar to objects selected by a name, such as “Douglas\nfir” other than that they are called Douglas fir. Opponents contend\nthat good names, or good accounts of nature, tell us something correct\nabout the world. This is related to the realism-antirealism debate\nconcerning the status of unobservable entities that has plagued\nphilosophers for millennia. For example Bas van Fraassen (1980), an\nantirealist, holds that we have no grounds for belief in unobservable\nentities such as the electron and that accepting theories about the\nelectron means only that we believe that the things the theory says\nabout observables is true. A realist claims that electrons really exist\nand that as, for example, Wilfred Sellars remarked, “to have good\nreason for holding a theory is ipso facto to have good reason\nfor holding that the entities postulated by the theory exist (Sellars\n1962, p. 97).” In Hacking’s view a scientific nominalist is more\nradical than an antirealist and is just as skeptical about fir trees as\nthey are about electrons. A nominalist further believes that the\nstructures we conceive of are properties of our representations of the\nworld and not of the world itself. Hacking refers to opponents of that\nview as inherent structuralists. \n\nHacking also remarks that this point is related to the question of\n“scientific facts.” Thus, constructivists Latour and\nWoolgar originally entitled their book Laboratory Life: The Social\nConstruction of Scientific Facts (1979). Andrew Pickering\nentitled his history of the quark model Constructing Quarks\n(Pickering 1984a).  Physicists argue that this demeans their\nwork. Steven Weinberg, a realist and a physicist, criticized\nPickering’s title by noting that no mountaineer would ever name a\nbook Constructing Everest. For Weinberg, quarks and Mount\nEverest have the same ontological status.  They are both facts about\nthe world. Hacking argues that constructivists do not, despite\nappearances, believe that facts do not exist, or that there is no such\nthing as reality. He cites Latour and Woolgar “that\n‘out-there-ness’ is a consequence of scientific work\nrather than its cause (Latour and Woolgar 1986, p. 180).”\nHacking reasonably concludes that, \n\nOne might add, however, that the reasons Hacking cites as supporting\nthat belief are given to us by valid experimental evidence and not by\nthe social and personal interests of scientists. \nLatour and Woolgar might not agree. Franklin argues that we have good\nreasons to believe in facts, and in the entities involved in our\ntheories, always remembering, of course, that science is fallible. \n\nHacking’s third sticking point is the external explanations of\nstability. \n\nRationalists think that most science proceeds as it does in the\nlight of good reasons produced by research. Some bodies of knowledge\nbecome stable because of the wealth of good theoretical and\nexperimental reasons that can be adduced for them. Constructivists\nthink that the reasons are not decisive for the course of science.\nNelson (1994) concludes that this issue will never be decided.\nRationalists, at least retrospectively, can always adduce reasons that\nsatisfy them. Constructivists, with equal ingenuity, can always find to\ntheir own satisfaction an openness where the upshot of research is\nsettled by something other than reason. Something external. That is one\nway of saying we have found an irresoluble “sticking point” (pp.\n91–92) \n\nThus, there is a rather severe disagreement on the reasons for the\nacceptance of experimental results. For some, like Staley, Galison and\nFranklin, it is because of epistemological arguments. For others, like\nPickering, the reasons are utility for future practice and agreement\nwith existing theoretical commitments. Although the history of science\nshows that the overthrow of a well-accepted theory leads to an enormous\namount of theoretical and experimental work, proponents of this view\nseem to accept it as unproblematical that it is always agreement with\nexisting theory that has more future utility. Hacking and Pickering\nalso suggest that experimental results are accepted on the basis of the\nmutual adjustment of elements which includes the theory of the\nphenomenon. \n\nNevertheless, everyone seems to agree that a consensus does arise on\nexperimental results. \n\nAuthors like Thomas Kuhn and Paul Feyerabend put forward the view that\nevidence does not confirm or refute a scientific theory since it is\nladen by it. Evidence is not a set of observational sentences\nautonomous from theoretical ones, as logical positivists\nbelieved. Each new theory or a theoretical paradigm, as Kuhn labeled\nlarger theoretical frameworks, produces, as it were, evidence\nanew.  Thus, theoretical concepts infect the entire experimental process\nfrom the stage of design and preparation to the production and\nanalysis of data. A simple example that is supposed to convincingly\nillustrate this view are measurements of temperature with a mercury\nthermometer one uses in order to test whether objects expand when\ntheir temperature increases. Note that in such a case one tests the\nhypothesis by relying on the very assumption that the expansion of\nmercury indicates increase in temperature.  There may be a fairly simple way out of the vicious circle in\nwhich theory and experiment are caught in this particular case of\ntheory-ladenness. It may suffice to calibrate the mercury thermometer\nwith a constant volume gas thermometer, for example, where its use\ndoes not rely on the tested hypothesis but on the proportionality of\nthe pressure of the gas and its absolute temperature (Franklin et\nal. 1989).  Although most experiments are far more complex than this toy\nexample, one could certainly approach the view that experimental\nresults are theory-laden on a case-by-case basis. Yet there may be a\nmore general problem with the view. \nBogen and Woodward (1988) argued that debate on the relationship\nbetween theory and observation overlooks a key ingredient in the\nproduction of experimental evidence, namely the experimental\nphenomena. The experimentalists distill experimental phenomena from\nraw experimental data (e.g. electronic or digital tracks in particle\ncolliders) using various tools of statistical analysis. Thus,\nidentification of an experimental phenomenon as significant (e.g. a\npeak at a particular energy of colliding beams) is free of the theory\nthat the experiment may be designed to test (e.g. the prediction of a\nparticular particle). Only when significant phenomenon has been\nidentified can a stage of data analysis begin in which the phenomenon\nis deemed to either support or refute a theory. Thus, the\ntheory-ladenness of evidence thesis fails at least in some experiments\nin physics. \n\nThe authors substantiate their argument in part through an analysis of\nexperiments that led to a breakthrough discovery of weak neutral\ncurrents. It is a type of force produced by so-called bosons —\nshort-lived particles responsible for energy transfer between other\nparticles such as hadrons and leptons. The relevant peaks were\nrecognized as significant via statistical analysis of data, and later\non interpreted as evidence for the existence of the bosons. \nThis view and the case study have recently been challenged by\nSchindler (2011). He argues that the tested theory was critical in the\nassessment of the reliability of data in the experiments with weak\nneutral currents. He also points out that, on occasion, experimental\ndata can even be ignored if they are deemed irrelevant from a\ntheoretical perspective that physicists find particularly\ncompelling. This was the case in experiments with so-called zebra\npattern magnetic anomalies on the ocean floor. The readings of new\napparatuses used to scan the ocean floor produced intriguing\nsignals. Yet the researchers could not interpret these signals\nmeaningfully or satisfyingly distinguish them from noise unless they\nrelied on some theoretical account of both the structure of the ocean\nfloor and the earth’s magnetic field. \n\nKaraca (2013) points out that a crude theory-observation distinction\nis particularly unhelpful in understanding high-energy physics\nexperiments. It fails to capture the complexity of relevant\ntheoretical structures and their relation to experimental data.\nTheoretical structures can be composed of background, model, and\nphenomenological theories. Background theories are very general\ntheories (e.g. quantum field theory or quantum electrodynamics) that\ndefine the general properties of physical particles and their\ninteractions. Models are specific instances of background theories\nthat define particular particles and their properties. While\nphenomenological theories develop testable predictions based on these\nmodels. \nNow, each of these theoretical segments stands in a different\nrelationship to experimental data—the experiments can be laden\nby a different segment to a different extent. This requires a nuanced\ncategorization of theory-ladeness, from weak to strong.  Thus, an experimental apparatus can be designed to test a very\nspecific theoretical model. UA1 and UA2 detectors at CERN’s Super\nProton Synchrotron were designed to detect particles only in a very\nspecific energy regime in which W and Z bosons of the Standard Model\nwere expected to exist. \nIn contrast, exploratory experiments approach phenomena without\nrelying on a particular theoretical model. Thus, sometimes a\ntheoretical framework for an experiment consists of phenomenological\ntheory alone. Karaca argues that experiments with deep-inelastic\nelectron-proton scattering in the late 1960s and early 1970s are\nexample of such weakly theory-laden experiments. The application of\nmerely phenomenological parameters in the experiment resulted in the\nvery important discovery of the composite rather than point-like\nstructure of hadrons (protons and neutrons), or the so-called scaling\nlaw. And this eventually led to a successful theoretical model of the\ncomposition of hadrons, namely quantum chromodynamics, or the\nquark-model of strong interactions. \n\nAlthough experiment often takes its importance from its relation to\ntheory, Hacking pointed out that it often has a life of its own,\nindependent of theory. He notes the pristine observations of Carolyn\nHerschel’s discovery of comets, William Herschel’s work on “radiant\nheat,” and Davy’s observation of the gas emitted by algae and the\nflaring of a taper in that gas. In none of these cases did the\nexperimenter have any theory of the phenomenon under investigation. One\nmay also note the nineteenth century measurements of atomic spectra and\nthe work on the masses and properties on elementary particles during\nthe 1960s. Both of these sequences were conducted without any guidance\nfrom theory.  \n\nIn deciding what experimental investigation to pursue, scientists may\nvery well be influenced by the equipment available and their own\nability to use that equipment (McKinney 1992). Thus, when the\nMann-O’Neill collaboration was doing high energy physics experiments\nat the Princeton-Pennsylvania Accelerator during the late 1960s, the\nsequence of experiments was (1) measurement of the \\(\\ce{K+}\\) decay\nrates, (2) measurement of the \\(\\ce{K+_{e 3}}\\) branching\nratio and decay spectrum, (3) measurement of the\n\\(\\ce{K+_{e 2}}\\) branching ratio, and (4) measurement of the\nform factor in \\(\\ce{K+_{e 3}}\\) decay. These experiments\nwere performed with basically the same experimental apparatus, but\nwith relatively minor modifications for each particular experiment. By\nthe end of the sequence the experimenters had become quite expert in\nthe use of the apparatus and knowledgeable about the backgrounds and\nexperimental problems. This allowed the group to successfully perform\nthe technically more difficult experiments later in the sequence. We\nmight refer to this as “instrumental loyalty” and the\n“recycling of expertise” (Franklin 1997b). This meshes\nnicely with Galison’s view of experimental traditions. Scientists,\nboth theorists and experimentalists, tend to pursue experiments and\nproblems in which their training and expertise can be used. \n\nHacking also remarks on the “noteworthy observations” on Iceland\nSpar by Bartholin, on diffraction by Hooke and Grimaldi, and on the\ndispersion of light by Newton. “Now of course Bartholin, Grimaldi,\nHooke, and Newton were not mindless empiricists without an\n‘idea’ in their heads. They saw what they saw because they\nwere curious, inquisitive, reflective people. They were attempting to\nform theories. But in all these cases it is clear that the observations\npreceded any formulation of theory” (Hacking 1983, p. 156). In all of\nthese cases we may say that these were observations waiting for, or\nperhaps even calling for, a theory. The discovery of any unexpected\nphenomenon calls for a theoretical explanation. \n\nNevertheless several of the important roles of experiment involve its\nrelation to theory. Experiment may confirm a theory, refute a theory,\nor give hints to the mathematical structure of a theory.  \n\nLet us consider first an episode in which the relation between theory\nand experiment was clear and straightforward. This was a “crucial”\nexperiment, one that decided unequivocally between two competing\ntheories, or classes of theory. The episode was that of the discovery\nthat parity, mirror-reflection symmetry or left-right symmetry, is not\nconserved in the weak interactions. (For details of this episode see\nFranklin (1986, Ch. 1) and\n Appendix 1).\n Experiments showed that in the beta\ndecay of nuclei the number of electrons emitted in the same direction\nas the nuclear spin was different from the number emitted opposite to\nthe spin direction. This was a clear demonstration of parity violation\nin the weak interactions.  \n\nAfter the discovery of parity and charge conjugation nonconservation,\nand following a suggestion by Landau, physicists considered CP\n(combined parity and particle-antiparticle symmetry), which was still\nconserved in the experiments, as the appropriate symmetry. One\nconsequence of this scheme, if CP were conserved, was that the\n\\(\\ce{K1^0}\\) meson could decay into two pions, whereas the\n\\(\\ce{K2^0}\\) meson could\n not.[10]\n Thus, observation of\nthe decay of \\(\\ce{K2^0}\\) into two pions would indicate CP\nviolation. The decay was observed by a group at Princeton University.\nAlthough several alternative explanations were offered, experiments\neliminated each of the alternatives leaving only CP violation as an\nexplanation of the experimental result. (For details of this episode\nsee Franklin (1986, Ch. 3) and\n Appendix 2.) \n\nIn both of the episodes discussed previously, those of parity\nnonconservation and of CP violation, we saw a decision between two\ncompeting classes of theories. This episode, the discovery of\nBose-Einstein condensation (BEC), illustrates the confirmation of a\nspecific theoretical prediction 70 years after the theoretical\nprediction was first made. Bose (1924) and Einstein (1924; 1925)\npredicted that a gas of noninteracting bosonic atoms will, below a\ncertain temperature, suddenly develop a macroscopic population in the\nlowest energy quantum\n state.[11]\n (For details of this episode see\n Appendix 3.) \n\nIn the three episodes discussed in the previous section, the relation\nbetween experiment and theory was clear. The experiments gave\nunequivocal results and there was no ambiguity about what theory was\npredicting. None of the conclusions reached has since been questioned.\nParity and CP symmetry are violated in the weak interactions and\nBose-Einstein condensation is an accepted phenomenon. In the practice\nof science things are often more complex. Experimental results may be\nin conflict, or may even be incorrect. Theoretical calculations may\nalso be in error or a correct theory may be incorrectly applied. There\nare even cases in which both experiment and theory are wrong. As noted\nearlier, science is fallible. In this section I will discuss\nseveral episodes which illustrate these complexities.  \n\nThe episode of the fifth force is the case of a refutation of an\nhypothesis, but only after a disagreement between experimental results\nwas resolved. The “Fifth Force” was a proposed modification of Newton’s\nLaw of Universal Gravitation. The initial experiments gave conflicting\nresults: one supported the existence of the Fifth Force whereas the\nother argued against it. After numerous repetitions of the experiment,\nthe discord was resolved and a consensus reached that the Fifth Force\ndid not exist. (For details of this episode see\n Appendix 4.) \n\nThe Stern-Gerlach experiment was regarded as crucial at the time it\nwas performed, but, in fact, \n wasn’t.[12]\n In the view of the physics\ncommunity it decided the issue between two theories, refuting one and\nsupporting the other. In the light of later work, however, the\nrefutation stood, but the confirmation was questionable. In fact, the\nexperimental result posed problems for the theory it had seemingly\nconfirmed. A new theory was proposed and although the Stern-Gerlach\nresult initially also posed problems for the new theory, after a\nmodification of that new theory, the result confirmed it. In a sense,\nit was crucial after all. It just took some time. \n\nThe Stern-Gerlach experiment provides evidence for the existence of\nelectron spin. These experimental results were first published in 1922,\nalthough the idea of electron spin wasn’t proposed by Goudsmit and\nUhlenbeck until 1925 (1925; 1926). One might say that electron spin was\ndiscovered before it was invented. (For details of this episode see\n Appendix 5). \n\nIn the last section we saw some of the difficulty inherent in\nexperiment-theory comparison. One is sometimes faced with the question\nof whether the experimental apparatus satisfies the conditions required\nby theory, or conversely, whether the appropriate theory is being\ncompared to the experimental result. A case in point is the history of\nexperiments on the double-scattering of electrons by heavy nuclei (Mott\nscattering) during the 1930s and the relation of these results to\nDirac’s theory of the electron, an episode in which the question of\nwhether or not the experiment satisfied the conditions of the\ntheoretical calculation was central. Initially, experiments disagreed\nwith Mott’s calculation, casting doubt on the underlying Dirac theory.\nAfter more than a decade of work, both experimental and theoretical, it\nwas realized that there was a background effect in the experiments that\nmasked the predicted effect. When the background was eliminated\nexperiment and theory agreed.\n (Appendix 6) \n\nEver vaster amounts of data have been produced by particle colliders\nas they have grown from room-size apparata, to tens of kilometers long\nmega-labs. Vast numbers of background interactions that are well\nunderstood and theoretically uninteresting occur in the\ndetector. These have to be combed in order to identify interactions of\npotential interest. This is especially true of hadron (proton-proton)\ncolliders like the Large Hadron Collider (LHC), where the Higgs boson\nwas discovered. Protons that collide in the LHC and similar hadron\ncolliders are composed of more elementary particles, collectively\nlabeled partons. Partons mutually interact, exponentially increasing\nthe number of background interactions. In fact, a minuscule number of\ninteractions are selected from the overwhelming number that occur in\nthe detector. (In contrast, lepton collisions, such as collisions of\nelectrons and positrons, produce much lower backgrounds, since leptons\nare not composed of more elementary particles.) \nThus, a successful search for new elementary particles critically\ndepends on successfully crafting selection criteria and techniques at\nthe stage of data collection and at the stage of data analysis. But\ngradual development and changes in data selection procedures in the\ncolliders raises an important epistemological concern. The main reason\nfor this concern is nicely anticipated by the following question,\nwhich was posed by one of the most prominent experimentalists in\nparticle physics: “What is the extent to which we are negating the\ndiscovery potential of very-high-energy proton machines by the\nnecessity of rejecting, a priori, the events we cannot afford to\nrecord?” (Panofsky 1994, 133). In other words, how does one decide\nwhich interactions to detect and analyze in a multitude, in order to\nminimize the possibility of throwing out novel and unexplored\nones? \n\nOne way of searching through vast amounts of data that are already in,\ni.e. those that the detector has already delivered, is to look for\noccurrences that remain robust under varying conditions of\ndetection. Physicists employ the technique of data cuts in such\nanalysis. They cut out data that may be unreliable—when, for\ninstance, a data set may be an artefact rather than a genuine particle\ninteraction the experimenters expect. E.g. a colliding beam may\ninteract with the walls of the detector and not with the other\ncolliding beam, while producing a signal identical to the signal the\nexperimenters expected the beam-beam interaction to produce. Thus, if\nunder various data cuts a result remains stable, then it is\nincreasingly likely to be correct and to represent the genuine\nphenomenon the physicists think it represents. The robustness of the\nresult under various data cuts minimizes the possibility that the\ndetected phenomenon only mimics the genuine one (Franklin 2013,\n224–5). \n\nAt the data-acquisition stage, however, this strategy does not seem\napplicable. As Panofsky suggests, one does not know with certainty\nwhich of the vast number of the events in the detector may be of\ninterest. \n\nYet, Karaca\n (2011)[13]\n argues that a form of robustness is in\nplay even at the acquisition stage. This experimental approach\namalgamates theoretical expectations and empirical results, as the\nexample of the hypothesis of specific heavy particles is supposed to\nillustrate. \nAlong with the Standard Model of particle physics, a number of\nalternative models have been proposed. Their predictions of how\nelementary particles should behave often differ substantially. Yet in\ncontrast to the Standard Model, they all share the hypothesis that\nthere exist heavy particles that decay into particles with high\ntransverse momentum. Physicists apply a robustness analysis in testing this hypothesis,\nthe argument goes. First, they check whether the apparatus can detect\nknown particles similar to those predicted. Second, guided by the\nhypothesis, they establish various trigger algorithms. (The trigger\nalgorithms, or “the triggers”, determine at what exact point in time\nand under which conditions a detector should record interactions. They\nare necessary because the frequency and the number of interactions far\nexceed the limited recording capacity.) And, finally, they observe\nwhether any results remain stable across the triggers. \n\nYet even in this theoretical-empirical form of robustness, as Franklin\n(2013, 225) points out, “there is an underlying assumption that any\nnew physics will resemble known physics”—usually a theory of the\nday. And one way around this problem is for physicists to produce as\nmany alternative models as possible, including those that may even\nseem implausible at the time. \nPerovic (2011) suggests that such a potential failure, namely to spot\npotentially relevant events occurring in the detector, may be also a\nconsequence of the gradual automation of the detection process.  \n\nThe early days of experimentation in particle physics, around WWII,\nsaw the direct involvement of the experimenters in the\nprocess. Experimental particle physics was a decentralized discipline\nwhere experimenters running individual labs had full control over the\ntriggers and analysis. The experimenters could also control the goals\nand the design of experiments. Fixed target accelerators, where the\nbeam hits the detector instead of another beam, produced a number of\nparticle interactions that was manageable for such labs. The chance of\nmissing an anomalous event not predicted by the current theory was not\na major concern in such an environment.  \n\nYet such labs could process a comparatively small amount of data. This\nhas gradually become an obstacle, with the advent of hadron\ncolliders. They work at ever-higher energies and produce an\never-vaster number of background interactions. That is why the\nexperimental process has become increasingly automated and much more\nindirect. Trained technicians instead of experimenters themselves at\nsome point started to scan the recordings. Eventually, these human\nscanners were replaced by computers, and a full automation of\ndetection in hadron colliders has enabled the processing of vast\nnumber of interactions. This was the first significant change in the\ntransition from small individual labs to mega-labs. \nThe second significant change concerned the organization and goals of\nthe labs. The mega-detectors and the amounts of data they produced\nrequired exponentially more staff and scientists. This in turn led to\neven more centralized and hierarchical labs and even longer periods of\ndesign and performance of the experiments. As a result, focusing on\nconfirming existing dominant hypotheses rather than on exploratory\nparticle searches was the least risky way of achieving results that\nwould justify unprecedented investments.  Now, an indirect detection process combined with mostly\nconfirmatory goals is conducive to overlooking of unexpected\ninteractions. As such, it may impede potentially crucial theoretical\nadvances stemming from missed interactions.  \n\nThis possibility that physicists such as Panofsky have acknowledged is\nnot a mere speculation. In fact, the use of semi-automated, rather\nthan fully-automated regimes of detection turned out to be essential\nfor a number of surprising discoveries that led to theoretical\nbreakthroughs. \nPerovic analyzes several such cases, most notably the discovery of the\nJ/psi particle that provided the first substantial piece of evidence\nfor the existence of the charmed quark. In the experiments, physicists\nwere able to perform exploratory detection and visual analysis of\npractically individual interactions due to low number of background\ninteractions in the linear electron-positron collider. And they could\nafford to do this in an energy range that the existing theory did not\nrecognize as significant, which led to them making the discovery. None\nof this could have been done in the fully automated detecting regime\nof hadron colliders that are indispensable when dealing with an\nenvironment that contains huge numbers of background\ninteractions. \nAnd in some cases, such as the Fermilab experiments that aimed to\ndiscover weak neutral currents, an automated and confirmatory regime\nof data analysis contributed to the failure to detect particles that\nwere readily produced in the apparatus. \nThe complexity of the discovery process in particle physics does not\nend with concerns about what exact data should be chosen out of the\nsea of interactions. The so-called look-elsewhere effect results in a\ntantalizing dilemma at the stage of data analysis. \nSuppose that our theory tells us that we will find a particle in an\nenergy range. And suppose we find a significant signal in a section of\nthat very range. Perhaps we should keep looking elsewhere within the\nrange to make sure it is not another particle altogether we have\ndiscovered. It may be a particle that left other undetected traces in\nthe range that our theory does not predict, along with the trace we\nfound. The question is to what extent we should look elsewhere before\nwe reach a satisfying level of certainty that it is the predicted\nparticle we have discovered. \n\nPhysicists faced such a dilemma during the search for the Higgs boson\nat the Large Hadron Collider at CERN (Dawid 2015). \n\nThe Higgs boson is a particle responsible for the mass of other\nparticles. It is a scalar field that “pulls back” moving and\ninteracting particles. This pull, which we call mass, is different for\ndifferent particles. It is predicted by the Standard Model, whereas\nalternative models predict somewhat similar Higgs-like particles.  \n\nA prediction based on the Standard Model tells us with high\nprobability that we will find the Higgs particle in a particular\nrange. Yet a simple and an inevitable fact of finding it in a\nparticular section of that range may prompt us to doubt whether we\nhave truly found the exact particle our theory predicted. Our initial\nexcitement may vanish when we realize that we are much more likely to\nfind a particle of any sort—not just the predicted\nparticle—within the entire range than in a particular section of\nthat range. Thus, the probability of finding the Higgs anywhere within\na given energy range (consisting of eighty energy ‘bins’) is much\nhigher than the probability of finding it at a particular energy scale\nwithin that range (i.e. in any individual bin). In fact, the\nlikelihood of us finding it in a particular bin of the range is about\nhundred times lower. \nIn other words, the fact that we will inevitably find the particle in\na particular bin, not only in a particular range, decreases the\ncertainty that it was the Higgs we found. Given this fact alone we\nshould keep looking elsewhere for other possible traces in the range\nonce we find a significant signal in a bin. We should not proclaim the\ndiscovery of a particle predicted by the Standard Model (or any model\nfor that matter) too soon. But for how long should we keep looking\nelsewhere? And what level of certainty do we need to achieve before we\nproclaim discovery? \n  \nThe answer boils down to the weight one gives the theory and its\npredictions. This is the reason the experimentalists and theoreticians\nhad divergent views on the criterion for determining the precise point\nat which they could justifiably state ‘Our data indicate that we have\ndiscovered the Higgs boson’. Theoreticians were confident that a\nfinding within the range (any of eighty bins) that was of standard\nreliability (of three or four sigma), coupled with the theoretical\nexpectations that Higgs would be found, would be sufficient. In\ncontrast, experimentalists argued that at no point of data analysis\nshould the pertinence of the look-elsewhere effect be reduced, and the\nsearch proclaimed successful, with the help of the theoretical\nexpectations concerning Higgs. One needs to be as careful in combing\nthe range as one practically may. As a result, the experimentalists’\npreferred value of sigmas for announcing the discovery was five. This\nis a standard under which very few findings have turned out to be a\nfluctuation in the past. \nDawid argues that a question of an appropriate statistical analysis of\ndata is at the heart of the dispute. The reasoning of the\nexperimentalists relied on a frequentist approach that does not\nspecify the probability of the tested hypothesis. It actually isolates\nstatistical analysis of data from the prior probabilities. The\ntheoreticians, however, relied on Bayesian analysis. It starts with\nprior probabilities of initial assumptions and ends with the\nassessment of the probability of tested hypothesis based on the\ncollected evidence. The question remains whether the experimentalists’\nreasoning was fully justified. The prior expectations that the\ntheoreticians had included in their analysis had already been\nempirically corroborated by previous experiments after all. \n\nExperiment can also provide us with evidence for the existence of the\nentities involved in our theories. J.J. Thomson’s experiments on\ncathode rays provided grounds for belief in the existence of electrons.\n(For details of this episode see\n Appendix 7).\n \n\nExperiment can also help to articulate a theory. Experiments on beta\ndecay during from the 1930s to the 1950s determined the precise\nmathematical form of Fermi’s theory of beta decay. (For details of this\nepisode see\n Appendix 8.)\n \n\nOne comment that has been made concerning the philosophy of\nexperiment is that all of the examples are taken from physics and are\ntherefore limited. In this section arguments will be presented that these\ndiscussions also apply to biology. \n\nAlthough all of the illustrations of the epistemology of experiment\ncome from physics, David Rudge (1998; 2001) has shown that they are\nalso used in biology. His example is Kettlewell’s (1955; 1956;\n1958) evolutionary biology experiments on the Peppered Moth, Biston\nbetularia. The typical form of the moth has a pale\nspeckled appearance and there are two darker forms, f.\ncarbonaria, which is nearly black, and f. insularia,\nwhich is intermediate in color. The typical form of the moth\nwas most prevalent in the British Isles and Europe until the middle of\nthe nineteenth century. At that time things began to change. Increasing\nindustrial pollution had both darkened the surfaces of trees and rocks\nand had also killed the lichen cover of the forests downwind of\npollution sources. Coincident with these changes, naturalists had found\nthat rare, darker forms of several moth species, in particular the\nPeppered Moth, had become common in areas downwind of pollution\nsources. \n\nKettlewell attempted to test a selectionist explanation of this\nphenomenon. E.B. Ford (1937; 1940) had suggested a two-part explanation\nof this effect: 1) darker moths had a superior physiology and 2) the\nspread of the melanic gene was confined to industrial areas because the\ndarker color made carbonaria more conspicuous to avian\npredators in rural areas and less conspicuous in polluted areas.\nKettlewell believed that Ford had established the superior viability of\ndarker moths and he wanted to test the hypothesis that the darker form\nof the moth was less conspicuous to predators in industrial areas. \n\nKettlewell’s investigations consisted of three parts. In the\nfirst part he used human observers to investigate whether his proposed\nscoring method would be accurate in assessing the relative\nconspicuousness of different types of moths against different\nbackgrounds. The tests showed that moths on “correct”\nbackgrounds, typical on lichen covered backgrounds and dark\nmoths on soot-blackened backgrounds were almost always judged\ninconspicuous, whereas moths on “incorrect” backgrounds\nwere judged conspicuous. \n\nThe second step involved releasing birds into a cage containing all\nthree types of moth and both soot-blackened and lichen covered pieces\nof bark as resting places. After some difficulties (see Rudge 1998 for\ndetails), Kettlewell found that birds prey on moths in an order of\nconspicuousness similar to that gauged by human observers. \n\nThe third step was to investigate whether birds preferentially prey\non conspicuous moths in the wild. Kettlewell used a\nmark-release-recapture experiment in both a polluted environment\n(Birmingham) and later in an unpolluted wood. He released 630 marked\nmale moths of all three types in an area near Birmingham, which\ncontained predators and natural boundaries. He then recaptured the\nmoths using two different types of trap, each containing virgin females\nof all three types to guard against the possibility of pheromone\ndifferences. \n\nKettlewell found that carbonaria was twice as likely to\nsurvive in soot-darkened environments (27.5 percent) as was\ntypical (12.7 percent). He worried, however, that his results\nmight be an artifact of his experimental procedures. Perhaps the traps\nused were more attractive to one type of moth, that one form of moth\nwas more likely to migrate, or that one type of moth just lived longer.\nHe eliminated the first alternative by showing that the recapture rates\nwere the same for both types of trap. The use of natural boundaries and\ntraps placed beyond those boundaries eliminated the second, and\nprevious experiments had shown no differences in longevity. Further\nexperiments in polluted environments confirmed that carbonaria\nwas twice as likely to survive as typical. An experiment in an\nunpolluted environment showed that typical was three times as\nlikely to survive as carbonaria. Kettlewell concluded that\nsuch selection was the cause of the prevalence of carbonaria\nin polluted environments. \n\nRudge also demonstrates that the strategies used by Kettlewell are\nthose described above in the epistemology of experiment. His examples\nare given in Table 1. (For more details see Rudge 1998). Table 1. Examples of epistemological strategies used\nby experimentalists in evolutionary biology, from H.B.D.\nKettlewell’s (1955, 1956, 1958) investigations of industrial\nmelanism. (See Rudge 1998). \n\nThe roles that experiment plays in physics are also those it plays\nin biology. In the previous section we have seen that\nKettlewell’s experiments both test and confirm a theory. I\ndiscussed earlier a set of crucial experiments that decided between two\ncompeting classes of theories, those that conserved parity and those\nthat did not. In this section I will discuss an experiment that decided\namong three competing mechanisms for the replication of DNA, the\nmolecule now believed to be responsible for heredity. This is another\ncrucial experiment. It strongly supported one proposed mechanism and\nargued against the other two. (For details of this episode see (Holmes\n2001)). \n\nIn 1953 Francis Crick and James Watson proposed a three-dimensional\nstructure for deoxyribonucleic acid (DNA) (Watson and Crick 1953a).\nTheir proposed structure consisted of two polynucleotide chains\nhelically wound about a common axis. This was the famous “Double\nHelix”. The chains were bound together by combinations of four\nnitrogen bases — adenine, thymine, cytosine, and guanine. Because of\nstructural requirements only the base pairs adenine-thymine and\ncytosine-guanine are allowed. Each chain is thus complementary to the\nother. If there is an adenine base at a location in one chain there is\na thymine base at the same location on the other chain, and vice versa.\nThe same applies to cytosine and guanine. The order of the bases along\na chain is not, however, restricted in any way, and it is the precise\nsequence of bases that carries the genetic information. \n\nThe significance of the proposed structure was not lost on Watson\nand Crick when they made their suggestion. They remarked, “It has\nnot escaped our notice that the specific pairing we have postulated\nimmediately suggests a possible copying mechanism for the genetic\nmaterial.” \n\nIf DNA was to play this crucial role in genetics, then there must be\na mechanism for the replication of the molecule. Within a short period\nof time following the Watson-Crick suggestion, three different\nmechanisms for the replication of the DNA molecule were proposed\n(Delbruck and Stent 1957). These are illustrated in Figure A. The\nfirst, proposed by Gunther Stent and known as conservative replication,\nsuggested that each of the two strands of the parent DNA molecule is\nreplicated in new material. This yields a first generation which\nconsists of the original parent DNA molecule and one newly-synthesized\nDNA molecule. The second generation will consist of the parental DNA\nand three new DNAs. Figure A:\nPossible mechanisms for DNA replication. (Left) Conservative\nreplication. Each of the two strands of the parent DNA is replicated to\nyield the unchanged parent DNA and one newly synthesized DNA. The\nsecond generation consists of one parent DNA and three new DNAs.\n(Center) Semiconservative replication. Each first generation DNA\nmolecule contains one strand of the parent DNA and one newly\nsynthesized strand. The second generation consists of two hybrid DNAs\nand two new DNAs. (Right) Dispersive replication. The parent chains\nbreak at intervals, and the parental segments combine with new segments\nto form the daughter chains. The darker segments are parental DNA and\nthe lighter segments are newly synthesized DNA. From Lehninger\n(1975). \n\nThe second proposed mechanism, known as semiconservative replication\nis when each strand of the parental DNA acts as a template for a second\nnewly-synthesized complementary strand, which then combines with the\noriginal strand to form a DNA molecule. This was proposed by Watson and\nCrick (1953b). The first generation consists of two hybrid molecules,\neach of which contains one strand of parental DNA and one newly\nsynthesized strand. The second generation consists of two hybrid\nmolecules and two totally new DNAs. The third mechanism, proposed by\nMax Delbruck, was dispersive replication, in which the parental DNA\nchains break at intervals and the parental segments combine with new\nsegments to form the daughter strands. \n\nIn this section the experiment performed by Matthew Meselson and\nFranklin Stahl, which has been called “the most beautiful\nexperiment in biology”, and which was designed to answer the\nquestion of the correct DNA replication mechanism will be discussed\n(Meselson and Stahl 1958). Meselson and Stahl described their proposed\nmethod. “We anticipated that a label which imparts to the DNA\nmolecule an increased density might permit an analysis of this\ndistribution by sedimentation techniques. To this end a method was\ndeveloped for the detection of small density differences among\nmacromolecules. By use of this method, we have observed the\ndistribution of the heavy nitrogen isotope \\(\\ce{^{15}N}\\) among\nmolecules of DNA following the transfer of a uniformly\n\\(\\ce{^{15}N}\\)-labeled, exponentially growing bacterial population to\na growth medium containing the ordinary nitrogen isotope\n\\(\\ce{^{14}N}\\)” (Meselson and Stahl 1958, pp. 671–672). Figure B:\nSchematic representation of the Meselson-Stahl experiment. From Watson\n(1965). \n\nThe experiment is described schematically in Figure B. Meselson and\nStahl placed a sample of DNA in a solution of cesium chloride. As the\nsample is rotated at high speed the denser material travels further\naway from the axis of rotation than does the less dense material. This\nresults in a solution of cesium chloride that has increasing density as\none goes further away from the axis of rotation. The DNA reaches\nequilibrium at the position where its density equals that of the\nsolution. Meselson and Stahl grew E. coli bacteria in a medium\nthat contained ammonium chloride \\((\\ce{NH4Cl})\\) as the sole source\nof nitrogen. They did this for media that contained either\n\\(\\ce{^{14}N}\\), ordinary nitrogen, or \\(\\ce{^{15}N}\\), a heavier\nisotope. By destroying the cell membranes they could obtain samples of\nDNA which contained either \n\\(\\ce{^{14}N}\\) or \\(\\ce{^{15}N}\\). They first\nshowed that they could indeed separate the two different mass molecules\nof DNA by centrifugation (Figure C). The separation of the two\ntypes of DNA is clear in both the photograph obtained by absorbing\nultraviolet light and in the graph showing the intensity of the signal,\nobtained with a densitometer. In addition, the separation between the\ntwo peaks suggested that they would be able to distinguish an\nintermediate band composed of hybrid DNA from the heavy and light\nbands. These early results argued both that the experimental apparatus\nwas working properly and that all of the results obtained were correct.\nIt is difficult to imagine either an apparatus malfunction or a source\nof experimental background that could reproduce those results. This is\nsimilar, although certainly not identical, to Galileo’s\nobservation of the moons of Jupiter or to Millikan’s measurement\nof the charge of the electron. In both of those episodes it was the\nresults themselves that argued for their correctness. Figure C:\nThe separation of \\(\\ce{^{14}N}\\) DNA from \\(\\ce{^{15}N}\\) DNA by\ncentrifugation. The band on the left is \\(\\ce{^{14}N}\\) DNA and that on\nthe right is from \\(\\ce{^{15}N}\\) DNA. From Meselson and Stahl (1958).\n \n\nMeselson and Stahl then produced a sample of E coli\nbacteria containing only \\(\\ce{^{15}N}\\) by growing it in a medium\ncontaining only ammonium chloride with \\(\\ce{^{15}N}\\)\n\\((\\ce{^{15}NH4Cl})\\) for fourteen generations. They then\nabruptly changed the medium to \\(\\ce{^{14}N}\\) by adding a tenfold\nexcess of \\(\\ce{^{14}NH_4Cl}\\). Samples were taken just before\nthe addition of \\(\\ce{^{14}N}\\) and at intervals afterward for several\ngenerations. The cell membranes were broken to release the DNA into the\nsolution and the samples were centrifuged and ultraviolet absorption\nphotographs taken. In addition, the photographs were scanned with a\nrecording densitometer. The results are shown in Figure D,\nshowing both the photographs and the densitometer traces. The figure\nshows that one starts only with heavy (fully-labeled) DNA. As time\nproceeds one sees more and more half-labeled DNA, until at one\ngeneration time only half-labeled DNA is present. “Subsequently\nonly half labeled DNA and completely unlabeled DNA are found. When two\ngeneration times have elapsed after the addition of \\(\\ce{^{14}N}\\)\nhalf-labeled and unlabeled DNA are present in equal amounts” (p.\n676). (This is exactly what the semiconservative replication mechanism\npredicts). By four generations the sample consists almost entirely of\nunlabeled DNA. A test of the conclusion that the DNA in the\nintermediate density band was half labeled was provided by examination\nof a sample containing equal amounts of generations 0 and 1.9. If the\nsemiconservative mechanism is correct then Generation 1.9 should have\napproximately equal amounts of unlabeled and half-labeled DNA, whereas\nGeneration 0 contains only fully-labeled DNA. As one can see, there are\nthree clear density bands and Meselson and Stahl found that the\nintermediate band was centered at \\((50 \\pm 2)\\) percent of the difference\nbetween the \\(\\ce{^{14}N}\\) and \\(\\ce{^{15}N}\\) bands, shown in the\nbottom photograph (Generations 0 and 4.1). This is precisely what one\nwould expect if that DNA were half labeled. Figure D:\n(Left) Ultraviolet absorption photographs showing DNA bands from\ncentrifugation of DNA from E. Coli sampled at various times\nafter the addition of an excess of \\(\\ce{^{14}N}\\) substrates to a\ngrowing \\(\\ce{^{15}N}\\) culture. (Right) Densitometer traces of the\nphotographs. The initial sample is all heavy (\\(\\ce{^{15}N}\\) DNA). As\ntime proceeds a second intermediate band begins to appear until at one\ngeneration all of the sample is of intermediate mass (Hybrid DNA). At\nlonger times a band of light DNA appears, until at 4.1 generations the\nsample is almost all lighter DNA. This is exactly what is predicted by\nthe Watson-Crick semiconservative mechanism. From Meselson and Stahl\n(1958) \n\nMeselson and Stahl stated their results as follows, “The\nnitrogen of DNA is divided equally between two subunits which remain\nintact through many generations…. Following replication,\neach daughter molecule has received one parental subunit” (p.\n676). \n\nMeselson and Stahl also noted the implications of their work\nfor deciding among the proposed mechanisms for DNA replication. In a\nsection labeled “The Watson-Crick Model” they noted that,\n“This [the structure of the DNA molecule] suggested to Watson and\nCrick a definite and structurally plausible hypothesis for the\nduplication of the DNA molecule. According to this idea, the two chains\nseparate, exposing the hydrogen-bonding sites of the bases. Then, in\naccord with base-pairing restrictions, each chain serves as a template\nfor the synthesis of its complement. Accordingly, each daughter\nmolecule contains one of the parental chains paired with a newly\nsynthesized chain…. The results of the present experiment are\nin exact accord with the expectations of the Watson-Crick model for DNA\nreplication” (pp. 677–678). \n\nIt also showed that the dispersive replication mechanism proposed by\nDelbruck, which had smaller subunits, was incorrect. “Since the\napparent molecular weight of the subunits so obtained is found to be\nclose to half that of the intact molecule, it may be further concluded\nthat the subunits of the DNA molecule which are conserved at\nduplication are single, continuous structures. The scheme for DNA\nduplication proposed by Delbruck is thereby ruled out” (p. 681).\nLater work by John Cairns and others showed that the subunits of DNA\nwere the entire single polynucleotide chains of the Watson-Crick model\nof DNA structure. \n\nThe Meselson-Stahl experiment is a crucial experiment in biology. It\ndecided between three proposed mechanisms for the replication of DNA.\nIt supported the Watson-Crick semiconservative mechanism and eliminated\nthe conservative and dispersive mechanisms. It played a similar role in\nbiology to that of the experiments that demonstrated the\nnonconservation of parity did in physics. Thus, we have seen evidence\nthat experiment plays similar roles in both biology and physics and\nalso that the same epistemological strategies are used in both\ndisciplines. \n\nOne interesting recent development in science, and thus in the\nphilosophy of science, has been the increasing use of, and importance\nof, computer simulations. In some fields, such as high-energy physics,\nsimulations are an essential part of all experiments. It is fair to\nsay that without computer simulations these experiments would be\nimpossible. There has been a considerable literature in the philosophy\nof science discussing whether computer simulations are experiments,\ntheory, or some new kind of hybrid method of doing science. But, as\nEric Winsberg remarked, “We have in other words, rejected the\noverly conservative intuition that computer simulation is nothing but\nboring and straightforward theory application. But we have avoided\nembracing the opposite, overly grandiose intuition that simulation is\na radically new kind of knowledge production, ”on a par“\nwith experimentation. In fact, we have seen that soberly locating\nsimulation ”on the methodological map“ is not a simple\nmatter (Winsberg 2010, p. 136).”  \n\nGiven the importance of computer simulations in science it is\nessential that we have good reasons to believe their results. Eric\nWinsberg (2010), Wendy Parker (2008) and others have shown that\nscientists use strategies quite similar to those discussed in Section\n1.1.1, to argue for the correctness of computer simulations.\n \nThe distinction between observation and experiment is relatively\nlittle discussed in philosophical literature, despite its continuous\nrelevance to the scientific community and beyond in understanding\nspecific traits and segments of the scientific process and the\nknowledge it produces. \nDaston and her coauthors (Daston 2011; Daston and Lunbeck 2011; Daston\nand Galison 2007) have convincingly demonstrated that the distinction\nhas played a role in delineating various features of scientific\npractice. It has helped scientists articulate their reflections on\ntheir own practice. \nObservation is philosophically a loaded term, yet the epistemic status\nof scientific observation has evolved gradually with the advance of\nscientific techniques of inquiry and the scientific communities\npursuing them. Daston succinctly summarizes this evolution in the\nfollowing passage: \nObservation gradually became juxtaposed to other, more complex modes\nof inquiry such as experiment, “whose meaning shifted from the\nbroad and heterogeneous sense of experimentum as recipe, trial, or\njust common experience to a concertedly artificial manipulation, often\nusing special instruments and designed to probe hidden causes”\n(Daston 2011, 82). \nIn the 17th century, observation and experiment were seen as “an\ninseparable pair” (Daston 2011, 82) and by the 19th century they were\nunderstood to be essentially opposed, with the observer increasingly\nseen as passive and thus epistemically inferior to the\nexperimenter. In fact, already Leibniz anticipated this view stating\nthat “[t]here are certain experiments that would be better called\nobservations, in which one considers rather than produces the work”\n(Daston 2011, 86). This aspect of the distinction has been a mainstay\nof understanding scientific practice ever since. \nApart from this historical analysis, there are currently two prominent\nand opposed views of the experiment-observation distinction. Ian\nHacking has characterized it as well-defined, while avoiding the claim\nthat observation and experiment are opposites (Hacking 1983,\n173). According to him, the notions signify different things in\nscientific practice. The experiment is a thorough manipulation that\ncreates a new phenomenon, and observation of the phenomenon is its\noutcome. If scientists can manipulate a domain of nature to such an\nextent that they can create a new phenomenon in a lab, a phenomenon\nthat normally cannot be observed in nature, then they have truly\nobserved the phenomenon (Hacking 1989, 1992). \nMeanwhile, other authors concur that “the familiar distinction between\nobservation and experiment … [is] an artefact of the\ndisembodied, reconstructed character of retrospective accounts”\n(Gooding 1992, 68). The distinction “collapses” when we are faced with\nactual scientific practice as a process, and “Hacking’s observation\nversus experiment framework does not survive intact when put to the\ntest in a range of cases of scientific experimentation” (Malik 2017,\n85).  First, the uses of the distinction cannot be compared across\nscientific fields. And second, as Gooding (1992) suggests, observation\nis a process too, not simply a static result of manipulation. Thus,\nboth observation and experiment are seen as concurrent processes\nblended together in scientific practice. \nMalik (2017, 86) states that these arguments are the reason why “very\nfew [authors] use Hacking’s nomenclature of observation/experiment”\nand goes so far to conclude that “to (try to) distinguish between\nobservation and experiment is futile.” There is no point delineating\nthe two except perhaps in certain narrow domains; e.g., Hacking’s\nnotion of the experiment based on creating phenomena might be useful\nwithin a narrow domain of particle physics. (See also Chang 2011.) He\nadvocates avoiding the distinction altogether and opting for “the\nterminology [that] underlines this sense of continuousness” (Malik\n2017, 88) instead. If we want to analyze scientific practice, the\nargument goes, we should leave behind the idea of the distinction as\nfundamental and turn to the characterization and analysis of various\n“epistemic activities” instead, e.g., along the lines suggested by\nChang (2011). \nA rather obvious danger of this approach is an over-emphasis on the\ncontinuousness of the notions of observation and experiment that\nresults in inadvertent equivocation. And this, in turn, results in\nsidelining the distinction and its subtleties in the analysis of the\nscientific practice, despite their crucial role in articulating and\ndeveloping that practice since the 17th century. This issue certainly\nrequires further philosophical and historical analysis. \n\nIn this entry varying views on the nature of experimental results have\nbeen presented. Some argue that the acceptance of experimental results\nis based on epistemological arguments, whereas others base acceptance\non future utility, social interests, or agreement with existing\ncommunity commitments. Everyone agrees , however, that for whatever\nreasons, a consensus is reached on experimental results. These results\nthen play many important roles in physics and we have examined several\nof these roles, although certainly not all of them. We have seen\nexperiment deciding between two competing theories, calling for a new\ntheory, confirming a theory, refuting a theory, providing evidence that\ndetermined the mathematical form of a theory, and providing evidence\nfor the existence of an elementary particle involved in an accepted\ntheory. We have also seen that experiment has a life of its own,\nindependent of theory. If, as I believe, epistemological procedures\nprovide grounds for reasonable belief in experimental results, then\nexperiment can legitimately play the roles I have discussed and can\nprovide the basis for scientific knowledge. ","contact.mail":"sperovic@f.bg.ac.rs","contact.domain":"f.bg.ac.rs"}]
