[{"date.published":"2008-04-16","date.changed":"2014-10-13","url":"https://plato.stanford.edu/entries/proof-theory-development/","author1":"Jan von Plato","entry":"proof-theory-development","body.text":"\n     \n\n\nThe development of \n proof theory\n can be naturally divided into: the prehistory of the notion of proof \nin ancient logic and mathematics; the discovery by \n Frege\n that mathematical proofs, and not only the propositions of \nmathematics, can (and should) be represented in a logical system; \n Hilbert's old axiomatic proof theory;\n failure of the aims of Hilbert through Gödel's incompleteness \ntheorems; Gentzen's creation of the two main types of logical systems\nof contemporary proof theory, natural deduction and sequent calculus \n(see the entry on \n automated reasoning);\n applications and extensions of natural deduction and sequent \ncalculus, up to the computational interpretation of natural deduction\nand its connections with computer science.\n\n\n\nProof theory can be described as the study of the general structure \nof mathematical proofs, and of arguments with demonstrative force as \nencountered in logic. The idea of such demonstrative arguments, i.e.,\nones the conclusion of which follows necessarily from the assumptions\nmade, is central in Aristotle's Analytica Posteriora: a \ndeductive science is organised around a number of basic concepts that\nare assumed understood without further explanation, and a number of \nbasic truths or axioms that are seen as true immediately. Defined \nconcepts and theorems are reduced to these two, the latter through \nproof. Aristotle's account of proof as demonstrative argument fits \nvery well to the structure of ancient geometry as axiomatized in \nEuclid. The specific form of \n Aristotle's logic,\n the theory of syllogism has instead, so it seems, almost nothing to \ndo with proofs in Euclidean geometry. These proofs remained intuitive\nfor more than two thousand years. \n\nBefore the work of Frege in 1879, no one seems to have maintained that\nthere could be a complete set of principles of proof, in the sense\nexpressed by Frege when he wrote that in his symbolic language,\n \nall that is necessary for a correct inference is expressed in\nfull, but what is not necessary is generally not indicated; nothing is\nleft to guesswork.\n(Begriffsschrift, p. 3) \n (One might contend that Boole is an\nexception as far as classical propositional logic is concerned.)\nFrege's step ahead was decisive for the development of logic and\nfoundational study. The contrast to the ancients is great: Aristotle\ngave a pattern for combining arguments, but the idea of a finite\nclosed set of rules was, philosophically, beyond the dreams of anyone\nbefore Frege, with the possible exception of Leibniz. \n\nAs we know today, Frege's principles of proof are complete for \nclassical predicate logic. \nAround 1890, Giuseppe Peano gave a formalization of logical inference,\nwith the aim of representing formally proofs in arithmetic. His\nseminal paper Arithmetices principia, nova methodo exposita,\nwritten originally in Latin, is included in English translation in the\ncollection  From Frege to Gödel (1967) that Jean van Heijenoort\nedited. Unfortunately, the editor failed to recognize what Peano did\nwith formal inference, and the view spread that Peano formalized only\nthe language of logic and arithmetic, not its principles of proof. If\nPeano's proofs are read with even a little care, it transpires that\nthey are purely formal derivations that use two principles: \nPeano is very careful to list, on every line of his\nderivations, what the formal ground for writing the line is. \n Russell\n took up Frege's logic, but used the notation and formal rules of\nproof of Peano, in a paper of 1906 with the title “The theory of\nimplication.”  Its formal machinery is exactly the same as\nPeano's. In later work, Russell changed the axiomatic system and the\none of Principia Mathematica (Whitehead and Russell 1910–13) became\nstandard. Russell's philosophical idea, and here he followed Frege,\nwas that the axioms express basic logical truths, and other logical\ntruths are derived from these through modus ponens and universal\ngeneralization, the two principles Frege had identified. Mathematics\nwas to be reduced to logic, so that its proofs would become presented\nin the same axiomatic pattern. \n\nFrege's and Peano-Russell's approach to logic became the universally \naccepted one, especially through the influence of Hilbert and his \nco-workers in the 1920s. In the 19th century, Frege was a marginal \nfigure, and the algebraic approach to logic, as in Boole and \nespecially Ernst Schröder, was the dominant one. It is clear \nthat there was a good understanding of the principles of predicate \nlogic in this tradition, for how could there have been a \nLöwenheim-Skolem theorem otherwise? Skolem found out about \nFrege's logic through Principia Mathematica only after having \nworked out the theorem in his paper of 1920. The first section of \nthat paper, widely read because of the English translation in Van \nHeijenoort's From Frege to Gödel, marks the end of the \nalgebraic tradition of logic that merged silently with lattice \ntheory. Other sections of the paper contain a remarkable beginning of\nthe analysis of proofs in lattice theory and in projective geometry: \nSkolem considered the axioms of a mathematical theory from a purely \ncombinatorial and formal point of view, as means for producing \nderivations of a formula from given formulas used as assumptions. It \nwas found out in the early 1990s that the part on lattice theory \ncontains the solution of a decision problem, called the word problem \nfor freely generated lattices, the known solution of which stemmed \nfrom 1988! Skolem's terminology and notation in lattice theory are \nthose of Schröder and that is part of the reason why his work \nwas a lost opportunity for proof theory. \n\nHilbert's book Grundlagen der Geometrie of 1899 set the stage\nfor the central foundational problems of mathematics of the early\ndecades of the 20th century. We can list these problems as\nfollows: \n\nAs to Hilbert's geometry, its attempted formalization fell short of\nthe ideal to which it gave birth. Hilbert found a much more important\nfield to which his “metamathematics” was to be applied,\nnamely arithmetic and analysis. The groundwork was a study of the four\nfoundational problems in axiomatic formulations of pure logic.\nPropositional logic was thus formalized, found to be consistent and\ncomplete, and decidable. The first results about predicate logic are\nfrom 1915, when Leopold Löwenheim gave his version of what later\nbecame the Löwenheim-Skolem theorem for predicate logic (see the entry\non classical logic).\nHe also solved special cases of the decision problem. This development\nwas independent of the Frege-Russell tradition, and was instead based\non the algebraic approach to logic of Ernst Schröder. Around 1920, the\n“Hilbert style” axiomatic approach, as it is often called,\nwas known to everyone and dominated the logical scene; the algebraic\napproach merged almost without notice with lattice theory. By 1928, in\nHilbert and Ackermann's Grundzüge der theoretischen Logik, an\naxiomatic formal system for predicate logic was presented, together\nwith the problem of its completeness. The latter was solved by Gödel\nin 1929, published a year later (Gödel 1930). The fourth\nfoundational problem, the decision problem for predicate logic, was\nshown to have a negative solution in a short paper by Church in 1936\nas a corollary to Gödel's incompleteness theorem. \n\nHilbert and his school, with Bernays, Ackermann and von Neumann as \nforemost, as well as young Herbrand in France, pursued the \nmetamathematical study of arithmetic in the latter part of the 1920s.\nHilbert developed a method for the study of consistency problems, \ncalled the \n epsilon substitution method,\n to deal with the quantifiers. He felt that indirect inferences with \nquantifiers in cases with an infinity of objects were the crucial \npoint of consistency proofs and needed a justification. Say, if the \nassumption that all natural numbers have the property P \nleads to an impossibility, the existence of a number with the \ncontrary property not-P can be inferred. The central problem\nwas thus to justify the use of classical logic in mathematical \nproofs, arithmetical ones in the first place. Ackermann was very \nclose to a solution towards the end of the 1920s and optimism reigned\nin the Hilbert school. Then, of course, the unexpected happened when \nGödel proved the impossibility of a complete formalization of \nelementary arithmetic, and, as it was soon interpreted, the \nimpossibility of proving the consistency of arithmetic by finitary \nmeans, the only ones judged “absolutely reliable” by \nHilbert. \n\nAfter Gödel had made public the incompleteness of arithmetic in\nSeptember 1930, von Neumann found that the consistency of arithmetic\nwould be among the Gödelian unprovable propositions. Alas, Gödel had\nmade the same discovery so von Neumann never published his proof. He\ndid, however, conjecture in correspondence with Gödel the\nunprovability of the consistency of arithmetic and therefore of\nmathematics as a whole, in some absolute sense. Von Neumann was the\nkey character in the reception of Gödel's results: He interrupted his\nlectures on Hilbert's proof theory in Berlin in the fall of 1930 to\nexplain the new discoveries. These events created an enormous\nexcitement among the mathematicians, as witnessed by Carl Hempel's\ntestimony: \n\nIn 1932–33, Gödel and Gentzen found independently of each \nother a translation from classical Peano arithmetic to \n intuitionistic Heyting arithmetic.\n Specifically, it shows that if a contradiction is provable in the \nformer, it is provable in the latter. Then the consistency of \nintuitionistic arithmetic would guarantee also the consistency of \nclassical arithmetic. This result was a surprise: as mentioned, \nHilbert had thought that the “transfinite” indirect \nexistence proofs would be the part of arithmetic that needs to be \nsecured of contradiction. By Gödel's and Gentzen's result, \nalready intuitionistic arithmetic contained principles that went \nbeyond finitary reasoning. A letter Gentzen wrote to Heyting on 25 \nFebruary 1933 summarizes the situation as follows: \n\nThe last-mentioned was the goal Gentzen had set to himself early in \n1932, when in a letter to his old teacher Hellmuth Kneser he wrote: \n\nIn pursuing his consistency program, Gentzen set as his first task the\nanalysis of purely logical deduction, to be extended later to\narithmetic and analysis. In his thesis (1934–1935), Gentzen\nstates that he set as his task the analysis of mathematical proofs as\nthey occur in practice.  The first observation is that actual proofs\nare not based on axioms expressed in a logical language, as in\nHilbert's axiomatic proof theory. The most typical feature is instead\nthat theorems make their claims under some assumptions. The\nassumptions are analyzed into parts and the conclusion is also\nanalyzed into parts until these two analyses meet and a proof can be\nsynthesized.  The latter analysis proceeds by what Gentzen called\nintroduction rules: they give sufficient conditions for deriving a\nproposition of a given form. For example, to derive a conjunction\nA & B, it is sufficient to\nderive the conjuncts A and B separately. The\ninference is given formally as in the rule \n\nAssumptions, instead, are analyzed into their components through \nelimination rules that give, by and large, immediate consequences of \nthe assumptions. For example, a conjunction used as an assumption can\nbe decomposed into its constituents, as in the rules \n\nGentzen developed and studied the system of natural deduction during \n1932, and had arrived by September 1932 at a calculus of natural \ndeduction (ND for short) that is standard today. By this time, he had\nnoticed that if an introduction, say a derivation of \nA & B from A and B separately, \nis followed by the corresponding elimination, say a derivation of \nA, the formula A & B constitutes a local\nmaximum, a “hillock”, that can be eliminated. He also \ncalled such hillocks “detours”, and what is now called a \ndetour conversion removes such unnecessary pairs of \nintroduction-elimination steps. The result of the steps of \n“normalization” is a derivation in “normal \nform”. \n\nImplication is perhaps more typical of ND than conjunction: to derive\nA ⊃ B, one temporarily assumes A, then \ntries to derive B. If this succeeds, the temporary \nassumption is closed or “discharged” when the conclusion \nto A ⊃ B is made, as in the schematic derivation \n\nIn the other direction, A ⊃ B can be eliminated \nif a derivation of A has been found, for then B can\nbe concluded: \n\nIf rule \n⊃I is followed by ⊃E,\n there is a non-normality that \nis removed by a detour conversion: a derivation of B (and \nwhat follows after it) is constructed by taking the derivation of the\nminor premiss A of the elimination rule and the derivation \nof B from the assumption A in the introduction. \nThese two pieces are combined together into a derivation of \nB that does not have the detour formula \nA ⊃ B. In Gentzen's thesis, all assumptions are \nin the end closed by implication introductions, but nowadays one \nconsiders also derivations that leave a collection of formulas as \nopen assumptions. \n\nLooking at the rules of conjunction and implication, one notes that \nthe premisses (the formulas immediately above the inference line) are\nsubformulas of the conclusion in the I-rules, whereas it is the other\nway around in the E-rules. Gentzen noticed that in normal \nderivations, this property of single steps is inherited by the whole \nderivation, in the sense that all formulas are subformulas of the \nconclusion. This result gave as a byproduct a decision method for \n intuitionistic propositional logic.\n Another corollary was a syntactic proof of consistency: if a \ncontradiction is provable, anything is provable, but an atomic \nformula, say, has no proof: if it has a proof, it has a normal proof,\nbut no E-rules apply to an atomic formula, and no I-rule concludes it\neither. \n\nGentzen's idea was to extend natural deduction to a system of \narithmetic by the addition of a rule that corresponds to the \nprinciple of complete induction. Consistency would then follow from \nthe normalization of derivations and the subformula property. By \nearly 1933, Gentzen realized that this proof strategy would not go \nthrough: the induction rule is schematic and has an infinity of \ninstances, with no bound on the complexity of the induction formulas.\nIt would be impossible to restrict these formulas in advance, thus no\nsubformula property can hold. After this failure, Gentzen took \nverbatim out of his early thesis manuscript the translation from \nclassical to intuitionistic arithmetic and submitted it as a paper in\nMarch 1933, but withdrew the paper after hearing of Gödel's \npublication of the result. \n\nGentzen wrote that he was unable to prove a normalization theorem for\na classical system of ND. He therefore invented another logical \ncalculus that he called sequent calculus (Sequenzenkalkul, \nliterally “calculus of sequences”) and made it the \ncentral topic of his thesis. The name of the calculus comes from the \nrepresentation of assumptions of a derivation as a list. The word \n“sequent” used as a noun is a suggestion of Kleene's in \nhis Introduction to Metamathematics (1952: 441), taken up \nin many languages in the form of purely invented words.  Sequent calculus, SC for short, can be seen as a formal\nrepresentation of the derivability relation in natural deduction. A\nsequent consists of a list Γ of formulas, an arrow (in Gentzen,\nlater also other markers have been used), and one formula as a\nconclusion. The list gives the assumptions the conclusion depends on\nin a derivation, in a local notation where in ND they are found in the\nleaves of a derivation tree. Gentzen also generalized sequents so that\nthey have, instead of one conclusion, a list of possible cases after\nthe arrow. This novelty led to the first satisfactory formulation of a\nproof system for classical logic. Gentzen's SC rules for conjunction\nand implication are, with commas separating elements in lists: \n\nThis is not the place to explain the details of ND and SC (but see the\nentry on automated\nreasoning).  Gentzen formulated the latter, denoted LK, so that it\ngave an intuitionistic calculus, denoted LJ, as a special case, the\none in which the conclusion is a list of at most one case. He then\nproved the analogue of the normalization theorem for the classical\ncalculus, the calculus and the proof carefully formulated so that the\nresult for the intuitionistic calculus was a special case of the one\nfor the classical calculus. In LJ and LK, L stands for\n“logistic”, a term by which Gentzen refers to the\naxiomatic calculi of logic of Frege, Russell, and Hilbert and\nBernays. In such calculi, each line in a derivation is correct in\nitself, i.e., a logical truth, whence the term. The letters K and J\ncome from the German words klassisch and\nintuitionistisch. (The latter should thus be upper case \"I\",\nbut older German uses upper case \"J\" for upper case \"I\".) \n\nGentzen called the analogue of normalization by the unimaginative \nname of Hauptsatz, “main theorem”. The standard \nterminology today is “cut elimination theorem” All of the\nlogical rules of SC have the subformula property in a very immediate \nsense: each formula in a premiss is a formula or subformula in the \nconclusion. The rule for combining derivations, analogous to the one \nexplained above for the case of detour conversions in ND, is called \n“cut”. In it, a formula A appears as a case in a\nfirst premiss and as an assumption in a second premiss. In the \nconclusion, this formula has disappeared and the assumptions of the \ntwo premisses collected together: \n\nThus, cut is the only rule that makes a formula \n“disappear” in a derivation. Gentzen showed that \ninstances of the rule of cut can be eliminated from derivations by \npermuting them upward until they reach points at which the derivation\nstarts. In ND, the starting points are assumptions, in SC they are \n“initial sequents” of the form \nA → A in which the assumption \nformula A is at the same time the conclusion. A cut with \nsuch a sequent as one premiss has the other premiss equal to the \nconclusion and can therefore be deleted. \n\nAfter the proof of cut elimination, Gentzen had no use for the proof \nof normalization for intuitionistic natural deduction. He gave the \nfirst handwritten version of his thesis, with the detailed proof of \nnormalization (equivalent to some 13 printed pages) to Bernays, but \nthe latter seems never to have realized what he had in his hands. The\nproof, among the papers of Bernays in Zurich, was discovered by the \npresent author in February 2005 and is now available in an English \ntranslation (Gentzen 1933 [2008]).  \n\nAfter his thesis work on ND and SC for pure logic, Gentzen continued\nhis plan of proving the consistency of arithmetic. The result was\nready by December 1934. What this very first proof was, is not known\nin detail. However, a letter to Bernays from 1938 indicates that the\nproof that Gentzen wrote down by the summer of 1935 was not this\noriginal one, but a second proof (see Menzler-Trott 2001, 79). This\nsecond proof was criticized by Bernays and Gödel who discussed it\nduring their Atlantic voyage to Princeton in September 1935. Gentzen's\nidea in the proof was as follows: first, take the\nconjunction-negation-universal quantification fragment of natural\ndeduction as the logic used in the formalization of arithmetic. Then\nwrite each rule instance so that the premisses and conclusion have the\nopen assumptions listed at left, with an arrow separating the\nconclusion, so, as sequents. This variant of ND is now called ND in SC\nstyle. Consider a sequent\nΓ → C. If its \nconclusion is an atomic formula, it is an equation between numbers. \nIn the worst case it is false, so then consider the list of \nassumptions. If one assumption is a conjunction, replace it by a \nconjunct of your choice, if a universal quantification, by an \ninstance. If it is a negation ¬A, replace the conclusion\nby A. If at any stage of this “reduction \nprocess” the conclusion of a sequent is a compound formula, you\nhave to consider any conjunct or any instance of universal \nquantification as a possible conclusion. In case of negation \n¬A as a conclusion, move A to the assumption \npart and replace the conclusion by 0 = 1. \nGentzen shows that by \nproceeding in this way under the assumption that the sequent in \nquestion is derivable, either a true equation is found as a \nconclusion, or a false equation as an assumption. Thus, there are no \nderivable sequents with all assumptions true and the conclusion \nfalse. \n\nIt was unclear to Gödel and Bernays what the proof assumed; they\nthought it assumed what is known in intuitionistic mathematics as the\nfan theorem, but this was false. Termination of Gentzen's reduction\nprocedure can be proved instead by induction on well-founded trees\n(“bar-induction”), a principle that was used by Gentzen on\nintuitive grounds. Anyway, the result of the criticism was that\nGentzen changed without further ado the proof into a third proof that\nuses the now famous principle of transfinite induction up to the first\nepsilon-number. This induction was presented through a coding that\nused decimal numbers. The concrete result of the changes for Gentzen's\npaper published in 1936 was not good, however: the logical calculus\nwas changed midway in an article of seventy-odd pages that became very\ndifficult to read. Therefore Gentzen gave yet another, by the present\ncount fourth proof of the consistency of arithmetic in 1938 (at the\nBernays archives of the ETH Zurich), this time based on the classical\nsequent calculus LK of 1933.  As mentioned, correspondence with\nBernays indicates that he thereby returned to the proof method that\nhad led to success in 1934. The use of transfinite induction is made\nclearly visible in the 1938 paper through an ordinal notation. Such\ninduction principles on Cantor's “second number class” are\ndiscussed in detail in Hilbert's 1925 lecture “Über das\nUnendliche” (“On the infinite”, published\n1926), a paper to which Gentzen referred. \n\nOne would have thought that that was that, but Gentzen had reason to \nproduce even a fourth proof of the consistency of arithmetic, in his \nlast paper published in 1943 but written before the war in 1939. He \nextended Peano arithmetic through transfinite ordinals and made the \ntransfinite induction principle part of this extended calculus. Then \nhe showed directly that transfinite induction up to the first \nepsilon-number ε0 is expressible but not provable \nin the system. Gödel's incompleteness theorem is thus proved in \na completely different way. The idea of the proof is, in brief terms,\nas follows: first it is laid down what it means to derive transfinite\ninduction to a specific ordinal number in the system. Secondly, \nordinal numbers below ε0 are associated to \nderivations. These are called “values”. It is then shown \nthat if transfinite induction to an ordinal number is derivable, this\nordinal number cannot be greater than the value of the derivation. \nTherefore transfinite induction to ε0 is not \nderivable. \n\nSince the induction principle can be expressed but not proved in \nordinary arithmetic, a formula unprovable in Peano arithmetic is \nfound. An easy consequence of Gentzen's version of the incompleteness\ntheorem is the consistency of Peano arithmetic, because anything \nwould be provable in an inconsistent system. Contrary to Gödel's\n“artificial” unprovable formula that was obtained through\na coding of the arithmetized provability predicate, Gentzen's \ntransfinite induction principle is a principle of \n“ordinary” mathematics. \n\nGentzen's last proof determined the “proof-theoretic \nordinal” of Peano arithmetic, namely the one that is needed to \nprove consistency, with the property that nothing less would suffice.\nThe work marked the beginning of ordinal proof theory. It was without\ndoubt the most remarkable foundational achievement in arithmetic \nafter Gödel's incompleteness theorems, but is still largely \nunknown—one can find many books about Gödel's theorems \nthat do not even mention Gentzen. \n\nGödel had, it seems, not thought of giving a consistency proof \nof arithmetic through the use of non-finitary but still constructive \nprinciples. In the late thirties, at least from 1938 on, he developed\nas a response to Gentzen's proof his own special interpretation of \nintuitionistic logic and arithmetic, what came to be known as the \nDialectica interpretation. It uses computable functionals to \ninterpret the proofs of intuitionistic arithmetic. Gödel \npublished the interpretation only in 1958, even though he had \npresented it in lectures in 1941. It is unknown if he discussed the \nmatter when he met Gentzen in December 1939. \n\nAt the request of Bernays, Ackermann reproduced Gentzen's proof in \nterms of Hilbert's \n epsilon-calculus\n in 1940. Ackermann's paper was the starting point of Kreisel's 1951 \n“no-counterexample” interpretation of arithmetic. It was \na surprise when the publication of Gödel's collected papers \nbrought to light his “Zilsel-lecture” in Vienna in 1938: \nhe outlines there this interpretation as a reformulation of Gentzen's\n1935 proof. (The matter is discussed in great detail in Tait (2005) \nwho himself had worked on the no-counterexample interpretation and \nits extension to analysis in the 1960s.) \n\nThe next obvious task in proof theory, after the proof of the \nconsistency of arithmetic, was to prove the consistency of analysis, \ni.e., of the theory of real numbers. Gentzen did some work in this \ndirection, but was then assigned to military service in the fall of \n1939. (He observed and reported the type, number, and direction of \naircraft that flew above the town of Brunswick, until he was hit by a\nnervous breakdown in early 1942.) From 1943 on he resumed the work on\nanalysis, but the difficulties intrinsic to the topic were great, as \nwere the practical difficulties of life caused by the war. Analysis \nwas to be formulated as a system of second-order arithmetic, which \nmeans that quantification is extended over number-theoretic \npredicates or, equivalently, over sets of natural numbers. \nSecond-order number theory is used in Gentzen's last paper, published\nin 1943, in which it is briefly shown that the principle of \ntransfinite induction up to ε0 is derivable in \nsecond-order number theory.  \n\nMore than half a century has passed with no constructive proof of the\nconsistency of full second-order arithmetic in sight. Early pioneers \nin this field included Kurt Schütte and Gaisi Takeuti. The \nformer created in 1951 an infinitary sequent calculus to present \nconsistency proofs in a perspicuous way, the latter instead used a \nmore traditional Gentzen-style calculus (see Takeuti 1987). \n\nIn the current research in the proof theory of second-order \narithmetic, one studies what are known as subsystems of second-order \narithmetic. The strongest results as of today are, in a very brief \noutline, the following: let X range over number-theoretic \npredicates. A formula such as X(x) states that x \nhas the property expressed by X. We can now use first- and \n second-order logic\n to form compound formulas such as \n∀X(Xx ∨ ¬ \nXx). The collection of natural numbers for which \nsuch a formula with one universal second-order quantifier holds is \ncalled a \nΠ11-set \n(in this case, the whole of\nthe natural numbers). More generally, a comprehension axiom is of the\nform \n∃X∀x(Xx \n↔ B(x)). If the formula B has no \nsecond-order quantifiers, the axiom gives what is called arithmetic \ncomprehension or ACA. If B can have the form \n∀Y∃Z C(x) with no \nother second-order quantifiers, the special case of \nΠ12-comprehension \nis obtained. Consistency  proofs for a subsystem of second-order\n arithmetic with \nΠ12-comprehension \nhave been given by Toshiyasu Arai and Michael Rathjen in the\nmid-1990s.  (see Rathjen 1995 for these developments). \n\nAt the time when Gentzen worked out his system of natural deduction, \nStanislaw Jaskowski was also developing a logical system for \nreasoning with assumptions. Formulas in derivations are arranged in a\nlinear succession, but Jaskowski's paper of 1934 remained fragmentary\nand without substantial results such as a subformula property. The \nlinear variant of natural deduction is followed in many pedagogical \nexpositions of elementary logic (sometimes called “Fitch \nsystems”). Gentzen found Jaskowski's work by June 1936, when \nboth were in Münster, and considered its linear arrangement of \nformulas an improvement, a “liberation from the straitjacket of\nthe tree form”, into one that reflects “the linearity of \nthinking” (the former from unpublished notes, the latter from \nGentzen's thesis). \n\nThe system of natural deduction lay mostly dormant for some thirty \nyears, until the thesis of Dag Prawitz of 1965, Natural Deduction:\nA Proof-Theoretical Study. The order in which Prawitz presented \nthe normalization theorem was different from the one in Gentzen's \nearly thesis manuscript. Prawitz gave first a normalization theorem \nand subformula property for a system of natural deduction for \nclassical logic. This system contains no disjunction or existence. In\na second stage, he considered intuitionistic natural deduction for \nthe full language of predicate logic and reduced its normalization to\nthe deletion of detour convertibilities as in the fragment of \nclassical logic. When Gentzen's proof of normalization came to light \nin 2005, Prawitz said, in conversation with the present author, that \nit is clear that Gentzen knew the result, because the remarks in the \nprinted thesis are so suggestive. \n\nIn the late 1960s, strong normalization became an issue: \nPrawitz, using previous work of William Tait and Jean-Yves Girard, \nproved in 1971 that non-normalities in a derivation can be converted \nin any order, with a terminating normalization process and a unique \nnormal derivation as a result. Gentzen seems not to have noticed the \nlatter, but seems to have thought rather the contrary, by the failure\nof this property for the elimination of cuts in sequent calculus. \n\nAt about the same time as strong normalization was studied, the\nCurry-Howard correspondence emerged. Curry had observed in his\nwork on combinatory logic in the late 1950s the analogy between\nimplication elimination in natural deduction and functional\napplication (Curry and Feys 1958). The idea was as old as\nintuitionistic logic: by the “BHK-explanation” of the\nconnectives and quantifiers (for Brouwer-Heyting-Kolmogorov), the\nforms of propositions in intuitionistic logic express prescriptions on\nhow to prove those propositions: a\nconjunction A &\nB is proved by proving A and B\nseparately, a disjunction A ∨\nB by proving one of A and B, and an\nimplication A ⊃ B by\nshowing how to convert any proof of A into some proof of\nB, and so on. These explanations come very close to the\nintroduction rules of natural deduction, but it remains unknown what\ntheir effect on Gentzen's thought was. \n\nThe Curry-Howard correspondence, from a paper by William Howard of \n1969, but published only in 1980, is based on the \n“formulas-as-types” principle, or in another jargon, on \nthe “propositions-as-sets” principle. A proposition is \nthought of as its set of proofs. Truth of a proposition corresponds \nto the non-emptyness of the set. Proofs of \nA ⊃ B are now functions from (proofs \nof) A to (proofs of) B and \nA ⊃ B itself the set of such \nfunctions. Thus, if f : \nA ⊃ B and \na : A, \nthen functional application gives \nf(a) : B. The reverse, corresponding to the introduction of an \nimplication, is captured by the principle of functional abstraction \nof Alonzo Church's λ-calculus. \n\nThe Curry-Howard correspondence has made intuitionistic natural \ndeduction part of the computer science curriculum: it gives a \ncomputational semantics for intuitionistic logic in which \ncomputations, and the executions of programs more generally, are \neffected through normalization. A proof of an implication \nA ⊃ B, say, is a program that \nconverts data of type A into an output of type B. \nThe construction of an object (proof, function, program) f \nof the type \nA ⊃ B ends with an \nabstraction. When an object a of type A is fed into\nf as an argument, the resulting expression is not normal, \nbut has a form that corresponds to an introduction followed by an \nelimination. Normalization now is the same as the execution of the \nprogram f. The use of intuitionistic logic is not tied to \nany intuitionistic philosophy of mathematics, but is just a \nsystematic guarantee for the termination of the execution of computer\nprograms. \n\nGentzen's doctoral thesis marked the birth of structural proof \ntheory, as contrasted to the old axiomatic proof theory of Hilbert. A\nremarkable step ahead in the development of systems of sequent \ncalculus was taken by Oiva Ketonen in his doctoral thesis of 1944. \nKetonen, a student of mathematics and philosophy in Helsinki, went to\nGöttingen in 1938 to study proof theory with Gentzen—being the closest to a student the latter ever had. The connection \nseems to have been established by Ketonen's philosophy professor Eino\nKaila who had met Gentzen in 1936 in Münster. Ketonen \nrecollected later that Gentzen was “a sympathetic young man of \nfew words” who gave him an introduction to the \nproof-theoretical systems and results. Ketonen's best-known discovery\nis a sequent calculus for classical propositional logic the logical \nrules of which are all invertible, meaning that whenever a sequent is\nof a form that matches the conclusion of a logical rule, the \ncorresponding premisses, defined uniquely from the given sequent and \nthe rule, are also derivable. The reverse is immediate (just apply \nthe rule). Rules L& and L⊃, for example, are modified into \n\nThere is just one left rule for conjunction (and dually just one \nright rule for disjunction). The left implication rule has what are \ncalled “shared contexts”: the assumptions and cases in \nthe conclusion, except for the formula with the connective, are \nrepeated identically in both premisses. Ketonen's idea was to define \na system of proof search: one starts from a given sequent to be \nderived, chooses a formula in it, and writes the premisses of a rule \nthat can conclude the given sequent. By invertibility, the question \nof derivability is replaced by one or two equivalent questions of \nderivability on simpler sequents. The new rules are needed to ensure \nuniquely defined premisses in such “root-first” \ndecomposition. \n\nKetonen's proof of invertibility of the logical rules of his sequent \ncalculus used the structural rule of cut. Later Kurt Schütte \n(1950) and Haskell Curry (1963) gave direct proofs of invertibility, \nthe latter with the explicit result that the inversions are height\npreserving: if a given sequent is derivable in at most n\nsteps, the premisses in a rule that can conclude that sequent also \nhave a derivation in at most n steps. \n\nHow much of Ketonen's work stems from suggestions on the part of \nGentzen remains unknown, because no correspondence has been found. \nKetonen writes in the preface of his thesis that “Dr. G. \nGentzen of Göttingen directed me towards the problem area of \nthis work”. The thesis was Ketonen's only original work in \nlogic, saved from oblivion by a long review that Bernays wrote of it \nfor The Journal of Symbolic Logic in 1945. \n\nOne person who knew Ketonen's calculus in the late 1940s was Evert \nBeth. When Beth later, in 1955, presented his well-known tableau \ncalculus, he seems to have forgotten the origin of the tableau \ncalculus as a reformulation of the one of Ketonen, but refers instead\nto Kleene's influential Introduction to Metamathematics of \n1952. Kleene had taken up Ketonen's calculus from the Bernays' review \nand also treated intuitionistic sequent calculus in which \ninvertibility is more restricted than in the classical calculus. With\nKleene's book, Gentzen's sequent calculi became generally known and \naccessible. \n\nKleene's work of the early 1950s also pioneered a remarkable \ndevelopment in sequent calculus, namely the \n“contraction-free” classical and intuitionistic calculi \ntoday denoted by G3c and G3i. These calculi have the \nproperty that none of Gentzen's original “structural \nrules” are needed. The rule of “weakening” permits \nthe addition of superfluous cases and assumptions, and the rule of \n“contraction” the deletion of one copy of a formula if \ntwo were present in a list, as in \n\nAnalogous rules permit weakening and contraction in the right, \nsuccedent parts of sequents. Weakening is made an eliminable rule by \nletting initial sequents have the form \nA, Γ → Δ,\nA  instead of Gentzen's \nA → A. Contraction is likewise made \neliminable by a suitable formulation of the rules. The import is that\nin root-first proof search, no rules need be applied that would \nproduce a duplication of a formula in a premiss. Without this result,\nnon-termination of proof search would not follow. \n\nThe classical calculus has the property, mentioned above, of \nheight-preserving invertibility of its logical rules. Albert Dragalin\nrefined in the late 1970s the calculus into one in which the \nstructural rules are moreover “height-preserving \nadmissible”, meaning that whenever the premiss of such a rule \nis derivable, the conclusion is derivable without the rule and with \nat most the same size (maximum number of rule instances in a \nderivation branch) of derivation. This property has profound effects \non cut elimination: in permuting cut up, Gentzen had to restore the \noriginal contexts (the Γ's and Δ's) through weakenings \nand contractions. With the height-preserving admissibility of these \nrules, the size of a derivation does not increase when the rules are \napplied. Dragalin gave also an intuitionistic multisuccedent calculus\nwith the same type of admissibility of the structural rules. \nTroelstra, finally, gave in the textbook Basic Proof Theory \n(2000, first ed. 1996) a single-succedent intuitionistic calculus \nwith height-preserving admissibility of weakening and contraction. \nThe contraction-free sequent calculi are powerful tools for the \nanalysis of formal derivations. Many difficult research results in \nlogic become just exercises through the control over the structure of\nproofs that the G3-calculi permit. \n\nThe earliest application of sequent calculus in mathematics was in \nthe proof theory of arithmetic, in Gentzen's thesis and in a decisive\nway in the 1938 proof of the consistency of arithmetic. Troelstra \nmentions Ketonen's work as  \nan early analysis of cutfree proofs\nin Gentzen calculi with axioms; but he considers the form of cutfree \nderivations in the pure calculus where axioms are present in the \nantecedent of the sequents derived. (Troelstra and \nSchwichtenberg 2000: 142) \n The axioms that Ketonen considers are \nthose of projective and affine geometry, the former taken from \nSkolem's 1920 paper discussed in the first section above. Ketonen \nwanted to formulate Skolem's formal rules of proof within sequent \ncalculus. However, Ketonen's work was mostly known only through its \nreview by Bernays and only the logical part on sequent calculus was \nexplained in detail there. \n\nA second way to apply the sequent calculus is to let sequents that\nbegin derivation branches have, in addition to initial sequents, also\nthe form → A,\nin which A is an axiom, or an instance of a universal\naxiom. Now, by Gentzen's “extended Hauptsatz”, cuts in\nderivations can be permuted up until one of their premisses is an\naxiom, but these cuts on axioms remain. Another, newer method is to\nconvert axioms into extra rules that are added to the logical rules of\nsequent calculus, with full cut elimination maintained (as explained\nin Negri and von Plato 2001, chapter 6, and in Troelstra and\nSchwichtenberg's 2000, chapter 4.7). \n\nTo what extent has proof theory achieved its original aims? For \nHilbert, the aims were a complete clarification of the foundational \nproblems through finitary proofs of consistency, etc, aims in which \nproof theory failed. Hilbert in his program was not interested in the\nstudy of mathematical proofs in themselves, but only in clearing the \ncentral foundational problems (and then to forget about them). A \nrecently found note by Hilbert gives a different picture: the note \nstates that Hilbert wanted to add as a 24th and last problem in his \nfamous Paris list of open mathematical problems of 1900 the \ndevelopment of “a theory of proof methods in \nmathematics”. This was before his metamathematical program for \nthe development of a proof theory emerged. \n\nFor Gentzen, the aims were, along with those of Hilbert, to \nunderstand the structure of mathematical proofs. This program was a \ncomplete success as far as pure logic and arithmetic are concerned. \nThe methods of sequent calculus, especially, permit the analysis of \nproofs with profound results. The grand aim of proof theory, a proof \nof the consistency of analysis as in Hilbert's second Paris problem, \nhas not been carried through but is not excluded, either. \n\nSome understanding of the notion of proof is necessary for any \nmathematician, if for nothing else, then at least for the \ncommunicability of mathematical results: publication rests on the \nunderstanding that proofs can be made as explicit as to be routinely \ncheckable for correctness. However, proof theory has so far not \nbecome a practical tool for the working mathematician; the \napplications in mathematics have been rather isolated cases. Recent \nwork on formalizing mathematical proofs with computerized systems, \ncalled proof editors, may gradually change this picture. \n\nProof theory has created new aims outside traditional mathematics, \nespecially in connection with computer science. Topics such as the \nverification of correctness of computer programs are an outgrowth of \nproof theory. Natural deduction has led to the Curry-Howard \ncorrespondence and to connections with functional programming, and \nsequent calculus is often used in systems of automatic proof search, \nas in logic programming.","contact.mail":"vonplato@mappi.helsinki.fi","contact.domain":"mappi.helsinki.fi"}]
