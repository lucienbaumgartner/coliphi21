[{"date.published":"2013-03-07","date.changed":"2019-03-26","url":"https://plato.stanford.edu/entries/logic-probability/","author1":"Lorenz Demey","author2":"Joshua Sack","author1.info":"https://sites.google.com/site/lorenzdemey/","author2.info":"http://www.philos.rug.nl/~barteld","entry":"logic-probability","body.text":"\n\n\nLogic and probability theory are two of the main tools in the formal\nstudy of reasoning, and have been fruitfully applied in areas as\ndiverse as philosophy, artificial intelligence, cognitive science and\nmathematics. This entry discusses the major proposals to combine logic\nand probability theory, and attempts to provide a classification of\nthe various approaches in this rapidly developing field.\n\nThe very idea of combining logic and probability might look strange at\nfirst sight (Hájek 2001). After all, logic is concerned with\nabsolutely certain truths and inferences, whereas probability theory\ndeals with uncertainties. Furthermore, logic offers a\nqualitative (structural) perspective on inference (the\ndeductive validity of an argument is based on the argument’s\nformal structure), whereas probabilities are quantitative\n(numerical) in nature. However, as will be shown in the next section,\nthere are natural senses in which probability theory\npresupposes and extends classical logic.\nFurthermore, historically speaking, several distinguished theorists\nsuch as De Morgan (1847), Boole (1854), Ramsey (1926), de Finetti\n(1937), Carnap (1950), Jeffrey (1992) and Howson (2003, 2007, 2009)\nhave emphasized the tight connections between logic and probability,\nor even considered their work on probability as a part of logic\nitself. \nBy integrating the complementary perspectives of qualitative logic and\nnumerical probability theory, probability logics are able to offer\nhighly expressive accounts of inference. It should therefore come as\nno surprise that they have been applied in all fields that study\nreasoning mechanisms, such as philosophy, artificial intelligence,\ncognitive science and mathematics. The downside to this\ncross-disciplinary popularity is that terms such as ‘probability\nlogic’ are used by different researchers in different,\nnon-equivalent ways. Therefore, before moving on to the actual\ndiscussion of the various approaches, we will first delineate the\nsubject matter of this entry. \nThe most important distinction is that between probability\nlogic and inductive logic. Classically, an argument is\nsaid to be (deductively) valid if and only if it is\nimpossible that the premises of \\(A\\) are all true, while its\nconclusion is false. In other words, deductive validity amounts to\ntruth preservation: in a valid argument, the truth of the\npremises guarantees the truth of the conclusion. In some arguments,\nhowever, the truth of the premises does not fully guarantee the truth\nof the conclusion, but it still renders it highly likely. A typical\nexample is the argument with premises ‘The first swan I saw was\nwhite’, …, ‘The 1000th swan I saw was white’,\nand conclusion ‘All swans are white’. Such arguments are\nstudied in inductive logic, which makes extensive use of\nprobabilistic notions, and is therefore considered by some authors to\nbe related to probability logic. There is some discussion about the\nexact relation between inductive logic and probability logic, which is\nsummarized in the introduction of Kyburg (1994). The dominant position\n(defended by Adams and Levine (1975), among others), which is also\nadopted here, is that probability logic entirely belongs to deductive\nlogic, and hence should not be concerned with inductive reasoning.\nStill, most work on inductive logic falls within the\n‘probability preservation’ approach, and is thus closely\nconnected to the systems discussed in\n Section 2.\n For more on inductive logic, the reader can consult Jaynes (2003),\nFitelson (2006), Romeijn (2011), and the entries on\n the problem of induction\n and\n inductive logic\n of this encyclopedia. \nWe will also steer clear of the philosophical debate over the exact\nnature of probability. The formal systems discussed here are\ncompatible with all of the common interpretations of probability, but\nobviously, in concrete applications, certain interpretations of\nprobability will fit more naturally than others. For example, the\nmodal probability logics discussed in \n Section 4\n are, by themselves, neutral about the nature of probability, but when\nthey are used to describe the behavior of a transition system, their\nprobabilities are typically interpreted in an objective way, whereas\nmodeling multi-agent scenarios is accompanied most naturally by a\nsubjective interpretation of probabilities (as agents’ degrees\nof belief). This topic is covered in detail in Gillies (2000), Eagle\n(2010), and the entry on\n interpretations of probability\n of this encyclopedia. \nA recent trend in the literature has been to focus less on integrating\nor combining logic and probability theory into a single, unified\nframework, but rather to establish bridges between the two\ndisciplines. This typically involves trying to capture the qualitative\nnotions of logic in the quantitative terms of probability theory, or\nthe other way around. We will not be able to do justice to the wide\nvariety of approaches in this booming area, but interested readers can\nconsult Leitgeb (2013, 2014), Lin and Kelly (2012a, 2012b), Douven and\nRott (2018), and Harrison-Trainor, Holliday and Icard (2016, 2018). A\n‘contemporary classic’ in this area is Leitgeb (2017),\nwhile van Benthem (2017) offers a useful survey and some interesting\nprogrammatic remarks. \nFinally, although the success of probability logic is largely due to\nits various applications, we will not deal with these applications in\nany detail. For example, we will not assess the use of probability as\na formal representation of belief in philosophy (Bayesian\nepistemology) or artificial intelligence (knowledge representation),\nand its advantages and disadvantages with respect to alternative\nrepresentations, such as generalized probability theory (for quantum\ntheory), \\(p\\)-adic probability, and fuzzy logic. For more information\nabout these topics, the reader can consult Gerla (1994), Vennekens et\nal. (2009), Hájek and Hartmann (2010), Hartmann and Sprenger\n(2010), Ilić-Stepić et al. (2012), and the entries on\n formal representations of belief,\n Bayesian epistemology,\n defeasible reasoning,\n quantum logic and probability theory, and\n fuzzy logic\n of this encyclopedia. \nWith these clarifications in place, we are now ready to look at what\nwill be discussed in this entry. The most common strategy to\nobtain a concrete system of probability logic is to start with a\nclassical (propositional/modal/etc.) system of logic and to\n‘probabilify’ it in one way or another, by adding\nprobabilistic features to it. There are various ways in which this\nprobabilification can be implemented. One can study probabilistic\nsemantics for classical languages (which do not have any explicit\nprobabilistic operators), in which case the consequence relation\nitself gets a probabilistic flavor: deductive validity becomes\n‘probability preservation’, rather than ‘truth\npreservation’. This direction will be discussed in \n Section 2. \n Alternatively, one can add various kinds of probabilistic\noperators to the syntax of the logic. In \n Section 3\n we will discuss some initial, rather basic examples of probabilistic\noperators. The full expressivity of modal probabilistic operators will\nbe explored in \n Section 4. \nFinally, languages with first-order probabilistic operators will be\ndiscussed in \n Section 5. \nIn this section, we will present a first family of probability logics,\nwhich are used to study questions of ‘probability\npreservation’ (or dually, ‘uncertainty\npropagation’). These systems do not extend the language with any\nprobabilistic operators, but rather deal with a\n‘classical’ propositional language \\(\\mathcal{L}\\), which\nhas a countable set of atomic propositions, and the usual\ntruth-functional (Boolean) connectives. \nThe main idea is that the premises of a valid argument can be\nuncertain, in which case (deductive) validity imposes no conditions on\nthe (un)certainty of the conclusion. For example, the argument with\npremises ‘if it will rain tomorrow, I will get wet’ and\n‘it will rain tomorrow’, and conclusion ‘I will get\nwet’ is valid, but if its second premise is uncertain, its\nconclusion will typically also be uncertain. Propositional probability\nlogics represent such uncertainties as probabilities, and study how\nthey ‘flow’ from the premises to the conclusion; in other\nwords, they do not study truth preservation, but rather\nprobability preservation. The following three subsections\ndiscuss systems that deal with increasingly more general versions of\nthis issue. \nWe begin by recalling the notion of a probability function for the\npropositional language \\(\\mathcal{L}\\). (In mathematics, probability\nfunctions are usually defined for a \\(\\sigma\\)-algebra of subsets of a\ngiven set \\(\\Omega\\), and required to satisfy countable additivity;\ncf. \n Section 4.3. \n In logical\ncontexts, however, it is often more natural to define probability\nfunctions ‘immediately’ for the logic’s object\nlanguage (Williamson 2002). Because this language is\nfinitary—all its formulas have finite length—, it also\nsuffices to require finite additivity.) A probability function\n(for \\(\\mathcal{L}\\)) is a function \\(P: \\mathcal{L}\\to\n\\mathbb{R}\\) satisfying the following constraints: \nNon-negativity. \\(P(\\phi)\\geq 0\\) for all\n\\(\\phi\\in\\mathcal{L}.\\)  \nTautologies. If \\(\\models\\phi\\), then\n\\(P(\\phi)=1.\\) \nFinite additivity. If\n\\(\\models\\neg(\\phi\\wedge\\psi)\\), then \\(P(\\phi\\vee\\psi) =\nP(\\phi)+P(\\psi).\\)  \nIn the second and third constraint, the \\(\\models\\)-symbol denotes\n(semantic) validity in classical propositional logic. The definition\nof probability functions thus requires notions from classical logic,\nand in this sense probability theory can be said to\npresuppose classical logic (Adams 1998, 22). It can easily be\nshown that if \\(P\\) satisfies these constraints, then \\(P(\\phi)\\in\n[0,1]\\) for all formulas \\(\\phi\\in\\mathcal{L}\\), and \\(P(\\phi) =\nP(\\psi)\\) for all formulas \\(\\phi,\\psi\\in\\mathcal{L}\\) that are\nlogically equivalent (i.e. such that\n\\(\\models\\phi\\leftrightarrow\\psi\\)). \nWe now turn to probabilistic semantics, as defined in Leblanc (1983).\nAn argument with premises \\(\\Gamma\\) and conclusion\n\\(\\phi\\)—henceforth denoted as \\((\\Gamma,\\phi)\\)—is said\nto be probabilistically valid, written\n\\(\\Gamma\\models_p\\phi\\), if and only if: \nfor all probability functions \\(P:\\mathcal{L}\\to\\mathbb{R}\\):\n\nif \\(P(\\gamma) = 1\\) for all \\(\\gamma\\in\\Gamma\\), then also \\(P(\\phi)\n= 1\\). \nProbabilistic semantics thus replaces the valuations\n\\(v:\\mathcal{L}\\to\\{0,1\\}\\) of classical propositional logic with\nprobability functions \\(P:\\mathcal{L}\\to \\mathbb{R}\\), which take\nvalues in the real unit interval \\([0,1]\\). The classical truth values\nof true (1) and false (0) can thus be regarded as\nthe endpoints of the unit interval \\([0,1]\\), and likewise, valuations\n\\(v:\\mathcal{L}\\to\\{0,1\\}\\) can be regarded as degenerate probability\nfunctions \\(P:\\mathcal{L}\\to[0,1]\\). In this sense, classical logic is\na special case of probability logic, or equivalently, probability\nlogic is an extension of classical logic. \nIt can be shown that classical propositional logic is (strongly) sound\nand complete with respect to probabilistic semantics: \nSome authors interpret probabilities as generalized truth values\n(Reichenbach 1949, Leblanc 1983). According to this view, probability\nlogic is just a particular kind of many-valued logic, and\nprobabilistic validity boils down to ‘truth preservation’:\ntruth (i.e. probability 1) carries over from the premises to the\nconclusion. Other logicians, such as Tarski (1936) and Adams (1998,\n15), have noted that probabilities cannot be seen as generalized truth\nvalues, because probability functions are not\n‘extensional’; for example, \\(P(\\phi\\wedge\\psi)\\) cannot\nbe expressed as a function of \\(P(\\phi)\\) and \\(P(\\psi)\\). More\ndiscussion on this topic can be found in Hailperin (1984). \nAnother possibility is to interpret a sentence’s probability as\na measure of its (un)certainty. For example, the sentence ‘Jones\nis in Spain at the moment’ can have any degree of certainty,\nranging from 0 (maximal uncertainty) to 1 (maximal certainty). (Note\nthat 0 is actually a kind of certainty, viz. certainty about\nfalsity; however, in this entry we follow Adams’ terminology\n(1998, 31) and interpret 0 as maximal uncertainty.) According to this\ninterpretation, the following theorem follows from the strong\nsoundness and completeness of probabilistic semantics: \nTheorem 1. Consider a deductively valid argument\n\\((\\Gamma,\\phi)\\). If all premises in \\(\\Gamma\\) have probability 1,\nthen the conclusion \\(\\phi\\) also has probability 1. \nThis theorem can be seen as a first, very partial clarification of the\nissue of probability preservation (or uncertainty propagation). It\nsays that if there is no uncertainty whatsoever about the premises,\nthen there cannot be any uncertainty about the conclusion either. In\nthe next two subsections we will consider more interesting cases, when\nthere is non-zero uncertainty about the premises, and ask how it\ncarries over to the conclusion. \nFinally, it should be noted that although this subsection only\ndiscussed probabilistic semantics for classical propositional logic,\nthere are also probabilistic semantics for a variety of other logics,\nsuch as intuitionistic propositional logic (van Fraassen 1981b, Morgan\nand Leblanc 1983), modal logics (Morgan 1982a, 1982b, 1983, Cross\n1993), classical first-order logic (Leblanc 1979, 1984, van Fraassen\n1981b), relevant logic (van Fraassen 1983) and nonmonotonic logic\n(Pearl 1991). All of these systems share a key feature: the\nlogic’s semantics is probabilistic in nature, but probabilities\nare not explicitly represented in the object language; hence,\nthey are much closer in nature to the propositional probability logics\ndiscussed here than to the systems presented in later sections. \nMost of these systems are not based on unary probabilities\n\\(P(\\phi)\\), but rather on conditional probabilities \\(P(\\phi,\\psi)\\).\nThe conditional probability \\(P(\\phi,\\psi)\\) is taken as primitive\n(rather than being defined as \\(P(\\phi\\wedge\\psi)/P(\\psi)\\), as is\nusually done) to avoid problems when \\(P(\\psi)=0\\). Goosens (1979)\nprovides an overview of various axiomatizations of probability theory\nin terms of such primitive notions of conditional probability. \nIn the previous subsection we discussed a first principle of\nprobability preservation, which says that if all premises have\nprobability 1, then the conclusion also has probability 1. Of course,\nmore interesting cases arise when the premises are less than\nabsolutely certain. Consider the valid argument with premises \\(p\\vee\nq\\) and \\(p\\to q\\), and conclusion \\(q\\) (the symbol\n‘\\(\\to\\)’ denotes the truth-conditional material\nconditional). One can easily show that  \nIn other words, if we know the probabilities of the argument’s\npremises, then we can calculate the exact probability of its\nconclusion, and thus provide a complete answer to the question of\nprobability preservation for this particular argument (for example, if\n\\(P(p \\vee q) = 6/7\\) and \\(P(p\\to q) = 5/7\\), then \\(P(q) = 4/7\\)).\nIn general, however, it will not be possible to calculate the\nexact probability of the conclusion, given the probabilities\nof the premises; rather, the best we can hope for is a (tight) upper\nand/or lower bound for the conclusion’s probability. We\nwill now discuss Adams’ (1998) methods to compute such\nbounds. \nAdams’ results can be stated more easily in terms of\nuncertainty rather than certainty (probability).\nGiven a probability function \\(P:\\mathcal{L}\\to [0,1]\\), the\ncorresponding uncertainty function \\(U_P\\) is defined as  \nIf the probability function \\(P\\) is clear from the context, we will\noften simply write \\(U\\) instead of \\(U_P\\). In the remainder of this\nsubsection (and in the next one as well) we will assume that all\narguments have only finitely many premises (which is not a significant\nrestriction, given the compactness property of classical propositional\nlogic). Adams’ first main result, which was originally\nestablished by Suppes (1966), can now be stated as follows: \nTheorem 2. Consider a valid argument\n\\((\\Gamma,\\phi)\\) and a probability function \\(P\\). Then the\nuncertainty of the conclusion \\(\\phi\\) cannot exceed the sum of the\nuncertainties of the premises \\(\\gamma\\in\\Gamma\\). Formally:  \nFirst of all, note that this theorem subsumes Theorem 1 as a special\ncase: if \\(P(\\gamma) = 1\\) for all \\(\\gamma\\in\\Gamma\\), then\n\\(U(\\gamma)=0\\) for all \\(\\gamma\\in\\Gamma\\), so \\(U(\\phi)\\leq \\sum\nU(\\gamma) = 0\\) and thus \\(P(\\phi) = 1\\). Furthermore, note that the\nupper bound on the uncertainty of the conclusion depends on\n\\(|\\Gamma|\\), i.e. on the number of premises. If a valid argument\nhas a small number of premises, each of which only has a small\nuncertainty (i.e. a high certainty), then its conclusion will\nalso have a reasonably small uncertainty (i.e. a reasonably high\ncertainty). Conversely, if a valid argument has premises with small\nuncertainties, then its conclusion can only be highly uncertain if the\nargument has a large number of premises (a famous illustration of this\nconverse principle is Kyburg’s (1965) lottery paradox,\nwhich is discussed in the entry on\n epistemic paradoxes\n of this encyclopedia). To put the matter more concretely, note that\nif a valid argument has three premises which each have uncertainty\n1/11, then adding a premise which also has uncertainty 1/11 will not\ninfluence the argument’s validity, but it will raise\nthe upper bound on the conclusion’s uncertainty from 3/11 to\n4/11—thus allowing the conclusion to be more uncertain than was\noriginally the case. Finally, the upper bound provided by Theorem 2 is\noptimal, in the sense that (under the right conditions) the\nuncertainty of the conclusion can coincide with its upper bound \\(\\sum\nU(\\gamma)\\): \nTheorem 3. Consider a valid argument\n\\((\\Gamma,\\phi)\\), and assume that the premise set \\(\\Gamma\\) is\nconsistent, and that every premise \\(\\gamma\\in\\Gamma\\) is relevant\n(i.e. \\(\\Gamma-\\{\\gamma\\}\\not\\models\\phi\\)). Then there exists a\nprobability function \\(P:\\mathcal{L}\\to[0,1]\\) such that \nThe upper bound provided by Theorem 2 can also be used to define a\nprobabilistic notion of validity. An argument \\((\\Gamma,\\phi)\\) is\nsaid to be Adams-probabilistically valid, written\n\\(\\Gamma\\models_a\\phi\\), if and only if \nfor all probability functions \\(P:\\mathcal{L}\\to\\mathbb{R}\\):\n\\(U_P(\\phi)\\leq \\sum_{\\gamma\\in\\Gamma}U_P(\\gamma)\\). \nAdams-probabilistic validity has an alternative, equivalent\ncharacterization in terms of probabilities rather than uncertainties.\nThis characterization says that \\((\\Gamma,\\phi)\\) is\nAdams-probabilistically valid if and only if the conclusion’s\nprobability can get arbitrarily close to 1 if the premises’\nprobabilities are sufficiently high. Formally: \\(\\Gamma\\models_a\\phi\\)\nif and only if \nfor all \\(\\epsilon>0\\) there exists a \\(\\delta>0\\) such that for\nall probability functions \\(P\\):\n\nif \\(P(\\gamma)>1-\\delta\\) for all \\(\\gamma\\in\\Gamma\\), then\n\\(P(\\phi)> 1-\\epsilon\\). \nIt can be shown that classical propositional logic is (strongly) sound\nand complete with respect to Adams’ probabilistic semantics:\n \nAdams (1998, 154) also defines another logic for which his\nprobabilistic semantics is sound and complete. However, this system\ninvolves a non-truth-functional connective (the probability\nconditional), and therefore falls outside the scope of this\nsection. (For more on probabilistic interpretations of conditionals,\nthe reader can consult the entries on\n conditionals\n and\n the logic of conditionals\n of this encyclopedia.) \nConsider the following example. The argument \\(A\\) with premises\n\\(p,q,r,s\\) and conclusion \\(p\\wedge(q\\vee r)\\) is valid. Assume that\n\\(P(p) = 10/11, P(q) = P(r) = 9/11\\) and \\(P(s) = 7/11\\). Then Theorem\n2 says that  \nThis upper bound on the uncertainty of the conclusion is rather\ndisappointing, and it exposes the main weakness of Theorem 2. One of\nthe reasons why the upper bound is so high, is that to compute it we\ntook into account the premise \\(s\\), which has a rather high\nuncertainty (\\(4/11\\)). However, this premise is irrelevant, in the\nsense that the conclusion already follows from the other three\npremises. Hence we can regard \\(p\\wedge (q\\vee r)\\) not only as the\nconclusion of the valid argument \\(A\\), but also as the conclusion of\nthe (equally valid) argument \\(A'\\), which has premises \\(p,q,r\\). In\nthe latter case Theorem 2 yields an upper bound of \\(1/11 + 2/11 +\n2/11 = 5/11\\), which is already much lower. \nThe weakness of Theorem 2 is thus that it takes into account (the\nuncertainty of) irrelevant or inessential premises. To obtain\nan improved version of this theorem, a more fine-grained notion of\n‘essentialness’ is necessary. In argument \\(A\\) in the\nexample above, premise \\(s\\) is absolutely irrelevant. Similarly,\npremise \\(p\\) is absolutely relevant, in the sense that without this\npremise, the conclusion \\(p\\wedge(q\\vee r)\\) is no longer derivable.\nFinally, the premise subset \\(\\{q,r\\}\\) is ‘in between’:\ntogether \\(q\\) and \\(r\\) are relevant (if both premises are left out,\nthe conclusion is no longer derivable), but each of them separately\ncan be left out (while keeping the conclusion derivable). \nThe notion of essentialness is formalized as follows: \nEssential premise set. Given a valid argument\n\\((\\Gamma,\\phi)\\), a set \\(\\Gamma' \\subseteq \\Gamma\\) is\nessential iff \\(\\Gamma - \\Gamma' \\not\\models\\phi\\). \nDegree of essentialness. Given a valid argument\n\\((\\Gamma,\\phi)\\) and a premise \\(\\gamma\\in\\Gamma\\), the degree of\nessentialness of \\(\\gamma\\), written \\(E(\\gamma)\\), is\n\\(1/|S_\\gamma|\\), where \\(|S_\\gamma|\\) is the cardinality of the\nsmallest essential premise set that contains \\(\\gamma\\). If \\(\\gamma\\)\ndoes not belong to any minimal essential premise set, then the degree\nof essentialness of \\(\\gamma\\) is 0. \nWith these definitions, a refined version of Theorem 2 can be\nestablished: \nTheorem 4. Consider a valid argument\n\\((\\Gamma,\\phi)\\). Then the uncertainty of the conclusion \\(\\phi\\)\ncannot exceed the weighted sum of the uncertainties of the premises\n\\(\\gamma\\in\\Gamma\\), with the degrees of essentialness as weights.\nFormally:  \nThe proof of Theorem 4 is significantly more difficult than that of\nTheorem 2: Theorem 2 requires only basic probability theory, whereas\nTheorem 4 is proved using methods from linear programming (Adams and\nLevine 1975; Goldman and Tucker 1956). Theorem 4 subsumes Theorem 2 as\na special case: if all premises are relevant (i.e. have degree of\nessentialness 1), then Theorem 4 yields the same upper bound as\nTheorem 2. Furthermore, Theorem 4 does not take into account\nirrelevant premises (i.e. premises with degree of essentialness\n0) to compute this upper bound; hence if a premise is irrelevant for\nthe validity of the argument, then its uncertainty will not carry over\nto the conclusion. Finally, note that since \\(E(\\gamma)\\in [0,1]\\) for\nall \\(\\gamma\\in\\Gamma\\), it holds that  \ni.e. Theorem 4 yields in general a tighter upper bound than\nTheorem 2. To illustrate this, consider again the argument with\npremises \\(p,q,r,s\\) and conclusion \\(p \\wedge (q\\vee r)\\). Recall\nthat \\(P(p)=10/11, P(q) = P(r)=9/11\\) and \\(P(s)=7/11\\). One can\ncalculate the degrees of essentialness of the premises: \\(E(p) = 1,\nE(q) = E(r) = 1/2\\) and \\(E(s) = 0\\). Hence Theorem 4 yields that  \nwhich is a tighter upper bound for the uncertainty of \\(p\\wedge(q \\vee\nr)\\) than any of the bounds obtained above via Theorem 2\n(viz. \\(9/11\\) and \\(5/11\\)). \nGiven the uncertainties (and degrees of essentialness) of the premises\nof a valid argument, Adams’ theorems allow us to compute an\nupper bound for the uncertainty of the conclusion. Of course\nthese results can also be expressed in terms of probabilities rather\nthan uncertainties; they then yield a lower bound for the\nprobability of the conclusion. For example, when expressed in terms of\nprobabilities rather than uncertainties, Theorem 4 looks as\nfollows: \nAdams’ results are restricted in at least two ways: \nThey only provide a lower bound for the probability of the\nconclusion (given the probabilities of the premises). In a sense this\nis the most important bound: it represents the conclusion’s\nprobability in the ‘worst-case scenario’, which might be\nuseful information in practical applications. However, in some\napplications it might also be informative to have an upper\nbound for the conclusion’s probability. For example, if one\nknows that this probability has an upper bound of 0.4, then one might\ndecide to refrain from certain actions (that one would have performed\nif this upper bound were (known to be) 0.9). \nThey presuppose that the premises’ exact probabilities are\nknown. In practical applications, however, there might only be partial\ninformation about the probability of a premise \\(\\gamma\\): its exact\nvalue is not known, but it is known to have a lower bound\n\\(a\\) and an upper bound \\(b\\) (Walley 1991). In such applications it\nwould be useful to have a method to calculate (optimal) lower and\nupper bounds for the probability of the conclusion in terms of the\nupper and lower bounds of the probabilities of the premises. \nHailperin (1965, 1984, 1986, 1996) and Nilsson (1986) use methods from\nlinear programming to show that these two restrictions can be\novercome. Their most important result is the following: \nTheorem 5. Consider an argument \\((\\Gamma,\\phi)\\),\nwith \\(|\\Gamma| = n\\). There exist functions \\(L_{\\Gamma,\\phi}:\n\\mathbb{R}^{2n} \\to \\mathbb{R}\\) and \\(U_{\\Gamma,\\phi}:\n\\mathbb{R}^{2n} \\to \\mathbb{R}\\) such that for any probability\nfunction \\(P\\), the following holds: if \\(a_i \\leq P(\\gamma_i) \\leq\nb_i\\) for \\(1\\leq i\\leq n\\), then: \n\\(L_{\\Gamma,\\phi}(a_1,\\dots,a_n,b_1,\\dots,b_n) \\leq P(\\phi) \\:\\leq\\)\n\\(U_{\\Gamma,\\phi}(a_1,\\dots,a_n,b_1,\\dots,b_n)\\). \nThe bounds in item 1 are optimal, in the sense that there exist\nprobability functions \\(P_L\\) and \\(P_U\\) such that \\(a_i \\leq\nP_L(\\gamma_i),\\) \\(P_U(\\gamma_i)\\leq b_i\\) for \\(1\\leq i\\leq n\\), and\n\\(L_{\\Gamma,\\phi}(a_1,\\dots,a_n,b_1,\\dots,b_n) = P_L(\\phi)\\) and\n\\(P_U(\\phi) = U_{\\Gamma,\\phi}(a_1,\\dots,a_n,b_1,\\dots,b_n)\\). \nThe functions \\(L_{\\Gamma,\\phi}\\) and \\(U_{\\Gamma,\\phi}\\) are\neffectively determinable from the Boolean structure of the sentences\nin \\(\\Gamma \\cup \\{\\phi\\}\\). \nThis result can also be used to define yet another probabilistic\nnotion of validity, which we will call Hailperin-probabilistic\nvalidity or simply h-validity. This notion is not\ndefined with respect to formulas, but rather with respect to\npairs consisting of a formula and a subinterval of \\([0,1]\\).\nIf \\(X_i\\) is the interval associated with premise \\(\\gamma_i\\in\n\\Gamma\\) and \\(Y\\) is the interval associated with the conclusion\n\\(\\phi\\), then the argument \\((\\Gamma,\\phi)\\) is said to be\nh-valid, written \\(\\Gamma\\models_h\\phi\\), if and only if \nfor all probability functions \\(P\\): \nIn Haenni et al. (2011) this is written as  \nand called the standard probabilistic semantics. \nNilsson’s work on probabilistic logic (1986, 1993) has sparked a\nlot of research on probabilistic reasoning in artificial intelligence\n(Hansen and Jaumard 2000; chapter 2 of Haenni et al. 2011).\nHowever, it should be noted that although Theorem 5 states that the\nfunctions \\(L_{\\Gamma,\\phi}\\) and \\(U_{\\Gamma,\\phi}\\) are\neffectively determinable from the sentences in\n\\(\\Gamma\\cup\\{\\phi\\}\\), the computational complexity of this\nproblem is quite high (Georgakopoulos et al. 1988, Kavvadias and\nPapadimitriou 1990), and thus finding these functions quickly becomes\ncomputationally unfeasible in real-world applications. Contemporary\napproaches based on probabilistic argumentation systems and\nprobabilistic networks are better capable of handling these\ncomputational challenges. Furthermore, probabilistic argumentation\nsystems are closely related to Dempster-Shafer theory (Dempster 1968;\nShafer 1976; Haenni and Lehmann 2003). However, an extended discussion\nof these approaches is beyond the scope of (the current version of)\nthis entry; see (Haenni et al. 2011) for a recent survey. \nIn this section we will study probability logics that extend the\npropositional language \\(\\mathcal{L}\\) with rather basic probability\noperators. They differ from the logics in \n Section 2 \n in that\nthe logics here involve probability operators in the object language.\n Section 3.1\n discusses qualitative probability\noperators; \n Section 3.2\n discusses quantitative probability operators. \nThere are several applications in which qualitative theories of\nprobability might be useful, or even necessary. In some situations\nthere are no frequencies available to use as estimates for the\nprobabilities, or it might be practically impossible to obtain those\nfrequencies. Furthermore, people are often willing to compare\nthe probabilities of two statements (‘\\(\\phi\\) is more probable\nthan \\(\\psi\\)’), without being able to assign explicit\nprobabilities to each of the statements individually\n(Szolovits and Pauker 1978, Halpern and Rabin 1987). In such\nsituations qualitative probability logics will be useful. \nOne of the earliest qualitative probability logics is Hamblin’s\n(1959). The language is extended with a unary operator \\(\\Box\\), which\nis to be read as ‘probably’. Hence a formula such as\n\\(\\Box\\phi\\) is to be read as ‘probably \\(\\phi\\)’. This\nnotion of ‘probable’ can be formalized as sufficiently\nhigh (numerical) probability (i.e. \\(P(\\phi)\\geq t\\), for\nsome threshold value \\(1/2 < t \\leq 1\\)), or alternatively in terms\nof plausibility, which is a non-metrical generalization of\nprobability. Burgess (1969) further develops these systems, focusing\non the ‘high numerical probability’-interpretation. Both\nHamblin and Burgess introduce additional operators into their systems\n(expressing, for example, metaphysical necessity and/or knowledge),\nand study the interaction between the ‘probably’-operator\nand these other modal operators. However, the\n‘probably’-operator already displays some interesting\nfeatures on its own (independent from any other operators). If it is\ninterpreted as ‘sufficiently high probability’, then it\nfails to satisfy the principle \\((\\Box\\phi\\wedge\\Box\\psi) \\to\n\\Box(\\phi\\wedge\\psi)\\). This means that it is not a normal\nmodal operator, and cannot be given a Kripke (relational) semantics.\nHerzig and Longin (2003) and Arló Costa (2005) provide weaker\nsystems of neighborhood semantics for such\n‘probably’-operators, while Yalcin (2010) discusses their\nbehavior from a more linguistically oriented perspective. \nAnother route is taken by Segerberg (1971) and Gärdenfors (1975a,\n1975b), who build on earlier work by de Finetti (1937), Kraft, Pratt\nand Seidenberg (1959) and Scott (1964). They introduce a\nbinary operator \\(\\geq\\); the formula \\(\\phi\\geq\\psi\\) is to\nbe read as ‘\\(\\phi\\) is at least as probable as \\(\\psi\\)’\n(formally: \\(P(\\phi)\\geq P(\\psi)\\)). The key idea is that one can\ncompletely axiomatize the behavior of \\(\\geq\\) without having to use\nthe ‘underlying’ probabilities of the individual formulas.\nIt should be noted that with comparative probability (a binary\noperator), one can also express some absolute probabilistic properties\n(unary operators). For example, \\(\\phi\\geq \\top\\) expresses that\n\\(\\phi\\) has probability 1, and \\(\\phi\\geq\\neg\\phi\\) expresses that\n\\(\\phi\\) has probability at least 1/2. In recent work, Delgrande and\nRenne (2015) further extend the qualitative approach, by allowing the\narguments of \\(\\geq\\) to be finite sequences of formulas (of\npotentially different lengths). The formula \\((\\phi_1,\\dots,\\phi_n)\n\\geq (\\psi_1,\\dots,\\psi_m)\\) is informally to be read as ‘the\nsum of the probabilities of the \\(\\phi_i\\)’s is at least as high\nas the sum of the probabilities of the \\(\\psi_j\\)’s’. The\nresulting logic can be axiomatized completely, and is so expressive\nthat it can even capture quantitative probabilistic logics,\nto which we turn now. \nPropositional probability logics are extensions of propositional logic\nthat express numerical relationships among probability terms\n\\(P(\\varphi)\\). A simple propositional probability logic adds to\npropositional logic formulas of the form \\(P(\\varphi)\\ge q\\), where\n\\(\\varphi\\) is a propositional formula and \\(q\\) is a number; such a\nformula asserts that the probability of \\(\\varphi\\) is at least \\(q\\).\nThe semantics is formalized using models consisting a probability\nfunction \\(\\mathcal{P}\\) over a set \\(\\Omega\\), whose elements are\neach given a truth assignment to the atomic propositions of the\npropositional logic. Thus a propositional formula is true at an\nelement of \\(\\Omega\\) if the truth assignment for that element makes\nthe propositional formula true. The formula \\(P(\\varphi)\\ge q\\) is\ntrue in the model if and only if the probability \\(\\mathcal{P}\\) of\nthe set of elements of \\(\\Omega\\) for which \\(\\varphi\\) is true is at\nleast \\(q\\). See Chapter 3 of Ognjanović et al. (2016) for\nan overview of such a propositional probability logic. \nSome propositional probability logics include other types of formulas\nin the object language, such as those involving sums and products of\nprobability terms. The appeal of involving sums can be clarified by\nthe additivity condition of probability functions (see \n Section 2.1),\n which can be expressed as \\(P(\\phi \\vee \\psi) =\nP(\\phi)+P(\\psi)\\) whenever \\(\\neg (\\phi \\wedge \\psi)\\) is a tautology,\nor equivalently as \\(P(\\phi \\wedge \\psi) + P(\\phi \\wedge \\neg \\psi) =\nP(\\phi)\\). Probability logics that explicitly involve sums of\nprobabilities tend to more generally include linear combinations of\nprobability terms, such as in Fagin et al. (1990). Here,\npropositional logic is extended with formulas of the form\n\\(a_1P(\\phi_1) + \\cdots + a_n P(\\phi_n) \\ge b\\), where \\(n\\) is a\npositive integer that may differ from formula to formula, and\n\\(a_1,\\ldots,a_n\\), and \\(b\\) are all rational numbers. Here are some\nexamples of what can be expressed. \n\\(P(\\phi) \\le q\\) by \\(-P(\\phi) \\ge -q\\), \n\\(P(\\phi) < q\\) by \\(\\neg (P(\\phi) \\ge q)\\), \n\\(P(\\phi) = q\\) by \\(P(\\phi)\\ge q \\wedge P(\\phi) \\le q\\). \n\\(P(\\phi) \\ge P(\\psi)\\) by \\(P(\\phi)-P(\\psi) \\ge 0\\). \nExpressive power with and without linear\ncombinations: \nAlthough linear combinations provide a convenient way of expressing\nnumerous relationships among probability terms, a language without\nsums of probability terms is still very powerful. Consider the\nlanguage restricted to formulas of the form \\(P(\\phi) \\ge q\\) for some\npropositional formula \\(\\phi\\) and rational \\(q\\). We can define \nwhich is reasonable considering that the probability of the complement\nof a proposition is equal to 1 minus the probability of the\nproposition. The formulas \\(P(\\phi) <q\\) and \\(P(\\phi) = q\\) can be\ndefined without linear combinations as we did above. Using this\nrestricted probability language, we can reason about additivity in a\nless direct way. The formula  \nstates that if the probability of \\(\\phi \\wedge \\psi\\) is \\(a\\) and\nthe probability of \\(\\phi\\wedge \\neg \\psi\\) is \\(b\\), then the\nprobability of the disjunction of the formulas (which is equivalent to\n\\(\\phi\\)) is \\(a+b\\). However, while the use of linear combinations\nallows us to assert that the probabilities of \\(\\varphi\\wedge\\psi\\)\nand \\(\\varphi\\wedge\\neg\\psi\\) are additive by using the formula\n\\(P(\\varphi\\wedge \\psi)+P(\\varphi\\wedge\\neg\\psi) = P(\\varphi)\\), the\nformula without linear combinations above only does so if we choose\nthe correct numbers \\(a\\) and \\(b\\). A formal comparison of the\nexpressiveness of propositional probability logic with linear\ncombinations and without is given in Demey and Sack (2015). While any\ntwo models agree on all formulas with linear combinations if and only\nif they agree on all formulas without (Lemma 4.1 of Demey and Sack\n(2015)), it is not the case that any class of models definable by a\nsingle formula with linear combinations can be defined by a single\nformula without (Lemma 4.2 of Demey and Sack (2015)). In particular,\nthe class of models defined by the formula \\(P(p)- P(q)\\ge 0\\) cannot\nbe defined by any single formula without the power of linear\ncombinations. \nProbabilities belonging to a given subset:\nOgnjanović and Rašković (1999) extend the language of\nprobability logic by means of a new type of operator: \\(Q_F\\).\nIntuitively, the formula \\(Q_F\\phi\\) means that the probability of\n\\(\\phi\\) belongs to \\(F\\), for some given set \\(F \\subseteq [0,1]\\).\nThis \\(Q_F\\)-operator cannot be defined in terms of formulas of the\nform \\(P(\\phi) \\ge a\\). Ognjanović and Rašković\n(1999) provide a sound and complete axiomatization of this type of\nlogical system. The key bridge principles, which connect the\n\\(Q_F\\)-operator to the more standard \\(P\\)-operator, are the axioms\n\\(P(\\phi) = a \\to Q_F\\phi\\) for all \\(a \\in F\\), as well as the\ninfinitary rule that specifies that from \\(P(\\phi) = a \\to \\psi\\) for\nall \\(a \\in F\\), one can infer \\(Q_F\\phi\\to\\psi\\). \nPolynomial weight formulas: Logics with polynomial\nweight formulas (involving both weighted sums and products of\nprobability terms), can allow for formulas of the form\n\\(P(\\phi)P(\\psi)-P(\\phi\\wedge \\psi) = 0\\), that is, the probability of\nboth \\(\\phi\\) and \\(\\psi\\) is equal to the product of the\nprobabilities of \\(\\phi\\) and \\(\\psi\\). This formula captures what it\nmeans for \\(\\phi\\) and \\(\\psi\\) to be statistically\nindependent. Such logics were investigated in Fagin et\nal. (1990), but mostly with first-order logic features included,\nand then again in a simpler context (without quantifiers) in\nPerović et al. (2008). \nCompactness and completeness: Compactness is a\nproperty of a logic where a set of formulas is satisfiable if every\nfinite subset is satisfiable. Propositional probability logics lack\nthe compactness property, as every finite subset of\n\\(\\{P(p)>0\\}\\cup\\{P(p)\\leq a\\,|\\,a>0\\}\\) is satisfiable, but the\nentire set is not. \nWithout compactness, a logic might be weakly complete (every valid\nformula is provable in the axiomatic system), but not strongly\ncomplete (for every set \\(\\Gamma\\) of formulas, every logical\nconsequence of \\(\\Gamma\\) is provable from \\(\\Gamma\\) in the axiomatic\nsystem). In Fagin et al. (1990), a proof system involving linear\ncombinations was given and the logic was shown to be both sound and\nweakly complete. In Ognjanović and Rašković (1999), a\nsound and strongly complete proof system is given for propositional\nprobability logic without linear combinations. In Heifetz and Mongin\n(2001), a proof system for a variation of the logic without linear\ncombinations that uses a system of types to allow for iteration of\nprobability formulas (we will see in \n Section 4\n how such\niteration can be achieved using possible worlds) was given and the\nlogic was shown to be sound and weakly complete. They also observe\nthat no finitary proof system for such a logic can be strongly\ncomplete. Ognjanović et al. (2008) present some qualitative\nprobabilistic logics with infinitary derivation rules (which require a\ncountably infinite number of premises), and prove strong completeness.\nGoldblatt (2010) presents a strongly complete proof system for a\nrelated coalgebraic logic. Perović et al. (2008) give a\nproof system and proof of strong completeness for propositional\nprobability logic with polynomial weight formulas. Finally, another\nstrategy for obtaining strong completeness involves restricting the\nrange of the probability functions to a fixed, finite set of numbers;\nfor example, Ognjanović et al. (2008) discuss a qualitative\nprobabilistic logic in which the range of the probability functions is\nnot the full real unit interval \\([0,1]\\), but rather the\n‘discretized’ version\n\\(\\{0,\\frac{1}{n},\\frac{2}{n},\\dots,\\frac{n-1}{n},1\\}\\) (for some\nfixed number \\(n\\in\\mathbb{N}\\)). See Chapter 7 of Ognjanović et\nal. (2016) for an overview of completeness results. \nMany probability logics are interpreted over a single, but arbitrary\nprobability space. Modal probability logic makes use of many\nprobability spaces, each associated with a possible world or state.\nThis can be viewed as a minor adjustment to the relational semantics\nof modal logic: rather than associate to every possible world a set of\naccessible worlds as is done in modal logic, modal probability logic\nassociates to every possible world a probability distribution, a\nprobability space, or a set of probability distributions. The language\nof modal probability logic allows for embedding of probabilities\nwithin probabilities, that is, it can for example reason about the\nprobability that (possibly a different) probability is \\(1/2\\). This\nmodal setting involving multiple probabilities has generally been\ngiven a (1) stochastic interpretation, concerning different\nprobabilities over the next states a system might transition into\n(Larsen and Skou 1991), and (2) a subjective interpretation,\nconcerning different probabilities that different agents may have\nabout a situation or each other’s probabilities (Fagin and\nHalpern 1988). Both interpretations can use exactly the same formal\nframework. \nA basic modal probability logic adds to propositional logic formulas\nof the form \\(P (\\phi)\\ge q\\), where \\(q\\) is typically a rational\nnumber, and \\(\\phi\\) is any formula of the language, possibly a\nprobability formula. The reading of such a formula is that the\nprobability of \\(\\phi\\) is at least \\(q\\). This general reading of the\nformula does not reflect any difference between modal probability\nlogic and other probability logics with the same formula; where the\ndifference lies is in the ability to embed probabilities in the\narguments of probability terms and in the semantics. The following\nsubsections provide an overview of the variations of how modal\nprobability logic is modeled. In one case the language is altered\nslightly \n (Section 4.2), and in other cases, the logic is\nextended to address interactions between qualitative and quantitative\nuncertainty \n (Section 4.4) or dynamics \n (Section 4.5). \nFormally, a Basic Finite Modal Probabilistic Model is a tuple\n\\(M=(W,\\mathcal{P},V)\\), where \\(W\\) is a finite set of possible\nworlds or states, \\(\\mathcal{P}\\) is a function associating a\ndistribution \\(\\mathcal{P}_w\\) over \\(W\\) to each world \\(w\\in W\\),\nand \\(V\\) is a ‘valuation function’ assigning atomic\npropositions from a set \\(\\Phi\\) to each world. The distribution is\nadditively extended from individual worlds to sets of worlds:\n\\(\\mathcal{P}_w(S) = \\sum_{s\\in S}\\mathcal{P}_w(s)\\). The first two\ncomponents of a basic modal probabilistic model are effectively the\nsame as a Kripke frame whose relation is decorated with numbers\n(probability values). Such a structure has different names, such as a\ndirected graph with labelled edges in mathematics, or a probabilistic\ntransition system in computer science. The valuation function, as in a\nKripke model, allows us to assign properties to the worlds. \nThe semantics for formulas are given on pairs \\((M,w)\\), where \\(M\\)\nis a model and \\(w\\) is an element of the model. A formula \\(P(\\phi)\n\\ge q\\) is true at a pair \\((M,w)\\), written \\((M,w)\\models P(\\phi)\\ge\nq\\), if and only if \\(\\mathcal{P}_w(\\{w'\\mid (M,w')\\models \\phi\\}) \\ge\nq\\). \nThe first generalization, which is most common in applications of\nmodal probabilistic logic, is to allow the distributions to be indexed\nby two sets rather than one. The first set is the set \\(W\\) of worlds\n(the base set of the model), but the other is an index set \\(A\\) often\nto be taken as a set of actions, agents, or players of a game.\nFormally, \\(\\mathcal{P}\\) associates a distribution\n\\(\\mathcal{P}_{a,w}\\) over \\(W\\) for each \\(w\\in W\\) and \\(a\\in A\\).\nFor the language, rather than involving formulas of the form\n\\(P(\\phi)\\ge q\\), we have \\(P_a(\\phi)\\ge q\\), and \\((M,w)\\models\nP_a(\\phi)\\ge q\\) if and only if \\(\\mathcal{P}_{a,w}(\\{w'\\mid\n(M,w')\\models \\phi\\}) \\ge q\\). \nExample: Suppose we have an index set \\(A = \\{a,\nb\\}\\), and a set \\(\\Phi = \\{p,q\\}\\) of atomic propositions. Consider\n\\((W,\\mathcal{P},V)\\), where \n\\(W = \\{w,x,y,z\\}\\) \n\\(\\mathcal{P}_{a,w}\\) and \\(\\mathcal{P}_{a,x}\\) map \\(w\\) to \\(1/2\\),\n\\(x\\) to \\(1/2\\), \\(y\\) to \\(0\\), and \\(z\\) to \\(0\\). \n\\(\\mathcal{P}_{a,y}\\) and \\(\\mathcal{P}_{a,z}\\) map \\(y\\) to \\(1/3\\),\n\\(z\\) to \\(2/3\\), \\(w\\) to \\(0\\), and \\(x\\) to \\(0\\). \n\\(\\mathcal{P}_{b,w}\\) and \\(\\mathcal{P}_{b,y}\\) map \\(w\\) to \\(1/2\\),\n\\(y\\) to \\(1/2\\), \\(x\\) to \\(0\\), and \\(z\\) to \\(0\\). \n\\(\\mathcal{P}_{b,x}\\) and \\(\\mathcal{P}_{b,z}\\) map \\(x\\) to \\(1/4\\),\n\\(z\\) to \\(3/4\\), \\(w\\) to \\(0\\), and \\(y\\) to \\(0\\). \n\\(V(p) = \\{w,x\\}\\) \n\\(V(q) = \\{w,y\\}\\). \nWe depict this example with the following diagram. Inside each circle\nis a labeling of the truth of each proposition letter for the world\nwhose name is labelled right outside the circle. The arrows indicate\nthe probabilities. For example, an arrow from world \\(x\\) to world\n\\(z\\) labeled by \\((b,3/4)\\) indicates that from \\(x\\), the probably\nof \\(z\\) under label \\(b\\) is \\(3/4\\). Probabilities of 0 are not\nlabelled. \nFigure \nStochastic Interpretation: Consider the elements\n\\(a\\) and \\(b\\) of \\(A\\) to be actions, for example, pressing buttons\non a machine. In this case, pressing a button does not have a certain\noutcome. For instance, if the machine is in state \\(x\\), there is a\n\\(1/2\\) probability it will remain in the same state after pressing\n\\(a\\), but a \\(1/4\\) probability of remaining in the same state after\npressing \\(b\\). That is,  \nA significant feature of modal logics in general (and this includes\nmodal probabilistic logic) is the ability to support higher-order\nreasoning, that is, the reasoning about probabilities of\nprobabilities. The importance of higher-order probabilities is clear\nfrom the role they play in, for example, Miller’s\nprinciple, which states that \\(P_1(\\phi\\mid P_2(\\phi) = b) = b\\).\nHere, \\(P_1\\) and \\(P_2\\) are probability functions, which can have\nvarious interpretations, such as the probabilities of two agents,\nlogical and statistical probability, or the probabilities of one agent\nat different moments in time (Miller 1966; Lewis 1980; van Fraassen\n1984; Halpern 1991). Higher-order probability also occurs for instance\nin the Judy Benjamin Problem (van Fraassen 1981a) where one\nconditionalizes on probabilistic information. Whether one agrees with\nthe principles proposed in the literature on higher-order\nprobabilities or not, the ability to represent them forces one to\ninvestigate the principles governing them. \nTo illustrate higher-order reasoning more concretely, we return to our\nexample and see that at \\(x\\), there is a \\(1/2\\) probability that\nafter pressing \\(a\\), there is a \\(1/2\\) probability that after\npressing \\(b\\), it will be the case that \\(\\neg p\\) is true, that is,\n \nSubjective Interpretation: Suppose the elements \\(a\\)\nand \\(b\\) of \\(A\\) are players of a game. \\(p\\) and \\(\\neg p\\) are\nstrategies for player \\(a\\) and \\(q\\) and \\(\\neg q\\) are both\nstrategies for player \\(b\\). In the model, each player is certain of\nher own strategy; for instance at \\(x\\), player \\(a\\) is certain that\nshe will play \\(p\\) and player \\(b\\) is certain that she will play\n\\(\\neg q\\), that is  \nBut the players randomize over their opponents. For instance at \\(x\\),\nthe probability that \\(b\\) has for \\(a\\)’s probability of \\(\\neg\nq\\) being \\(1/2\\) is \\(1/4\\), that is  \nProbabilities are generally defined as measures in a measure space. A\nmeasure space is a set \\(\\Omega\\) (the sample space) together with a\n\\(\\sigma\\)-algebra (also called \\(\\sigma\\)-field) \\(\\mathcal{A}\\) over\n\\(\\Omega\\), which is a non-empty set of subsets of \\(\\Omega\\) such\nthat \\(A\\in \\mathcal{A}\\) implies that \\(\\Omega-A\\in \\mathcal{A}\\),\nand \\(A_i\\in \\mathcal{A}\\) for all natural numbers \\(i\\), implies that\n\\(\\bigcup_i A_i\\in \\mathcal{A}\\). A measure is a function \\(\\mu\\)\ndefined on the \\(\\sigma\\)-algebra \\(\\mathcal{A}\\), such that \\(\\mu(A)\n\\ge 0\\) for every set \\(A \\in\\mathcal{A}\\) and \\(\\mu(\\bigcup_i A_i) =\n\\sum_i\\mu(A_i)\\) whenever \\(A_i\\cap A_j = \\emptyset\\) for each\n\\(i,j\\). \nThe effect of the \\(\\sigma\\)-algebra is to restrict the domain so that\nnot every subset of \\(\\Omega\\) need have a probability. This is\ncrucial for some probabilities to be defined on uncountably infinite\nsets; for example, a uniform distribution over a unit interval cannot\nbe defined on all subsets of the interval while also maintaining the\ncountable additivity condition for probability measures. \nThe same basic language as was used for the basic finite probability\nlogic need not change, but the semantics is slightly different: for\nevery state \\(w\\in W\\), the component \\(\\mathcal{P}_w\\) of a modal\nprobabilistic model is replaced by an entire probability space\n\\((\\Omega_w,\\mathcal{A}_w,\\mu_w)\\), such that \\(\\Omega_w\\subseteq W\\)\nand \\(\\mathcal{A}_w\\) is a \\(\\sigma\\)-algebra over \\(\\Omega_w\\). The\nreason we may want entire spaces to differ from one world to another\nis to reflect uncertainty about what probability space is the right\none. For the semantics of probability formulas, \\((M,w)\\models P(\\phi)\n\\ge q\\) if and only if \\(\\mu_w(\\{w'\\mid (M,w')\\models \\phi\\})\\ge q\\).\nSuch a definition is not well defined in the event that \\(\\{w'\\mid\n(M,w')\\models \\phi\\}\\not\\in \\mathcal{A}_w\\). Thus constraints are\noften placed on the models to ensure that such sets are always in the\n\\(\\sigma\\)-algebras. \nAlthough probabilities reflect quantitative uncertainty at one level,\nthere can also be qualitative uncertainty about probabilities. We\nmight want to have qualitative and quantitative uncertainty because we\nmay be so uncertain about some situations that we do not want to\nassign numbers to the probabilities of their events, while there are\nother situations where we do have a sense of the probabilities of\ntheir events; and these situations can interact. \nThere are many situations in which we might not want to assign\nnumerical values to uncertainties. One example is where a computer\nselects a bit 0 or 1, and we know nothing about how this bit is\nselected. Results of coin flips, on the other hand, are often used\nexamples of where we would assign probabilities to individual\noutcomes. \nAn example of how these might interact is where the result of the bit\ndetermines whether a fair coin or a weighted coin (say, heads with\nprobability \\(2/3\\)) be used for a coin flip. Thus there is\nqualitative uncertainty as to whether the action of flipping a coin\nyields heads with probability \\(1/2\\) or \\(2/3\\). \nOne way to formalize the interaction between probability and\nqualitative uncertainty is by adding another relation to the model and\na modal operator to the language as is done in Fagin and Halpern\n(1988, 1994). Formally, we add to a basic finite probability model a\nrelation \\(R\\subseteq W^2\\). Then we add to the language a modal\noperator \\(\\Box\\), such that \\((M,w)\\models \\Box\\phi\\) if and only if\n\\((M,w')\\models \\phi\\) whenever \\(w R w'\\). \nConsider the following example: \n\\(W = \\{(0,H),(0,T),(1,H),(1,T)\\}\\), \n\\(\\Phi = \\{h,t\\}\\) is the set of atomic propositions, \n\\(R = W^2\\), \n\\(P\\) associates with \\((0,H)\\) and \\((0,T)\\) the distribution mapping\n\\((0,H)\\) and \\((0,T)\\) each to \\(1/2\\), and associates with \\((1,H)\\)\nand \\((1,T)\\) the distribution mapping \\((1,H)\\) to \\(2/3\\) and\n\\((1,T)\\) to \\(1/3\\), \n\\(V\\) maps \\(h\\) to the set \\(\\{(0,H),(1,H)\\}\\) and \\(t\\) to the set\n\\(\\{(0,T),(1,T)\\}\\). \nThen the following formula is true at \\((0,H)\\): \\(\\neg \\Box h \\wedge\n(\\neg \\Box P(h)= 1/2) \\wedge (\\Diamond P(h) = 1/2)\\). This can be read\nas it is not known that \\(h\\) is true, and it is not known that\nthe probability of \\(h\\) is \\(1/2\\), but it is possible that the\nprobability of \\(h\\) is \\(1/2\\). \nWe have discussed two views of modal probability logic. One is\ntemporal or stochastic, where the probability distribution associated\nwith each state determines the likelihood of transitioning into other\nstates; another is concerned with subjective perspectives of agents,\nwho may reason about probabilities of other agents. A stochastic\nsystem is dynamic in that it represents probabilities of different\ntransitions, and this can be conveyed by the modal probabilistic\nmodels themselves. But from a subjective view, the modal probabilistic\nmodels are static: the probabilities are concerned with what currently\nis the case. Although static in their interpretation, the modal\nprobabilistic setting can be put in a dynamic context. \nDynamics in a modal probabilistic setting is generally concerned with\nsimultaneous changes to probabilities in potentially all possible\nworlds. Intuitively, such a change may be caused by new information\nthat invokes a probabilistic revision at each possible world. The\ndynamics of subjective probabilities is often modeled using\nconditional probabilities, such as in Kooi (2003), Baltag and Smets\n(2008), and van Benthem et al. (2009). The probability of \\(E\\)\nconditional on \\(F\\), written \\(P(E\\mid F)\\), is \\(P(E\\cap F)/P(F)\\).\nWhen updating by a set \\(F\\), a probability distribution \\(P\\) is\nreplaced by the probability distribution \\(P'\\), such that \\(P'(E)=\nP(E \\mid F)\\), so long as \\(P(F)\\neq 0\\). Let us assume for the\nremainder of this dynamics subsection that every relevant set\nconsidered has positive probability. \nUsing a probability logic with linear combinations, we can abbreviate\nthe conditional probability \\(P(\\phi\\mid \\psi)\\ge q\\) by \\(P(\\phi\n\\wedge \\psi) - qP(\\psi)\\ge 0\\). In a modal setting, an operator\n\\([!\\psi]\\) can be added to the language, such that \\(M,w\\models\n[!\\psi]\\phi\\) if and only if \\(M',w\\models \\phi\\), where \\(M'\\) is the\nmodel obtained from \\(M\\) by revising the probabilities of each world\nby \\(\\psi\\). Note that \\([!\\psi](P(\\phi)\\ge q)\\) differs from\n\\(P(\\phi\\mid \\psi)\\ge q\\), in that in \\([!\\psi](P(\\phi)\\ge q)\\), the\ninterpretation of probability terms inside \\(\\phi\\) are affected by\nthe revision by \\(\\psi\\), whereas in \\(P(\\phi\\mid \\psi)\\ge q\\), they\nare not, which is why \\(P(\\phi\\mid \\psi)\\ge q\\) nicely unfolds into\nanother probability formula. However, \\([!\\psi]\\phi\\) does unfold too,\nbut in more steps:  \nFor other overviews of modal probability logics and its dynamics, see\nDemey and Kooi (2014), Demey and Sack (2015), and\n appendix L on probabilistic update in dynamic epistemic logic\n of the entry on dynamic epistemic logic. \nIn this section we will discuss first-order probability logics. As was\nexplained in \n Section 1\n of this entry, there are many ways in\nwhich a logic can have probabilistic features. The models of the logic\ncan have probabilistic aspects, the notion of consequence can have a\nprobabilistic flavor, or the language of the logic can contain\nprobabilistic operators. In this section we will focus on those\nlogical operators that have a first-order flavor. The first-order\nflavor is what distinguishes these operators from the probabilistic\nmodal operators of the previous section. \nConsider the following example from Bacchus (1990): \nMore than 75% of all birds fly. \nThere is a straightforward probabilistic interpretation of this\nsentence, namely when one randomly selects a bird, then the\nprobability that the selected bird flies is more than 3/4. First-order\nprobabilistic operators are needed to express these sort of\nstatements. \nThere is another type of sentence, such as the following sentence\ndiscussed in Halpern (1990): \nThe probability that Tweety flies is greater than \\(0.9\\). \nThis sentence considers the probability that Tweety (a particular\nbird) can fly. These two types of sentences are addressed by two\ndifferent types of semantics, where the former involves probabilities\nover a domain, while the latter involves probabilities over a set of\npossible worlds that is separate from the domain. \nIn this subsection we will have a closer look at a particular\nfirst-order probability logic, whose language is as simple as\npossible, in order to focus on the probabilistic quantifiers. The\nlanguage is very much like the language of classical first-order\nlogic, but rather than the familiar universal and existential\nquantifier, the language contains a probabilistic quantifier. \nThe language is built on a set of of individual variables\n(denoted by \\(x, y, z, x_1, x_2, \\ldots\\)), a set of function\nsymbols (denoted by \\(f, g, h, f_1, \\ldots\\)) where an arity is\nassociated with each symbol (nullary function symbols are also called\nindividual constants), and a set of predicate\nletters (denoted by \\( R, P_1, \\ldots\\)) where an arity is\nassociated with each symbol. The language contains two kinds of\nsyntactical objects, namely terms and formulas. The\nterms are defined inductively as follows: \nEvery individual variable \\(x\\) is a term. \nEvery function symbol \\(f\\) of arity \\(n\\) followed by an \\(n\\)-tuple\nof terms \\((t_1,\\ldots,t_n)\\) is a term. \nGiven this definition of terms, the formulas are defined inductively\nas follows: \nEvery predicate letter \\(R\\) of arity \\(n\\) followed by an \\(n\\)-tuple\nof terms \\((t_1,\\ldots,t_n)\\) is a formula. \nIf \\(\\phi\\) is a formula, then so is \\(\\neg \\phi\\). \nIf \\(\\phi\\) and \\(\\psi\\) are formulas, then so is \\((\\phi \\wedge\n\\psi)\\). \nIf \\(\\phi\\) is a formula and \\(q\\) is a rational number in the\ninterval \\([0,1]\\), then so is \\(Px (\\phi) \\geq q\\). \nFormulas of the form \\(Px (\\phi) \\geq q\\) should be read as:\n“the probability of selecting an \\(x\\) such that \\(x\\) satisfies\n\\(\\phi\\) is at least \\(q\\)”. The formula \\(Px(\\phi) \\leq q\\) is\nan abbreviation of \\(Px(\\neg \\phi) \\geq 1-q\\) and \\(Px(\\phi)=q\\) is an\nabbreviation of \\(Px(\\phi) \\geq q \\wedge Px(\\phi) \\leq q\\). Every free\noccurrence of \\(x\\) in \\(\\phi\\) is bound by the operator. \nThis language is interpreted on very simple first-order models, which\nare triples \\(M=(D,I,P)\\), where the domain of discourse\n\\(D\\) is a finite nonempty set of objects, the interpretation\n\\(I\\) associates an \\(n\\)-ary function on \\(D\\) with every \\(n\\)-ary\nfunction symbol occurring in the language, and an \\(n\\)-ary relation\non \\(D\\) with every \\(n\\)-ary predicate letter. \\(P\\) is a\nprobability function that assigns a probability \\(P(d)\\) to\nevery element \\(d\\) in \\(D\\) such that \\(\\sum_{d \\in D} P(d)=1\\). \nIn order to interpret formulas containing free variables one also\nneeds an assignment \\(g\\) which assigns an element of \\(D\\)\nto every variable. The interpretation \\([\\![t]\\!]_{M,g}\\) of a term\n\\(t\\) given a model \\(M=(D,I,P)\\) and an assignment \\(g\\) is defined\ninductively as follows: \n\\([\\![ x ]\\!]_{M,g}=g(x)\\) \n\\([\\![ f (t_1,\\ldots,t_n)]\\!]_{M,g}= I(f) ([\\![t_1]\\!], \\ldots,\n[\\![t_n]\\!])\\) \nTruth is defined as a relation \\(\\models\\) between models with\nassignments and formulas: \n\\(M,g \\models R(t_1,\\ldots,t_n)\\) iff \\(([\\![t_1]\\!], \\ldots,\n[\\![t_n]\\!]) \\in I(R)\\) \n\\(M,g \\models \\neg \\phi\\) iff \\(M,g \\not \\models \\phi\\) \n\\(M,g \\models (\\phi \\wedge \\psi)\\) iff \\(M,g \\models \\phi\\) and \\(M,g\n\\models \\psi\\) \n\\(M,g \\models Px(\\phi) \\geq q\\) iff \\(\\sum_{d :M,g[x \\mapsto d]\n\\models \\phi} P(d) \\geq q\\) \nAs an example, consider a model of a vase containing nine marbles:\nfive are black and four are white. Let us assume that \\(P\\) assigns a\nprobability of 1/9 to each marble, which captures the idea that one is\nequally likely to pick any marble. Suppose the language contains a\nunary predicate \\(B\\) whose interpretation is the set of black\nmarbles. The sentence \\(Px(B(x)) = 5/9\\) is true in this model\nregardless of the assignment. \nThe logic that we just presented is too simple to capture many forms\nof reasoning about probabilities. We will discuss three extensions\nhere. \nFirst of all one would like to reason about cases where more than one\nobject is selected from the domain. Consider for example the\nprobability of first picking a black marble, putting it back, and then\npicking a white marble from the vase. This probability is 5/9\n\\(\\times\\) 4/9 = 20/81, but we cannot express this in the language\nabove. For this we need one operator that deals with multiple\nvariables simultaneously, written as \\(Px_1,\\ldots x_n (\\phi) \\geq\nq\\). The semantics for such operators will then have to provide a\nprobability measure on subsets of \\(D^n\\). The simplest way to do this\nis by simply taking the product of the probability function \\(P\\) on\n\\(D\\), which can be taken as an extension of \\(P\\) to tuples, where\n\\(P(d_1,\\ldots d_n)= P(d_1) \\times \\cdots \\times P(d_n)\\), which\nyields the following semantics: \n\\(M,g \\models Px_1\\ldots x_n (\\phi) \\geq q\\) iff\n\\(\\sum_{(d_1,\\ldots,d_n) :M,g[x_1 \\mapsto d_1, \\ldots, x_n \\mapsto\nd_n] \\models \\phi} P(d_1,\\ldots,d_n) \\geq q\\) \nThis approach is taken by Bacchus (1990) and Halpern (1990),\ncorresponding to the idea that selections are independent and with\nreplacements. With these semantics the example above can be formalized\nas \\(Px,y (B(x) \\wedge \\neg B(y))= 20/81\\). There are also more\ngeneral approaches to extending the measure on the domain to tuples\nfrom the domain such as by Hoover (1978) and Keisler (1985). \nWhen one considers the initial example that more than 75% of all birds\nfly, one finds that this cannot be adequately captured in a model\nwhere the domain contains objects that are not birds. These objects\nshould not matter to what one wishes to express, but the probability\nquantifiers, quantify over the whole domain. In order to restrict\nquantification one must add conditional probability operators \\(Px\n(\\phi | \\psi) \\geq q\\) with the following semantics: \n\\(M,g \\models Px (\\phi | \\psi) \\geq q\\) iff if there is a \\(d \\in D\\)\nsuch that \\(M,g[x \\mapsto d] \\models \\psi\\) then \nWith these operators, the formula \\(Px(F(x) \\mid B(x)) > 3/4\\)\nexpresses that more than 75% of all birds fly. \nWhen one wants to compare the probability of different events, say of\nselecting a black ball and selecting a white ball, it may be more\nconvenient to consider probabilities to be terms in their own right.\nThat is, an expression \\(Px(\\phi)\\) is interpreted as referring to\nsome rational number. Then one can extend the language with\narithmetical operations such as addition and multiplication, and with\noperators such as equality and inequalities to compare probability\nterms. One can then say that one is twice as likely to select a black\nball compared to a white ball as \\(Px(B(x))=2 \\times Px (W(x))\\). Such\nan extension requires that the language contains two separate classes\nof terms: one for probabilities, numbers and the results of\narithmetical operations on such terms, and one for the domain of\ndiscourse which the probabilistic operators quantify over. We will not\npresent such a language and semantics in detail here. One can find\nsuch a system in Bacchus (1990). \nIn this subsection, we consider a first-order probability logic with a\npossible-world semantics (which we abbreviate FOPL). The language of\nFOPL is similar to the example we gave in\n Section 5.1\n related to that of\nBacchus, except here we have full quantifier formulas of the form\n\\((\\forall x)\\phi\\) for any formula \\(\\phi\\), and instead of\nprobability formulas of the form \\(Px(\\phi)\\ge q\\), we have\nprobability formulas of the form \\(P(\\phi)\\ge q\\) (similar to the\nprobability formulas in propositional probability logic). \nThe models of FOPL are of the form \\(M = (W,D,I,P)\\), where \\(W\\) is a\nset of possible worlds, \\(D\\) is a domain of\ndiscourse, \\(I\\) is a localized interpretation function mapping\nevery \\(w\\in W\\) to a interpretation function \\(I(w)\\) that associates\nto every function and predicate symbol, a function or predicate of\nappropriate arity, and \\(P\\) is a probability function that assign a\nprobability \\(P(w)\\) to every \\(w\\) in \\(W\\). \nSimilarly to the simple example before, we involve an assignment\nfunction \\(g\\) mapping each variable to an element of the domain\n\\(D\\). To interpret terms, for every model \\(M\\), world \\(w\\in W\\),\nand assignment function \\(g\\), we map each term \\(t\\) to domain\nelements as follows: \nTruth is defined according to a relation \\(\\models\\) between pointed\nmodels (models with designated worlds) with assignments and formulas\nas follows: \n\\(M,w,g \\models R(t_1,\\ldots,t_n)\\) iff \\(([\\![t_1]\\!], \\ldots,\n[\\![t_n]\\!]) \\in I(w)(R)\\) \n\\(M,w,g \\models \\neg \\phi\\) iff \\(M,w,g \\not \\models \\phi\\) \n\\(M,w,g \\models (\\phi \\wedge \\psi)\\) iff \\(M,w,g \\models \\phi\\) and\n\\(M,w,g \\models \\psi\\) \n\\(M,w,g\\models (\\forall x)\\varphi\\) iff \\(M,w,g[x/d]\\models \\varphi\\)\nfor all \\(d\\in D\\), where \\(g[x/d]\\) is the same as \\(g\\) except that\nit maps \\(x\\) to \\(d\\). \n\\(M,w,g\\models P(\\varphi)\\ge q\\) iff \\(P(\\{w'\\mid (M,w',g)\\models\n\\varphi\\})\\ge q\\). \nAs an example, consider a model where there are two possible vases: 4\nwhite marbles and 4 black marbles were put in both possible vases. But\nthen another marble, called , was placed in the vase, but in one\npossible vase, was white, and in the other it was black. Thus in the\nend, there are two possible vases: one with 5 black marbles and 4\nwhite marbles, and the other with 4 black marbles and 5 white marbles.\nSuppose \\(P\\) assigns \\(1/2\\) probability to the two possible vases.\nThen \\(P(B(\\mathsf{last})) = 1/2\\) is true for this variable\nassignment, and if any other variable assignment were chosen, the\nformula \\((\\exists x) P(B(x)) = 1/2\\) would still be true. \nGenerally it is hard to provide proof systems for first-order\nprobability logics, because the validity problem for these logics is\ngenerally undecidable. It is even not the case, as it is the case in\nclassical first-order logic, that if an inference is valid, then one\ncan find out in finite time (see Abadi and Halpern (1994)). \nNonetheless there are many results for first-order probability logic.\nFor instance, Hoover (1978) and Keisler (1985) study completeness\nresults. Bacchus (1990) and Halpern (1990) also provide complete\naxiomatizations as well as combinations of first-order probability\nlogics and possible-world first-order probability logics respectively.\nIn Ognjanović and Rašković (2000), an infinitary\ncomplete axiomatization is given for a more general version of the\npossible-world first-order probability logic presented here.","contact.mail":"lorenz.demey@hiw.kuleuven.be","contact.domain":"hiw.kuleuven.be"},{"date.published":"2013-03-07","date.changed":"2019-03-26","url":"https://plato.stanford.edu/entries/logic-probability/","author1":"Lorenz Demey","author2":"Joshua Sack","author1.info":"https://sites.google.com/site/lorenzdemey/","author2.info":"http://www.philos.rug.nl/~barteld","entry":"logic-probability","body.text":"\n\n\nLogic and probability theory are two of the main tools in the formal\nstudy of reasoning, and have been fruitfully applied in areas as\ndiverse as philosophy, artificial intelligence, cognitive science and\nmathematics. This entry discusses the major proposals to combine logic\nand probability theory, and attempts to provide a classification of\nthe various approaches in this rapidly developing field.\n\nThe very idea of combining logic and probability might look strange at\nfirst sight (Hájek 2001). After all, logic is concerned with\nabsolutely certain truths and inferences, whereas probability theory\ndeals with uncertainties. Furthermore, logic offers a\nqualitative (structural) perspective on inference (the\ndeductive validity of an argument is based on the argument’s\nformal structure), whereas probabilities are quantitative\n(numerical) in nature. However, as will be shown in the next section,\nthere are natural senses in which probability theory\npresupposes and extends classical logic.\nFurthermore, historically speaking, several distinguished theorists\nsuch as De Morgan (1847), Boole (1854), Ramsey (1926), de Finetti\n(1937), Carnap (1950), Jeffrey (1992) and Howson (2003, 2007, 2009)\nhave emphasized the tight connections between logic and probability,\nor even considered their work on probability as a part of logic\nitself. \nBy integrating the complementary perspectives of qualitative logic and\nnumerical probability theory, probability logics are able to offer\nhighly expressive accounts of inference. It should therefore come as\nno surprise that they have been applied in all fields that study\nreasoning mechanisms, such as philosophy, artificial intelligence,\ncognitive science and mathematics. The downside to this\ncross-disciplinary popularity is that terms such as ‘probability\nlogic’ are used by different researchers in different,\nnon-equivalent ways. Therefore, before moving on to the actual\ndiscussion of the various approaches, we will first delineate the\nsubject matter of this entry. \nThe most important distinction is that between probability\nlogic and inductive logic. Classically, an argument is\nsaid to be (deductively) valid if and only if it is\nimpossible that the premises of \\(A\\) are all true, while its\nconclusion is false. In other words, deductive validity amounts to\ntruth preservation: in a valid argument, the truth of the\npremises guarantees the truth of the conclusion. In some arguments,\nhowever, the truth of the premises does not fully guarantee the truth\nof the conclusion, but it still renders it highly likely. A typical\nexample is the argument with premises ‘The first swan I saw was\nwhite’, …, ‘The 1000th swan I saw was white’,\nand conclusion ‘All swans are white’. Such arguments are\nstudied in inductive logic, which makes extensive use of\nprobabilistic notions, and is therefore considered by some authors to\nbe related to probability logic. There is some discussion about the\nexact relation between inductive logic and probability logic, which is\nsummarized in the introduction of Kyburg (1994). The dominant position\n(defended by Adams and Levine (1975), among others), which is also\nadopted here, is that probability logic entirely belongs to deductive\nlogic, and hence should not be concerned with inductive reasoning.\nStill, most work on inductive logic falls within the\n‘probability preservation’ approach, and is thus closely\nconnected to the systems discussed in\n Section 2.\n For more on inductive logic, the reader can consult Jaynes (2003),\nFitelson (2006), Romeijn (2011), and the entries on\n the problem of induction\n and\n inductive logic\n of this encyclopedia. \nWe will also steer clear of the philosophical debate over the exact\nnature of probability. The formal systems discussed here are\ncompatible with all of the common interpretations of probability, but\nobviously, in concrete applications, certain interpretations of\nprobability will fit more naturally than others. For example, the\nmodal probability logics discussed in \n Section 4\n are, by themselves, neutral about the nature of probability, but when\nthey are used to describe the behavior of a transition system, their\nprobabilities are typically interpreted in an objective way, whereas\nmodeling multi-agent scenarios is accompanied most naturally by a\nsubjective interpretation of probabilities (as agents’ degrees\nof belief). This topic is covered in detail in Gillies (2000), Eagle\n(2010), and the entry on\n interpretations of probability\n of this encyclopedia. \nA recent trend in the literature has been to focus less on integrating\nor combining logic and probability theory into a single, unified\nframework, but rather to establish bridges between the two\ndisciplines. This typically involves trying to capture the qualitative\nnotions of logic in the quantitative terms of probability theory, or\nthe other way around. We will not be able to do justice to the wide\nvariety of approaches in this booming area, but interested readers can\nconsult Leitgeb (2013, 2014), Lin and Kelly (2012a, 2012b), Douven and\nRott (2018), and Harrison-Trainor, Holliday and Icard (2016, 2018). A\n‘contemporary classic’ in this area is Leitgeb (2017),\nwhile van Benthem (2017) offers a useful survey and some interesting\nprogrammatic remarks. \nFinally, although the success of probability logic is largely due to\nits various applications, we will not deal with these applications in\nany detail. For example, we will not assess the use of probability as\na formal representation of belief in philosophy (Bayesian\nepistemology) or artificial intelligence (knowledge representation),\nand its advantages and disadvantages with respect to alternative\nrepresentations, such as generalized probability theory (for quantum\ntheory), \\(p\\)-adic probability, and fuzzy logic. For more information\nabout these topics, the reader can consult Gerla (1994), Vennekens et\nal. (2009), Hájek and Hartmann (2010), Hartmann and Sprenger\n(2010), Ilić-Stepić et al. (2012), and the entries on\n formal representations of belief,\n Bayesian epistemology,\n defeasible reasoning,\n quantum logic and probability theory, and\n fuzzy logic\n of this encyclopedia. \nWith these clarifications in place, we are now ready to look at what\nwill be discussed in this entry. The most common strategy to\nobtain a concrete system of probability logic is to start with a\nclassical (propositional/modal/etc.) system of logic and to\n‘probabilify’ it in one way or another, by adding\nprobabilistic features to it. There are various ways in which this\nprobabilification can be implemented. One can study probabilistic\nsemantics for classical languages (which do not have any explicit\nprobabilistic operators), in which case the consequence relation\nitself gets a probabilistic flavor: deductive validity becomes\n‘probability preservation’, rather than ‘truth\npreservation’. This direction will be discussed in \n Section 2. \n Alternatively, one can add various kinds of probabilistic\noperators to the syntax of the logic. In \n Section 3\n we will discuss some initial, rather basic examples of probabilistic\noperators. The full expressivity of modal probabilistic operators will\nbe explored in \n Section 4. \nFinally, languages with first-order probabilistic operators will be\ndiscussed in \n Section 5. \nIn this section, we will present a first family of probability logics,\nwhich are used to study questions of ‘probability\npreservation’ (or dually, ‘uncertainty\npropagation’). These systems do not extend the language with any\nprobabilistic operators, but rather deal with a\n‘classical’ propositional language \\(\\mathcal{L}\\), which\nhas a countable set of atomic propositions, and the usual\ntruth-functional (Boolean) connectives. \nThe main idea is that the premises of a valid argument can be\nuncertain, in which case (deductive) validity imposes no conditions on\nthe (un)certainty of the conclusion. For example, the argument with\npremises ‘if it will rain tomorrow, I will get wet’ and\n‘it will rain tomorrow’, and conclusion ‘I will get\nwet’ is valid, but if its second premise is uncertain, its\nconclusion will typically also be uncertain. Propositional probability\nlogics represent such uncertainties as probabilities, and study how\nthey ‘flow’ from the premises to the conclusion; in other\nwords, they do not study truth preservation, but rather\nprobability preservation. The following three subsections\ndiscuss systems that deal with increasingly more general versions of\nthis issue. \nWe begin by recalling the notion of a probability function for the\npropositional language \\(\\mathcal{L}\\). (In mathematics, probability\nfunctions are usually defined for a \\(\\sigma\\)-algebra of subsets of a\ngiven set \\(\\Omega\\), and required to satisfy countable additivity;\ncf. \n Section 4.3. \n In logical\ncontexts, however, it is often more natural to define probability\nfunctions ‘immediately’ for the logic’s object\nlanguage (Williamson 2002). Because this language is\nfinitary—all its formulas have finite length—, it also\nsuffices to require finite additivity.) A probability function\n(for \\(\\mathcal{L}\\)) is a function \\(P: \\mathcal{L}\\to\n\\mathbb{R}\\) satisfying the following constraints: \nNon-negativity. \\(P(\\phi)\\geq 0\\) for all\n\\(\\phi\\in\\mathcal{L}.\\)  \nTautologies. If \\(\\models\\phi\\), then\n\\(P(\\phi)=1.\\) \nFinite additivity. If\n\\(\\models\\neg(\\phi\\wedge\\psi)\\), then \\(P(\\phi\\vee\\psi) =\nP(\\phi)+P(\\psi).\\)  \nIn the second and third constraint, the \\(\\models\\)-symbol denotes\n(semantic) validity in classical propositional logic. The definition\nof probability functions thus requires notions from classical logic,\nand in this sense probability theory can be said to\npresuppose classical logic (Adams 1998, 22). It can easily be\nshown that if \\(P\\) satisfies these constraints, then \\(P(\\phi)\\in\n[0,1]\\) for all formulas \\(\\phi\\in\\mathcal{L}\\), and \\(P(\\phi) =\nP(\\psi)\\) for all formulas \\(\\phi,\\psi\\in\\mathcal{L}\\) that are\nlogically equivalent (i.e. such that\n\\(\\models\\phi\\leftrightarrow\\psi\\)). \nWe now turn to probabilistic semantics, as defined in Leblanc (1983).\nAn argument with premises \\(\\Gamma\\) and conclusion\n\\(\\phi\\)—henceforth denoted as \\((\\Gamma,\\phi)\\)—is said\nto be probabilistically valid, written\n\\(\\Gamma\\models_p\\phi\\), if and only if: \nfor all probability functions \\(P:\\mathcal{L}\\to\\mathbb{R}\\):\n\nif \\(P(\\gamma) = 1\\) for all \\(\\gamma\\in\\Gamma\\), then also \\(P(\\phi)\n= 1\\). \nProbabilistic semantics thus replaces the valuations\n\\(v:\\mathcal{L}\\to\\{0,1\\}\\) of classical propositional logic with\nprobability functions \\(P:\\mathcal{L}\\to \\mathbb{R}\\), which take\nvalues in the real unit interval \\([0,1]\\). The classical truth values\nof true (1) and false (0) can thus be regarded as\nthe endpoints of the unit interval \\([0,1]\\), and likewise, valuations\n\\(v:\\mathcal{L}\\to\\{0,1\\}\\) can be regarded as degenerate probability\nfunctions \\(P:\\mathcal{L}\\to[0,1]\\). In this sense, classical logic is\na special case of probability logic, or equivalently, probability\nlogic is an extension of classical logic. \nIt can be shown that classical propositional logic is (strongly) sound\nand complete with respect to probabilistic semantics: \nSome authors interpret probabilities as generalized truth values\n(Reichenbach 1949, Leblanc 1983). According to this view, probability\nlogic is just a particular kind of many-valued logic, and\nprobabilistic validity boils down to ‘truth preservation’:\ntruth (i.e. probability 1) carries over from the premises to the\nconclusion. Other logicians, such as Tarski (1936) and Adams (1998,\n15), have noted that probabilities cannot be seen as generalized truth\nvalues, because probability functions are not\n‘extensional’; for example, \\(P(\\phi\\wedge\\psi)\\) cannot\nbe expressed as a function of \\(P(\\phi)\\) and \\(P(\\psi)\\). More\ndiscussion on this topic can be found in Hailperin (1984). \nAnother possibility is to interpret a sentence’s probability as\na measure of its (un)certainty. For example, the sentence ‘Jones\nis in Spain at the moment’ can have any degree of certainty,\nranging from 0 (maximal uncertainty) to 1 (maximal certainty). (Note\nthat 0 is actually a kind of certainty, viz. certainty about\nfalsity; however, in this entry we follow Adams’ terminology\n(1998, 31) and interpret 0 as maximal uncertainty.) According to this\ninterpretation, the following theorem follows from the strong\nsoundness and completeness of probabilistic semantics: \nTheorem 1. Consider a deductively valid argument\n\\((\\Gamma,\\phi)\\). If all premises in \\(\\Gamma\\) have probability 1,\nthen the conclusion \\(\\phi\\) also has probability 1. \nThis theorem can be seen as a first, very partial clarification of the\nissue of probability preservation (or uncertainty propagation). It\nsays that if there is no uncertainty whatsoever about the premises,\nthen there cannot be any uncertainty about the conclusion either. In\nthe next two subsections we will consider more interesting cases, when\nthere is non-zero uncertainty about the premises, and ask how it\ncarries over to the conclusion. \nFinally, it should be noted that although this subsection only\ndiscussed probabilistic semantics for classical propositional logic,\nthere are also probabilistic semantics for a variety of other logics,\nsuch as intuitionistic propositional logic (van Fraassen 1981b, Morgan\nand Leblanc 1983), modal logics (Morgan 1982a, 1982b, 1983, Cross\n1993), classical first-order logic (Leblanc 1979, 1984, van Fraassen\n1981b), relevant logic (van Fraassen 1983) and nonmonotonic logic\n(Pearl 1991). All of these systems share a key feature: the\nlogic’s semantics is probabilistic in nature, but probabilities\nare not explicitly represented in the object language; hence,\nthey are much closer in nature to the propositional probability logics\ndiscussed here than to the systems presented in later sections. \nMost of these systems are not based on unary probabilities\n\\(P(\\phi)\\), but rather on conditional probabilities \\(P(\\phi,\\psi)\\).\nThe conditional probability \\(P(\\phi,\\psi)\\) is taken as primitive\n(rather than being defined as \\(P(\\phi\\wedge\\psi)/P(\\psi)\\), as is\nusually done) to avoid problems when \\(P(\\psi)=0\\). Goosens (1979)\nprovides an overview of various axiomatizations of probability theory\nin terms of such primitive notions of conditional probability. \nIn the previous subsection we discussed a first principle of\nprobability preservation, which says that if all premises have\nprobability 1, then the conclusion also has probability 1. Of course,\nmore interesting cases arise when the premises are less than\nabsolutely certain. Consider the valid argument with premises \\(p\\vee\nq\\) and \\(p\\to q\\), and conclusion \\(q\\) (the symbol\n‘\\(\\to\\)’ denotes the truth-conditional material\nconditional). One can easily show that  \nIn other words, if we know the probabilities of the argument’s\npremises, then we can calculate the exact probability of its\nconclusion, and thus provide a complete answer to the question of\nprobability preservation for this particular argument (for example, if\n\\(P(p \\vee q) = 6/7\\) and \\(P(p\\to q) = 5/7\\), then \\(P(q) = 4/7\\)).\nIn general, however, it will not be possible to calculate the\nexact probability of the conclusion, given the probabilities\nof the premises; rather, the best we can hope for is a (tight) upper\nand/or lower bound for the conclusion’s probability. We\nwill now discuss Adams’ (1998) methods to compute such\nbounds. \nAdams’ results can be stated more easily in terms of\nuncertainty rather than certainty (probability).\nGiven a probability function \\(P:\\mathcal{L}\\to [0,1]\\), the\ncorresponding uncertainty function \\(U_P\\) is defined as  \nIf the probability function \\(P\\) is clear from the context, we will\noften simply write \\(U\\) instead of \\(U_P\\). In the remainder of this\nsubsection (and in the next one as well) we will assume that all\narguments have only finitely many premises (which is not a significant\nrestriction, given the compactness property of classical propositional\nlogic). Adams’ first main result, which was originally\nestablished by Suppes (1966), can now be stated as follows: \nTheorem 2. Consider a valid argument\n\\((\\Gamma,\\phi)\\) and a probability function \\(P\\). Then the\nuncertainty of the conclusion \\(\\phi\\) cannot exceed the sum of the\nuncertainties of the premises \\(\\gamma\\in\\Gamma\\). Formally:  \nFirst of all, note that this theorem subsumes Theorem 1 as a special\ncase: if \\(P(\\gamma) = 1\\) for all \\(\\gamma\\in\\Gamma\\), then\n\\(U(\\gamma)=0\\) for all \\(\\gamma\\in\\Gamma\\), so \\(U(\\phi)\\leq \\sum\nU(\\gamma) = 0\\) and thus \\(P(\\phi) = 1\\). Furthermore, note that the\nupper bound on the uncertainty of the conclusion depends on\n\\(|\\Gamma|\\), i.e. on the number of premises. If a valid argument\nhas a small number of premises, each of which only has a small\nuncertainty (i.e. a high certainty), then its conclusion will\nalso have a reasonably small uncertainty (i.e. a reasonably high\ncertainty). Conversely, if a valid argument has premises with small\nuncertainties, then its conclusion can only be highly uncertain if the\nargument has a large number of premises (a famous illustration of this\nconverse principle is Kyburg’s (1965) lottery paradox,\nwhich is discussed in the entry on\n epistemic paradoxes\n of this encyclopedia). To put the matter more concretely, note that\nif a valid argument has three premises which each have uncertainty\n1/11, then adding a premise which also has uncertainty 1/11 will not\ninfluence the argument’s validity, but it will raise\nthe upper bound on the conclusion’s uncertainty from 3/11 to\n4/11—thus allowing the conclusion to be more uncertain than was\noriginally the case. Finally, the upper bound provided by Theorem 2 is\noptimal, in the sense that (under the right conditions) the\nuncertainty of the conclusion can coincide with its upper bound \\(\\sum\nU(\\gamma)\\): \nTheorem 3. Consider a valid argument\n\\((\\Gamma,\\phi)\\), and assume that the premise set \\(\\Gamma\\) is\nconsistent, and that every premise \\(\\gamma\\in\\Gamma\\) is relevant\n(i.e. \\(\\Gamma-\\{\\gamma\\}\\not\\models\\phi\\)). Then there exists a\nprobability function \\(P:\\mathcal{L}\\to[0,1]\\) such that \nThe upper bound provided by Theorem 2 can also be used to define a\nprobabilistic notion of validity. An argument \\((\\Gamma,\\phi)\\) is\nsaid to be Adams-probabilistically valid, written\n\\(\\Gamma\\models_a\\phi\\), if and only if \nfor all probability functions \\(P:\\mathcal{L}\\to\\mathbb{R}\\):\n\\(U_P(\\phi)\\leq \\sum_{\\gamma\\in\\Gamma}U_P(\\gamma)\\). \nAdams-probabilistic validity has an alternative, equivalent\ncharacterization in terms of probabilities rather than uncertainties.\nThis characterization says that \\((\\Gamma,\\phi)\\) is\nAdams-probabilistically valid if and only if the conclusion’s\nprobability can get arbitrarily close to 1 if the premises’\nprobabilities are sufficiently high. Formally: \\(\\Gamma\\models_a\\phi\\)\nif and only if \nfor all \\(\\epsilon>0\\) there exists a \\(\\delta>0\\) such that for\nall probability functions \\(P\\):\n\nif \\(P(\\gamma)>1-\\delta\\) for all \\(\\gamma\\in\\Gamma\\), then\n\\(P(\\phi)> 1-\\epsilon\\). \nIt can be shown that classical propositional logic is (strongly) sound\nand complete with respect to Adams’ probabilistic semantics:\n \nAdams (1998, 154) also defines another logic for which his\nprobabilistic semantics is sound and complete. However, this system\ninvolves a non-truth-functional connective (the probability\nconditional), and therefore falls outside the scope of this\nsection. (For more on probabilistic interpretations of conditionals,\nthe reader can consult the entries on\n conditionals\n and\n the logic of conditionals\n of this encyclopedia.) \nConsider the following example. The argument \\(A\\) with premises\n\\(p,q,r,s\\) and conclusion \\(p\\wedge(q\\vee r)\\) is valid. Assume that\n\\(P(p) = 10/11, P(q) = P(r) = 9/11\\) and \\(P(s) = 7/11\\). Then Theorem\n2 says that  \nThis upper bound on the uncertainty of the conclusion is rather\ndisappointing, and it exposes the main weakness of Theorem 2. One of\nthe reasons why the upper bound is so high, is that to compute it we\ntook into account the premise \\(s\\), which has a rather high\nuncertainty (\\(4/11\\)). However, this premise is irrelevant, in the\nsense that the conclusion already follows from the other three\npremises. Hence we can regard \\(p\\wedge (q\\vee r)\\) not only as the\nconclusion of the valid argument \\(A\\), but also as the conclusion of\nthe (equally valid) argument \\(A'\\), which has premises \\(p,q,r\\). In\nthe latter case Theorem 2 yields an upper bound of \\(1/11 + 2/11 +\n2/11 = 5/11\\), which is already much lower. \nThe weakness of Theorem 2 is thus that it takes into account (the\nuncertainty of) irrelevant or inessential premises. To obtain\nan improved version of this theorem, a more fine-grained notion of\n‘essentialness’ is necessary. In argument \\(A\\) in the\nexample above, premise \\(s\\) is absolutely irrelevant. Similarly,\npremise \\(p\\) is absolutely relevant, in the sense that without this\npremise, the conclusion \\(p\\wedge(q\\vee r)\\) is no longer derivable.\nFinally, the premise subset \\(\\{q,r\\}\\) is ‘in between’:\ntogether \\(q\\) and \\(r\\) are relevant (if both premises are left out,\nthe conclusion is no longer derivable), but each of them separately\ncan be left out (while keeping the conclusion derivable). \nThe notion of essentialness is formalized as follows: \nEssential premise set. Given a valid argument\n\\((\\Gamma,\\phi)\\), a set \\(\\Gamma' \\subseteq \\Gamma\\) is\nessential iff \\(\\Gamma - \\Gamma' \\not\\models\\phi\\). \nDegree of essentialness. Given a valid argument\n\\((\\Gamma,\\phi)\\) and a premise \\(\\gamma\\in\\Gamma\\), the degree of\nessentialness of \\(\\gamma\\), written \\(E(\\gamma)\\), is\n\\(1/|S_\\gamma|\\), where \\(|S_\\gamma|\\) is the cardinality of the\nsmallest essential premise set that contains \\(\\gamma\\). If \\(\\gamma\\)\ndoes not belong to any minimal essential premise set, then the degree\nof essentialness of \\(\\gamma\\) is 0. \nWith these definitions, a refined version of Theorem 2 can be\nestablished: \nTheorem 4. Consider a valid argument\n\\((\\Gamma,\\phi)\\). Then the uncertainty of the conclusion \\(\\phi\\)\ncannot exceed the weighted sum of the uncertainties of the premises\n\\(\\gamma\\in\\Gamma\\), with the degrees of essentialness as weights.\nFormally:  \nThe proof of Theorem 4 is significantly more difficult than that of\nTheorem 2: Theorem 2 requires only basic probability theory, whereas\nTheorem 4 is proved using methods from linear programming (Adams and\nLevine 1975; Goldman and Tucker 1956). Theorem 4 subsumes Theorem 2 as\na special case: if all premises are relevant (i.e. have degree of\nessentialness 1), then Theorem 4 yields the same upper bound as\nTheorem 2. Furthermore, Theorem 4 does not take into account\nirrelevant premises (i.e. premises with degree of essentialness\n0) to compute this upper bound; hence if a premise is irrelevant for\nthe validity of the argument, then its uncertainty will not carry over\nto the conclusion. Finally, note that since \\(E(\\gamma)\\in [0,1]\\) for\nall \\(\\gamma\\in\\Gamma\\), it holds that  \ni.e. Theorem 4 yields in general a tighter upper bound than\nTheorem 2. To illustrate this, consider again the argument with\npremises \\(p,q,r,s\\) and conclusion \\(p \\wedge (q\\vee r)\\). Recall\nthat \\(P(p)=10/11, P(q) = P(r)=9/11\\) and \\(P(s)=7/11\\). One can\ncalculate the degrees of essentialness of the premises: \\(E(p) = 1,\nE(q) = E(r) = 1/2\\) and \\(E(s) = 0\\). Hence Theorem 4 yields that  \nwhich is a tighter upper bound for the uncertainty of \\(p\\wedge(q \\vee\nr)\\) than any of the bounds obtained above via Theorem 2\n(viz. \\(9/11\\) and \\(5/11\\)). \nGiven the uncertainties (and degrees of essentialness) of the premises\nof a valid argument, Adams’ theorems allow us to compute an\nupper bound for the uncertainty of the conclusion. Of course\nthese results can also be expressed in terms of probabilities rather\nthan uncertainties; they then yield a lower bound for the\nprobability of the conclusion. For example, when expressed in terms of\nprobabilities rather than uncertainties, Theorem 4 looks as\nfollows: \nAdams’ results are restricted in at least two ways: \nThey only provide a lower bound for the probability of the\nconclusion (given the probabilities of the premises). In a sense this\nis the most important bound: it represents the conclusion’s\nprobability in the ‘worst-case scenario’, which might be\nuseful information in practical applications. However, in some\napplications it might also be informative to have an upper\nbound for the conclusion’s probability. For example, if one\nknows that this probability has an upper bound of 0.4, then one might\ndecide to refrain from certain actions (that one would have performed\nif this upper bound were (known to be) 0.9). \nThey presuppose that the premises’ exact probabilities are\nknown. In practical applications, however, there might only be partial\ninformation about the probability of a premise \\(\\gamma\\): its exact\nvalue is not known, but it is known to have a lower bound\n\\(a\\) and an upper bound \\(b\\) (Walley 1991). In such applications it\nwould be useful to have a method to calculate (optimal) lower and\nupper bounds for the probability of the conclusion in terms of the\nupper and lower bounds of the probabilities of the premises. \nHailperin (1965, 1984, 1986, 1996) and Nilsson (1986) use methods from\nlinear programming to show that these two restrictions can be\novercome. Their most important result is the following: \nTheorem 5. Consider an argument \\((\\Gamma,\\phi)\\),\nwith \\(|\\Gamma| = n\\). There exist functions \\(L_{\\Gamma,\\phi}:\n\\mathbb{R}^{2n} \\to \\mathbb{R}\\) and \\(U_{\\Gamma,\\phi}:\n\\mathbb{R}^{2n} \\to \\mathbb{R}\\) such that for any probability\nfunction \\(P\\), the following holds: if \\(a_i \\leq P(\\gamma_i) \\leq\nb_i\\) for \\(1\\leq i\\leq n\\), then: \n\\(L_{\\Gamma,\\phi}(a_1,\\dots,a_n,b_1,\\dots,b_n) \\leq P(\\phi) \\:\\leq\\)\n\\(U_{\\Gamma,\\phi}(a_1,\\dots,a_n,b_1,\\dots,b_n)\\). \nThe bounds in item 1 are optimal, in the sense that there exist\nprobability functions \\(P_L\\) and \\(P_U\\) such that \\(a_i \\leq\nP_L(\\gamma_i),\\) \\(P_U(\\gamma_i)\\leq b_i\\) for \\(1\\leq i\\leq n\\), and\n\\(L_{\\Gamma,\\phi}(a_1,\\dots,a_n,b_1,\\dots,b_n) = P_L(\\phi)\\) and\n\\(P_U(\\phi) = U_{\\Gamma,\\phi}(a_1,\\dots,a_n,b_1,\\dots,b_n)\\). \nThe functions \\(L_{\\Gamma,\\phi}\\) and \\(U_{\\Gamma,\\phi}\\) are\neffectively determinable from the Boolean structure of the sentences\nin \\(\\Gamma \\cup \\{\\phi\\}\\). \nThis result can also be used to define yet another probabilistic\nnotion of validity, which we will call Hailperin-probabilistic\nvalidity or simply h-validity. This notion is not\ndefined with respect to formulas, but rather with respect to\npairs consisting of a formula and a subinterval of \\([0,1]\\).\nIf \\(X_i\\) is the interval associated with premise \\(\\gamma_i\\in\n\\Gamma\\) and \\(Y\\) is the interval associated with the conclusion\n\\(\\phi\\), then the argument \\((\\Gamma,\\phi)\\) is said to be\nh-valid, written \\(\\Gamma\\models_h\\phi\\), if and only if \nfor all probability functions \\(P\\): \nIn Haenni et al. (2011) this is written as  \nand called the standard probabilistic semantics. \nNilsson’s work on probabilistic logic (1986, 1993) has sparked a\nlot of research on probabilistic reasoning in artificial intelligence\n(Hansen and Jaumard 2000; chapter 2 of Haenni et al. 2011).\nHowever, it should be noted that although Theorem 5 states that the\nfunctions \\(L_{\\Gamma,\\phi}\\) and \\(U_{\\Gamma,\\phi}\\) are\neffectively determinable from the sentences in\n\\(\\Gamma\\cup\\{\\phi\\}\\), the computational complexity of this\nproblem is quite high (Georgakopoulos et al. 1988, Kavvadias and\nPapadimitriou 1990), and thus finding these functions quickly becomes\ncomputationally unfeasible in real-world applications. Contemporary\napproaches based on probabilistic argumentation systems and\nprobabilistic networks are better capable of handling these\ncomputational challenges. Furthermore, probabilistic argumentation\nsystems are closely related to Dempster-Shafer theory (Dempster 1968;\nShafer 1976; Haenni and Lehmann 2003). However, an extended discussion\nof these approaches is beyond the scope of (the current version of)\nthis entry; see (Haenni et al. 2011) for a recent survey. \nIn this section we will study probability logics that extend the\npropositional language \\(\\mathcal{L}\\) with rather basic probability\noperators. They differ from the logics in \n Section 2 \n in that\nthe logics here involve probability operators in the object language.\n Section 3.1\n discusses qualitative probability\noperators; \n Section 3.2\n discusses quantitative probability operators. \nThere are several applications in which qualitative theories of\nprobability might be useful, or even necessary. In some situations\nthere are no frequencies available to use as estimates for the\nprobabilities, or it might be practically impossible to obtain those\nfrequencies. Furthermore, people are often willing to compare\nthe probabilities of two statements (‘\\(\\phi\\) is more probable\nthan \\(\\psi\\)’), without being able to assign explicit\nprobabilities to each of the statements individually\n(Szolovits and Pauker 1978, Halpern and Rabin 1987). In such\nsituations qualitative probability logics will be useful. \nOne of the earliest qualitative probability logics is Hamblin’s\n(1959). The language is extended with a unary operator \\(\\Box\\), which\nis to be read as ‘probably’. Hence a formula such as\n\\(\\Box\\phi\\) is to be read as ‘probably \\(\\phi\\)’. This\nnotion of ‘probable’ can be formalized as sufficiently\nhigh (numerical) probability (i.e. \\(P(\\phi)\\geq t\\), for\nsome threshold value \\(1/2 < t \\leq 1\\)), or alternatively in terms\nof plausibility, which is a non-metrical generalization of\nprobability. Burgess (1969) further develops these systems, focusing\non the ‘high numerical probability’-interpretation. Both\nHamblin and Burgess introduce additional operators into their systems\n(expressing, for example, metaphysical necessity and/or knowledge),\nand study the interaction between the ‘probably’-operator\nand these other modal operators. However, the\n‘probably’-operator already displays some interesting\nfeatures on its own (independent from any other operators). If it is\ninterpreted as ‘sufficiently high probability’, then it\nfails to satisfy the principle \\((\\Box\\phi\\wedge\\Box\\psi) \\to\n\\Box(\\phi\\wedge\\psi)\\). This means that it is not a normal\nmodal operator, and cannot be given a Kripke (relational) semantics.\nHerzig and Longin (2003) and Arló Costa (2005) provide weaker\nsystems of neighborhood semantics for such\n‘probably’-operators, while Yalcin (2010) discusses their\nbehavior from a more linguistically oriented perspective. \nAnother route is taken by Segerberg (1971) and Gärdenfors (1975a,\n1975b), who build on earlier work by de Finetti (1937), Kraft, Pratt\nand Seidenberg (1959) and Scott (1964). They introduce a\nbinary operator \\(\\geq\\); the formula \\(\\phi\\geq\\psi\\) is to\nbe read as ‘\\(\\phi\\) is at least as probable as \\(\\psi\\)’\n(formally: \\(P(\\phi)\\geq P(\\psi)\\)). The key idea is that one can\ncompletely axiomatize the behavior of \\(\\geq\\) without having to use\nthe ‘underlying’ probabilities of the individual formulas.\nIt should be noted that with comparative probability (a binary\noperator), one can also express some absolute probabilistic properties\n(unary operators). For example, \\(\\phi\\geq \\top\\) expresses that\n\\(\\phi\\) has probability 1, and \\(\\phi\\geq\\neg\\phi\\) expresses that\n\\(\\phi\\) has probability at least 1/2. In recent work, Delgrande and\nRenne (2015) further extend the qualitative approach, by allowing the\narguments of \\(\\geq\\) to be finite sequences of formulas (of\npotentially different lengths). The formula \\((\\phi_1,\\dots,\\phi_n)\n\\geq (\\psi_1,\\dots,\\psi_m)\\) is informally to be read as ‘the\nsum of the probabilities of the \\(\\phi_i\\)’s is at least as high\nas the sum of the probabilities of the \\(\\psi_j\\)’s’. The\nresulting logic can be axiomatized completely, and is so expressive\nthat it can even capture quantitative probabilistic logics,\nto which we turn now. \nPropositional probability logics are extensions of propositional logic\nthat express numerical relationships among probability terms\n\\(P(\\varphi)\\). A simple propositional probability logic adds to\npropositional logic formulas of the form \\(P(\\varphi)\\ge q\\), where\n\\(\\varphi\\) is a propositional formula and \\(q\\) is a number; such a\nformula asserts that the probability of \\(\\varphi\\) is at least \\(q\\).\nThe semantics is formalized using models consisting a probability\nfunction \\(\\mathcal{P}\\) over a set \\(\\Omega\\), whose elements are\neach given a truth assignment to the atomic propositions of the\npropositional logic. Thus a propositional formula is true at an\nelement of \\(\\Omega\\) if the truth assignment for that element makes\nthe propositional formula true. The formula \\(P(\\varphi)\\ge q\\) is\ntrue in the model if and only if the probability \\(\\mathcal{P}\\) of\nthe set of elements of \\(\\Omega\\) for which \\(\\varphi\\) is true is at\nleast \\(q\\). See Chapter 3 of Ognjanović et al. (2016) for\nan overview of such a propositional probability logic. \nSome propositional probability logics include other types of formulas\nin the object language, such as those involving sums and products of\nprobability terms. The appeal of involving sums can be clarified by\nthe additivity condition of probability functions (see \n Section 2.1),\n which can be expressed as \\(P(\\phi \\vee \\psi) =\nP(\\phi)+P(\\psi)\\) whenever \\(\\neg (\\phi \\wedge \\psi)\\) is a tautology,\nor equivalently as \\(P(\\phi \\wedge \\psi) + P(\\phi \\wedge \\neg \\psi) =\nP(\\phi)\\). Probability logics that explicitly involve sums of\nprobabilities tend to more generally include linear combinations of\nprobability terms, such as in Fagin et al. (1990). Here,\npropositional logic is extended with formulas of the form\n\\(a_1P(\\phi_1) + \\cdots + a_n P(\\phi_n) \\ge b\\), where \\(n\\) is a\npositive integer that may differ from formula to formula, and\n\\(a_1,\\ldots,a_n\\), and \\(b\\) are all rational numbers. Here are some\nexamples of what can be expressed. \n\\(P(\\phi) \\le q\\) by \\(-P(\\phi) \\ge -q\\), \n\\(P(\\phi) < q\\) by \\(\\neg (P(\\phi) \\ge q)\\), \n\\(P(\\phi) = q\\) by \\(P(\\phi)\\ge q \\wedge P(\\phi) \\le q\\). \n\\(P(\\phi) \\ge P(\\psi)\\) by \\(P(\\phi)-P(\\psi) \\ge 0\\). \nExpressive power with and without linear\ncombinations: \nAlthough linear combinations provide a convenient way of expressing\nnumerous relationships among probability terms, a language without\nsums of probability terms is still very powerful. Consider the\nlanguage restricted to formulas of the form \\(P(\\phi) \\ge q\\) for some\npropositional formula \\(\\phi\\) and rational \\(q\\). We can define \nwhich is reasonable considering that the probability of the complement\nof a proposition is equal to 1 minus the probability of the\nproposition. The formulas \\(P(\\phi) <q\\) and \\(P(\\phi) = q\\) can be\ndefined without linear combinations as we did above. Using this\nrestricted probability language, we can reason about additivity in a\nless direct way. The formula  \nstates that if the probability of \\(\\phi \\wedge \\psi\\) is \\(a\\) and\nthe probability of \\(\\phi\\wedge \\neg \\psi\\) is \\(b\\), then the\nprobability of the disjunction of the formulas (which is equivalent to\n\\(\\phi\\)) is \\(a+b\\). However, while the use of linear combinations\nallows us to assert that the probabilities of \\(\\varphi\\wedge\\psi\\)\nand \\(\\varphi\\wedge\\neg\\psi\\) are additive by using the formula\n\\(P(\\varphi\\wedge \\psi)+P(\\varphi\\wedge\\neg\\psi) = P(\\varphi)\\), the\nformula without linear combinations above only does so if we choose\nthe correct numbers \\(a\\) and \\(b\\). A formal comparison of the\nexpressiveness of propositional probability logic with linear\ncombinations and without is given in Demey and Sack (2015). While any\ntwo models agree on all formulas with linear combinations if and only\nif they agree on all formulas without (Lemma 4.1 of Demey and Sack\n(2015)), it is not the case that any class of models definable by a\nsingle formula with linear combinations can be defined by a single\nformula without (Lemma 4.2 of Demey and Sack (2015)). In particular,\nthe class of models defined by the formula \\(P(p)- P(q)\\ge 0\\) cannot\nbe defined by any single formula without the power of linear\ncombinations. \nProbabilities belonging to a given subset:\nOgnjanović and Rašković (1999) extend the language of\nprobability logic by means of a new type of operator: \\(Q_F\\).\nIntuitively, the formula \\(Q_F\\phi\\) means that the probability of\n\\(\\phi\\) belongs to \\(F\\), for some given set \\(F \\subseteq [0,1]\\).\nThis \\(Q_F\\)-operator cannot be defined in terms of formulas of the\nform \\(P(\\phi) \\ge a\\). Ognjanović and Rašković\n(1999) provide a sound and complete axiomatization of this type of\nlogical system. The key bridge principles, which connect the\n\\(Q_F\\)-operator to the more standard \\(P\\)-operator, are the axioms\n\\(P(\\phi) = a \\to Q_F\\phi\\) for all \\(a \\in F\\), as well as the\ninfinitary rule that specifies that from \\(P(\\phi) = a \\to \\psi\\) for\nall \\(a \\in F\\), one can infer \\(Q_F\\phi\\to\\psi\\). \nPolynomial weight formulas: Logics with polynomial\nweight formulas (involving both weighted sums and products of\nprobability terms), can allow for formulas of the form\n\\(P(\\phi)P(\\psi)-P(\\phi\\wedge \\psi) = 0\\), that is, the probability of\nboth \\(\\phi\\) and \\(\\psi\\) is equal to the product of the\nprobabilities of \\(\\phi\\) and \\(\\psi\\). This formula captures what it\nmeans for \\(\\phi\\) and \\(\\psi\\) to be statistically\nindependent. Such logics were investigated in Fagin et\nal. (1990), but mostly with first-order logic features included,\nand then again in a simpler context (without quantifiers) in\nPerović et al. (2008). \nCompactness and completeness: Compactness is a\nproperty of a logic where a set of formulas is satisfiable if every\nfinite subset is satisfiable. Propositional probability logics lack\nthe compactness property, as every finite subset of\n\\(\\{P(p)>0\\}\\cup\\{P(p)\\leq a\\,|\\,a>0\\}\\) is satisfiable, but the\nentire set is not. \nWithout compactness, a logic might be weakly complete (every valid\nformula is provable in the axiomatic system), but not strongly\ncomplete (for every set \\(\\Gamma\\) of formulas, every logical\nconsequence of \\(\\Gamma\\) is provable from \\(\\Gamma\\) in the axiomatic\nsystem). In Fagin et al. (1990), a proof system involving linear\ncombinations was given and the logic was shown to be both sound and\nweakly complete. In Ognjanović and Rašković (1999), a\nsound and strongly complete proof system is given for propositional\nprobability logic without linear combinations. In Heifetz and Mongin\n(2001), a proof system for a variation of the logic without linear\ncombinations that uses a system of types to allow for iteration of\nprobability formulas (we will see in \n Section 4\n how such\niteration can be achieved using possible worlds) was given and the\nlogic was shown to be sound and weakly complete. They also observe\nthat no finitary proof system for such a logic can be strongly\ncomplete. Ognjanović et al. (2008) present some qualitative\nprobabilistic logics with infinitary derivation rules (which require a\ncountably infinite number of premises), and prove strong completeness.\nGoldblatt (2010) presents a strongly complete proof system for a\nrelated coalgebraic logic. Perović et al. (2008) give a\nproof system and proof of strong completeness for propositional\nprobability logic with polynomial weight formulas. Finally, another\nstrategy for obtaining strong completeness involves restricting the\nrange of the probability functions to a fixed, finite set of numbers;\nfor example, Ognjanović et al. (2008) discuss a qualitative\nprobabilistic logic in which the range of the probability functions is\nnot the full real unit interval \\([0,1]\\), but rather the\n‘discretized’ version\n\\(\\{0,\\frac{1}{n},\\frac{2}{n},\\dots,\\frac{n-1}{n},1\\}\\) (for some\nfixed number \\(n\\in\\mathbb{N}\\)). See Chapter 7 of Ognjanović et\nal. (2016) for an overview of completeness results. \nMany probability logics are interpreted over a single, but arbitrary\nprobability space. Modal probability logic makes use of many\nprobability spaces, each associated with a possible world or state.\nThis can be viewed as a minor adjustment to the relational semantics\nof modal logic: rather than associate to every possible world a set of\naccessible worlds as is done in modal logic, modal probability logic\nassociates to every possible world a probability distribution, a\nprobability space, or a set of probability distributions. The language\nof modal probability logic allows for embedding of probabilities\nwithin probabilities, that is, it can for example reason about the\nprobability that (possibly a different) probability is \\(1/2\\). This\nmodal setting involving multiple probabilities has generally been\ngiven a (1) stochastic interpretation, concerning different\nprobabilities over the next states a system might transition into\n(Larsen and Skou 1991), and (2) a subjective interpretation,\nconcerning different probabilities that different agents may have\nabout a situation or each other’s probabilities (Fagin and\nHalpern 1988). Both interpretations can use exactly the same formal\nframework. \nA basic modal probability logic adds to propositional logic formulas\nof the form \\(P (\\phi)\\ge q\\), where \\(q\\) is typically a rational\nnumber, and \\(\\phi\\) is any formula of the language, possibly a\nprobability formula. The reading of such a formula is that the\nprobability of \\(\\phi\\) is at least \\(q\\). This general reading of the\nformula does not reflect any difference between modal probability\nlogic and other probability logics with the same formula; where the\ndifference lies is in the ability to embed probabilities in the\narguments of probability terms and in the semantics. The following\nsubsections provide an overview of the variations of how modal\nprobability logic is modeled. In one case the language is altered\nslightly \n (Section 4.2), and in other cases, the logic is\nextended to address interactions between qualitative and quantitative\nuncertainty \n (Section 4.4) or dynamics \n (Section 4.5). \nFormally, a Basic Finite Modal Probabilistic Model is a tuple\n\\(M=(W,\\mathcal{P},V)\\), where \\(W\\) is a finite set of possible\nworlds or states, \\(\\mathcal{P}\\) is a function associating a\ndistribution \\(\\mathcal{P}_w\\) over \\(W\\) to each world \\(w\\in W\\),\nand \\(V\\) is a ‘valuation function’ assigning atomic\npropositions from a set \\(\\Phi\\) to each world. The distribution is\nadditively extended from individual worlds to sets of worlds:\n\\(\\mathcal{P}_w(S) = \\sum_{s\\in S}\\mathcal{P}_w(s)\\). The first two\ncomponents of a basic modal probabilistic model are effectively the\nsame as a Kripke frame whose relation is decorated with numbers\n(probability values). Such a structure has different names, such as a\ndirected graph with labelled edges in mathematics, or a probabilistic\ntransition system in computer science. The valuation function, as in a\nKripke model, allows us to assign properties to the worlds. \nThe semantics for formulas are given on pairs \\((M,w)\\), where \\(M\\)\nis a model and \\(w\\) is an element of the model. A formula \\(P(\\phi)\n\\ge q\\) is true at a pair \\((M,w)\\), written \\((M,w)\\models P(\\phi)\\ge\nq\\), if and only if \\(\\mathcal{P}_w(\\{w'\\mid (M,w')\\models \\phi\\}) \\ge\nq\\). \nThe first generalization, which is most common in applications of\nmodal probabilistic logic, is to allow the distributions to be indexed\nby two sets rather than one. The first set is the set \\(W\\) of worlds\n(the base set of the model), but the other is an index set \\(A\\) often\nto be taken as a set of actions, agents, or players of a game.\nFormally, \\(\\mathcal{P}\\) associates a distribution\n\\(\\mathcal{P}_{a,w}\\) over \\(W\\) for each \\(w\\in W\\) and \\(a\\in A\\).\nFor the language, rather than involving formulas of the form\n\\(P(\\phi)\\ge q\\), we have \\(P_a(\\phi)\\ge q\\), and \\((M,w)\\models\nP_a(\\phi)\\ge q\\) if and only if \\(\\mathcal{P}_{a,w}(\\{w'\\mid\n(M,w')\\models \\phi\\}) \\ge q\\). \nExample: Suppose we have an index set \\(A = \\{a,\nb\\}\\), and a set \\(\\Phi = \\{p,q\\}\\) of atomic propositions. Consider\n\\((W,\\mathcal{P},V)\\), where \n\\(W = \\{w,x,y,z\\}\\) \n\\(\\mathcal{P}_{a,w}\\) and \\(\\mathcal{P}_{a,x}\\) map \\(w\\) to \\(1/2\\),\n\\(x\\) to \\(1/2\\), \\(y\\) to \\(0\\), and \\(z\\) to \\(0\\). \n\\(\\mathcal{P}_{a,y}\\) and \\(\\mathcal{P}_{a,z}\\) map \\(y\\) to \\(1/3\\),\n\\(z\\) to \\(2/3\\), \\(w\\) to \\(0\\), and \\(x\\) to \\(0\\). \n\\(\\mathcal{P}_{b,w}\\) and \\(\\mathcal{P}_{b,y}\\) map \\(w\\) to \\(1/2\\),\n\\(y\\) to \\(1/2\\), \\(x\\) to \\(0\\), and \\(z\\) to \\(0\\). \n\\(\\mathcal{P}_{b,x}\\) and \\(\\mathcal{P}_{b,z}\\) map \\(x\\) to \\(1/4\\),\n\\(z\\) to \\(3/4\\), \\(w\\) to \\(0\\), and \\(y\\) to \\(0\\). \n\\(V(p) = \\{w,x\\}\\) \n\\(V(q) = \\{w,y\\}\\). \nWe depict this example with the following diagram. Inside each circle\nis a labeling of the truth of each proposition letter for the world\nwhose name is labelled right outside the circle. The arrows indicate\nthe probabilities. For example, an arrow from world \\(x\\) to world\n\\(z\\) labeled by \\((b,3/4)\\) indicates that from \\(x\\), the probably\nof \\(z\\) under label \\(b\\) is \\(3/4\\). Probabilities of 0 are not\nlabelled. \nFigure \nStochastic Interpretation: Consider the elements\n\\(a\\) and \\(b\\) of \\(A\\) to be actions, for example, pressing buttons\non a machine. In this case, pressing a button does not have a certain\noutcome. For instance, if the machine is in state \\(x\\), there is a\n\\(1/2\\) probability it will remain in the same state after pressing\n\\(a\\), but a \\(1/4\\) probability of remaining in the same state after\npressing \\(b\\). That is,  \nA significant feature of modal logics in general (and this includes\nmodal probabilistic logic) is the ability to support higher-order\nreasoning, that is, the reasoning about probabilities of\nprobabilities. The importance of higher-order probabilities is clear\nfrom the role they play in, for example, Miller’s\nprinciple, which states that \\(P_1(\\phi\\mid P_2(\\phi) = b) = b\\).\nHere, \\(P_1\\) and \\(P_2\\) are probability functions, which can have\nvarious interpretations, such as the probabilities of two agents,\nlogical and statistical probability, or the probabilities of one agent\nat different moments in time (Miller 1966; Lewis 1980; van Fraassen\n1984; Halpern 1991). Higher-order probability also occurs for instance\nin the Judy Benjamin Problem (van Fraassen 1981a) where one\nconditionalizes on probabilistic information. Whether one agrees with\nthe principles proposed in the literature on higher-order\nprobabilities or not, the ability to represent them forces one to\ninvestigate the principles governing them. \nTo illustrate higher-order reasoning more concretely, we return to our\nexample and see that at \\(x\\), there is a \\(1/2\\) probability that\nafter pressing \\(a\\), there is a \\(1/2\\) probability that after\npressing \\(b\\), it will be the case that \\(\\neg p\\) is true, that is,\n \nSubjective Interpretation: Suppose the elements \\(a\\)\nand \\(b\\) of \\(A\\) are players of a game. \\(p\\) and \\(\\neg p\\) are\nstrategies for player \\(a\\) and \\(q\\) and \\(\\neg q\\) are both\nstrategies for player \\(b\\). In the model, each player is certain of\nher own strategy; for instance at \\(x\\), player \\(a\\) is certain that\nshe will play \\(p\\) and player \\(b\\) is certain that she will play\n\\(\\neg q\\), that is  \nBut the players randomize over their opponents. For instance at \\(x\\),\nthe probability that \\(b\\) has for \\(a\\)’s probability of \\(\\neg\nq\\) being \\(1/2\\) is \\(1/4\\), that is  \nProbabilities are generally defined as measures in a measure space. A\nmeasure space is a set \\(\\Omega\\) (the sample space) together with a\n\\(\\sigma\\)-algebra (also called \\(\\sigma\\)-field) \\(\\mathcal{A}\\) over\n\\(\\Omega\\), which is a non-empty set of subsets of \\(\\Omega\\) such\nthat \\(A\\in \\mathcal{A}\\) implies that \\(\\Omega-A\\in \\mathcal{A}\\),\nand \\(A_i\\in \\mathcal{A}\\) for all natural numbers \\(i\\), implies that\n\\(\\bigcup_i A_i\\in \\mathcal{A}\\). A measure is a function \\(\\mu\\)\ndefined on the \\(\\sigma\\)-algebra \\(\\mathcal{A}\\), such that \\(\\mu(A)\n\\ge 0\\) for every set \\(A \\in\\mathcal{A}\\) and \\(\\mu(\\bigcup_i A_i) =\n\\sum_i\\mu(A_i)\\) whenever \\(A_i\\cap A_j = \\emptyset\\) for each\n\\(i,j\\). \nThe effect of the \\(\\sigma\\)-algebra is to restrict the domain so that\nnot every subset of \\(\\Omega\\) need have a probability. This is\ncrucial for some probabilities to be defined on uncountably infinite\nsets; for example, a uniform distribution over a unit interval cannot\nbe defined on all subsets of the interval while also maintaining the\ncountable additivity condition for probability measures. \nThe same basic language as was used for the basic finite probability\nlogic need not change, but the semantics is slightly different: for\nevery state \\(w\\in W\\), the component \\(\\mathcal{P}_w\\) of a modal\nprobabilistic model is replaced by an entire probability space\n\\((\\Omega_w,\\mathcal{A}_w,\\mu_w)\\), such that \\(\\Omega_w\\subseteq W\\)\nand \\(\\mathcal{A}_w\\) is a \\(\\sigma\\)-algebra over \\(\\Omega_w\\). The\nreason we may want entire spaces to differ from one world to another\nis to reflect uncertainty about what probability space is the right\none. For the semantics of probability formulas, \\((M,w)\\models P(\\phi)\n\\ge q\\) if and only if \\(\\mu_w(\\{w'\\mid (M,w')\\models \\phi\\})\\ge q\\).\nSuch a definition is not well defined in the event that \\(\\{w'\\mid\n(M,w')\\models \\phi\\}\\not\\in \\mathcal{A}_w\\). Thus constraints are\noften placed on the models to ensure that such sets are always in the\n\\(\\sigma\\)-algebras. \nAlthough probabilities reflect quantitative uncertainty at one level,\nthere can also be qualitative uncertainty about probabilities. We\nmight want to have qualitative and quantitative uncertainty because we\nmay be so uncertain about some situations that we do not want to\nassign numbers to the probabilities of their events, while there are\nother situations where we do have a sense of the probabilities of\ntheir events; and these situations can interact. \nThere are many situations in which we might not want to assign\nnumerical values to uncertainties. One example is where a computer\nselects a bit 0 or 1, and we know nothing about how this bit is\nselected. Results of coin flips, on the other hand, are often used\nexamples of where we would assign probabilities to individual\noutcomes. \nAn example of how these might interact is where the result of the bit\ndetermines whether a fair coin or a weighted coin (say, heads with\nprobability \\(2/3\\)) be used for a coin flip. Thus there is\nqualitative uncertainty as to whether the action of flipping a coin\nyields heads with probability \\(1/2\\) or \\(2/3\\). \nOne way to formalize the interaction between probability and\nqualitative uncertainty is by adding another relation to the model and\na modal operator to the language as is done in Fagin and Halpern\n(1988, 1994). Formally, we add to a basic finite probability model a\nrelation \\(R\\subseteq W^2\\). Then we add to the language a modal\noperator \\(\\Box\\), such that \\((M,w)\\models \\Box\\phi\\) if and only if\n\\((M,w')\\models \\phi\\) whenever \\(w R w'\\). \nConsider the following example: \n\\(W = \\{(0,H),(0,T),(1,H),(1,T)\\}\\), \n\\(\\Phi = \\{h,t\\}\\) is the set of atomic propositions, \n\\(R = W^2\\), \n\\(P\\) associates with \\((0,H)\\) and \\((0,T)\\) the distribution mapping\n\\((0,H)\\) and \\((0,T)\\) each to \\(1/2\\), and associates with \\((1,H)\\)\nand \\((1,T)\\) the distribution mapping \\((1,H)\\) to \\(2/3\\) and\n\\((1,T)\\) to \\(1/3\\), \n\\(V\\) maps \\(h\\) to the set \\(\\{(0,H),(1,H)\\}\\) and \\(t\\) to the set\n\\(\\{(0,T),(1,T)\\}\\). \nThen the following formula is true at \\((0,H)\\): \\(\\neg \\Box h \\wedge\n(\\neg \\Box P(h)= 1/2) \\wedge (\\Diamond P(h) = 1/2)\\). This can be read\nas it is not known that \\(h\\) is true, and it is not known that\nthe probability of \\(h\\) is \\(1/2\\), but it is possible that the\nprobability of \\(h\\) is \\(1/2\\). \nWe have discussed two views of modal probability logic. One is\ntemporal or stochastic, where the probability distribution associated\nwith each state determines the likelihood of transitioning into other\nstates; another is concerned with subjective perspectives of agents,\nwho may reason about probabilities of other agents. A stochastic\nsystem is dynamic in that it represents probabilities of different\ntransitions, and this can be conveyed by the modal probabilistic\nmodels themselves. But from a subjective view, the modal probabilistic\nmodels are static: the probabilities are concerned with what currently\nis the case. Although static in their interpretation, the modal\nprobabilistic setting can be put in a dynamic context. \nDynamics in a modal probabilistic setting is generally concerned with\nsimultaneous changes to probabilities in potentially all possible\nworlds. Intuitively, such a change may be caused by new information\nthat invokes a probabilistic revision at each possible world. The\ndynamics of subjective probabilities is often modeled using\nconditional probabilities, such as in Kooi (2003), Baltag and Smets\n(2008), and van Benthem et al. (2009). The probability of \\(E\\)\nconditional on \\(F\\), written \\(P(E\\mid F)\\), is \\(P(E\\cap F)/P(F)\\).\nWhen updating by a set \\(F\\), a probability distribution \\(P\\) is\nreplaced by the probability distribution \\(P'\\), such that \\(P'(E)=\nP(E \\mid F)\\), so long as \\(P(F)\\neq 0\\). Let us assume for the\nremainder of this dynamics subsection that every relevant set\nconsidered has positive probability. \nUsing a probability logic with linear combinations, we can abbreviate\nthe conditional probability \\(P(\\phi\\mid \\psi)\\ge q\\) by \\(P(\\phi\n\\wedge \\psi) - qP(\\psi)\\ge 0\\). In a modal setting, an operator\n\\([!\\psi]\\) can be added to the language, such that \\(M,w\\models\n[!\\psi]\\phi\\) if and only if \\(M',w\\models \\phi\\), where \\(M'\\) is the\nmodel obtained from \\(M\\) by revising the probabilities of each world\nby \\(\\psi\\). Note that \\([!\\psi](P(\\phi)\\ge q)\\) differs from\n\\(P(\\phi\\mid \\psi)\\ge q\\), in that in \\([!\\psi](P(\\phi)\\ge q)\\), the\ninterpretation of probability terms inside \\(\\phi\\) are affected by\nthe revision by \\(\\psi\\), whereas in \\(P(\\phi\\mid \\psi)\\ge q\\), they\nare not, which is why \\(P(\\phi\\mid \\psi)\\ge q\\) nicely unfolds into\nanother probability formula. However, \\([!\\psi]\\phi\\) does unfold too,\nbut in more steps:  \nFor other overviews of modal probability logics and its dynamics, see\nDemey and Kooi (2014), Demey and Sack (2015), and\n appendix L on probabilistic update in dynamic epistemic logic\n of the entry on dynamic epistemic logic. \nIn this section we will discuss first-order probability logics. As was\nexplained in \n Section 1\n of this entry, there are many ways in\nwhich a logic can have probabilistic features. The models of the logic\ncan have probabilistic aspects, the notion of consequence can have a\nprobabilistic flavor, or the language of the logic can contain\nprobabilistic operators. In this section we will focus on those\nlogical operators that have a first-order flavor. The first-order\nflavor is what distinguishes these operators from the probabilistic\nmodal operators of the previous section. \nConsider the following example from Bacchus (1990): \nMore than 75% of all birds fly. \nThere is a straightforward probabilistic interpretation of this\nsentence, namely when one randomly selects a bird, then the\nprobability that the selected bird flies is more than 3/4. First-order\nprobabilistic operators are needed to express these sort of\nstatements. \nThere is another type of sentence, such as the following sentence\ndiscussed in Halpern (1990): \nThe probability that Tweety flies is greater than \\(0.9\\). \nThis sentence considers the probability that Tweety (a particular\nbird) can fly. These two types of sentences are addressed by two\ndifferent types of semantics, where the former involves probabilities\nover a domain, while the latter involves probabilities over a set of\npossible worlds that is separate from the domain. \nIn this subsection we will have a closer look at a particular\nfirst-order probability logic, whose language is as simple as\npossible, in order to focus on the probabilistic quantifiers. The\nlanguage is very much like the language of classical first-order\nlogic, but rather than the familiar universal and existential\nquantifier, the language contains a probabilistic quantifier. \nThe language is built on a set of of individual variables\n(denoted by \\(x, y, z, x_1, x_2, \\ldots\\)), a set of function\nsymbols (denoted by \\(f, g, h, f_1, \\ldots\\)) where an arity is\nassociated with each symbol (nullary function symbols are also called\nindividual constants), and a set of predicate\nletters (denoted by \\( R, P_1, \\ldots\\)) where an arity is\nassociated with each symbol. The language contains two kinds of\nsyntactical objects, namely terms and formulas. The\nterms are defined inductively as follows: \nEvery individual variable \\(x\\) is a term. \nEvery function symbol \\(f\\) of arity \\(n\\) followed by an \\(n\\)-tuple\nof terms \\((t_1,\\ldots,t_n)\\) is a term. \nGiven this definition of terms, the formulas are defined inductively\nas follows: \nEvery predicate letter \\(R\\) of arity \\(n\\) followed by an \\(n\\)-tuple\nof terms \\((t_1,\\ldots,t_n)\\) is a formula. \nIf \\(\\phi\\) is a formula, then so is \\(\\neg \\phi\\). \nIf \\(\\phi\\) and \\(\\psi\\) are formulas, then so is \\((\\phi \\wedge\n\\psi)\\). \nIf \\(\\phi\\) is a formula and \\(q\\) is a rational number in the\ninterval \\([0,1]\\), then so is \\(Px (\\phi) \\geq q\\). \nFormulas of the form \\(Px (\\phi) \\geq q\\) should be read as:\n“the probability of selecting an \\(x\\) such that \\(x\\) satisfies\n\\(\\phi\\) is at least \\(q\\)”. The formula \\(Px(\\phi) \\leq q\\) is\nan abbreviation of \\(Px(\\neg \\phi) \\geq 1-q\\) and \\(Px(\\phi)=q\\) is an\nabbreviation of \\(Px(\\phi) \\geq q \\wedge Px(\\phi) \\leq q\\). Every free\noccurrence of \\(x\\) in \\(\\phi\\) is bound by the operator. \nThis language is interpreted on very simple first-order models, which\nare triples \\(M=(D,I,P)\\), where the domain of discourse\n\\(D\\) is a finite nonempty set of objects, the interpretation\n\\(I\\) associates an \\(n\\)-ary function on \\(D\\) with every \\(n\\)-ary\nfunction symbol occurring in the language, and an \\(n\\)-ary relation\non \\(D\\) with every \\(n\\)-ary predicate letter. \\(P\\) is a\nprobability function that assigns a probability \\(P(d)\\) to\nevery element \\(d\\) in \\(D\\) such that \\(\\sum_{d \\in D} P(d)=1\\). \nIn order to interpret formulas containing free variables one also\nneeds an assignment \\(g\\) which assigns an element of \\(D\\)\nto every variable. The interpretation \\([\\![t]\\!]_{M,g}\\) of a term\n\\(t\\) given a model \\(M=(D,I,P)\\) and an assignment \\(g\\) is defined\ninductively as follows: \n\\([\\![ x ]\\!]_{M,g}=g(x)\\) \n\\([\\![ f (t_1,\\ldots,t_n)]\\!]_{M,g}= I(f) ([\\![t_1]\\!], \\ldots,\n[\\![t_n]\\!])\\) \nTruth is defined as a relation \\(\\models\\) between models with\nassignments and formulas: \n\\(M,g \\models R(t_1,\\ldots,t_n)\\) iff \\(([\\![t_1]\\!], \\ldots,\n[\\![t_n]\\!]) \\in I(R)\\) \n\\(M,g \\models \\neg \\phi\\) iff \\(M,g \\not \\models \\phi\\) \n\\(M,g \\models (\\phi \\wedge \\psi)\\) iff \\(M,g \\models \\phi\\) and \\(M,g\n\\models \\psi\\) \n\\(M,g \\models Px(\\phi) \\geq q\\) iff \\(\\sum_{d :M,g[x \\mapsto d]\n\\models \\phi} P(d) \\geq q\\) \nAs an example, consider a model of a vase containing nine marbles:\nfive are black and four are white. Let us assume that \\(P\\) assigns a\nprobability of 1/9 to each marble, which captures the idea that one is\nequally likely to pick any marble. Suppose the language contains a\nunary predicate \\(B\\) whose interpretation is the set of black\nmarbles. The sentence \\(Px(B(x)) = 5/9\\) is true in this model\nregardless of the assignment. \nThe logic that we just presented is too simple to capture many forms\nof reasoning about probabilities. We will discuss three extensions\nhere. \nFirst of all one would like to reason about cases where more than one\nobject is selected from the domain. Consider for example the\nprobability of first picking a black marble, putting it back, and then\npicking a white marble from the vase. This probability is 5/9\n\\(\\times\\) 4/9 = 20/81, but we cannot express this in the language\nabove. For this we need one operator that deals with multiple\nvariables simultaneously, written as \\(Px_1,\\ldots x_n (\\phi) \\geq\nq\\). The semantics for such operators will then have to provide a\nprobability measure on subsets of \\(D^n\\). The simplest way to do this\nis by simply taking the product of the probability function \\(P\\) on\n\\(D\\), which can be taken as an extension of \\(P\\) to tuples, where\n\\(P(d_1,\\ldots d_n)= P(d_1) \\times \\cdots \\times P(d_n)\\), which\nyields the following semantics: \n\\(M,g \\models Px_1\\ldots x_n (\\phi) \\geq q\\) iff\n\\(\\sum_{(d_1,\\ldots,d_n) :M,g[x_1 \\mapsto d_1, \\ldots, x_n \\mapsto\nd_n] \\models \\phi} P(d_1,\\ldots,d_n) \\geq q\\) \nThis approach is taken by Bacchus (1990) and Halpern (1990),\ncorresponding to the idea that selections are independent and with\nreplacements. With these semantics the example above can be formalized\nas \\(Px,y (B(x) \\wedge \\neg B(y))= 20/81\\). There are also more\ngeneral approaches to extending the measure on the domain to tuples\nfrom the domain such as by Hoover (1978) and Keisler (1985). \nWhen one considers the initial example that more than 75% of all birds\nfly, one finds that this cannot be adequately captured in a model\nwhere the domain contains objects that are not birds. These objects\nshould not matter to what one wishes to express, but the probability\nquantifiers, quantify over the whole domain. In order to restrict\nquantification one must add conditional probability operators \\(Px\n(\\phi | \\psi) \\geq q\\) with the following semantics: \n\\(M,g \\models Px (\\phi | \\psi) \\geq q\\) iff if there is a \\(d \\in D\\)\nsuch that \\(M,g[x \\mapsto d] \\models \\psi\\) then \nWith these operators, the formula \\(Px(F(x) \\mid B(x)) > 3/4\\)\nexpresses that more than 75% of all birds fly. \nWhen one wants to compare the probability of different events, say of\nselecting a black ball and selecting a white ball, it may be more\nconvenient to consider probabilities to be terms in their own right.\nThat is, an expression \\(Px(\\phi)\\) is interpreted as referring to\nsome rational number. Then one can extend the language with\narithmetical operations such as addition and multiplication, and with\noperators such as equality and inequalities to compare probability\nterms. One can then say that one is twice as likely to select a black\nball compared to a white ball as \\(Px(B(x))=2 \\times Px (W(x))\\). Such\nan extension requires that the language contains two separate classes\nof terms: one for probabilities, numbers and the results of\narithmetical operations on such terms, and one for the domain of\ndiscourse which the probabilistic operators quantify over. We will not\npresent such a language and semantics in detail here. One can find\nsuch a system in Bacchus (1990). \nIn this subsection, we consider a first-order probability logic with a\npossible-world semantics (which we abbreviate FOPL). The language of\nFOPL is similar to the example we gave in\n Section 5.1\n related to that of\nBacchus, except here we have full quantifier formulas of the form\n\\((\\forall x)\\phi\\) for any formula \\(\\phi\\), and instead of\nprobability formulas of the form \\(Px(\\phi)\\ge q\\), we have\nprobability formulas of the form \\(P(\\phi)\\ge q\\) (similar to the\nprobability formulas in propositional probability logic). \nThe models of FOPL are of the form \\(M = (W,D,I,P)\\), where \\(W\\) is a\nset of possible worlds, \\(D\\) is a domain of\ndiscourse, \\(I\\) is a localized interpretation function mapping\nevery \\(w\\in W\\) to a interpretation function \\(I(w)\\) that associates\nto every function and predicate symbol, a function or predicate of\nappropriate arity, and \\(P\\) is a probability function that assign a\nprobability \\(P(w)\\) to every \\(w\\) in \\(W\\). \nSimilarly to the simple example before, we involve an assignment\nfunction \\(g\\) mapping each variable to an element of the domain\n\\(D\\). To interpret terms, for every model \\(M\\), world \\(w\\in W\\),\nand assignment function \\(g\\), we map each term \\(t\\) to domain\nelements as follows: \nTruth is defined according to a relation \\(\\models\\) between pointed\nmodels (models with designated worlds) with assignments and formulas\nas follows: \n\\(M,w,g \\models R(t_1,\\ldots,t_n)\\) iff \\(([\\![t_1]\\!], \\ldots,\n[\\![t_n]\\!]) \\in I(w)(R)\\) \n\\(M,w,g \\models \\neg \\phi\\) iff \\(M,w,g \\not \\models \\phi\\) \n\\(M,w,g \\models (\\phi \\wedge \\psi)\\) iff \\(M,w,g \\models \\phi\\) and\n\\(M,w,g \\models \\psi\\) \n\\(M,w,g\\models (\\forall x)\\varphi\\) iff \\(M,w,g[x/d]\\models \\varphi\\)\nfor all \\(d\\in D\\), where \\(g[x/d]\\) is the same as \\(g\\) except that\nit maps \\(x\\) to \\(d\\). \n\\(M,w,g\\models P(\\varphi)\\ge q\\) iff \\(P(\\{w'\\mid (M,w',g)\\models\n\\varphi\\})\\ge q\\). \nAs an example, consider a model where there are two possible vases: 4\nwhite marbles and 4 black marbles were put in both possible vases. But\nthen another marble, called , was placed in the vase, but in one\npossible vase, was white, and in the other it was black. Thus in the\nend, there are two possible vases: one with 5 black marbles and 4\nwhite marbles, and the other with 4 black marbles and 5 white marbles.\nSuppose \\(P\\) assigns \\(1/2\\) probability to the two possible vases.\nThen \\(P(B(\\mathsf{last})) = 1/2\\) is true for this variable\nassignment, and if any other variable assignment were chosen, the\nformula \\((\\exists x) P(B(x)) = 1/2\\) would still be true. \nGenerally it is hard to provide proof systems for first-order\nprobability logics, because the validity problem for these logics is\ngenerally undecidable. It is even not the case, as it is the case in\nclassical first-order logic, that if an inference is valid, then one\ncan find out in finite time (see Abadi and Halpern (1994)). \nNonetheless there are many results for first-order probability logic.\nFor instance, Hoover (1978) and Keisler (1985) study completeness\nresults. Bacchus (1990) and Halpern (1990) also provide complete\naxiomatizations as well as combinations of first-order probability\nlogics and possible-world first-order probability logics respectively.\nIn Ognjanović and Rašković (2000), an infinitary\ncomplete axiomatization is given for a more general version of the\npossible-world first-order probability logic presented here.","contact.mail":"joshua.sack@gmail.com","contact.domain":"gmail.com"}]
