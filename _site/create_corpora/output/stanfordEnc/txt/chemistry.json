[{"date.published":"2011-03-14","date.changed":"2019-01-16","url":"https://plato.stanford.edu/entries/chemistry/","author1":"Michael Weisberg","author1.info":"http://www.phil.upenn.edu/~weisberg","author2.info":"http://people.su.se/~pneedham/PNEng.html","entry":"chemistry","body.text":"\n\n\nChemistry is the study of the structure and transformation of matter.\nWhen Aristotle wrote the first systematic treatises on chemistry in\nthe 4th century BCE, his conceptual grasp of the nature of\nmatter was tailored to accommodate a relatively simple range of\nobservable phenomena. In the 21st century, chemistry has\nbecome the largest scientific discipline, producing over half a\nmillion publications a year ranging from direct empirical\ninvestigations to substantial theoretical work. However, the\nspecialized interest in the conceptual issues arising in chemistry,\nhereafter Philosophy of Chemistry, is a relatively recent\naddition to philosophy of science.\n\n\nPhilosophy of chemistry has two major parts. In the first, conceptual\nissues arising within chemistry are carefully articulated and\nanalyzed. Such questions which are internal to chemistry include the\nnature of substance, atomism, the chemical bond, and synthesis. In the\nsecond, traditional topics in philosophy of science such as realism,\nreduction, explanation, confirmation, and modeling are taken up within\nthe context of chemistry.\n\nOur contemporary understanding of chemical substances is elemental and\natomic: All substances are composed of atoms of elements such as\nhydrogen and oxygen. These atoms are the building blocks of the\nmicrostructures of compounds and hence are the fundamental units of\nchemical analysis. However, the reality of chemical atoms was\ncontroversial until the beginning of the 20th century and\nthe phrase “fundamental building blocks” has always\nrequired careful interpretation. So even today, the claim that all\nsubstances are composed of elements does not give us sufficient\nguidance about the ontological status of elements and how the elements\nare to be individuated. \nIn this section, we will begin with the issue of elements.\nHistorically, chemists have offered two answers to the question\n“What is it for something to be an element?” \nThese two theses describe elements in different ways. In the first,\nelements are explicitly identified by a procedure. Elements are simply\nthe ingredients in a mixture that can be separated no further. The\nsecond conception is more theoretical, positing elements as\nconstituents of composite bodies. In the pre-modern Aristotelian\nsystem, the end of analysis thesis was the favored option. Aristotle\nbelieved that elements were the building blocks of chemical\nsubstances, only potentially present in these substances. The modern\nconception of elements asserts that they are actual components,\nalthough, as we will see, aspects of the end of analysis thesis\nlinger. This section will explain the conceptual background behind\nchemistry’s progression from one conception to the other. Along the\nway, we will discuss the persistence of elements in chemical\ncombination, the connection between element individuation and\nclassification, and criteria for determining pure substances. \nThe earliest conceptual analyses concerning matter and its\ntransformations come in the Aristotelian tradition. As in modern\nchemistry, the focus of Aristotle’s theories was the nature of\nsubstances and their transformations. He offered the first systematic\ntreatises of chemical theory in On Generation and Corruption\n(De Generatione et Corruptione), Meteorology, and\nparts of Physics and On the Heavens (De\nCaelo). \nAristotle recognized that most ordinary, material things are composed\nof multiple substances, although he thought that some of them could be\ncomposed of a single, pure substance. Thus, he needed to give a\ncriterion of purity that would individuate a single substance. His\ncriterion was that pure substances are homoeomerous: they are\ncomposed of like parts at every level. “[I]f combination has\ntaken place, the compound must be uniform—any part of\nsuch a compound is the same as the whole, just as any part of water is\nwater” (De Generatione et Corruptione,\nhenceforth DG, I.10,\n328a10ff).[1] So\nwhen we encounter diamond in rock, oil in water, or smoke in air,\nAristotelian chemistry tells us that there is more than one substance\npresent. \nLike some of his predecessors, Aristotle held that the elements Fire,\nWater, Air, and Earth were the building blocks of all substances. But\nunlike his predecessors, Aristotle established this list from\nfundamental principles. He argued that “it is impossible for the\nsame thing to be hot and cold, or moist and dry … Fire is hot\nand dry, whereas Air is hot and moist …; and Water is cold and\nmoist, while Earth is cold and dry” (DG II.3,\n330a30–330b5). Aristotle supposed hot and moist to be maximal\ndegrees of heat and humidity, and cold and dry to be minimal degrees.\nNon-elemental substances are characterized by intermediate degrees of\nthe primary qualities of warmth and humidity. \nAristotle used this elemental theory to account for many properties of\nsubstances. For example he distinguished between liquids and solids by\nnoting the different properties imposed by two characteristic\nproperties of elements, moist and dry. “[M]oist is that which,\nbeing readily adaptable in shape, is not determinable by any limit of\nits own; while dry is that which is readily determinable by its own\nlimit, but not readily adaptable in shape” (DG II.2,\n329b30f.). Solid bodies have a shape and volume of their own, liquids\nonly have a volume of their own. He further distinguished liquids from\ngases, which don’t even have their own volume. He reasoned that while\nwater and air are both fluid because they are moist, cold renders\nwater liquid and hot makes air gas. On the other hand, dry together\nwith cold makes earth solid, but together with hot we get fire. \nChemistry focuses on more than just the building blocks of substances:\nIt attempts to account for the transformations that change substances\ninto other kinds of substances. Aristotle also contributed the first\nimportant analyses of this process, distinguishing between\ntransmutation, where one substance overwhelms and eliminates\nanother and proper mixing. The former is closest to what we\nwould now call change of phase and the latter to what we would now\ncall chemical combination. \nAristotle thought that proper mixing could occur when substances of\ncomparable amounts are brought together to yield other substances\ncalled\n ‘compounds.’[2]\n Accordingly, the substances we typically encounter are compounds, and\nall compounds have the feature that there are some ingredients from\nwhich they could be made. \nWhat happens to the original ingredients when they are mixed together\nto form a compound? Like modern chemists, Aristotle argued that the\noriginal ingredients can, at least in principle, be obtained by\nfurther transformations. He presumably knew that salt and water can be\nobtained from sea water and metals can be obtained from alloys. But he\nexplains this with a conceptual argument, not a detailed list of\nobservations. \nAristotle first argues that heterogeneous mixtures can be\ndecomposed: \nHe then goes on to offer an explicit definition of the concept of an\nelement in terms of simple bodies, specifically mentioning recovery in\nanalysis.  \nThe notion of simplicity implicit here is introduced late in\nDG where in book II Aristotle claims that “All the\ncompound bodies … are composed of all the simple bodies”\n(334b31). But if all simple bodies (elements) are present in all\ncompounds, how are the various compounds distinguished? With an eye to\nmore recent chemistry, it is natural to think that the differing\ndegrees of the primary qualities of warmth and humidity that\ncharacterize different substances arise from mixing different\nproportions of the elements. Perhaps Aristotle makes a fleeting\nreference to this idea when he expresses the uniformity of a product\nof mixing by saying that “the part exhibit[s] the same ratio\nbetween its constituents as the whole” (DG I.10,\n328a8–9 and again at DG II.7, 334b15). \nBut what does “proportions of the elements” mean? The\ncontemporary laws of constant and multiple proportions deal with a\nconcept of elemental proportions understood on the basis of the\nconcept of mass. No such concept was available to Aristotle. The\nextant texts give little indication of how Aristotle might have\nunderstood the idea of elemental proportions, and we have to resort to\nspeculation (Needham 2009a). \nRegardless of how he understood elemental proportions, Aristotle was\nquite explicit that while recoverable, elements were not actually\npresent in compounds. In DG I.10 he argues that the original\ningredients are only potentially, and not actually, present in the\nresulting compounds of a mixing process. \nThere are two reasons why in Aristotle’s theory the elements are not\nactually present in compounds. The first concerns the manner in which\nmixing occurs. Mixing only occurs because of the primary powers and\nsusceptibilities of substances to affect and be affected by other\nsubstances. This implies that all of the original matter is\nchanged when a new compound is formed. Aristotle tells us\nthat compounds are formed when the opposing contraries are neutralized\nand an intermediate state results: \nThe second reason has to do with the homogeneity requirement of pure\nsubstances. Aristotle tells us that “if combination has taken\nplace, the compound must be uniform—any part of such a\ncompound is the same as the whole, just as any part of water is\nwater” (DG I.10, 328a10f.). Since the elements are\ndefined in terms of the extremes of warmth and humidity, what has\nintermediate degrees of these qualities is not an element. Being\nhomogeneous, every part of a compound has the same intermediate\ndegrees of these qualities. Thus, there are no parts with extremal\nqualities, and hence no elements actually present. His theory of the\nappearance of new substances therefore implies that the elements are\nnot actually present in compounds. \nSo we reach an interesting theoretical impasse. Aristotle defined the\nelements by conditions they exhibit in isolation and argued that all\ncompounds are composed of the elements. However, the properties\nelements have in isolation are nothing that any part of an actually\nexisting compound could have. So how is it possible to recover the\nelements? \nIt is certainly not easy to understand what would induce a compound to\ndissociate into its elements on Aristotle’s theory, which seems\nentirely geared to showing how a stable equilibrium results from\nmixing. The overwhelming kind of mixing process doesn’t seem to be\napplicable. How, for example, could it explain the separation of salt\nand water from sea water? But the problem for the advocates of the\nactual presence of elements is to characterize them in terms of\nproperties exhibited in both isolated and combined states.\nThe general problem of adequately meeting this challenge, either in\ndefense of the potential presence or actual presence view, is the\nproblem of mixture (Cooper 2004; Fine 1995, Wood &\nWeisberg 2004). \nIn summary, Aristotle laid the philosophical groundwork for all\nsubsequent discussions of elements, pure substances, and chemical\ncombination. He asserted that all pure substances were homoeomerous\nand composed of the elements air, earth, fire, and water. These\nelements were not actually present in these substances; rather, the\nfour elements were potentially present. Their potential presence could\nbe revealed by further analysis and transformation. \nAntoine Lavoisier (1743–1794) is often called the father of\nmodern chemistry, and by 1789 he had produced a list of the elements\nthat a modern chemist would recognize. Lavoisier’s list, however, was\nnot identical to our modern one. Some items such as hydrogen and\noxygen gases were regarded as compounds by Lavoisier, although we now\nknow regard hydrogen and oxygen as elements and their gases as\nmolecules. \nOther items on his list were remnants of the Aristotelian system which\nhave no place at all in the modern system. For example, fire remained\non his list, although in the somewhat altered form of caloric. Air is\nanalyzed into several components: the respirable part called oxygen\nand the remainder called azote or nitrogen. Four types of earth found\na place on his list: lime, magnesia, barytes, and argill. The\ncomposition of these earths are “totally unknown, and, until by\nnew discoveries their constituent elements are ascertained, we are\ncertainly authorized to consider them as simple bodies” (1789,\np. 157), although Lavoisier goes on to speculate that “all the\nsubstances we call earths may be only metallic oxyds” (1789, p.\n159). \nWhat is especially important about Lavoisier’s system is his\ndiscussion of how the elemental basis of particular compounds is\ndetermined. For example, he describes how water can be shown to be a\ncompound of hydrogen and oxygen (1789, pp. 83–96). He\nwrites: \nThe metaphysical principle of the conservation of matter—that\nmatter can be neither created nor destroyed in chemical\nprocesses—called upon here is at least as old as Aristotle\n(Weisheipl 1963). What the present passage illustrates is the\nemployment of a criterion of conservation: the preservation of mass.\nThe total mass of the products must come from the mass of the\nreactants, and if this is not to be found in the easily visible ones,\nthen there must be other, less readily visible reactants. \nThis principle enabled Lavoisier to put what was essentially\nAristotle’s notion of simple substances (302a15ff., quoted in section\n1.1) to much more effective experimental use. Directly after rejecting\natomic theories, he says “if we apply the term\nelements, or principles of bodies, to express our\nidea of the last point which analysis is capable of reaching, we must\nadmit, as elements, all the substances into which we are capable, by\nany means, to reduce bodies by decomposition” (1789, p. xxiv).\nIn other words, elements are identified as the smallest components of\nsubstances that we can produce experimentally. The principle of the\nconservation of mass provided for a criterion of when a chemical\nchange was a decomposition into simpler substances, which was decisive\nin disposing of the phlogiston theory. The increase in weight on\ncalcination meant, in the light of this principle, that calcination\nwas not a decomposition, as the phlogiston theorists would have it,\nbut the formation of a more complex compound. \nDespite the pragmatic character of this definition, Lavoisier felt\nfree to speculate about the compound nature of the earths, as well as\nthe formation of metal oxides which required the decomposition of\noxygen gas. Thus, Lavoisier also developed the notion of an element as\na theoretical, last point of analysis concept. While this last point\nof analysis conception remained an important notion for Lavoisier as\nit was for Aristotle, his notion was a significant advance over\nAristotle’s and provided the basis for further theoretical advance in\nthe 19th century (Hendry 2005). \nLavoisier’s list of elements was corrected and elaborated with the\ndiscovery of many new elements in the 19th century. For example,\nHumphrey Davy (1778–1829) isolated sodium and potassium by\nelectrolysis, demonstrating that Lavoisier’s earths were actually\ncompounds. In addition, caloric disappeared from the list of accepted\nelements with the discovery of the first law of thermodynamics in the\n1840s. Thus with this changing, but growing, number of elements,\nchemists increasingly recognized the need for a systematization. Many\nattempts were made, but an early influential account was given by John\nNewlands (1837–98) who prepared the first periodic table showing\nthat 62 of the 63 then known elements follow an “octave”\nrule according to which every eighth element has similar\nproperties. \nLater, Lothar Meyer (1830–95) and Dmitrij Mendeleev\n(1834–1907) independently presented periodic tables covering all\n63 elements known in 1869. In 1871, Mendeleev published his periodic\ntable in the form it was subsequently acclaimed. This table was\norganized on the idea of periodically recurring general features as\nthe elements are followed when sequentially ordered by relative atomic\nweight. The periodically recurring similarities of chemical behavior\nprovided the basis of organizing elements into groups. He identified 8\nsuch groups across 12 horizontal periods, which, given that he was\nworking with just 63 elements, meant there were several holes. Figure 1.\nThe International Union of Pure and Applied Chemistry’s Periodic\nTable of the Elements. \nThe modern Periodic Table depicted in Figure 1 is based on Mendeleev’s\ntable, but now includes 92 naturally occurring elements and some dozen\nartificial elements (see Scerri 2006). The lightest element,\nhydrogen, is difficult to place, but is generally placed at the top of\nthe first group. Next comes helium, the lightest of the noble gases,\nwhich were not discovered until the end of the 19th\ncentury. Then the second period begins with lithium, the first of the\ngroup 1 (alkali metal) elements. As we cross the second period,\nsuccessively heavier elements are first members of other groups until\nwe reach neon, which is a noble gas like helium. Then with the next\nheaviest element sodium we return to the group 1 alkali metals and\nbegin the third period, and so on. \nOn the basis of his systematization, Mendeleev was able to correct the\nvalues of the atomic weights of certain known elements and also to\npredict the existence of then unknown elements corresponding to gaps\nin his Periodic Table. His system first began to seriously attract\nattention in 1875 when he was able to point out that gallium, the\nnewly discovered element by Lecoq de Boisbaudran (1838–1912),\nwas the same as the element he predicted under the name eka-aluminium,\nbut that its density should be considerably greater than the value\nLecoq de Boisbaudran reported. Repeating the measurement proved\nMendeleev to be right. The discovery of scandium in 1879 and germanium\nin 1886 with the properties Mendeleev predicted for what he called\n“eka-bor” and “eka-silicon” were further\ntriumphs (Scerri 2006). \nIn addition to providing the systematization of the elements used in\nmodern chemistry, Mendeleev also gave an account of the nature of\nelements which informs contemporary philosophical understanding. He\nexplicitly distinguished between the end of analysis and actual\ncomponents conceptions of elements and while he thought that both\nnotions have chemical importance, he relied on the actual components\nthesis when constructing the Periodic Table. He assumed that the\nelements remained present in compounds and that the weights of\ncompounds is the sum of the weights of their constituent atoms. He was\nthus able to use atomic weights as the primary ordering property of\nthe Periodic\n Table.[3] \nNowadays, chemical nomenclature, including the definition of the\nelement, is regulated by The International Union of Pure and Applied\nChemistry (IUPAC). In 1923, IUPAC followed Mendeleev and standardized\nthe individuation criteria for the elements by explicitly endorsing\nthe actual components thesis. Where they differed from Mendeleev is in\nwhat property they thought could best individuate the elements. Rather\nthan using atomic weights, they ordered elements according to\natomic number, the number of protons and of electrons of\nneutral elemental atoms, allowing for the occurrence of\nisotopes with the same atomic number but different atomic\nweights. They chose to order elements by atomic number because of the\ngrowing recognition that electronic structure was the atomic feature\nresponsible for governing how atoms combine to form molecules, and the\nnumber of electrons is governed by the requirement of overall\nelectrical neutrality (Kragh 2000). \nMendeleev’s periodic system was briefly called into question with the\ndiscovery of the inert gas argon in 1894, which had to be placed\noutside the existing system after chlorine. But William Ramsay\n(1852–1916) suspected there might be a whole group of chemically\ninert substances separating the electronegative halogen group 17 (to\nwhich chlorine belongs) and the electropositive alkali metals, and by\n1898 he had discovered the other noble gases, which became group 18 on\nthe modern Table. \nA more serious challenge arose when the English radiochemist Frederick\nSoddy (1877–1956) established in 1913 that according to the\natomic weight criterion of sameness, positions in the periodic table\nwere occupied by several elements. Adopting Margaret Todd’s\n(1859–1918) suggestion, Soddy called these elements\n‘isotopes,’ meaning “same place.” At the same\ntime, Bohr’s conception of the atom as comprising a positively charged\nnucleus around which much lighter electrons circulated was gaining\nacceptance. After some discussion about criteria (van der Vet 1979),\ndelegates to the 1923 IUPAC meeting saved the Periodic Table by\ndecreeing that positions should be correlated with atomic number\n(number of protons in the nucleus) rather than atomic weight. \nCorrelating positions in the Periodic Table with whole numbers finally\nprovided a criterion determining whether any gaps remained in the\ntable below the position corresponding to the highest known atomic\nnumber. The variation in atomic weight for fixed atomic number was\nexplained in 1932 when James Chadwick (1891–1974) discovered the\nneutron—a neutral particle occurring alongside the proton in\natomic nuclei with approximately the same mass as the proton. \nContemporary philosophical discussion about the nature of the elements\nbegins with the work of Friedrich Paneth (1887–1958), whose work\nheavily influenced IUPAC standards and definitions. He was among the\nfirst chemists in modern times to make explicit the distinction\nbetween the last point of analysis and actual components analyses, and\nargued that the last point in analysis thesis could not be the proper\nbasis for the chemical explanation of the nature of compounds.\nSomething that wasn’t actually present in a substance couldn’t be\ninvoked to explain the properties in a real substance. He went on to\nsay that the chemically important notion of element was\n“transcendental,” which we interpret to mean “an\nabstraction over the properties in compounds” (Paneth\n1962). \nAnother strand of the philosophical discussion probes at the\ncontemporary IUPAC definition of elements. According to IUPAC, to be\ngold is to have atomic number 79, regardless of atomic weight. A\nlogical and intended consequence of this definition is that all\nisotopes sharing an atomic number count as the same element. Needham\n(2008) has recently challenged this identification by pointing to\nchemically salient differences among the isotopes. These differences\nare best illustrated by the three isotopes of hydrogen: protium,\ndeuterium and tritium. The most striking chemical difference among the\nisotopes of hydrogen is their different rate of chemical reactions.\nBecause of the sensitivity of biochemical processes to rates of\nreaction, heavy water (deuterium oxide) is poisonous whereas ordinary\nwater (principally protium oxide) is not. With the development of more\nsensitive measuring techniques, it has become clear that this is a\ngeneral phenomenon. Isotopic variation affects the rate of chemical\nreactions, although these effects are less marked with increasing\natomic number. In view of the way chemists understand these\ndifferences in behavior, Needham argues that they can reasonably be\nsaid to underlie differences in chemical substance. He further argues\nthat the criteria of sameness and difference provided by\nthermodynamics also suggest that the isotopes should be considered\ndifferent substances. However, notwithstanding his own view, the\nplaces in Mendeleev’s periodic table were determined by atomic number\n(or nuclear charge), so a concentration on atomic weight would be\nhighly revisionary of chemical classification (Hendry 2006a). It can\nalso be argued that the thermodynamic criteria underlying the view\nthat isotopes are different substances distinguish among substances\nmore finely than is appropriate for chemistry (Hendry 2010c). \nContemporary theories of chemical combination arose from a fusion of\nancient theories of proper mixing and hundreds of years of\nexperimental work, which refined those theories. Yet even by the time\nthat Lavoisier inaugurated modern chemistry, chemists had little in\nthe way of rules or principles that govern how elements combine to\nform compounds. In this section, we discuss theoretical efforts to\nprovide such criteria. \nA first step towards a theory of chemical combination was implicit in\nLavoisier’s careful experimental work on water. In his Elements of\nChemistry, Lavoisier established the mass proportions of hydrogen\nand oxygen obtained by the complete reduction of water to its\nelements. The fact that his results were based on multiple repetitions\nof this experiment suggests that he assumed compounds like water are\nalways composed of the same elements in the same proportions. This\nwidely shared view about the constant proportions of elements in\ncompounds was first explicitly proclaimed as the law of constant\nproportions by Joseph Louis Proust (1754–1826) in the first\nyears of the 19th century. Proust did so in response to Claude Louis\nBerthollet (1748–1822), one of Lavoisier’s colleagues and\nsupporters, who argued that compounds could vary in their elemental\ncomposition. \nAlthough primarily a theoretical and conceptual posit, the law of\nconstant proportions became an important tool for chemical analysis.\nFor example, chemists had come to understand that atmospheric air is\ncomposed of both nitrogen and oxygen and is not an element. But was\nair a genuine compound of these elements or some looser mixture of\nnitrogen and oxygen, that could vary at different times and in\ndifferent places? The law of constant proportions gave a criterion for\ndistinguishing compounds from genuine mixtures. If air was a compound,\nthen it would always have the same proportion of nitrogen and oxygen\nand it should further be distinguishable from other compounds of\nnitrogen and oxygen such as nitrous oxide. If air was not a genuine\ncompound, then it would be an example of a solution, a\nhomogenous mixture of oxygen and nitrogen that could vary in\nproportions. \nBerthollet didn’t accept this rigid distinction between solutions and\ncompounds. He believed that whenever a substance is brought into\ncontact with another, it forms a homogeneous union until further\naddition of the substance leaves the union in excess. For example,\nwhen water and sugar are combined, they initially form a homogenous\nunion. At a certain point, the affinities of water and sugar for one\nanother are saturated, and a second phase of solid sugar will form\nupon the addition of more sugar. This point of saturation will vary\nwith the pressure and temperature of the solution. Berthollet\nmaintained that just as the amount of sugar in a saturated solution\nvaries with temperature and pressure, the proportions of elements in\ncompounds are sensitive to ambient conditions. Thus, he argued, it is\nnot true that substances are always composed of the same proportions\nof the element and this undermines the law of constant proportions.\nBut after a lengthy debate, chemists came to accept that the evidence\nProust adduced established the law of constant proportions for\ncompounds, which were thereby distinguished from solutions. \nChemists’ attention was largely directed towards the investigation of\ncompounds in the first half of the 19th century, initially with a view\nto broadening the evidential basis which Proust had provided. For a\ntime, the law of constant proportions seemed a satisfactory criterion\nof the occurrence of chemical combination. But towards the end of the\n19th century, chemists turned their attention to solutions.\nTheir investigation of solutions drew on the new science of\nthermodynamics, which said that changes of state undergone by\nsubstances when they are brought into contact were subjected to its\nlaws governing energy and entropy. \nAlthough thermodynamics provided no sharp distinction between\ncompounds and solutions, it did allow the formulation of a concept for\na special case called an ideal solution. An ideal solution\nforms because its increased stability compared with the separated\ncomponents is entirely due to the entropy of mixing. This can be\nunderstood as a precisification of the idea of a purely mechanical\nmixture. In contrast, compounds were stabilized by interactions\nbetween their constituent components over and above the entropy of\nmixing. For example, solid sodium chloride is stabilized by the\ninteractions of sodium and chlorine, which react to form sodium\nchloride. The behavior of real solutions could be compared with that\nof an ideal solution, and it turned out that non-ideality was the rule\nrather than the exception. Ideality is approached only in certain\ndilute binary solutions. More often, solutions exhibited behavior\nwhich could only be understood in terms of significant chemical\ninteractions between the components, of the sort characteristic of\nchemical combination. \nLong after his death, in the first decades of the 20th century,\nBerthollet was partially vindicated with the careful characterization\nof a class of substances that we now call Berthollides. These are\ncompounds whose proportions of elements do not stand in simple\nrelations to one another. Their elemental proportions are not fixed,\nbut vary with temperature and pressure. For example, the mineral\nwüstite, or ferrous oxide, has an approximate compositional\nformula of FeO, but typically has somewhat less iron than oxygen. \nFrom a purely macroscopic, thermodynamic perspective, Berthollides can\nbe understood in terms of the minimization of the thermodynamic\nfunction called the Gibbs free energy, which accommodates the\ninterplay of energy and entropy as functions of temperature and\npressure. Stable substances are ones with minimal Gibbs free energy.\nOn the microscopic scale, the basic microstructure of ferrous oxide is\na three-dimensional lattice of ferrous (Fe2+) and oxide\n(O2-) ions. However, some of the ferrous ions are replaced\nby holes randomly distributed in the crystal lattice, which generates\nan increase in entropy compared with a uniform crystal structure. An\noverall imbalance of electrical charge would be created by the missing\nions. But this is countered in ferrous oxide by twice that number of\nions from those remaining being converted to ferric (Fe3+)\nions. This removal of electrons requires an input of\nenergy, which would make for a less stable structure were it not for\nthe increased entropy afforded by the holes in the crystal structure.\nThe optimal balance between these forces depends on the temperature\nand pressure, and this is described by the Gibbs free energy function.\n \nAlthough the law of constant proportions has not survived the\ndiscovery of Berthollides and more careful analyses of solutions\nshowed that chemical combination or affinity is not confined to\ncompounds, it gave chemists a principled way of studying how elements\ncombine to form compounds through the 19th century. This\naccount of Berthollides also illustrates the interplay between\nmacroscopic and microscopic theory which is a regular feature of\nmodern chemistry, and which we turn to in the next section. \nChemistry has traditionally distinguished itself from classical\nphysics by its interest in the division of matter into different\nsubstances and in chemical combination, the process whereby substances\nare held together in compounds and solutions. In this section, we have\ndescribed how chemists came to understand that all substances were\ncomposed of the Periodic Table’s elements, and that these elements are\nactual components of substances. Even with this knowledge,\ndistinguishing pure substances from heterogeneous mixtures and\nsolutions remained a very difficult chemical challenge. And despite\nchemists’ acceptance of the law of definite proportions as a criterion\nfor substancehood, chemical complexities such as the discovery of the\nBerthollides muddied the waters. \nModern chemistry is thoroughly atomistic. All substances are thought\nto be composed of small particles, or atoms, of the Periodic Table’s\nelements. Yet until the beginning of the 20th century, much\ndebate surrounded the status of atoms and other microscopic\nconstituents of matter. As with many other issues in philosophy of\nchemistry, the discussion of atomism begins with Aristotle, who\nattacked the coherence of the notion and disputed explanations\nsupposedly built on the idea of indivisible constituents of matter\ncapable only of change in respect of position and motion, but not\nintrinsic qualities. We will discuss Aristotle’s critiques of atomism\nand Boyle’s response as well as the development of atomism in the 19th\nand 20th centuries. \nIn Aristotle’s time, atomists held that matter was fundamentally\nconstructed out of atoms. These atoms were indivisible and uniform, of\nvarious sizes and shapes, and capable only of change in respect of\nposition and motion, but not intrinsic qualities. Aristotle rejected\nthis doctrine, beginning his critique of it with a simple question:\nWhat are atoms made of? Atomists argue that they are all made of\nuniform matter. But why should uniform matter split into portions not\nthemselves further divisible? What makes atoms different from\nmacroscopic substances which are also uniform, but can be divided into\nsmaller portions? Atomism, he argued, posits a particular size as the\nfinal point of division in completely ad hoc fashion, without giving\nany account of this smallest size or why atoms are this smallest\nsize. \nApart from questions of coherence, Aristotle argued that it was\nunclear and certainly unwarranted to assume that atoms have or lack\nparticular properties. Why shouldn’t atoms have some degree of warmth\nand humidity like any observable body? But if they do, why shouldn’t\nthe degree of warmth of a cold atom be susceptible to change by the\napproach of a warm atom, in contradiction with the postulate that\natoms only change their position and motion? On the other hand, if\natoms don’t possess warmth and humidity, how can changes in degrees of\nwarmth and humidity between macroscopic substances be explained purely\non the basis of change in position and motion? \nThese and similar considerations led Aristotle to question whether the\natomists had a concept of substance at all. There are a large variety\nof substances discernible in the world—the flesh, blood and bone\nof animal bodies; the water, rock, sand and vegetable matter by the\ncoast, etc. Atomism apparently makes no provision for accommodating\nthe differing properties of these substances, and their\ninterchangeability, when for example white solid salt and tasteless\nliquid water are mixed to form brine or bronze statues slowly become\ngreen. Aristotle recognized the need to accommodate the creation of\nnew substances with the destruction of old by combination involving\nthe mutual interaction and consequent modification of the primary\nfeatures of bodies brought into contact. In spite of the weaknesses of\nhis own theory, he displays a grasp of the issue entirely lacking on\nthe part of the atomists. His conception of elements as being few in\nnumber and of such a character that all the other substances are\ncompounds derived from them by combination and reducible to them by\nanalysis provided the seeds of chemical theory. Ancient atomism\nprovided none. \nRobert Boyle (1627–1691) is often credited with first breaking\nwith ancient and medieval traditions and inaugurating modern chemistry\nby fusing an experimental approach with mechanical philosophy. Boyle’s\nchemical theory attempts to explain the diversity of substances,\nincluding the elements, in terms of variations of shape and size and\nmechanical arrangements of what would now be called sub-atomic atoms\nor corpuscles. Although Boyle’s celebrated experimental work attempted\nto respond to Aristotelian orthodoxy, his theorizing about atoms had\nlittle impact on his experimental work. Chalmers (1993, 2002)\ndocuments the total absence of any connection between Boyle’s atomic\nspeculations and his experimental work on the effects of pressure on\ngases. This analysis applies equally to Boyle’s chemical experiments\nand chemical theorizing, which was primarily driven by a desire to\ngive a mechanical philosophy of chemical combination (Chalmers 2009,\nCh. 6). No less a commentator than Antoine Lavoisier (1743–1794)\nwas quite clear that Boyle’s corpuscular theories did nothing to\nadvance chemistry. As he noted towards the end of the next century,\n“… if, by the term elements, we mean to express\nthose simple and indivisible atoms of which matter is composed, it is\nextremely probable we know nothing at all about them” (1789, p.\nxxiv). Many commentators thus regard Boyle’s empirically-based\ncriticisms of the Aristotelian chemists more important than his own\natomic theories. \nContemporary textbooks typically locate discussions of chemical\natomism in the 19th century work of John Dalton\n(1766–1844). Boyle’s ambitions of reducing elemental minima to\nstructured constellations of mechanical atoms had been abandoned by\nthis time, and Dalton’s theory simply assumes that each element has\nsmallest parts of characteristic size and mass which have the property\nof being of that elemental kind. Lavoisier’s elements\nare considered to be collections of such characteristic atoms. Dalton\nargued that this atomic hypothesis explained the law of constant\nproportions (see section 1.5). \nDalton’s theory gives expression to the idea of the real presence of\nelements in compounds. He believed that atoms survive chemical change,\nwhich underwrites the claim that elements are actually present in\ncompounds. He assumed that atoms of the same element are alike in\ntheir weight. On the assumption that atoms combine with the atoms of\nother elements in fixed ratios, Dalton claimed to explain why, when\nelements combine, they do so with fixed proportions between their\nweights. He also introduced the law of multiple proportions,\naccording to which the elements in distinct compounds of the same\nelements stand in simple proportions. He argued that this principle\nwas also explained by his atomic theory. \nDalton’s theory divided the chemical community and while he had many\nsupporters, a considerable number of chemists remained anti-atomistic.\nPart of the reason for this was controversy surrounding the empirical\napplication of Dalton’s atomic theory: How should one estimate atomic\nweights since atoms were such small quantities of matter? Daltonians\nargued that although such tiny quantities could not be measured\nabsolutely, they could be measured relative to a reference atom (the\nnatural choice being hydrogen as 1). This still left a problem in\nsetting the ratio between the weights of different atoms in compounds.\nDalton assumed that, if only one compound of two elements is known, it\nshould be assumed that they combine in equal proportions. Thus, he\nunderstood water, for instance, as though it would have been\nrepresented by HO in terms of the formulas that Berzelius was to\nintroduce (Berzelius, 1813). But Dalton’s response to this problem\nseemed arbitrary. Finding a more natural solution became pressing\nduring the first half of the nineteenth century as more and more\nelements were being discovered, and the elemental compositions of more\nand more chemical substances were being determined qualitatively\n(Duhem 2002; Needham 2004; Chalmers 2005a, 2005b, and 2008). \nDalton’s contemporaries raised other objections as well. Jacob\nBerzelius (1779–1848) argued that Daltonian atomism provided no\nexplanation of chemical combination, how elements hold together to\nform compounds (Berzelius, 1815). Since his atoms are intrinsically\nunchanging, they can suffer no modification of the kind Aristotle\nthought necessary for combination to occur. Lacking anything like the\nmodern idea of a molecule, Dalton was forced to explain chemical\ncombination in terms of atomic packing. He endowed his atoms with\natmospheres of caloric whose mutual repulsion was supposed to explain\nhow atoms pack together efficiently. But few were persuaded by this\nidea, and what came later to be known as Daltonian atomism abandoned\nthe idea of caloric shells altogether. \nThe situation was made more complex when chemists realized that\nelemental composition was not in general sufficient to distinguish\nsubstances. Dalton was aware that the same elements sometimes give\nrise to several compounds; there are several oxides of nitrogen, for\nexample. But given the law of constant proportions, these can be\ndistinguished by specifying the combining proportions, which is what\nis represented by distinct chemical formulas, for example\nN2O, NO and N2O3 for different oxides\nof nitrogen. However, as more organic compounds were isolated and\nanalyzed, it became clear that elemental composition doesn’t uniquely\ndistinguish substances. Distinct compounds with the same elemental\ncomposition are called isomers. The term was coined by\nBerzelius in 1832 when organic compounds with the same composition,\nbut different properties, were first recognized. It was later\ndiscovered that isomerism is ubiquitous, and not confined to organic\ncompounds. \nIsomers may differ radically in “physical” properties such\nas melting points and boiling points as well as patterns of chemical\nreactivity. This is the case with dimethyl ether and ethyl alcohol,\nwhich have the compositional formula\nC2H6O in common, but are represented by two\ndistinct structural formulas: (CH3)2O\nand C2H5OH. These formulas identify different\nfunctional groups, which govern patterns of chemical\nreactivity. The notion of a structural formula was developed to\naccommodate other isomers that are even more similar. This was the\ncase with a subgroup of stereoisomers called optical\nisomers, which are alike in many of their physical properties such as\nmelting points and boiling points and (when first discovered) seemed\nto be alike in chemical reactivity too. Pasteur famously separated\nenantiomers (stereoisomers of one another) of tartaric acid\nby preparing a solution of the sodium ammonium salt and allowing\nrelatively large crystals to form by slow evaporation. Using tweezers,\nhe assembled the crystals into two piles, members of the one having\nshapes which are mirror images of the shapes of those in the other\npile. Optical isomers are so called because they have the\ndistinguishing feature of rotating the plane of plane polarized light\nin opposite directions, a phenomenon first observed in quartz crystals\nat the beginning of the 19th century. These isomers are\nrepresented by three-dimensional structural formulas which are mirror\nimages of one another as we show in Figure 2. Figure 2.\nThe enantiomers of tartaric acid. D-tartaric acid is on the\nleft and L-tartaric acid is on the right. The dotted vertical line\nrepresents a mirror plane. The solid wedges represent bonds coming out\nof the plane, while the dashed wedges represent bonds going behind the\nplane. These molecular structures are mirror images of one another. \nAlthough these discoveries are often presented as having been\nexplained by the atomic or molecular hypothesis, skepticism about the\nstatus of atomism persisted throughout the 19th century. Late 19th\ncentury skeptics such as Ernst Mach, Georg Helm, Wilhelm Ostwald, and\nPierre Duhem did not see atomism as an adequate explanation of these\nphenomena, nor did they believe that there was sufficient evidence to\naccept the existence of atoms. Instead, they advocated non-atomistic\ntheories of chemical change grounded in thermodynamics (on Helm and\nOstwald, see the introduction to Deltete 2000). \nDuhem’s objections to atomism are particularly instructive. Despite\nbeing represented as a positivist in some literature (e.g. Fox 1971),\nhis objections to atomism in chemistry made no appeal to the\nunobservability of atoms. Instead, he argued that a molecule was a\ntheoretical impossibility according to 19th century physics, which\ncould say nothing about how atoms can hold together but could give\nmany reasons why they couldn’t be stable entities over reasonable\nperiods of time. He also argued that the notion of valency attributed\nto atoms to explain their combining power was simply a macroscopic\ncharacterization projected into the microscopic level. He showed that\nchemical formulae could be interpreted without resorting to atoms and\nthe notion of valency could be defined on this basis (Duhem 1892,\n1902; for an exposition, see Needham 1996). Atomists failed to meet\nthis challenge, and he criticized them for not saying what the\nfeatures of their atoms were beyond simply reading into them\nproperties defined on a macroscopic basis (Needham 2004). Duhem did\nrecognize that an atomic theory was developed in the 19th\ncentury, the vortex theory (Kragh 2002), but rejected it as\ninadequate for explaining chemical phenomena. \nSkeptics about atomism finally became convinced at the beginning of\nthe 20th century by careful experimental and theoretical\nwork on Brownian motion, the fluctuation of particles in an\nemulsion. With the development of kinetic theory it was suspected that\nthis motion was due to invisible particles within the emulsion pushing\nthe visible particles. But it wasn’t until the first decade of the\n20th century that Einstein’s theoretical analysis and Perrin’s\nexperimental work gave substance to those suspicions and provided an\nestimate of Avogadro’s number, which Perrin famously argued was\nsubstantially correct because it agreed with determinations made by\nseveral other, independent, methods. This was the decisive argument\nfor the existence of microentities which led most of those still\nskeptical of the atomic hypotheses to change their views (Einstein\n1905; Perrin 1913; Nye 1972; Maiocchi 1990). \nIt is important to appreciate, however, that establishing the\nexistence of atoms in this way left many of the questions raised by\nthe skeptics unanswered. A theory of the nature of atoms which would\nexplain how they can combine to form molecules was yet to be\nformulated. And it remains to this day an open question whether a\npurely microscopic theory is available which is adequate to explain\nthe whole range of chemical phenomena. This issue is pursued in\nSection 6 where we discuss reduction. \nAs we discussed in Section 1, by the end of the 18th century the\nmodern conception of chemical substances began to take form in\nLavoisier’s work. Contemporary looking lists of elements were being\ndrawn up and also the notion of mass was introduced into chemistry.\nDespite these advances, chemists continued to develop theories about\ntwo substances which we no longer accept: caloric and phlogiston.\nLavoisier famously rejected phlogiston, but he accepted caloric. It\nwould be another 60 years until the notion of caloric was finally\nabandoned with the development of thermodynamics. \nIn 1761, Joseph Black discovered that heating a body doesn’t always\nraise its temperature. In particular, he noticed that heating ice at\n0°C converts it to liquid at the same temperature. Similarly,\nthere is a latent heat of vaporization which must be supplied for the\nconversion of liquid water into steam at the boiling point without\nraising the temperature. It was some time before the modern\ninterpretation of Black’s ground-breaking discovery was fully\ndeveloped. He had shown that heat must be distinguished from the state\nof warmth of a body and even from the changes in that state. But it\nwasn’t until the development of thermodynamics that heating was\ndistinguished as a process from the property or quality of being warm\nwithout reference to a transferred substance. \nBlack himself was apparently wary of engaging in hypothetical\nexplanations of heat phenomena (Fox 1971), but he does suggest an\ninterpretation of the latent heat of fusion of water as a chemical\nreaction involving the combination of the heat fluid with ice to yield\nthe new substance water. Lavoisier incorporated Black’s conception of\nlatent heat into his caloric theory of heat, understanding latent heat\ntransferred to a body without raising its temperature as caloric fluid\nbound in chemical combination with that body and not contributing to\nthe body’s degree of warmth or temperature. Lavoisier’s theory thus\nretains something of Aristotle’s, understanding what we would call a\nphase change of the same substance as a transformation of one\nsubstance into another. \nCaloric figures in Lavoisier’s list of elements as the “element\nof heat or fire” (Lavoisier 1789, p. 175), “becom[ing]\nfixed in bodies … [and] act[ing] upon them with a repulsive\nforce, from which, or from its accumulation in bodies to a greater or\nlesser degree, the transformation of solids into fluids, and of fluids\nto aeriform elasticity, is entirely owing” (1789, p. 183). He\ngoes on to define ‘gas’ as “this aeriform state of\nbodies produced by a sufficient accumulation of caloric.” Under\nthe list of binary compounds formed with hydrogen, caloric is said to\nyield hydrogen gas (1789, p. 198). Similarly, under the list of binary\ncompounds formed with phosphorus, caloric yields phosphorus gas (1789,\np. 204). The Lavoisian element base of oxygen combines with the\nLavoisian element caloric to form the compound oxygen gas. The\ncompound of base of oxygen with a smaller amount of caloric is oxygen\nliquid (known only in principle to Lavoisier). What we would call the\nphase change of liquid to gaseous oxygen is thus for him a change of\nsubstance. Light also figures in his list of elements, and is said\n“to have a great affinity with oxygen, … and contributes\nalong with caloric to change it into the state of gas” (1789, p.\n185). \nAnother substance concept from roughly the same period is phlogiston,\nwhich served as the basis for 18th century theories of processes that\ncame to be called oxidation and reduction. Georg Ernst Stahl\n(1660–1734) introduced the theory, drawing on older theoretical\nideas. Alchemists thought that metals lose the mercury principle under\ncalcination and that when substances are converted to slag, rust, or\nash by heating, they lose the sulphur principle. Johann Joackim Becher\n(1635–82) modified these ideas at the end of the 17th century,\narguing that the calcination of metals is a kind of combustion\ninvolving the loss of what he called the principle of flammability.\nStahl subsequently renamed this principle ‘phlogiston’ and\nfurther modified the theory, maintaining that phlogiston could be\ntransferred from one substance to another in chemical reactions, but\nthat it could never be isolated. \nFor example, metals were thought to be compounds of the metal’s calx\nand phlogiston, sulphur was thought to be a compound of sulphuric acid\nand phlogiston, and phosphorus was thought to be a compound of\nphosphoric acid and phlogiston. Substances such as carbon which left\nlittle or no ash after burning were taken to be rich in phlogiston.\nThe preparation of metals from their calxes with the aid of wood\ncharcoal was understood as the transfer of phlogiston from carbon to\nthe metal. \nRegarding carbon as a source of phlogiston and no longer merely as a\nsource of warmth was a step forward in understanding chemical\nreactions (which Ladyman 2011 emphasizes in support of his structural\nrealist interpretation of phlogiston chemistry). The phlogiston theory\nsuggested that reactions could involve the replacement of one part of\na substance with another, where previously all reactions were thought\nto be simple associations or dissociations. \nPhlogiston theory was developed further by Henry Cavendish\n(1731–1810) and Joseph Priestley (1733–1804), who both\nattempted to better characterize the properties of phlogiston itself.\nAfter 1760, phlogiston was commonly identified with what they called\n‘inflammable air’ (hydrogen), which they successfully\ncaptured by reacting metals with muriatic (hydrochloric) acid. Upon\nfurther experimental work on the production and characterizations of\nthese “airs,” Cavendish and Priestley identified what we\nnow call oxygen as ‘dephlogisticated air’ and nitrogen as\n‘phlogiston-saturated air.’ \nAs reactants and products came to be routinely weighed, it became\nclear that metals gain weight when they become a calx. But according\nto the phlogiston theory, the calx involves the loss of phlogiston.\nAlthough the idea that a process involving the loss of a substance\ncould involve the gain of weight seems strange to us, phlogiston\ntheorists were not immediately worried. Some phlogiston theorists\nproposed explanations based on the ‘levitation’ properties\nof phlogiston, what Priestly later referred to as phlogiston’s\n‘negative weight.’ Another explanation of the phenomenon\nwas that the nearly weightless phlogiston drove out heavy, condensed\nair from the pores of the calx. The net result was a lighter product.\nSince the concept of mass did not yet play a central role in\nchemistry, these explanations were thought to be quite reasonable. \nHowever, by the end of the 1770s, Torbern Olaf Bergman\n(1735–1784) made a series of careful measurements of the weights\nof metals and their calxes. He showed that the calcination of metals\nled to a gain in their weight equal to the weight of oxygen lost by\nthe surrounding air. This ruled out the two explanations given above,\nbut interestingly, he took this in his stride, arguing that, as metals\nwere being transformed into their calxes, they lost weightless\nphlogiston. This phlogiston combines with the air’s oxygen to form\nponderable warmth, which in turn combines with what remains of the\nmetal after loss of phlogiston to form the calx. Lavoisier simplified\nthis explanation by removing the phlogiston from this scheme. This\nmoment is what many call the Chemical Revolution. \nModern chemistry primarily deals with microstructure, not elemental\ncomposition. This section will explore the history and consequences of\nchemistry’s focus on structure. The first half of this section\ndescribes chemistry’s transition from a science concerned with\nelemental composition to a science concerned with structure. The\nsecond half will focus on the conceptual puzzles raised by\ncontemporary accounts of bonding and molecular structure. \nIn the 18th and early 19th centuries, chemical\nanalyses of substances consisted in the decomposition of substances\ninto their elemental components. Careful weighing combined with an\napplication of the law of constant proportions allowed chemists to\ncharacterize substances in terms of the mass ratios of their\nconstituent elements, which is what chemists mean by the composition\nof a compound. During this period, Berzelius developed a notation of\ncompositional formulas for these mass ratios where letters stand for\nelements and subscripts stand for proportions on a scale which\nfacilitates comparison of different substances. Although these\nproportions reflect the proportion by weight in grams, the simple\nnumbers are a result of reexpressing gravimetric proportions in terms\nof chemical equivalents. For example, the formulas\n‘H2O’ and ‘H2S’ say that\nthere is just as much oxygen in combination with hydrogen in water as\nthere is sulphur in combination with hydrogen in hydrogen sulphide.\nHowever, when measured in weight, ‘H2O’\ncorresponds to combining proportions of 8 grams of oxygen to 1 gram of\nhydrogen and ‘H2S’ corresponds to 16 grams of\nsulphur to 1 of hydrogen in weight. \nBy the first decades of the 19th century, the nascent\nsub-discipline of organic chemistry began identifying and synthesizing\never increasing numbers of compounds (Klein 2003). As indicated in\nsection 2.2, it was during this period that the phenomenon of\nisomerism was recognized, and structural formulas were introduced to\ndistinguish substances with the same compositional formula that differ\nin their macroscopic properties. Although some chemists thought\nstructural formulas could be understood on a macroscopic basis, others\nsought to interpret them as representations of microscopic entities\ncalled molecules, corresponding to the smallest unit of a compound as\nan atom was held to be the smallest unit of an element. \nIn the first half of the nineteenth century there was no general\nagreement about how the notion of molecular structure could be\ndeployed in understanding isomerism. But during the second half of the\ncentury, consensus built around the structural theories of August\nKekulé (1829–1896). Kekulé noted that carbon\ntended to combine with univalent elements in a 1:4 ratio. He argued\nthat this was because each carbon atom could form bonds to four other\natoms, even other carbon atoms (1858 [1963], 127). In later papers,\nKekulé dealt with apparent exceptions to carbon’s valency of\nfour by introducing the concept of double bonds between carbon atoms.\nHe extended his treatment to aromatic compounds, producing the famous\nhexagonal structure for benzene (see Rocke 2010), although this was to\ncreate a lasting problem for the universality of carbon’s valency of 4\n(Brush 1999a, 1999b). \nKekulé’s ideas about bonding between atoms were important steps\ntoward understanding isomerism. Yet his presentations of structure\ntheory lacked a clear system of diagrammatic representation so most\nmodern systems of structural representation originate with Alexander\nCrum Brown’s (1838–1932) paper about isomerism among organic\nacids (1864 [1865]). Here, structure was shown as linkages between\natoms (see Figure 3). Figure 3. Depictions of ethane and formic acid in Crum Brown’s graphic\nnotation. (1864 [1865], 232)\n \nEdward Frankland (1825–1899) simplified and popularized Crum\nBrown’s notation in successive editions of his Lecture Notes for\nChemical Students (Russell 1971; Ritter 2001). Frankland was\nalso the first to introduce the term ‘bond’ for the\nlinkages between atoms (Ramberg 2003). \nThe next step in the development of structural theory came when James\nDewar (1842–1943) and August Hofmann (1818–1892) developed\nphysical models corresponding closely to Crum Brown’s formulae\n(Meinel 2004). Dewar’s molecules were built from carbon atoms\nrepresented by black discs placed at the centre of pairs of copper\nbands. In Hofmann’s models, atoms were colored billiard balls (black\nfor carbon, white for hydrogen, red for oxygen etc.) linked by bonds.\nEven though they were realized by concrete three-dimensional\nstructures of croquet balls and connecting arms, the\nthree-dimensionality of these models was artificial. The medium itself\nforced the representations of atoms to be spread out in space. But did\nthis correspond to chemical reality? \nKekulé, Crum Brown, and Frankland were extremely cautious when\nanswering this question. Kekulé distinguished between the\napparent atomic arrangement which could be deduced from chemical\nproperties, which he called “chemical structure,” and the\ntrue spatial arrangement of the atoms (Rocke 1984, 2010). Crum Brown\nmade a similar distinction, cautioning that in his graphical formulae\nhe did not “mean to indicate the physical, but merely the\nchemical position of the atoms” (Crum Brown, 1864, 232).\nFrankland noted that “It must carefully be borne in mind that\nthese graphic formulae are intended to represent neither the shape of\nthe molecules, nor the relative position of the constituent\natoms” (Biggs et al. 1976, 59). \nOne way to interpret these comments is that they reflect a kind of\nanti-realism: Structural formulae are merely theoretical tools for\nsummarizing a compound’s chemical behavior. Or perhaps they are simply\nagnostic, avoiding definite commitment to a microscopic realm about\nwhich little can be said. However, other comments suggest a realist\ninterpretation, but one in which structural formulae represent only\nthe topological structure of the spatial arrangement: \nThe move towards a fully spatial interpretation was advanced by the\nsimultaneous postulation in 1874 of a tetrahedral structure for the\norientation of carbon’s four bonds by Jacobus van ’t Hoff\n(1852–1911) and Joseph Achille Le Bel (1847–1930) to\naccount for optical isomerism (see Figure 4 and section 2.2). When\ncarbon atoms are bonded to four different constituents, they cannot be\nsuperimposed on their mirror images, just as your left and right hands\ncannot be. This gives rise to two possible configurations of\nchiral molecules, thus providing for a distinction between\ndistinct substances whose physical and chemical properties are the\nsame except for their ability to rotate plane polarized light in\ndifferent directions.  \nvan ’t Hoff and Le Bel provided no account of the mechanism by which\nchiral molecules affect the rotation of plane polarized light\n(Needham 2004). But by the end of the century, spatial structure was\nbeing put to use in explaining the aspects of the reactivity and\nstability of organic compounds with Viktor Meyer’s (1848–1897)\nconception of steric hindrance and Adolf von Baeyer’s\n(1835–1917) conception of internal molecular strain (Ramberg\n2003). Figure 4. A schematic representation of the tetrahedral arrangement of\nsubstituents around the carbon atom. Compare the positions of\nsubstituents Y and Z.\n \nGiven that these theories were intrinsically spatial, traditional\nquestions about chemical combination and valency took a new direction:\nWhat is it that holds the atoms together in a particular spatial\narrangement? The answer, of course, is the chemical bond. \nAs structural theory gained widespread acceptance at the end of the\n19th century, chemists began focusing their attention on\nwhat connects the atoms together, constraining the spatial\nrelationships between these atoms. In other words, they began\ninvestigating the chemical bond. Modern theoretical accounts of\nchemical bonding are quantum mechanical, but even contemporary\nconceptions of bonds owe a huge amount to the classical conception of\nbonds developed by G.N. Lewis at the very beginning of the\n20th century. \nG.N. Lewis (1875–1946) was responsible for the first influential\ntheory of the chemical bond (Lewis 1923; see Kohler 1971, 1975 for\nbackground). His theory said that chemical bonds are pairs of\nelectrons shared between atoms. Lewis also distinguished between what\ncame to be called ionic and covalent compounds,\nwhich has proved to be remarkably resilient in modern chemistry. \nIonic compounds are composed of electrically charged ions, usually\narranged in a neutral crystal lattice. Neutrality is achieved when the\npositively charged ions (cations) are of exactly the right number to\nbalance the negatively charged ions (anions). Crystals of common salt,\nfor example, comprise as many sodium cations (Na+) as there\nare chlorine anions (Cl−). Compared to the isolated\natoms, the sodium cation has lost an electron and the chlorine anion\nhas gained an electron. \nCovalent compounds, on the other hand, are either individual molecules\nor indefinitely repeating structures. In either case, Lewis thought\nthat they are formed from atoms bound together by shared pairs of\nelectrons. Hydrogen gas is said to consist of molecules composed of\ntwo hydrogen atoms held together by a single, covalent bond; oxygen\ngas, of molecules composed of two oxygen atoms and a double bond;\nmethane, of molecules composed of four equivalent carbon-hydrogen\nsingle bonds, and silicon dioxide (sand) crystals of indefinitely\nrepeating covalently bonded arrays of SiO2 units. \nAn important part of Lewis’ account of molecular structure concerns\ndirectionality of bonding. In ionic compounds, bonding is\nelectrostatic and therefore radially symmetrical. Hence an individual\nion bears no special relationship to any one of its neighbors. On the\nother hand, in covalent or non-polar bonding, bonds have a definite\ndirection; they are located between atomic centers. \nThe nature of the covalent bond has been the subject of considerable\ndiscussion in the recent philosophy of chemistry literature (Berson\n2008; Hendry 2008; Weisberg 2008). While the chemical bond plays a\ncentral role in chemical predictions, interventions, and explanations,\nit is a difficult concept to define precisely. Fundamental\ndisagreements exist between classical and quantum mechanical\nconceptions of the chemical bond, and even between different quantum\nmechanical models. Once one moves beyond introductory textbooks to\nadvanced treatments, one finds many theoretical approaches to bonding,\nbut few if any definitions or direct characterizations of the bond\nitself. While some might attribute this lack of definitional clarity\nto common background knowledge shared among all chemists, we believe\nthis reflects uncertainty or maybe even ambivalence about the status\nof the chemical bond itself. \nThe new philosophical literature about the chemical bond begins with\nthe structural conception of chemical bonding (Hendry 2008).\nOn the structural conception, chemical bonds are sub-molecular,\nmaterial parts of molecules, which are localized between individual\natomic centers and are responsible for holding the molecule together.\nThis is the notion of the chemical bond that arose at the end of the\n19th century, which continues to inform the practice of\nsynthetic and analytical chemistry. But is the structural conception\nof bonding correct? Several distinct challenges have been raised in\nthe philosophical literature. \nThe first challenge comes from the incompatibility between the\nontology of quantum mechanics and the apparent ontology of the\nchemical bonds. Electrons cannot be distinguished in principle\n (Identity and Individuality in Quantum Theory)\n and hence quantum mechanical descriptions of bonds cannot depend on\nthe identity of particular electrons. If we interpret the structural\nconception of bonding in a Lewis-like fashion, where bonds are\ncomposed of specific pairs of electrons donated by particular atoms,\nwe can see that this picture is incompatible with quantum mechanics. A\nrelated objection notes that both experimental and theoretical\nevidence suggest that electrons are delocalized,\n“smeared out” over whole molecules. Quantum mechanics\ntells us not to expect pairs of electrons to be localized between\nbonded atoms. Furthermore, Mulliken argued that pairing was\nunnecessary for covalent bond formation. Electrons in a hydrogen\nmolecule “are more firmly bound when they have two hydrogen\nnuclei to run around than when each has only one. The fact that two\nelectrons become paired … seems to be largely incidental”\n(1931, p. 360). Later authors point to the stability of the\nH2+ ion in support of this contention. \nDefenders of the structural conception of bonding respond to these\nchallenges by noting that G.N. Lewis’ particular structural account\nisn’t the only possible one. While bonds on the structural conception\nmust be sub-molecular and directional, they need not be electron\npairs. Responding specifically to the challenge from quantum ontology,\nthey argue that bonds should be individuated by the atomic centers\nthey link, not by the electrons. Insofar as electrons participate\nphysically in the bond, they do so not as individuals. All of the\nelectrons are associated with the whole molecule, but portions of the\nelectron density can be localized. To the objection from\ndelocalization, they argue that all the structural account requires is\nthat some part of the total electron density of the molecule\nis responsible for the features associated with the bond and there\nneed be no assumption that it is localized directly between the atoms\nas in Lewis’ model (Hendry 2008, 2010b). \nA second challenge to the structural conception of bonding comes from\ncomputational chemistry, the application of quantum mechanics to make\npredictions about chemical phenomena. Drawing on the work of quantum\nchemist Charles Coulson (1910–1974), Weisberg (2008) has argued\nthat the structural conception of chemical bonding is not robust in\nquantum chemistry. This argument looks to the history of quantum\nmechanical models of molecular structure. In the earliest quantum\nmechanical models, something very much like the structural conception\nof bonding was preserved; electron density was, for the most part,\nlocalized between atomic centers and was responsible for holding\nmolecules together. However, these early models made empirical\npredictions about bond energies and bond lengths that were only in\nqualitative accord with experiment. \nSubsequent models of molecular structure yielded much better agreement\nwith experiment when electron density was “allowed” to\nleave the area between the atoms and delocalize throughout the\nmolecule. As the models were further improved, bonding came to be seen\nas a whole-molecule, not sub-molecular, phenomenon. Weisberg argues\nthat such considerations should lead us to reject the structural\nconception of bonding and replace it with a molecule-wide conception.\nOne possibility is the energetic conception of bonding that\nsays that bonding is the energetic stabilization of molecules.\nStrictly speaking, according to this view, chemical bonds do not\nexist; bonding is real, bonds are not (Weisberg 2008; also see\nCoulson 1952, 1960). \nThe challenges to the structural view of bonding have engendered\nseveral responses in the philosophical and chemical literatures. The\nfirst appeals to chemical practice: Chemists engaged in synthetic and\nanalytic activities rely on the structural conception of bonding.\nThere are well over 100,000,000 compounds that have been discovered or\nsynthesized, all of which have been formally characterized. How can\nthis success be explained if a central chemical concept such as the\nstructural conception of the bond does not pick out anything real in\nnature?  Throughout his life, Linus Pauling\n(1901–1994) defended this view. \nAnother line of objection comes from Berson (2008), who discusses the\nsignificance of very weakly bonded molecules. For example, there are\nfour structural isomers of 2-methylenecyclopentane-1,3-diyl. The most\nstable of the structures does not correspond to a normal bonding\ninteraction because of an unusually stable singlet state, a state\nwhere the electron spins are parallel. Berson suggests that this is a\ncase where “the formation of a bond actually produces a\ndestabilized molecule.” In other words, the energetic conception\nbreaks down because bonding and molecule-wide stabilization come\napart. \nFinally, the “Atoms in Molecules” program (Bader 1991;\nsee Gillespie and Popelier 2001, Chs. 6 & 7 for an exposition)\nsuggests that we can hold on to the structural conception of the bond\nunderstood functionally, but reject Lewis’ ideas about how electrons\nrealize this relationship. Bader, for example, argues that we can\ndefine ‘bond paths’ in terms of topological features of\nthe molecule-wide electron density. Such bond paths have physical\nlocations, and generally correspond closely to classical covalent\nbonds. Moreover they partially vindicate the idea that bonding\ninvolves an increase in electron density between atoms: a bond path is\nan axis of maximal electron density (leaving a bond path in a\ndirection perpendicular to it involves a decrease in electron\ndensity). There are also many technical advantages to this approach.\nMolecule-wide electron density exists within the ontology of quantum\nmechanics, so no quantum-mechanical model could exclude it. Further,\nelectron density is considerably easier to calculate than other\nquantum mechanical properties, and it can be measured empirically\nusing X-ray diffraction techniques. Figure 5. Too many bonds? 60 bond paths from each carbon atom in\nC60 to a trapped Ar atom in the interior.\n \nUnfortunately, Bader’s approach does not necessarily save the day for\nthe structural conception of the bond. His critics point out that his\naccount is extremely permissive and puts bond paths in places that\nseem chemically suspect. For example, his account says that when you\ntake the soccer-ball shaped buckminster fullerene molecule\n(C60) and trap an argon atom inside it, there are 60 bonds\nbetween the carbon atoms and the argon atom as depicted in Figure 5\n(Cerpa et al. 2008). Most chemists would think this implausible\nbecause one of the most basic principles of chemical combination is\nthe fact that argon almost never forms bonds (see Bader 2009 for a\nresponse). \nA generally acknowledged problem for the delocalized account is the\nlack of what chemists call transferability. Central to the structural\nview, as we saw, is the occurrence of functional groups common to\ndifferent substances. Alcohols, for example, are characterized by\nhaving the hydroxyl OH group in common. This is reflected in the\nstrong infra red absorption at 3600cm–1 being taken\nas a tell-tale sign of the OH group. But ab initio QM treatments just\nsee different problems posed by different numbers of electrons, and\nfail to reflect that there are parts of a molecular structure, such as\nan OH group, which are transferable from one molecule to another, and\nwhich they may have in common (Woody 2000, 2012). \nA further issue is the detailed understanding of the cause of chemical\nbonding. For many years, the dominant view, based on the\nHellman-Feynman theorem, has been that it is essentially an\nelectrostatic attraction between positive nuclei and negative electron\nclouds (Feynman 1939). But an alternative, originally suggested by\nHellman and developed by Rüdenberg, has recently come into\nprominence. This emphasizes the quantum mechanical analogue of the\nkinetic energy (Needham 2014). Contemporary accounts may draw on a\nnumber of subtle quantum mechanical features. But these details\nshouldn’t obscure the overriding thermodynamic principle governing\nthe formation of stable compounds by chemical reaction. As Atkins puts\nit, \nThe difficulties faced by this and every other model of bonding have\nled a number of chemists and philosophers to argue for pluralism.\nQuantum chemist Roald Hoffmann writes “A bond will be a bond by\nsome criteria and not by others … have fun with the concept and\nspare us the hype” (Hoffmann 2009, Other Internet Resources). \nWhile most of the philosophical literature about molecular structure\nand geometry is about bonding, there are a number of important\nquestions concerning the notion of molecular structure itself. The\nfirst issue involves the correct definition of molecular structure.\nTextbooks typically describe a molecule’s structure as the equilibrium\nposition of its atoms. Water’s structure is thus characterized by\n104.5º angles between the hydrogen atoms and the oxygen atom. But\nthis is a problematic notion because molecules are not static\nentities. Atoms are constantly in motion, moving in ways that we might\ndescribe as bending, twisting, rocking, and scissoring. Bader\ntherefore argues that we should think of molecular structure as the\ntopology of bond paths, or the relationships between the atoms that\nare preserved by continuous transformations (Bader 1991). \nA second issue concerning molecular structure is even more\nfundamental: Do molecules have the kinds of shapes and directional\nfeatures that structural formulas represent? Given the history we have\ndiscussed so far it seems like the answer is obviously yes. Indeed, a\nnumber of indirect experimental techniques including x-ray\ncrystallography, spectroscopy, and product analysis provide converging\nevidence of not only the existence of shape, but specific shapes for\nspecific molecular species. \nDespite this, quantum mechanics poses a challenge to the notion of\nmolecular shape. In quantum mechanical treatments of molecular\nspecies, shape doesn’t seem to arise unless it is put in by hand.\n(Woolley 1978; Primas 1981; Sutcliffe & Woolley 2012). \nThis tension the between the familiar theories of chemical structure\nand quantum- mechanical accounts of molecules might be resolved in\nseveral ways. One might embrace eliminativism about molecular\nstructure: Quantum mechanics is a more fundamental theory, we might\nargue, and its ontology has no place for molecular structure.\nTherefore, molecular structure doesn’t exist. No philosopher or\nchemist that we are aware of has endorsed this option. Another\npossible response makes a different appeal to the underlying physics.\nSomething must be breaking the wavefunction symmetries and giving\natoms locations in molecules. This might be interactions with other\nmolecules or interactions with measuring devices. Thus, molecular\nshape is partially constituted by interaction and is a relational, not\nintrinsic property (Ramsey 1997). \nA related option is a kind of pragmatism. Hans Primas argues that,\nstrictly speaking, a quantum mechanical description of a molecule has\nto be a whole-universe description. No matter how we draw the\nboundaries of interest around some target molecular system, in\nreality, the system is open and interacting with everything else in\nthe universe. Thus the shape of any particular molecule could be the\nresult of its interactions with anything else in the universe. We only\nget the paradox of molecules having no shape when we treat systems as\nclosed—say a single methane molecule alone in the universe. It\nis fine to treat open systems as closed for pragmatic purposes, but we\nshould always understand that this is an idealization. We shouldn’t\ntreat our idealizations, such as open systems being closed, as\nveridical. Hence there is no incompatibility between quantum mechanics\nand molecular shape (Primas 1981). \nSo despite the ubiquity of structural representations of molecules, it\nturns out that even the notion of molecular shape is not ambiguous.\nPrimas’ approach, which points to the idealization in many quantum\nmechanical models, is accepted by many chemists. But there is nothing\nlike a consensus in the philosophical literature about how to\nunderstand molecular shape. \nIn the final part of this section about structure, we consider a\nfavorite example of philosophers: the thesis that “Water is\nH2O.” This thesis is often taken to be\nuncontroversially true and is used as evidence for semantic\nexternalism and for essentialism about natural kinds (Kripke 1980;\nPutnam 1975,  1990). Since general theses about the theory of reference\nand semantic externalism are beyond the scope of this article, we\nfocus narrowly on chemical essentialism. Is having a common essential\nmicrostructure sufficient to individuate chemical kinds and explain\ntheir general features? And if so, is “being\nH2O” sufficient to individuate water? \nThe essentialist thesis is often stylized by writing “water =\nH2O” or “(all and only) water is\nH2O”. Ignoring the issue of whether the identity\nmakes sense (Needham 2000) and of understanding what the predicates\napply to in the second formulation (Needham 2010a), it is not clear\nthat either formulation expresses the kind of thesis that\nessentialists intend. “H2O” is not a\ndescription of any microstructure. Rather,\n“H2O” is a compositional formula, describing\nthe combining proportions of hydrogen and oxygen to make water. \nA reasonable paraphrase of the standard formulation would be\n“Water is a collection of H2O molecules.”\nHowever, although the expression “H2O molecule”\ndescribes a particular microentity, it by no means exhausts the kinds\nof microparticles in water, and says nothing of the\nmicrostructure by which they are related in water. Describing\nthe microstructure of water completely involves elaborating the\ndetails of this interconnected structure, as well as detailing how\nthey depend on temperature and pressure, and how they change over time\n(Finney 2004). \nLike many other substances, water cannot simply be described as a\ncollection of individual molecules. Here are just a few examples of\nthe complexities of its microstructure: water self-ionizes, which\nmeans that hydrogen and hydroxide ions co-exist with H2O\nmolecules in liquid water, continually recombining to form\nH2O molecules. At the same time, the H2O\nmolecules associate into larger polymeric species. Mentioning these\ncomplexities isn’t just pedantic because they are often what give rise\nto the most striking characteristics of substances. For example, the\nelectrical conductivity of water is due to a mechanism in which a\npositive charge (hydrogen ion) attaches at one point of a polymeric\ncluster, inducing a co-ordinated transfer of charge across the\ncluster, releasing a hydrogen ion at some distant point. The effect is\nthat charge is transferred from one point to another without a\ntransfer of matter to carry it. The hydrogen bonding underlying the\nformation of clusters is also at the root of many other distinctive\nproperties of water including its high melting and boiling points and\nits increase in density upon melting. As van Brakel has argued (1986,\n2000), water is practically the poster child for such\n“non-molecular” substances. \nMaybe water isn’t simply a collection of H2O molecules, but\nit certainly has a microstructure and perhaps the essentialist thesis\ncould be recast along the lines of “Water is whatever has its\nmicrostructure,” writing in the information that would save this\nfrom tautology. But this thesis still endorses the idea that\n“water” is a predicate characterized by what Putnam calls\nstereotypical features. This neglects the importance of macroscopic,\nyet scientifically important, properties such as boiling points,\nspecific heats, latent heats, and so on, from which much of the\nmicrostructure is actually inferred. Indeed, many of the criteria that\nchemists use to determine the sameness and purity of substances are\nmacroscopic, not microscopic. In fact, international standards for\ndetermining the purity of substances like water depend on the careful\ndetermination of macroscopic properties such as the triple-point, the\ntemperature and pressure where liquid, gaseous, and solid phases exist\nsimultaneously (Needham 2011). \nSo is water H2O? In the end, the answer to this question\ncomes down to how one interprets this sentence. Many chemists would be\nsurprised to find out that water wasn’t H2O, but perhaps\nthis is because they read “H2O” as a shorthand\n(Weisberg 2005) or as a compositional formula in the manner we\ndiscussed in the opening of this section. Water is actually\ncharacterized by making reference to both its microstructural and\nmacroscopic features, so this can’t on its own provide a justification\nfor microessentialism. \nFor these reasons, microessentialist claims would need to be grounded\nin chemical classification and explanation: the systems of\nnomenclature developed by IUPAC are based entirely on microstructure,\nas are theoretical explanations of the chemical and spectroscopic\nbehaviour of substances (see Hendry 2016). On the other hand,\nH2O content fails to track usage of the term\n“water” by ordinary-language speakers, who seem to have\ndifferent interests to chemists (Malt 1994). Pluralism is one\nresponse to these tensions: Hasok Chang (2012) urges that even within\nscience, water’s identity with H2O should be left open;\nJulia Bursten (2014) tries to reconcile the special role of\nmicrostructure in chemistry with the failure of microessentialism; and\nJoyce Havstad (2018) argues that chemists’ use of substance concepts\nis just as messy and disunified as biologists’ use of various species\nconcepts. \nOur discussion so far has focused on “static” chemistry:\naccounts of the nature of matter and its structure. But much of\nchemistry involves the transformation of matter from one form to\nanother. This section describes the philosophical issues surrounding\nthe synthesis of one substance from another, as well as chemical\nmechanisms, the explanatory framework chemists use to describe these\ntransformations. \nThere has been a profusion of discussion in the literatures of\nphilosophy of biology and philosophy of neuroscience about the notion\nof mechanisms and mechanistic explanations (e.g., Machamer, Darden,\n& Craver 2000). Yet the production of mechanisms as explanatory\nschemes finds its original home in chemistry, especially organic\nchemistry. Chemical mechanisms are used to classify reactions into\ntypes, to explain chemical behavior, and to make predictions about\nnovel reactions or reactions taking place in novel circumstances\n(Weininger 2014). \nGoodwin (2012) identifies two notions of chemical mechanism at play in\nchemistry. The first or thick notion of mechanism is like a\nmotion picture of a chemical reaction. Such a mechanism traces out the\npositions of all of the electrons and atomic cores of some set of\nmolecules during the course of a reaction, and correlates these\npositions to the potential energy or free energy of the system. One\nmight think of this as an ideal reaction mechanism, as it would\ncontain all information about the time course of a chemical\nreaction. \nIn contrast, the thin notion of a reaction mechanism focuses\non a discrete set of steps. In each step, a set of reactive\nintermediates are generated. These intermediates are quasi-stable\nmolecular species that will ultimately yield the products of the\nreaction. For example, the much studied biomolecular nucleophilic\nsubstitution (SN2) reaction is said to have a single\nreactive intermediate with the incoming nucleophile and outgoing\nleaving group both partially bonded to the reactive carbon center (see\nFigure 6). Such a description of the reaction mechanism is not only\nabstract in that it leaves out much detail, but it is also highly\nidealized. Reactions do not take actually place as a series of\ndiscrete steps, each of which generates a quasi-stable reaction\nintermediate. Figure 6. Thin reaction mechanism for the SN2 reaction.\n \nWhile most textbook treatments of reaction mechanisms begin by\nmentioning the thick notion, the details nearly always turn to thin\nnotions of mechanism. At the same time, formal theoretical treatments\nof reaction mechanisms deal exclusively with the thick notion. Such\ntreatments often attempt to calculate the potential energy function\nfor the reaction from quantum mechanics. Given the importance of the\nthick notion to formal chemical theorizing, why does the thin notion\ndominate the practice of chemistry and find expression in textbooks\nand research articles? \nPart of the reason that thin reaction mechanisms are widely used is\nthat determining thick reaction mechanisms is essentially impossible\nexperimentally, and extremely difficult theoretically. But this cannot\nbe the whole story because when necessary, chemists have been able to\nproduce relevant portions of the thick mechanism. \nAlternatively, Goodwin (2012, p. 311) has argued that, given the\nexplanatory and predictive goals of chemists, not all of the thick\nmechanism is needed. In fact, only a characterization of specific\nstructures, the transition state and stable reactive intermediates,\nare necessary to produce chemical explanations and predictions.\nConstructing mechanisms as a discrete series of steps between stable\nand reactive structures allows the chemist: \nSo chemists’ explanatory goals require that specific features of\nreaction mechanisms can be identified. The rest of the thick mechanism\nwouldn’t necessarily add any explanatorily relevant detail to the\nexplanation. \nChemists typically do not engage in philosophical discussions about\ntheir work. Yet, when discussing the confirmation of reaction\nmechanisms, it is not uncommon to see mention of philosophical issues\nsurrounding confirmation. So why does the study of reaction mechanisms\nmake chemists more philosophically reflective? \nFor one thing, almost all studies aimed at elucidating reaction\nmechanisms rely on indirect techniques. Ideally, elucidating a\nreaction mechanism would be like doing experiments in biomechanics.\nSlow motion video could give direct information about the movement of\nparts and how these movements give rise to the motion of the whole.\nBut we have nothing like a video camera for chemical reactions.\nInstead, after an experimental determination of the reaction products\nand possible isolation of stable intermediate species, chemists rely\non measurements of reaction rates in differing conditions,\nspectroscopy, and isotopic labeling, among other techniques. These\ntechniques help eliminate candidate reaction mechanisms, but do not\nthemselves directly suggest new ones. This emphasis on eliminating\npossibilities has led some chemists to endorse a Popperian,\nfalsificationist analysis of reaction mechanism elucidation (e.g.,\nCarpenter 1984). \nAlthough some chemists have been attracted to a falsificationist\nanalysis, a better analysis of reaction mechanism elucidation is the\naccount of confirmation known as eliminative induction. This\naccount shares falsification’s emphasis on trying to reject\nhypotheses, but argues that the hypotheses not rejected receive some\ndegree of confirmation. So in the case of reaction mechanisms, we\nmight see eliminative induction as a processes whereby chemists: \nIn following this procedure, chemists do more than simply falsify:\nthey add confirmatory power to the mechanisms that haven’t been\neliminated. Indeed, in discussing similar issues, biochemist John\nPlatt (1964) argued that good scientific inference is strong\ninference, whereby the goal in an experiment is to eliminate one\nor more hypotheses. Several contemporary philosophers have endorsed\nthe role of eliminative induction in science (e.g., Bird 2010, Dorling\n1995, Kitcher 1993, Norton 1995). It is easy to see how it can be\nmodeled in Bayesian and other quantitative frameworks for\nconfirmation. Specifically, as particular candidate reaction\nmechanisms are eliminated, the probability that one of the remaining\nmechanisms is correct goes up (see Earman 1992 for details). \nOne difficulty with eliminative induction is the source of the\nrelevant alternative hypotheses, in this case reaction mechanisms.\nThere is no algorithmic procedure for generating these mechanisms, and\nthere is always the possibility that the correct mechanism has not\nbeen considered at all. This is a genuine problem, and we believe that\nit is the very issue that motivates chemists to turn towards\nfalsification when thinking about mechanisms; all they can do is\nevaluate the plausible mechanisms that they have thought of. However,\nwe see eliminative induction as a more plausible reflection of the\nepistemic situation of mechanistic chemists. This problem is not\nuncertainty about mechanisms compatible with\nexperiments—chemists have evidence that weighs in favor of\nthose. Rather, the problem is with unconceived alternatives. Structure\noffers one way to delineate such mechanistic possibilities: Hoffmann\n(1997, Chapter 29) provides a beautiful example of explicitly\neliminative reasoning in his discussion of how H. Okabe and J. R.\nMcNesby used isotopic labelling to eliminate two out of three possible\nmechanisms for the photolysis of ethane to ethylene. But this is an\nissue in all parts of science, not just mechanistic chemistry, and\neliminative induction has always played a role in chemists’ reasoning\nabout structure. How did van ’t Hoff argue for the tetrahedral\ncarbon atom? He argued first that it was possible to account for the\nobserved number and variety of the isomers of certain organic\nsubstances only by taking into account the arrangement of atoms in\nspace. He then defended a tetrahedral geometry for the carbon atom by\nrejecting a square planar arrangement: if carbon’s geometry were\nsquare planar, there would be more isomers of substituted methane than\nare observed. Thus, for instance, disubstituted methane (of the form\nCH2X2) should have two separable isomers if it\nis square planar, whereas only one can be found. Assuming a\ntetrahedral arrangement, in contrast, would be in accord with the\nobserved number of isomers (Brock 1992). \nIn his classic discussion, Hans Reichenbach distinguished between the\ncontext of discovery and the context of\njustification. His distinction was intended to highlight the fact\nthat we could have a logical analysis of scientific justification in\nthe form of confirmation theory, but there could never be a logical\nprocedure for generating hypotheses. Hypothesis generation is the\ncreative part of science, while confirmation is the logical part. This\ndistinction has been challenged in recent years by those that see the\npath of discovery contributing to justification. But chemistry\nprovides a more interesting challenge to Reichenbach: It apparently\ngives us logics of discovery. \nThere are two subfields in which chemists sometimes speak of logics or\nprocedures for discovery. The first is synthetic chemistry. E.J. Corey\n(Corey & Cheng 1989) has proposed that the synthesis of organic\nmolecules can be rationally planned according to the logic of\nretrosynthetic analysis. Systematizing a long tradition in\nsynthetic organic chemistry, Corey shows how one can reason backwards\nfrom a target molecule by finding a series of\n“disconnections,” bonds which one knows how to make. The\nresulting tree of disconnections gives potential pathways for\nsynthesis that can then be evaluated by plausibility, or simply tried\nout in the laboratory. \nAnother area where chemists have developed a logic for discovery is in\nthe area of drug design. Murray Goodman (Goodman & Ro 1995), for\nexample, proposed a four-step procedure for developing candidate\nmolecules for new medication. Say that you were interested in making a\ndrug that would more effectively target one of the morphine receptors\nin the brain. You start by making a molecular analogue of morphine,\nperhaps with a more constrained structure. After successful synthesis,\nyou study the molecule’s three-dimensional structure by spectroscopy\nand computer simulation. You then test your molecule in a biological\nassay to see if you have successfully targeted the receptor and to\nwhat extent. Then based on the information you get, you modify the\nstructure, hopefully improving in each iteration. \nThese examples from chemistry put pressure on Reichenbach’s claim that\nthere cannot be a logic of discovery. Moreover, they illustrate how,\nwhen a science is concerned with creating new things, procedures for\ndiscovery may become essential. \nOne of the perennial topics in philosophy of science concerns\ninter-theoretic relations. In the course of debating whether biology\nis reducible to the physical sciences or whether psychology is\nreducible to biology, many philosophers assume that chemistry has\nalready been reduced to physics. In the past, this assumption was so\npervasive that it was common to read about\n“physico/chemical” laws and explanations, as if the\nreduction of chemistry to physics was complete. Although most\nphilosophers of chemistry would accept that there is no conflict\nbetween the sciences of chemistry and physics (Needham 2010b), many\nwould reject a stronger unity thesis. Most believe that chemistry has\nnot been reduced to physics nor is it likely to be (see Le Poidevin\n2005, for the opposite view, and Hendry & Needham 2007, for a\nrejoinder). \nWhen thinking about the question of reducibility in chemistry, it is\nuseful to break this question into two parts: The first, and more\nfamiliar one to philosophers, concerns the relationship between\nelements, atoms, molecules, and the fundamental particles of physics.\nWe might ask, “Are atomic and molecular species reducible to\nsystems of fundamental particles interacting according to quantum\nmechanics?” A second, less familiar question concerns the\nrelationship between the macroscopic and microscopic descriptions of\nchemical substances. “Are chemical substances reducible to\nmolecular species?” Here, the main question is whether all\nchemical properties that have been defined macroscopically can be\nredefined in terms of the properties of atoms, molecules, and their\ninteractions. \nBogaard (1978), Scerri (1991, 1994) and Hendry (1998) have all\nquestioned the possibility of fully reducing chemical theories about\natoms and molecules to quantum mechanics. Bogaard argues that many key\nchemical concepts such as valence and bonding do not find a natural\nhome in quantum mechanics. In a similar spirit, Scerri points out that\nthe quantum mechanical calculations of atomic spectra standardly\npresented in chemistry textbooks make highly idealized assumptions\nabout the structure of many-electron systems. These approximations are\nwell-motivated on pragmatic grounds. However, they do not allow\nquantum mechanics to “approximately reduce” chemical\nfacts, because the errors introduced by these approximations cannot be\nestimated (Scerri 1991, 1994). Further, one of the most important\nchemical trends, the length of periods in the Periodic Table, cannot\nbe derived from quantum mechanics, unless experimentally derived\nchemical information is specifically introduced (Scerri 1997).\nDrawing on the work of Woolley (1978) and Primas (1981), Hendry (1998)\nargues that there are principled difficulties in accommodating\nmolecular shape within quantum mechanics: the Born-Oppenheimer\napproximation effectively adds structure by hand. Although quantum\nchemistry can be extremely illuminating, these authors argue that it\nhas not reduced chemistry to physics. \nIf one thinks that reduction means deriving the phenomenon of the\nhigher level exclusively from the lower level, then these arguments\nshould settle the question of reduction. More than 80 years after the\ndiscovery of quantum mechanics, chemistry has not been reduced to it.\nBut there are two possible reductionist responses to this\nargument. \nFirst, reductionists can argue that there are no principled reasons\nthat chemical phenomena have not been derived from quantum mechanics.\nThe problem is a lack of computational power and appropriate\napproximation schemes, not anything fundamental. Schwarz (2007) has\nmade this argument against Scerri, claiming that the electronic\nstructure of atoms, and hence the Periodic Table, is in principle\nderivable from quantum mechanics. He believes that quantum chemistry’s\ninability to reduce chemical properties is simply a manifestation of\nthe problems shared by all of the computationally complex sciences.\nDebate then turns to the plausibility of such “reducibility in\nprinciple” claims. \nThere are also arguments that focus, at least implicitly, on\nchemistry’s ontology. A well-known strand of contemporary metaphysics\ndefends physicalism, the doctrine that everything in the\nuniverse is physical (see the entry on\n physicalism).\n According to the physicalist, chemistry is “nothing but”\nphysics, even though chemical explanations and theories are not\nderivable from physics. The physical world is simply composed of the\nfundamental particles of physics. Chemical entities and their\nproperties have no independent reality. \nThe status of arguments for physicalism and the supervenience of\neverything on the physical are contentious within metaphysics proper,\nbut beyond the scope of this entry. Yet we think that the failure of\nchemical theory to be fully derivable from physics raises interesting\nquestions about the doctrine of physicalism. Minimally, it points to\nlongstanding worries that the domain of the physical is not\nwell-defined. If chemical entities such as molecules and ions end up\nbeing part of the physical ontology, one might argue that this was not\na case of the reduction of chemistry to physics at all but simply the\nexpansion of the ontology of physics to encompass the ontology of\nchemistry. \nIndependent studies of the ontology of chemistry on the basis of\nmereology have been undertaken by several authors (Earley 2005,\nHarré and Llored 2011, Needham 2010a). In disputing Scerri’s\n(2000, 2001) argument against claims (Zuo et al. 1999) that orbitals\nhave been observed, Mulder (2010) appeals to a general ontological\ndistinction between entities, which can appropriately be said to\nexist, and states which don’t exist independently but are features of\nentities that exist. Ostrovsky (2005) and Schwarz (2006) take issue\nwith the role of approximations in Scerri’s argument. \nMore controversially, some philosophers of chemistry have argued that\nchemical properties may constrain the behavior of physical systems,\nsomething akin to what philosophers of mind call strong emergence, or\ndownwards causation (Kim 1999). While acknowledging the central role\nof quantum mechanics in understanding structure, Hendry argues that in\nsome cases, molecular structure is an unexplained explainer. The issue\narises when we consider the quantum-mechanical description of\nstructural isomers, molecules with the same atoms, but with different\nmolecular structures. For example, dimethyl ether and ethanol share a\nHamiltonian, the quantum mechanical description of their physical\nstates. Nevertheless, they are very different molecules. Ethanol is\nextremely soluble in water, whereas dimethyl ether is only partially\nsoluble in water. Ethanol boils at 78.4°C, while dimethyl ether\nboils at 34.6°C. Drinking ethanol leads to intoxication, while\ndimethyl ether is toxic in quite different ways. Quantum mechanics can\nshow how each of these structures is energetically stable, and\nilluminate how they interact with other molecules and radiation to\nexplain the chemical and spectroscopic behaviour of ethanol and\ndimethyl ether, but the different structures are introduced as\nunexplained initial conditions. While he acknowledges that these facts\nare not incompatible with the claim that structure is reducible,\nHendry argues that strong emergence is just as plausible an\ninterpretation as reduction of the explanatory relationship between\nchemistry and quantum mechanics (2006b, 2010a). \nSo far we have considered intertheoretic relationships between\nchemistry and physics. What about within chemistry itself? Do the\nmacroscopic and microscopic theories of chemistry align perfectly? Are\nall macroscopic properties of substances ultimately reducible to\nmicroscopic properties? In other words, if we have a macroscopic\ndescription of matter and a thermodynamic theory about how it behaves,\ncan all of this be reduced to a molecular description? The answer has\nseemed to be “yes” to many philosophers and chemists, but\nphilosophers of chemistry have urged caution here. \nConsider first the relatively simple case of gas temperature, which\nhas often been supposed reducible to the average kinetic energy of the\ngas’s molecules (cf. Nagel 1961, p. 343). A particular average\nkinetic energy of the molecules is only a necessary condition for\nhaving a given temperature, however. Only gases at equilibrium have a\ndefinite temperature, when all the spatial parts have the same\ntemperature as the whole (reflecting the fact that temperature is an\nintensive property). A sufficient condition would need to complement\nthe average kinetic energy with a microscopic correlate of the\nmacroscopic condition of being at thermodynamic equilibrium.\nStatistical mechanics specifies the relevant correlative condition as\nthat of the energy being distributed over the gas molecules in\naccordance with the Boltzmann distribution. But the Boltzmann\ndistribution is expressed as a function of the temperature, and its\nderivation from Boltzmann’s microscopic construal of entropy appeals\nto the thermodynamic law connecting entropy with temperature.\nAccordingly, the necessary and sufficient microscopic condition for\ngas temperature becomes circular when construed as a reduction of the\nconcept of temperature (Needham 2009b; Bishop 2010) \nAlthough the reduction of temperature to microscopic properties is\nproblematic, it is a relatively easy candidate for reduction.\nProperties concerned with chemical changes such as phase transitions,\nsolubility, and reactivity, are considerably more complex. As we\ndiscussed in Section 4.5, a purely microscopic description of matter\nis not coextensive with all chemical properties. Solubility, for\nexample, is not fully explained by microscopic properties. While we\ncan explain in rough qualitative fashion that substances dissolve when\ntheir ions or molecules have more affinity for the solvent than they\ndo for each other, this doesn’t recover the subtle, quantitative\nfeatures of solubility. It also leaves the solubility of nonionic\nsubstances untouched. Predicting these features requires appeals to\nthermodynamics, and the alleged reduction of thermodynamics to\nstatistical mechanics is considered highly contentious (Sklar\n1993). \nAs we have seen in this case, even very fruitful applications of\nphysical and chemical theory at the microscopic level are often\ninsufficient to reduce chemically important properties. Whether the\ngeneral notion of chemical substance, or the property of being a\nparticular substance for each of the millions of known substances, can\nbe reduced to microstructure needs to be demonstrated and not merely\nassumed. While there is no in-principle argument that reductions will\nalways be impossible, essential reference is made back to some\nmacroscopically observable chemical property in every formal attempt\nof reduction that we are aware of. In the absence of definite\narguments to the contrary, it seems reasonable to suppose that\nchemistry employs both macroscopic and microscopic concepts in\ndetailed theories which it strives to integrate into a unified view.\nAlthough plenty of chemistry is conducted at the microscopic level\nalone, macroscopic chemical properties continue to play important\nexperimental and theoretical roles throughout chemistry. \nIn the background of all of these debates about chemical reduction are\nissues concerning the criteria for successful reduction. All of the\nliterature that we have discussed make explicit or implicit reference\nto Nagel’s influential account of reduction. Beyond the philosophy of\nchemistry literature, this account has also been presupposed by\ncritics of particular reductionist theses (e.g. Davidson 1970), even\nwhen making points about the inapplicability of Nagel’s account to\nparticular sciences (Kitcher 1984). But Nagel’s account of reduction\nis thought by many to be unrealistic and inapplicable to actual\nscience because of the logical requirements it assumes. \nPerhaps part of the anti-reductionist consensus in the philosophy of\nchemistry literature is driven by the stringent demands of Nagel’s\naccount. But even if Nagel’s account is weakened to allow\napproximative arguments (as Hempel modified his DN model of\nexplanation), as some advocates of reductionism have urged (e.g.,\nSchaffner 1967; Churchland 1985), this still doesn’t circumvent the\nproblem of the appeal to macroscopic properties in the explanation of\nmicroscopic properties. Current chemical theory makes essential\nreference to both microscopic and macroscopic chemical concepts with\nboth chemical and quantum mechanical origins. We know of no convincing\nsubstantial examples where either of these aspects have been entirely\nexcised. \nAlmost all contemporary chemical theorizing involves modeling, the\nindirect description and analysis of real chemical phenomena by way of\nmodels. From the 19th century onwards, chemistry was\ncommonly taught and studied with physical models of molecular\nstructure. Beginning in the 20th century, mathematical\nmodels based on classical and quantum mechanics were successfully\napplied to chemical systems. This section discusses some of the\nphilosophical questions that arise when we consider modeling in\nchemistry more directly. \nChemistry’s modeling tradition began with physical models of atoms and\nmolecules. In contemporary chemical education, much emphasis is placed\non the construction and manipulation of such models. Students in\norganic chemistry classes are often required to purchase plastic\nmolecular modeling kits, and it is not uncommon to see complex\nmolecular structures built from such kits in professional laboratory\nsettings. \nThe use of molecular models gained special prominence in the middle of\nthe 19th, helping chemists to understand the significance\nof molecular shape (Brock 2000). While such structures could be\nrepresented on paper, physical models gave an immediacy and an ease of\nvisualization that sketches alone did not provide. In the middle of\nthe twentieth century, the discovery of the double helical structure\nof DNA was aided by the manipulation of physical models (Watson\n1968). \nWhile physical modeling has been important historically, and is still\na central part of chemical education and some investigations in\nstereochemistry, contemporary chemical models are almost always\nmathematical. Families of partially overlapping, partially\nincompatible models such as the valence bond, molecular\norbital, and semi-empirical models are used to explain\nand predict molecular structure and reactivity. Molecular\nmechanical models are used to explain some aspects of reaction\nkinetics and transport processes. And lattice models are used\nto explain thermodynamic properties such as phase. These and other\nmathematical models are ubiquitous in chemistry textbooks and\narticles, and chemists see them as central to chemical theory. \nChemists are very permissive about which kinds of mathematical\nstructures can serve as models. But while just about any kind of\nmathematical structure can serve as a chemical model, different types\nof systems lend themselves to particular kinds of mathematical\nstructures used in modeling. For example, the most common kinds of\nmathematical structures employed in quantum chemistry are state\nspaces, which typically correlate sub-molecular particle distances\nwith the total energy of chemical systems. Other parts of chemical\nmodeling are dynamic, hence they employ trajectory spaces, which can\nrepresent the course of a reaction over time. Still other kinds of\nmathematical structures such as graphs and groups can be employed to\nmodel molecular structure and symmetry. \nThe purpose of many exercises in chemical modeling is to learn about\nreal systems. In these cases, the model must bear certain\nrelationships to real-world systems. But these relationships needn’t\nalways be of extremely high fidelity. For example, Linus Pauling\n(1939) and early proponents of the simple valence bond model believed\nthat this model captured the essential physical interactions that give\nrise to chemical bonding. This method is closely related to Lewis’\nconception of bonding, treating molecules as composed of atomic cores\n(nuclei together with inner-shell electrons) and valence electrons\nwhich give rise to localized bonds. It stands in contrast to the\nmolecular orbital method, which doesn’t localize the bonding electrons\nto any particular part of the molecule. Modern quantum chemists think\nof the valence bond model as a template for building models of greater\ncomplexity. Thus if a modern quantum chemist deploys the simple\nvalence bond model to study a real molecule, she does so with a much\nlower standard of fidelity than Pauling would have. Her use of the\nmodel is only intended to give a first approximation to the most\nimportant features of the system. \nMuch of contemporary theoretical research in chemistry involves the\napplication of quantum mechanics to chemistry. While exact solutions\nto the quantum mechanical descriptions of chemical phenomena have not\nbeen achieved, advances in theoretical physics, applied mathematics,\nand computation have made it possible to calculate the chemical\nproperties of many molecules very accurately and with few\nidealizations. The approach of striving for ever more accurate\ncalculations with decreasing levels of idealization is endorsed by\nmany quantum chemists. For example, the development team of Gaussian,\none of the leading packages for doing quantum chemical calculations,\nexplicitly endorses this position. While they admit that there are\nmany considerations that enter into the choice of the degree of\napproximation or “level of theory” for any calculation,\nthe goal is to de-idealize the models as much as possible. They argue\nthat quantum chemical calculations which are arbitrarily close to the\nexact solutions are the “limit to which all approximate methods\nstrive” (Foresman & Frisch 1996). \nThis method of developing chemical theory relies on a systematic\nrefinement of theories, attempting to bring them closer to the truth.\nPhilosophers of science have called this process Galilean\nidealization, because as in Galileo’s work, idealizations are\nintroduced for reasons of tractability and are removed as soon as\npossible (McMullin 1985; Weisberg 2007b). But not all chemists have\nshared this focus on ever more accurate calculations. Reflecting on\nwhy he didn’t choose this path in his own career, theorist Roald\nHoffmann wrote: \nElsewhere in this article, Hoffmann admits that quantum chemistry is\nenormously successful in its predictive power, and continues to give\nus better approximations to the fundamental theory. Yet the attitude\nexpressed in this paragraph seems to be that simple, idealized models\nare needed for chemical theorizing. Thus, the central philosophical\nquestion arises: Given the availability of models that are closer to\nthe truth, why work with idealized ones? \nOne answer is given by Felix Carroll, a physical organic chemist: \nCarroll does not elaborate on these issues, but this passage contains\nthe central message: Simple models prevent our theories from having a\n“black-box” character, meaning that they will not simply\nbe a recipe for calculating without giving any physical insight.\nCarroll claims that simple models are necessary in order to expose the\nmechanisms by which chemical phenomena come about. High-level\ntheoretical calculations are not capable of showing us these\nmechanistic relationships, even though they are based on the quantum\nmechanical principles that describe the fundamental physics of the\nsystem. Or, as Hoffmann puts the point: “[I]f understanding is\nsought, simpler models, not necessarily the best in predicting all\nobservables in detail, will have value. Such models may highlight the\nimportant causes and channels” (Hoffmann, Minkin, &\nCarpenter 1996). \nWhy should it be the case that simple models have less black-box\ncharacter than others? One explanation appeals to our cognitive\nlimitations. We can only hold a couple of steps of an argument in our\nmind at once. Modern, high-level calculations can take hours or days\nto compute using fast computers. Even if every step was made explicit\nby the computer, it would be impossible to hold the calculational\nsteps in mind and hence hard to understand the reason for the result,\neven if one was convinced that the answer was correct. Paul Humphreys\nhas called this the epistemic opacity of simulations\n(2004). \nThere is a second reason for employing simple, more highly idealized\nmodels in chemistry, which stems from the explanatory traditions of\nchemistry. In developing this point, Hoffmann argues that there are\ntwo modes of explanation that can be directed at chemical systems:\nhorizontal and vertical (Hoffmann 1997). Vertical\nexplanations are what philosophers of science call deductive\nnomological explanations. These explain a chemical phenomenon by\nderiving its occurrence from quantum mechanics. Calculations in\nquantum chemistry are often used to make predictions, but insofar as\nthey are taken to explain chemical phenomena, they follow this\npattern. By showing that a molecular structure is stable, the quantum\nchemist is reasoning that this structure was to be expected given the\nunderlying physics. \nIn contrast with vertical mode, the horizontal mode of explanation\nattempts to explain chemical phenomena with chemical concepts. For\nexample, all first year organic chemistry students learn about the\nrelative reaction rates of different substrates undergoing the\nSN2 reaction. An organic chemist might ask “Why does\nmethyl bromide undergo the SN2 reaction faster than methyl\nchloride?” One answer is that “the leaving group\nBr− is a weaker base than Cl−, and\nall things being equal, weaker bases are better leaving groups.”\nThis explains a chemical reaction by appealing to a chemical property,\nin this case, the weakness of bases. \nHoffmann doesn’t say much about the differing value of the horizontal\nand vertical explanations, but one important difference is that they\ngive us different kinds of explanatory information. Vertical\nexplanations demonstrate that chemical phenomena can be derived from\nquantum mechanics. They show that, given the (approximate) truth of\nquantum mechanics, the phenomenon observed had to have happened.\nHorizontal explanations are especially good for making\ncontrastive explanations, which allows the explanation of\ntrends. Consider again our example of the rate of an SN2\nreaction. By appealing to the weakness of Br− as a\nbase, the chemist invokes a chemical property, shared across other\nmolecules. This allows her to explain methyl bromide’s reactivity as\ncompared to methyl chloride, and also methyl fluoride, methyl iodide,\netc. Insofar as chemists want to explain trends, they make contrastive\nexplanations using chemical concepts. \nReflecting on the nature of chemical theorizing, the eminent chemical\ntheorist Charles Coulson (1910–1974) makes a similar point. He\nwrote: \nAlthough Coulson, Carroll, and Hoffmann defend the use of simple,\nidealized models to generate horizontal explanations, it is not clear\nthat quantum calculations can never generate contrastive explanations.\nAlthough single vertical explanations are not contrastive, a theorist\ncan conduct multiple calculations and in so doing, generate the\ninformation needed to make contrastive explanations. Many of the best\nexamples of quantum chemistry have this character: a series of closely\nrelated calculations, attempting to get at chemically relevant\ntrends.","contact.mail":"weisberg@phil.upenn.edu","contact.domain":"phil.upenn.edu"},{"date.published":"2011-03-14","date.changed":"2019-01-16","url":"https://plato.stanford.edu/entries/chemistry/","author1":"Michael Weisberg","author1.info":"http://www.phil.upenn.edu/~weisberg","author2.info":"http://people.su.se/~pneedham/PNEng.html","entry":"chemistry","body.text":"\n\n\nChemistry is the study of the structure and transformation of matter.\nWhen Aristotle wrote the first systematic treatises on chemistry in\nthe 4th century BCE, his conceptual grasp of the nature of\nmatter was tailored to accommodate a relatively simple range of\nobservable phenomena. In the 21st century, chemistry has\nbecome the largest scientific discipline, producing over half a\nmillion publications a year ranging from direct empirical\ninvestigations to substantial theoretical work. However, the\nspecialized interest in the conceptual issues arising in chemistry,\nhereafter Philosophy of Chemistry, is a relatively recent\naddition to philosophy of science.\n\n\nPhilosophy of chemistry has two major parts. In the first, conceptual\nissues arising within chemistry are carefully articulated and\nanalyzed. Such questions which are internal to chemistry include the\nnature of substance, atomism, the chemical bond, and synthesis. In the\nsecond, traditional topics in philosophy of science such as realism,\nreduction, explanation, confirmation, and modeling are taken up within\nthe context of chemistry.\n\nOur contemporary understanding of chemical substances is elemental and\natomic: All substances are composed of atoms of elements such as\nhydrogen and oxygen. These atoms are the building blocks of the\nmicrostructures of compounds and hence are the fundamental units of\nchemical analysis. However, the reality of chemical atoms was\ncontroversial until the beginning of the 20th century and\nthe phrase “fundamental building blocks” has always\nrequired careful interpretation. So even today, the claim that all\nsubstances are composed of elements does not give us sufficient\nguidance about the ontological status of elements and how the elements\nare to be individuated. \nIn this section, we will begin with the issue of elements.\nHistorically, chemists have offered two answers to the question\n“What is it for something to be an element?” \nThese two theses describe elements in different ways. In the first,\nelements are explicitly identified by a procedure. Elements are simply\nthe ingredients in a mixture that can be separated no further. The\nsecond conception is more theoretical, positing elements as\nconstituents of composite bodies. In the pre-modern Aristotelian\nsystem, the end of analysis thesis was the favored option. Aristotle\nbelieved that elements were the building blocks of chemical\nsubstances, only potentially present in these substances. The modern\nconception of elements asserts that they are actual components,\nalthough, as we will see, aspects of the end of analysis thesis\nlinger. This section will explain the conceptual background behind\nchemistry’s progression from one conception to the other. Along the\nway, we will discuss the persistence of elements in chemical\ncombination, the connection between element individuation and\nclassification, and criteria for determining pure substances. \nThe earliest conceptual analyses concerning matter and its\ntransformations come in the Aristotelian tradition. As in modern\nchemistry, the focus of Aristotle’s theories was the nature of\nsubstances and their transformations. He offered the first systematic\ntreatises of chemical theory in On Generation and Corruption\n(De Generatione et Corruptione), Meteorology, and\nparts of Physics and On the Heavens (De\nCaelo). \nAristotle recognized that most ordinary, material things are composed\nof multiple substances, although he thought that some of them could be\ncomposed of a single, pure substance. Thus, he needed to give a\ncriterion of purity that would individuate a single substance. His\ncriterion was that pure substances are homoeomerous: they are\ncomposed of like parts at every level. “[I]f combination has\ntaken place, the compound must be uniform—any part of\nsuch a compound is the same as the whole, just as any part of water is\nwater” (De Generatione et Corruptione,\nhenceforth DG, I.10,\n328a10ff).[1] So\nwhen we encounter diamond in rock, oil in water, or smoke in air,\nAristotelian chemistry tells us that there is more than one substance\npresent. \nLike some of his predecessors, Aristotle held that the elements Fire,\nWater, Air, and Earth were the building blocks of all substances. But\nunlike his predecessors, Aristotle established this list from\nfundamental principles. He argued that “it is impossible for the\nsame thing to be hot and cold, or moist and dry … Fire is hot\nand dry, whereas Air is hot and moist …; and Water is cold and\nmoist, while Earth is cold and dry” (DG II.3,\n330a30–330b5). Aristotle supposed hot and moist to be maximal\ndegrees of heat and humidity, and cold and dry to be minimal degrees.\nNon-elemental substances are characterized by intermediate degrees of\nthe primary qualities of warmth and humidity. \nAristotle used this elemental theory to account for many properties of\nsubstances. For example he distinguished between liquids and solids by\nnoting the different properties imposed by two characteristic\nproperties of elements, moist and dry. “[M]oist is that which,\nbeing readily adaptable in shape, is not determinable by any limit of\nits own; while dry is that which is readily determinable by its own\nlimit, but not readily adaptable in shape” (DG II.2,\n329b30f.). Solid bodies have a shape and volume of their own, liquids\nonly have a volume of their own. He further distinguished liquids from\ngases, which don’t even have their own volume. He reasoned that while\nwater and air are both fluid because they are moist, cold renders\nwater liquid and hot makes air gas. On the other hand, dry together\nwith cold makes earth solid, but together with hot we get fire. \nChemistry focuses on more than just the building blocks of substances:\nIt attempts to account for the transformations that change substances\ninto other kinds of substances. Aristotle also contributed the first\nimportant analyses of this process, distinguishing between\ntransmutation, where one substance overwhelms and eliminates\nanother and proper mixing. The former is closest to what we\nwould now call change of phase and the latter to what we would now\ncall chemical combination. \nAristotle thought that proper mixing could occur when substances of\ncomparable amounts are brought together to yield other substances\ncalled\n ‘compounds.’[2]\n Accordingly, the substances we typically encounter are compounds, and\nall compounds have the feature that there are some ingredients from\nwhich they could be made. \nWhat happens to the original ingredients when they are mixed together\nto form a compound? Like modern chemists, Aristotle argued that the\noriginal ingredients can, at least in principle, be obtained by\nfurther transformations. He presumably knew that salt and water can be\nobtained from sea water and metals can be obtained from alloys. But he\nexplains this with a conceptual argument, not a detailed list of\nobservations. \nAristotle first argues that heterogeneous mixtures can be\ndecomposed: \nHe then goes on to offer an explicit definition of the concept of an\nelement in terms of simple bodies, specifically mentioning recovery in\nanalysis.  \nThe notion of simplicity implicit here is introduced late in\nDG where in book II Aristotle claims that “All the\ncompound bodies … are composed of all the simple bodies”\n(334b31). But if all simple bodies (elements) are present in all\ncompounds, how are the various compounds distinguished? With an eye to\nmore recent chemistry, it is natural to think that the differing\ndegrees of the primary qualities of warmth and humidity that\ncharacterize different substances arise from mixing different\nproportions of the elements. Perhaps Aristotle makes a fleeting\nreference to this idea when he expresses the uniformity of a product\nof mixing by saying that “the part exhibit[s] the same ratio\nbetween its constituents as the whole” (DG I.10,\n328a8–9 and again at DG II.7, 334b15). \nBut what does “proportions of the elements” mean? The\ncontemporary laws of constant and multiple proportions deal with a\nconcept of elemental proportions understood on the basis of the\nconcept of mass. No such concept was available to Aristotle. The\nextant texts give little indication of how Aristotle might have\nunderstood the idea of elemental proportions, and we have to resort to\nspeculation (Needham 2009a). \nRegardless of how he understood elemental proportions, Aristotle was\nquite explicit that while recoverable, elements were not actually\npresent in compounds. In DG I.10 he argues that the original\ningredients are only potentially, and not actually, present in the\nresulting compounds of a mixing process. \nThere are two reasons why in Aristotle’s theory the elements are not\nactually present in compounds. The first concerns the manner in which\nmixing occurs. Mixing only occurs because of the primary powers and\nsusceptibilities of substances to affect and be affected by other\nsubstances. This implies that all of the original matter is\nchanged when a new compound is formed. Aristotle tells us\nthat compounds are formed when the opposing contraries are neutralized\nand an intermediate state results: \nThe second reason has to do with the homogeneity requirement of pure\nsubstances. Aristotle tells us that “if combination has taken\nplace, the compound must be uniform—any part of such a\ncompound is the same as the whole, just as any part of water is\nwater” (DG I.10, 328a10f.). Since the elements are\ndefined in terms of the extremes of warmth and humidity, what has\nintermediate degrees of these qualities is not an element. Being\nhomogeneous, every part of a compound has the same intermediate\ndegrees of these qualities. Thus, there are no parts with extremal\nqualities, and hence no elements actually present. His theory of the\nappearance of new substances therefore implies that the elements are\nnot actually present in compounds. \nSo we reach an interesting theoretical impasse. Aristotle defined the\nelements by conditions they exhibit in isolation and argued that all\ncompounds are composed of the elements. However, the properties\nelements have in isolation are nothing that any part of an actually\nexisting compound could have. So how is it possible to recover the\nelements? \nIt is certainly not easy to understand what would induce a compound to\ndissociate into its elements on Aristotle’s theory, which seems\nentirely geared to showing how a stable equilibrium results from\nmixing. The overwhelming kind of mixing process doesn’t seem to be\napplicable. How, for example, could it explain the separation of salt\nand water from sea water? But the problem for the advocates of the\nactual presence of elements is to characterize them in terms of\nproperties exhibited in both isolated and combined states.\nThe general problem of adequately meeting this challenge, either in\ndefense of the potential presence or actual presence view, is the\nproblem of mixture (Cooper 2004; Fine 1995, Wood &\nWeisberg 2004). \nIn summary, Aristotle laid the philosophical groundwork for all\nsubsequent discussions of elements, pure substances, and chemical\ncombination. He asserted that all pure substances were homoeomerous\nand composed of the elements air, earth, fire, and water. These\nelements were not actually present in these substances; rather, the\nfour elements were potentially present. Their potential presence could\nbe revealed by further analysis and transformation. \nAntoine Lavoisier (1743–1794) is often called the father of\nmodern chemistry, and by 1789 he had produced a list of the elements\nthat a modern chemist would recognize. Lavoisier’s list, however, was\nnot identical to our modern one. Some items such as hydrogen and\noxygen gases were regarded as compounds by Lavoisier, although we now\nknow regard hydrogen and oxygen as elements and their gases as\nmolecules. \nOther items on his list were remnants of the Aristotelian system which\nhave no place at all in the modern system. For example, fire remained\non his list, although in the somewhat altered form of caloric. Air is\nanalyzed into several components: the respirable part called oxygen\nand the remainder called azote or nitrogen. Four types of earth found\na place on his list: lime, magnesia, barytes, and argill. The\ncomposition of these earths are “totally unknown, and, until by\nnew discoveries their constituent elements are ascertained, we are\ncertainly authorized to consider them as simple bodies” (1789,\np. 157), although Lavoisier goes on to speculate that “all the\nsubstances we call earths may be only metallic oxyds” (1789, p.\n159). \nWhat is especially important about Lavoisier’s system is his\ndiscussion of how the elemental basis of particular compounds is\ndetermined. For example, he describes how water can be shown to be a\ncompound of hydrogen and oxygen (1789, pp. 83–96). He\nwrites: \nThe metaphysical principle of the conservation of matter—that\nmatter can be neither created nor destroyed in chemical\nprocesses—called upon here is at least as old as Aristotle\n(Weisheipl 1963). What the present passage illustrates is the\nemployment of a criterion of conservation: the preservation of mass.\nThe total mass of the products must come from the mass of the\nreactants, and if this is not to be found in the easily visible ones,\nthen there must be other, less readily visible reactants. \nThis principle enabled Lavoisier to put what was essentially\nAristotle’s notion of simple substances (302a15ff., quoted in section\n1.1) to much more effective experimental use. Directly after rejecting\natomic theories, he says “if we apply the term\nelements, or principles of bodies, to express our\nidea of the last point which analysis is capable of reaching, we must\nadmit, as elements, all the substances into which we are capable, by\nany means, to reduce bodies by decomposition” (1789, p. xxiv).\nIn other words, elements are identified as the smallest components of\nsubstances that we can produce experimentally. The principle of the\nconservation of mass provided for a criterion of when a chemical\nchange was a decomposition into simpler substances, which was decisive\nin disposing of the phlogiston theory. The increase in weight on\ncalcination meant, in the light of this principle, that calcination\nwas not a decomposition, as the phlogiston theorists would have it,\nbut the formation of a more complex compound. \nDespite the pragmatic character of this definition, Lavoisier felt\nfree to speculate about the compound nature of the earths, as well as\nthe formation of metal oxides which required the decomposition of\noxygen gas. Thus, Lavoisier also developed the notion of an element as\na theoretical, last point of analysis concept. While this last point\nof analysis conception remained an important notion for Lavoisier as\nit was for Aristotle, his notion was a significant advance over\nAristotle’s and provided the basis for further theoretical advance in\nthe 19th century (Hendry 2005). \nLavoisier’s list of elements was corrected and elaborated with the\ndiscovery of many new elements in the 19th century. For example,\nHumphrey Davy (1778–1829) isolated sodium and potassium by\nelectrolysis, demonstrating that Lavoisier’s earths were actually\ncompounds. In addition, caloric disappeared from the list of accepted\nelements with the discovery of the first law of thermodynamics in the\n1840s. Thus with this changing, but growing, number of elements,\nchemists increasingly recognized the need for a systematization. Many\nattempts were made, but an early influential account was given by John\nNewlands (1837–98) who prepared the first periodic table showing\nthat 62 of the 63 then known elements follow an “octave”\nrule according to which every eighth element has similar\nproperties. \nLater, Lothar Meyer (1830–95) and Dmitrij Mendeleev\n(1834–1907) independently presented periodic tables covering all\n63 elements known in 1869. In 1871, Mendeleev published his periodic\ntable in the form it was subsequently acclaimed. This table was\norganized on the idea of periodically recurring general features as\nthe elements are followed when sequentially ordered by relative atomic\nweight. The periodically recurring similarities of chemical behavior\nprovided the basis of organizing elements into groups. He identified 8\nsuch groups across 12 horizontal periods, which, given that he was\nworking with just 63 elements, meant there were several holes. Figure 1.\nThe International Union of Pure and Applied Chemistry’s Periodic\nTable of the Elements. \nThe modern Periodic Table depicted in Figure 1 is based on Mendeleev’s\ntable, but now includes 92 naturally occurring elements and some dozen\nartificial elements (see Scerri 2006). The lightest element,\nhydrogen, is difficult to place, but is generally placed at the top of\nthe first group. Next comes helium, the lightest of the noble gases,\nwhich were not discovered until the end of the 19th\ncentury. Then the second period begins with lithium, the first of the\ngroup 1 (alkali metal) elements. As we cross the second period,\nsuccessively heavier elements are first members of other groups until\nwe reach neon, which is a noble gas like helium. Then with the next\nheaviest element sodium we return to the group 1 alkali metals and\nbegin the third period, and so on. \nOn the basis of his systematization, Mendeleev was able to correct the\nvalues of the atomic weights of certain known elements and also to\npredict the existence of then unknown elements corresponding to gaps\nin his Periodic Table. His system first began to seriously attract\nattention in 1875 when he was able to point out that gallium, the\nnewly discovered element by Lecoq de Boisbaudran (1838–1912),\nwas the same as the element he predicted under the name eka-aluminium,\nbut that its density should be considerably greater than the value\nLecoq de Boisbaudran reported. Repeating the measurement proved\nMendeleev to be right. The discovery of scandium in 1879 and germanium\nin 1886 with the properties Mendeleev predicted for what he called\n“eka-bor” and “eka-silicon” were further\ntriumphs (Scerri 2006). \nIn addition to providing the systematization of the elements used in\nmodern chemistry, Mendeleev also gave an account of the nature of\nelements which informs contemporary philosophical understanding. He\nexplicitly distinguished between the end of analysis and actual\ncomponents conceptions of elements and while he thought that both\nnotions have chemical importance, he relied on the actual components\nthesis when constructing the Periodic Table. He assumed that the\nelements remained present in compounds and that the weights of\ncompounds is the sum of the weights of their constituent atoms. He was\nthus able to use atomic weights as the primary ordering property of\nthe Periodic\n Table.[3] \nNowadays, chemical nomenclature, including the definition of the\nelement, is regulated by The International Union of Pure and Applied\nChemistry (IUPAC). In 1923, IUPAC followed Mendeleev and standardized\nthe individuation criteria for the elements by explicitly endorsing\nthe actual components thesis. Where they differed from Mendeleev is in\nwhat property they thought could best individuate the elements. Rather\nthan using atomic weights, they ordered elements according to\natomic number, the number of protons and of electrons of\nneutral elemental atoms, allowing for the occurrence of\nisotopes with the same atomic number but different atomic\nweights. They chose to order elements by atomic number because of the\ngrowing recognition that electronic structure was the atomic feature\nresponsible for governing how atoms combine to form molecules, and the\nnumber of electrons is governed by the requirement of overall\nelectrical neutrality (Kragh 2000). \nMendeleev’s periodic system was briefly called into question with the\ndiscovery of the inert gas argon in 1894, which had to be placed\noutside the existing system after chlorine. But William Ramsay\n(1852–1916) suspected there might be a whole group of chemically\ninert substances separating the electronegative halogen group 17 (to\nwhich chlorine belongs) and the electropositive alkali metals, and by\n1898 he had discovered the other noble gases, which became group 18 on\nthe modern Table. \nA more serious challenge arose when the English radiochemist Frederick\nSoddy (1877–1956) established in 1913 that according to the\natomic weight criterion of sameness, positions in the periodic table\nwere occupied by several elements. Adopting Margaret Todd’s\n(1859–1918) suggestion, Soddy called these elements\n‘isotopes,’ meaning “same place.” At the same\ntime, Bohr’s conception of the atom as comprising a positively charged\nnucleus around which much lighter electrons circulated was gaining\nacceptance. After some discussion about criteria (van der Vet 1979),\ndelegates to the 1923 IUPAC meeting saved the Periodic Table by\ndecreeing that positions should be correlated with atomic number\n(number of protons in the nucleus) rather than atomic weight. \nCorrelating positions in the Periodic Table with whole numbers finally\nprovided a criterion determining whether any gaps remained in the\ntable below the position corresponding to the highest known atomic\nnumber. The variation in atomic weight for fixed atomic number was\nexplained in 1932 when James Chadwick (1891–1974) discovered the\nneutron—a neutral particle occurring alongside the proton in\natomic nuclei with approximately the same mass as the proton. \nContemporary philosophical discussion about the nature of the elements\nbegins with the work of Friedrich Paneth (1887–1958), whose work\nheavily influenced IUPAC standards and definitions. He was among the\nfirst chemists in modern times to make explicit the distinction\nbetween the last point of analysis and actual components analyses, and\nargued that the last point in analysis thesis could not be the proper\nbasis for the chemical explanation of the nature of compounds.\nSomething that wasn’t actually present in a substance couldn’t be\ninvoked to explain the properties in a real substance. He went on to\nsay that the chemically important notion of element was\n“transcendental,” which we interpret to mean “an\nabstraction over the properties in compounds” (Paneth\n1962). \nAnother strand of the philosophical discussion probes at the\ncontemporary IUPAC definition of elements. According to IUPAC, to be\ngold is to have atomic number 79, regardless of atomic weight. A\nlogical and intended consequence of this definition is that all\nisotopes sharing an atomic number count as the same element. Needham\n(2008) has recently challenged this identification by pointing to\nchemically salient differences among the isotopes. These differences\nare best illustrated by the three isotopes of hydrogen: protium,\ndeuterium and tritium. The most striking chemical difference among the\nisotopes of hydrogen is their different rate of chemical reactions.\nBecause of the sensitivity of biochemical processes to rates of\nreaction, heavy water (deuterium oxide) is poisonous whereas ordinary\nwater (principally protium oxide) is not. With the development of more\nsensitive measuring techniques, it has become clear that this is a\ngeneral phenomenon. Isotopic variation affects the rate of chemical\nreactions, although these effects are less marked with increasing\natomic number. In view of the way chemists understand these\ndifferences in behavior, Needham argues that they can reasonably be\nsaid to underlie differences in chemical substance. He further argues\nthat the criteria of sameness and difference provided by\nthermodynamics also suggest that the isotopes should be considered\ndifferent substances. However, notwithstanding his own view, the\nplaces in Mendeleev’s periodic table were determined by atomic number\n(or nuclear charge), so a concentration on atomic weight would be\nhighly revisionary of chemical classification (Hendry 2006a). It can\nalso be argued that the thermodynamic criteria underlying the view\nthat isotopes are different substances distinguish among substances\nmore finely than is appropriate for chemistry (Hendry 2010c). \nContemporary theories of chemical combination arose from a fusion of\nancient theories of proper mixing and hundreds of years of\nexperimental work, which refined those theories. Yet even by the time\nthat Lavoisier inaugurated modern chemistry, chemists had little in\nthe way of rules or principles that govern how elements combine to\nform compounds. In this section, we discuss theoretical efforts to\nprovide such criteria. \nA first step towards a theory of chemical combination was implicit in\nLavoisier’s careful experimental work on water. In his Elements of\nChemistry, Lavoisier established the mass proportions of hydrogen\nand oxygen obtained by the complete reduction of water to its\nelements. The fact that his results were based on multiple repetitions\nof this experiment suggests that he assumed compounds like water are\nalways composed of the same elements in the same proportions. This\nwidely shared view about the constant proportions of elements in\ncompounds was first explicitly proclaimed as the law of constant\nproportions by Joseph Louis Proust (1754–1826) in the first\nyears of the 19th century. Proust did so in response to Claude Louis\nBerthollet (1748–1822), one of Lavoisier’s colleagues and\nsupporters, who argued that compounds could vary in their elemental\ncomposition. \nAlthough primarily a theoretical and conceptual posit, the law of\nconstant proportions became an important tool for chemical analysis.\nFor example, chemists had come to understand that atmospheric air is\ncomposed of both nitrogen and oxygen and is not an element. But was\nair a genuine compound of these elements or some looser mixture of\nnitrogen and oxygen, that could vary at different times and in\ndifferent places? The law of constant proportions gave a criterion for\ndistinguishing compounds from genuine mixtures. If air was a compound,\nthen it would always have the same proportion of nitrogen and oxygen\nand it should further be distinguishable from other compounds of\nnitrogen and oxygen such as nitrous oxide. If air was not a genuine\ncompound, then it would be an example of a solution, a\nhomogenous mixture of oxygen and nitrogen that could vary in\nproportions. \nBerthollet didn’t accept this rigid distinction between solutions and\ncompounds. He believed that whenever a substance is brought into\ncontact with another, it forms a homogeneous union until further\naddition of the substance leaves the union in excess. For example,\nwhen water and sugar are combined, they initially form a homogenous\nunion. At a certain point, the affinities of water and sugar for one\nanother are saturated, and a second phase of solid sugar will form\nupon the addition of more sugar. This point of saturation will vary\nwith the pressure and temperature of the solution. Berthollet\nmaintained that just as the amount of sugar in a saturated solution\nvaries with temperature and pressure, the proportions of elements in\ncompounds are sensitive to ambient conditions. Thus, he argued, it is\nnot true that substances are always composed of the same proportions\nof the element and this undermines the law of constant proportions.\nBut after a lengthy debate, chemists came to accept that the evidence\nProust adduced established the law of constant proportions for\ncompounds, which were thereby distinguished from solutions. \nChemists’ attention was largely directed towards the investigation of\ncompounds in the first half of the 19th century, initially with a view\nto broadening the evidential basis which Proust had provided. For a\ntime, the law of constant proportions seemed a satisfactory criterion\nof the occurrence of chemical combination. But towards the end of the\n19th century, chemists turned their attention to solutions.\nTheir investigation of solutions drew on the new science of\nthermodynamics, which said that changes of state undergone by\nsubstances when they are brought into contact were subjected to its\nlaws governing energy and entropy. \nAlthough thermodynamics provided no sharp distinction between\ncompounds and solutions, it did allow the formulation of a concept for\na special case called an ideal solution. An ideal solution\nforms because its increased stability compared with the separated\ncomponents is entirely due to the entropy of mixing. This can be\nunderstood as a precisification of the idea of a purely mechanical\nmixture. In contrast, compounds were stabilized by interactions\nbetween their constituent components over and above the entropy of\nmixing. For example, solid sodium chloride is stabilized by the\ninteractions of sodium and chlorine, which react to form sodium\nchloride. The behavior of real solutions could be compared with that\nof an ideal solution, and it turned out that non-ideality was the rule\nrather than the exception. Ideality is approached only in certain\ndilute binary solutions. More often, solutions exhibited behavior\nwhich could only be understood in terms of significant chemical\ninteractions between the components, of the sort characteristic of\nchemical combination. \nLong after his death, in the first decades of the 20th century,\nBerthollet was partially vindicated with the careful characterization\nof a class of substances that we now call Berthollides. These are\ncompounds whose proportions of elements do not stand in simple\nrelations to one another. Their elemental proportions are not fixed,\nbut vary with temperature and pressure. For example, the mineral\nwüstite, or ferrous oxide, has an approximate compositional\nformula of FeO, but typically has somewhat less iron than oxygen. \nFrom a purely macroscopic, thermodynamic perspective, Berthollides can\nbe understood in terms of the minimization of the thermodynamic\nfunction called the Gibbs free energy, which accommodates the\ninterplay of energy and entropy as functions of temperature and\npressure. Stable substances are ones with minimal Gibbs free energy.\nOn the microscopic scale, the basic microstructure of ferrous oxide is\na three-dimensional lattice of ferrous (Fe2+) and oxide\n(O2-) ions. However, some of the ferrous ions are replaced\nby holes randomly distributed in the crystal lattice, which generates\nan increase in entropy compared with a uniform crystal structure. An\noverall imbalance of electrical charge would be created by the missing\nions. But this is countered in ferrous oxide by twice that number of\nions from those remaining being converted to ferric (Fe3+)\nions. This removal of electrons requires an input of\nenergy, which would make for a less stable structure were it not for\nthe increased entropy afforded by the holes in the crystal structure.\nThe optimal balance between these forces depends on the temperature\nand pressure, and this is described by the Gibbs free energy function.\n \nAlthough the law of constant proportions has not survived the\ndiscovery of Berthollides and more careful analyses of solutions\nshowed that chemical combination or affinity is not confined to\ncompounds, it gave chemists a principled way of studying how elements\ncombine to form compounds through the 19th century. This\naccount of Berthollides also illustrates the interplay between\nmacroscopic and microscopic theory which is a regular feature of\nmodern chemistry, and which we turn to in the next section. \nChemistry has traditionally distinguished itself from classical\nphysics by its interest in the division of matter into different\nsubstances and in chemical combination, the process whereby substances\nare held together in compounds and solutions. In this section, we have\ndescribed how chemists came to understand that all substances were\ncomposed of the Periodic Table’s elements, and that these elements are\nactual components of substances. Even with this knowledge,\ndistinguishing pure substances from heterogeneous mixtures and\nsolutions remained a very difficult chemical challenge. And despite\nchemists’ acceptance of the law of definite proportions as a criterion\nfor substancehood, chemical complexities such as the discovery of the\nBerthollides muddied the waters. \nModern chemistry is thoroughly atomistic. All substances are thought\nto be composed of small particles, or atoms, of the Periodic Table’s\nelements. Yet until the beginning of the 20th century, much\ndebate surrounded the status of atoms and other microscopic\nconstituents of matter. As with many other issues in philosophy of\nchemistry, the discussion of atomism begins with Aristotle, who\nattacked the coherence of the notion and disputed explanations\nsupposedly built on the idea of indivisible constituents of matter\ncapable only of change in respect of position and motion, but not\nintrinsic qualities. We will discuss Aristotle’s critiques of atomism\nand Boyle’s response as well as the development of atomism in the 19th\nand 20th centuries. \nIn Aristotle’s time, atomists held that matter was fundamentally\nconstructed out of atoms. These atoms were indivisible and uniform, of\nvarious sizes and shapes, and capable only of change in respect of\nposition and motion, but not intrinsic qualities. Aristotle rejected\nthis doctrine, beginning his critique of it with a simple question:\nWhat are atoms made of? Atomists argue that they are all made of\nuniform matter. But why should uniform matter split into portions not\nthemselves further divisible? What makes atoms different from\nmacroscopic substances which are also uniform, but can be divided into\nsmaller portions? Atomism, he argued, posits a particular size as the\nfinal point of division in completely ad hoc fashion, without giving\nany account of this smallest size or why atoms are this smallest\nsize. \nApart from questions of coherence, Aristotle argued that it was\nunclear and certainly unwarranted to assume that atoms have or lack\nparticular properties. Why shouldn’t atoms have some degree of warmth\nand humidity like any observable body? But if they do, why shouldn’t\nthe degree of warmth of a cold atom be susceptible to change by the\napproach of a warm atom, in contradiction with the postulate that\natoms only change their position and motion? On the other hand, if\natoms don’t possess warmth and humidity, how can changes in degrees of\nwarmth and humidity between macroscopic substances be explained purely\non the basis of change in position and motion? \nThese and similar considerations led Aristotle to question whether the\natomists had a concept of substance at all. There are a large variety\nof substances discernible in the world—the flesh, blood and bone\nof animal bodies; the water, rock, sand and vegetable matter by the\ncoast, etc. Atomism apparently makes no provision for accommodating\nthe differing properties of these substances, and their\ninterchangeability, when for example white solid salt and tasteless\nliquid water are mixed to form brine or bronze statues slowly become\ngreen. Aristotle recognized the need to accommodate the creation of\nnew substances with the destruction of old by combination involving\nthe mutual interaction and consequent modification of the primary\nfeatures of bodies brought into contact. In spite of the weaknesses of\nhis own theory, he displays a grasp of the issue entirely lacking on\nthe part of the atomists. His conception of elements as being few in\nnumber and of such a character that all the other substances are\ncompounds derived from them by combination and reducible to them by\nanalysis provided the seeds of chemical theory. Ancient atomism\nprovided none. \nRobert Boyle (1627–1691) is often credited with first breaking\nwith ancient and medieval traditions and inaugurating modern chemistry\nby fusing an experimental approach with mechanical philosophy. Boyle’s\nchemical theory attempts to explain the diversity of substances,\nincluding the elements, in terms of variations of shape and size and\nmechanical arrangements of what would now be called sub-atomic atoms\nor corpuscles. Although Boyle’s celebrated experimental work attempted\nto respond to Aristotelian orthodoxy, his theorizing about atoms had\nlittle impact on his experimental work. Chalmers (1993, 2002)\ndocuments the total absence of any connection between Boyle’s atomic\nspeculations and his experimental work on the effects of pressure on\ngases. This analysis applies equally to Boyle’s chemical experiments\nand chemical theorizing, which was primarily driven by a desire to\ngive a mechanical philosophy of chemical combination (Chalmers 2009,\nCh. 6). No less a commentator than Antoine Lavoisier (1743–1794)\nwas quite clear that Boyle’s corpuscular theories did nothing to\nadvance chemistry. As he noted towards the end of the next century,\n“… if, by the term elements, we mean to express\nthose simple and indivisible atoms of which matter is composed, it is\nextremely probable we know nothing at all about them” (1789, p.\nxxiv). Many commentators thus regard Boyle’s empirically-based\ncriticisms of the Aristotelian chemists more important than his own\natomic theories. \nContemporary textbooks typically locate discussions of chemical\natomism in the 19th century work of John Dalton\n(1766–1844). Boyle’s ambitions of reducing elemental minima to\nstructured constellations of mechanical atoms had been abandoned by\nthis time, and Dalton’s theory simply assumes that each element has\nsmallest parts of characteristic size and mass which have the property\nof being of that elemental kind. Lavoisier’s elements\nare considered to be collections of such characteristic atoms. Dalton\nargued that this atomic hypothesis explained the law of constant\nproportions (see section 1.5). \nDalton’s theory gives expression to the idea of the real presence of\nelements in compounds. He believed that atoms survive chemical change,\nwhich underwrites the claim that elements are actually present in\ncompounds. He assumed that atoms of the same element are alike in\ntheir weight. On the assumption that atoms combine with the atoms of\nother elements in fixed ratios, Dalton claimed to explain why, when\nelements combine, they do so with fixed proportions between their\nweights. He also introduced the law of multiple proportions,\naccording to which the elements in distinct compounds of the same\nelements stand in simple proportions. He argued that this principle\nwas also explained by his atomic theory. \nDalton’s theory divided the chemical community and while he had many\nsupporters, a considerable number of chemists remained anti-atomistic.\nPart of the reason for this was controversy surrounding the empirical\napplication of Dalton’s atomic theory: How should one estimate atomic\nweights since atoms were such small quantities of matter? Daltonians\nargued that although such tiny quantities could not be measured\nabsolutely, they could be measured relative to a reference atom (the\nnatural choice being hydrogen as 1). This still left a problem in\nsetting the ratio between the weights of different atoms in compounds.\nDalton assumed that, if only one compound of two elements is known, it\nshould be assumed that they combine in equal proportions. Thus, he\nunderstood water, for instance, as though it would have been\nrepresented by HO in terms of the formulas that Berzelius was to\nintroduce (Berzelius, 1813). But Dalton’s response to this problem\nseemed arbitrary. Finding a more natural solution became pressing\nduring the first half of the nineteenth century as more and more\nelements were being discovered, and the elemental compositions of more\nand more chemical substances were being determined qualitatively\n(Duhem 2002; Needham 2004; Chalmers 2005a, 2005b, and 2008). \nDalton’s contemporaries raised other objections as well. Jacob\nBerzelius (1779–1848) argued that Daltonian atomism provided no\nexplanation of chemical combination, how elements hold together to\nform compounds (Berzelius, 1815). Since his atoms are intrinsically\nunchanging, they can suffer no modification of the kind Aristotle\nthought necessary for combination to occur. Lacking anything like the\nmodern idea of a molecule, Dalton was forced to explain chemical\ncombination in terms of atomic packing. He endowed his atoms with\natmospheres of caloric whose mutual repulsion was supposed to explain\nhow atoms pack together efficiently. But few were persuaded by this\nidea, and what came later to be known as Daltonian atomism abandoned\nthe idea of caloric shells altogether. \nThe situation was made more complex when chemists realized that\nelemental composition was not in general sufficient to distinguish\nsubstances. Dalton was aware that the same elements sometimes give\nrise to several compounds; there are several oxides of nitrogen, for\nexample. But given the law of constant proportions, these can be\ndistinguished by specifying the combining proportions, which is what\nis represented by distinct chemical formulas, for example\nN2O, NO and N2O3 for different oxides\nof nitrogen. However, as more organic compounds were isolated and\nanalyzed, it became clear that elemental composition doesn’t uniquely\ndistinguish substances. Distinct compounds with the same elemental\ncomposition are called isomers. The term was coined by\nBerzelius in 1832 when organic compounds with the same composition,\nbut different properties, were first recognized. It was later\ndiscovered that isomerism is ubiquitous, and not confined to organic\ncompounds. \nIsomers may differ radically in “physical” properties such\nas melting points and boiling points as well as patterns of chemical\nreactivity. This is the case with dimethyl ether and ethyl alcohol,\nwhich have the compositional formula\nC2H6O in common, but are represented by two\ndistinct structural formulas: (CH3)2O\nand C2H5OH. These formulas identify different\nfunctional groups, which govern patterns of chemical\nreactivity. The notion of a structural formula was developed to\naccommodate other isomers that are even more similar. This was the\ncase with a subgroup of stereoisomers called optical\nisomers, which are alike in many of their physical properties such as\nmelting points and boiling points and (when first discovered) seemed\nto be alike in chemical reactivity too. Pasteur famously separated\nenantiomers (stereoisomers of one another) of tartaric acid\nby preparing a solution of the sodium ammonium salt and allowing\nrelatively large crystals to form by slow evaporation. Using tweezers,\nhe assembled the crystals into two piles, members of the one having\nshapes which are mirror images of the shapes of those in the other\npile. Optical isomers are so called because they have the\ndistinguishing feature of rotating the plane of plane polarized light\nin opposite directions, a phenomenon first observed in quartz crystals\nat the beginning of the 19th century. These isomers are\nrepresented by three-dimensional structural formulas which are mirror\nimages of one another as we show in Figure 2. Figure 2.\nThe enantiomers of tartaric acid. D-tartaric acid is on the\nleft and L-tartaric acid is on the right. The dotted vertical line\nrepresents a mirror plane. The solid wedges represent bonds coming out\nof the plane, while the dashed wedges represent bonds going behind the\nplane. These molecular structures are mirror images of one another. \nAlthough these discoveries are often presented as having been\nexplained by the atomic or molecular hypothesis, skepticism about the\nstatus of atomism persisted throughout the 19th century. Late 19th\ncentury skeptics such as Ernst Mach, Georg Helm, Wilhelm Ostwald, and\nPierre Duhem did not see atomism as an adequate explanation of these\nphenomena, nor did they believe that there was sufficient evidence to\naccept the existence of atoms. Instead, they advocated non-atomistic\ntheories of chemical change grounded in thermodynamics (on Helm and\nOstwald, see the introduction to Deltete 2000). \nDuhem’s objections to atomism are particularly instructive. Despite\nbeing represented as a positivist in some literature (e.g. Fox 1971),\nhis objections to atomism in chemistry made no appeal to the\nunobservability of atoms. Instead, he argued that a molecule was a\ntheoretical impossibility according to 19th century physics, which\ncould say nothing about how atoms can hold together but could give\nmany reasons why they couldn’t be stable entities over reasonable\nperiods of time. He also argued that the notion of valency attributed\nto atoms to explain their combining power was simply a macroscopic\ncharacterization projected into the microscopic level. He showed that\nchemical formulae could be interpreted without resorting to atoms and\nthe notion of valency could be defined on this basis (Duhem 1892,\n1902; for an exposition, see Needham 1996). Atomists failed to meet\nthis challenge, and he criticized them for not saying what the\nfeatures of their atoms were beyond simply reading into them\nproperties defined on a macroscopic basis (Needham 2004). Duhem did\nrecognize that an atomic theory was developed in the 19th\ncentury, the vortex theory (Kragh 2002), but rejected it as\ninadequate for explaining chemical phenomena. \nSkeptics about atomism finally became convinced at the beginning of\nthe 20th century by careful experimental and theoretical\nwork on Brownian motion, the fluctuation of particles in an\nemulsion. With the development of kinetic theory it was suspected that\nthis motion was due to invisible particles within the emulsion pushing\nthe visible particles. But it wasn’t until the first decade of the\n20th century that Einstein’s theoretical analysis and Perrin’s\nexperimental work gave substance to those suspicions and provided an\nestimate of Avogadro’s number, which Perrin famously argued was\nsubstantially correct because it agreed with determinations made by\nseveral other, independent, methods. This was the decisive argument\nfor the existence of microentities which led most of those still\nskeptical of the atomic hypotheses to change their views (Einstein\n1905; Perrin 1913; Nye 1972; Maiocchi 1990). \nIt is important to appreciate, however, that establishing the\nexistence of atoms in this way left many of the questions raised by\nthe skeptics unanswered. A theory of the nature of atoms which would\nexplain how they can combine to form molecules was yet to be\nformulated. And it remains to this day an open question whether a\npurely microscopic theory is available which is adequate to explain\nthe whole range of chemical phenomena. This issue is pursued in\nSection 6 where we discuss reduction. \nAs we discussed in Section 1, by the end of the 18th century the\nmodern conception of chemical substances began to take form in\nLavoisier’s work. Contemporary looking lists of elements were being\ndrawn up and also the notion of mass was introduced into chemistry.\nDespite these advances, chemists continued to develop theories about\ntwo substances which we no longer accept: caloric and phlogiston.\nLavoisier famously rejected phlogiston, but he accepted caloric. It\nwould be another 60 years until the notion of caloric was finally\nabandoned with the development of thermodynamics. \nIn 1761, Joseph Black discovered that heating a body doesn’t always\nraise its temperature. In particular, he noticed that heating ice at\n0°C converts it to liquid at the same temperature. Similarly,\nthere is a latent heat of vaporization which must be supplied for the\nconversion of liquid water into steam at the boiling point without\nraising the temperature. It was some time before the modern\ninterpretation of Black’s ground-breaking discovery was fully\ndeveloped. He had shown that heat must be distinguished from the state\nof warmth of a body and even from the changes in that state. But it\nwasn’t until the development of thermodynamics that heating was\ndistinguished as a process from the property or quality of being warm\nwithout reference to a transferred substance. \nBlack himself was apparently wary of engaging in hypothetical\nexplanations of heat phenomena (Fox 1971), but he does suggest an\ninterpretation of the latent heat of fusion of water as a chemical\nreaction involving the combination of the heat fluid with ice to yield\nthe new substance water. Lavoisier incorporated Black’s conception of\nlatent heat into his caloric theory of heat, understanding latent heat\ntransferred to a body without raising its temperature as caloric fluid\nbound in chemical combination with that body and not contributing to\nthe body’s degree of warmth or temperature. Lavoisier’s theory thus\nretains something of Aristotle’s, understanding what we would call a\nphase change of the same substance as a transformation of one\nsubstance into another. \nCaloric figures in Lavoisier’s list of elements as the “element\nof heat or fire” (Lavoisier 1789, p. 175), “becom[ing]\nfixed in bodies … [and] act[ing] upon them with a repulsive\nforce, from which, or from its accumulation in bodies to a greater or\nlesser degree, the transformation of solids into fluids, and of fluids\nto aeriform elasticity, is entirely owing” (1789, p. 183). He\ngoes on to define ‘gas’ as “this aeriform state of\nbodies produced by a sufficient accumulation of caloric.” Under\nthe list of binary compounds formed with hydrogen, caloric is said to\nyield hydrogen gas (1789, p. 198). Similarly, under the list of binary\ncompounds formed with phosphorus, caloric yields phosphorus gas (1789,\np. 204). The Lavoisian element base of oxygen combines with the\nLavoisian element caloric to form the compound oxygen gas. The\ncompound of base of oxygen with a smaller amount of caloric is oxygen\nliquid (known only in principle to Lavoisier). What we would call the\nphase change of liquid to gaseous oxygen is thus for him a change of\nsubstance. Light also figures in his list of elements, and is said\n“to have a great affinity with oxygen, … and contributes\nalong with caloric to change it into the state of gas” (1789, p.\n185). \nAnother substance concept from roughly the same period is phlogiston,\nwhich served as the basis for 18th century theories of processes that\ncame to be called oxidation and reduction. Georg Ernst Stahl\n(1660–1734) introduced the theory, drawing on older theoretical\nideas. Alchemists thought that metals lose the mercury principle under\ncalcination and that when substances are converted to slag, rust, or\nash by heating, they lose the sulphur principle. Johann Joackim Becher\n(1635–82) modified these ideas at the end of the 17th century,\narguing that the calcination of metals is a kind of combustion\ninvolving the loss of what he called the principle of flammability.\nStahl subsequently renamed this principle ‘phlogiston’ and\nfurther modified the theory, maintaining that phlogiston could be\ntransferred from one substance to another in chemical reactions, but\nthat it could never be isolated. \nFor example, metals were thought to be compounds of the metal’s calx\nand phlogiston, sulphur was thought to be a compound of sulphuric acid\nand phlogiston, and phosphorus was thought to be a compound of\nphosphoric acid and phlogiston. Substances such as carbon which left\nlittle or no ash after burning were taken to be rich in phlogiston.\nThe preparation of metals from their calxes with the aid of wood\ncharcoal was understood as the transfer of phlogiston from carbon to\nthe metal. \nRegarding carbon as a source of phlogiston and no longer merely as a\nsource of warmth was a step forward in understanding chemical\nreactions (which Ladyman 2011 emphasizes in support of his structural\nrealist interpretation of phlogiston chemistry). The phlogiston theory\nsuggested that reactions could involve the replacement of one part of\na substance with another, where previously all reactions were thought\nto be simple associations or dissociations. \nPhlogiston theory was developed further by Henry Cavendish\n(1731–1810) and Joseph Priestley (1733–1804), who both\nattempted to better characterize the properties of phlogiston itself.\nAfter 1760, phlogiston was commonly identified with what they called\n‘inflammable air’ (hydrogen), which they successfully\ncaptured by reacting metals with muriatic (hydrochloric) acid. Upon\nfurther experimental work on the production and characterizations of\nthese “airs,” Cavendish and Priestley identified what we\nnow call oxygen as ‘dephlogisticated air’ and nitrogen as\n‘phlogiston-saturated air.’ \nAs reactants and products came to be routinely weighed, it became\nclear that metals gain weight when they become a calx. But according\nto the phlogiston theory, the calx involves the loss of phlogiston.\nAlthough the idea that a process involving the loss of a substance\ncould involve the gain of weight seems strange to us, phlogiston\ntheorists were not immediately worried. Some phlogiston theorists\nproposed explanations based on the ‘levitation’ properties\nof phlogiston, what Priestly later referred to as phlogiston’s\n‘negative weight.’ Another explanation of the phenomenon\nwas that the nearly weightless phlogiston drove out heavy, condensed\nair from the pores of the calx. The net result was a lighter product.\nSince the concept of mass did not yet play a central role in\nchemistry, these explanations were thought to be quite reasonable. \nHowever, by the end of the 1770s, Torbern Olaf Bergman\n(1735–1784) made a series of careful measurements of the weights\nof metals and their calxes. He showed that the calcination of metals\nled to a gain in their weight equal to the weight of oxygen lost by\nthe surrounding air. This ruled out the two explanations given above,\nbut interestingly, he took this in his stride, arguing that, as metals\nwere being transformed into their calxes, they lost weightless\nphlogiston. This phlogiston combines with the air’s oxygen to form\nponderable warmth, which in turn combines with what remains of the\nmetal after loss of phlogiston to form the calx. Lavoisier simplified\nthis explanation by removing the phlogiston from this scheme. This\nmoment is what many call the Chemical Revolution. \nModern chemistry primarily deals with microstructure, not elemental\ncomposition. This section will explore the history and consequences of\nchemistry’s focus on structure. The first half of this section\ndescribes chemistry’s transition from a science concerned with\nelemental composition to a science concerned with structure. The\nsecond half will focus on the conceptual puzzles raised by\ncontemporary accounts of bonding and molecular structure. \nIn the 18th and early 19th centuries, chemical\nanalyses of substances consisted in the decomposition of substances\ninto their elemental components. Careful weighing combined with an\napplication of the law of constant proportions allowed chemists to\ncharacterize substances in terms of the mass ratios of their\nconstituent elements, which is what chemists mean by the composition\nof a compound. During this period, Berzelius developed a notation of\ncompositional formulas for these mass ratios where letters stand for\nelements and subscripts stand for proportions on a scale which\nfacilitates comparison of different substances. Although these\nproportions reflect the proportion by weight in grams, the simple\nnumbers are a result of reexpressing gravimetric proportions in terms\nof chemical equivalents. For example, the formulas\n‘H2O’ and ‘H2S’ say that\nthere is just as much oxygen in combination with hydrogen in water as\nthere is sulphur in combination with hydrogen in hydrogen sulphide.\nHowever, when measured in weight, ‘H2O’\ncorresponds to combining proportions of 8 grams of oxygen to 1 gram of\nhydrogen and ‘H2S’ corresponds to 16 grams of\nsulphur to 1 of hydrogen in weight. \nBy the first decades of the 19th century, the nascent\nsub-discipline of organic chemistry began identifying and synthesizing\never increasing numbers of compounds (Klein 2003). As indicated in\nsection 2.2, it was during this period that the phenomenon of\nisomerism was recognized, and structural formulas were introduced to\ndistinguish substances with the same compositional formula that differ\nin their macroscopic properties. Although some chemists thought\nstructural formulas could be understood on a macroscopic basis, others\nsought to interpret them as representations of microscopic entities\ncalled molecules, corresponding to the smallest unit of a compound as\nan atom was held to be the smallest unit of an element. \nIn the first half of the nineteenth century there was no general\nagreement about how the notion of molecular structure could be\ndeployed in understanding isomerism. But during the second half of the\ncentury, consensus built around the structural theories of August\nKekulé (1829–1896). Kekulé noted that carbon\ntended to combine with univalent elements in a 1:4 ratio. He argued\nthat this was because each carbon atom could form bonds to four other\natoms, even other carbon atoms (1858 [1963], 127). In later papers,\nKekulé dealt with apparent exceptions to carbon’s valency of\nfour by introducing the concept of double bonds between carbon atoms.\nHe extended his treatment to aromatic compounds, producing the famous\nhexagonal structure for benzene (see Rocke 2010), although this was to\ncreate a lasting problem for the universality of carbon’s valency of 4\n(Brush 1999a, 1999b). \nKekulé’s ideas about bonding between atoms were important steps\ntoward understanding isomerism. Yet his presentations of structure\ntheory lacked a clear system of diagrammatic representation so most\nmodern systems of structural representation originate with Alexander\nCrum Brown’s (1838–1932) paper about isomerism among organic\nacids (1864 [1865]). Here, structure was shown as linkages between\natoms (see Figure 3). Figure 3. Depictions of ethane and formic acid in Crum Brown’s graphic\nnotation. (1864 [1865], 232)\n \nEdward Frankland (1825–1899) simplified and popularized Crum\nBrown’s notation in successive editions of his Lecture Notes for\nChemical Students (Russell 1971; Ritter 2001). Frankland was\nalso the first to introduce the term ‘bond’ for the\nlinkages between atoms (Ramberg 2003). \nThe next step in the development of structural theory came when James\nDewar (1842–1943) and August Hofmann (1818–1892) developed\nphysical models corresponding closely to Crum Brown’s formulae\n(Meinel 2004). Dewar’s molecules were built from carbon atoms\nrepresented by black discs placed at the centre of pairs of copper\nbands. In Hofmann’s models, atoms were colored billiard balls (black\nfor carbon, white for hydrogen, red for oxygen etc.) linked by bonds.\nEven though they were realized by concrete three-dimensional\nstructures of croquet balls and connecting arms, the\nthree-dimensionality of these models was artificial. The medium itself\nforced the representations of atoms to be spread out in space. But did\nthis correspond to chemical reality? \nKekulé, Crum Brown, and Frankland were extremely cautious when\nanswering this question. Kekulé distinguished between the\napparent atomic arrangement which could be deduced from chemical\nproperties, which he called “chemical structure,” and the\ntrue spatial arrangement of the atoms (Rocke 1984, 2010). Crum Brown\nmade a similar distinction, cautioning that in his graphical formulae\nhe did not “mean to indicate the physical, but merely the\nchemical position of the atoms” (Crum Brown, 1864, 232).\nFrankland noted that “It must carefully be borne in mind that\nthese graphic formulae are intended to represent neither the shape of\nthe molecules, nor the relative position of the constituent\natoms” (Biggs et al. 1976, 59). \nOne way to interpret these comments is that they reflect a kind of\nanti-realism: Structural formulae are merely theoretical tools for\nsummarizing a compound’s chemical behavior. Or perhaps they are simply\nagnostic, avoiding definite commitment to a microscopic realm about\nwhich little can be said. However, other comments suggest a realist\ninterpretation, but one in which structural formulae represent only\nthe topological structure of the spatial arrangement: \nThe move towards a fully spatial interpretation was advanced by the\nsimultaneous postulation in 1874 of a tetrahedral structure for the\norientation of carbon’s four bonds by Jacobus van ’t Hoff\n(1852–1911) and Joseph Achille Le Bel (1847–1930) to\naccount for optical isomerism (see Figure 4 and section 2.2). When\ncarbon atoms are bonded to four different constituents, they cannot be\nsuperimposed on their mirror images, just as your left and right hands\ncannot be. This gives rise to two possible configurations of\nchiral molecules, thus providing for a distinction between\ndistinct substances whose physical and chemical properties are the\nsame except for their ability to rotate plane polarized light in\ndifferent directions.  \nvan ’t Hoff and Le Bel provided no account of the mechanism by which\nchiral molecules affect the rotation of plane polarized light\n(Needham 2004). But by the end of the century, spatial structure was\nbeing put to use in explaining the aspects of the reactivity and\nstability of organic compounds with Viktor Meyer’s (1848–1897)\nconception of steric hindrance and Adolf von Baeyer’s\n(1835–1917) conception of internal molecular strain (Ramberg\n2003). Figure 4. A schematic representation of the tetrahedral arrangement of\nsubstituents around the carbon atom. Compare the positions of\nsubstituents Y and Z.\n \nGiven that these theories were intrinsically spatial, traditional\nquestions about chemical combination and valency took a new direction:\nWhat is it that holds the atoms together in a particular spatial\narrangement? The answer, of course, is the chemical bond. \nAs structural theory gained widespread acceptance at the end of the\n19th century, chemists began focusing their attention on\nwhat connects the atoms together, constraining the spatial\nrelationships between these atoms. In other words, they began\ninvestigating the chemical bond. Modern theoretical accounts of\nchemical bonding are quantum mechanical, but even contemporary\nconceptions of bonds owe a huge amount to the classical conception of\nbonds developed by G.N. Lewis at the very beginning of the\n20th century. \nG.N. Lewis (1875–1946) was responsible for the first influential\ntheory of the chemical bond (Lewis 1923; see Kohler 1971, 1975 for\nbackground). His theory said that chemical bonds are pairs of\nelectrons shared between atoms. Lewis also distinguished between what\ncame to be called ionic and covalent compounds,\nwhich has proved to be remarkably resilient in modern chemistry. \nIonic compounds are composed of electrically charged ions, usually\narranged in a neutral crystal lattice. Neutrality is achieved when the\npositively charged ions (cations) are of exactly the right number to\nbalance the negatively charged ions (anions). Crystals of common salt,\nfor example, comprise as many sodium cations (Na+) as there\nare chlorine anions (Cl−). Compared to the isolated\natoms, the sodium cation has lost an electron and the chlorine anion\nhas gained an electron. \nCovalent compounds, on the other hand, are either individual molecules\nor indefinitely repeating structures. In either case, Lewis thought\nthat they are formed from atoms bound together by shared pairs of\nelectrons. Hydrogen gas is said to consist of molecules composed of\ntwo hydrogen atoms held together by a single, covalent bond; oxygen\ngas, of molecules composed of two oxygen atoms and a double bond;\nmethane, of molecules composed of four equivalent carbon-hydrogen\nsingle bonds, and silicon dioxide (sand) crystals of indefinitely\nrepeating covalently bonded arrays of SiO2 units. \nAn important part of Lewis’ account of molecular structure concerns\ndirectionality of bonding. In ionic compounds, bonding is\nelectrostatic and therefore radially symmetrical. Hence an individual\nion bears no special relationship to any one of its neighbors. On the\nother hand, in covalent or non-polar bonding, bonds have a definite\ndirection; they are located between atomic centers. \nThe nature of the covalent bond has been the subject of considerable\ndiscussion in the recent philosophy of chemistry literature (Berson\n2008; Hendry 2008; Weisberg 2008). While the chemical bond plays a\ncentral role in chemical predictions, interventions, and explanations,\nit is a difficult concept to define precisely. Fundamental\ndisagreements exist between classical and quantum mechanical\nconceptions of the chemical bond, and even between different quantum\nmechanical models. Once one moves beyond introductory textbooks to\nadvanced treatments, one finds many theoretical approaches to bonding,\nbut few if any definitions or direct characterizations of the bond\nitself. While some might attribute this lack of definitional clarity\nto common background knowledge shared among all chemists, we believe\nthis reflects uncertainty or maybe even ambivalence about the status\nof the chemical bond itself. \nThe new philosophical literature about the chemical bond begins with\nthe structural conception of chemical bonding (Hendry 2008).\nOn the structural conception, chemical bonds are sub-molecular,\nmaterial parts of molecules, which are localized between individual\natomic centers and are responsible for holding the molecule together.\nThis is the notion of the chemical bond that arose at the end of the\n19th century, which continues to inform the practice of\nsynthetic and analytical chemistry. But is the structural conception\nof bonding correct? Several distinct challenges have been raised in\nthe philosophical literature. \nThe first challenge comes from the incompatibility between the\nontology of quantum mechanics and the apparent ontology of the\nchemical bonds. Electrons cannot be distinguished in principle\n (Identity and Individuality in Quantum Theory)\n and hence quantum mechanical descriptions of bonds cannot depend on\nthe identity of particular electrons. If we interpret the structural\nconception of bonding in a Lewis-like fashion, where bonds are\ncomposed of specific pairs of electrons donated by particular atoms,\nwe can see that this picture is incompatible with quantum mechanics. A\nrelated objection notes that both experimental and theoretical\nevidence suggest that electrons are delocalized,\n“smeared out” over whole molecules. Quantum mechanics\ntells us not to expect pairs of electrons to be localized between\nbonded atoms. Furthermore, Mulliken argued that pairing was\nunnecessary for covalent bond formation. Electrons in a hydrogen\nmolecule “are more firmly bound when they have two hydrogen\nnuclei to run around than when each has only one. The fact that two\nelectrons become paired … seems to be largely incidental”\n(1931, p. 360). Later authors point to the stability of the\nH2+ ion in support of this contention. \nDefenders of the structural conception of bonding respond to these\nchallenges by noting that G.N. Lewis’ particular structural account\nisn’t the only possible one. While bonds on the structural conception\nmust be sub-molecular and directional, they need not be electron\npairs. Responding specifically to the challenge from quantum ontology,\nthey argue that bonds should be individuated by the atomic centers\nthey link, not by the electrons. Insofar as electrons participate\nphysically in the bond, they do so not as individuals. All of the\nelectrons are associated with the whole molecule, but portions of the\nelectron density can be localized. To the objection from\ndelocalization, they argue that all the structural account requires is\nthat some part of the total electron density of the molecule\nis responsible for the features associated with the bond and there\nneed be no assumption that it is localized directly between the atoms\nas in Lewis’ model (Hendry 2008, 2010b). \nA second challenge to the structural conception of bonding comes from\ncomputational chemistry, the application of quantum mechanics to make\npredictions about chemical phenomena. Drawing on the work of quantum\nchemist Charles Coulson (1910–1974), Weisberg (2008) has argued\nthat the structural conception of chemical bonding is not robust in\nquantum chemistry. This argument looks to the history of quantum\nmechanical models of molecular structure. In the earliest quantum\nmechanical models, something very much like the structural conception\nof bonding was preserved; electron density was, for the most part,\nlocalized between atomic centers and was responsible for holding\nmolecules together. However, these early models made empirical\npredictions about bond energies and bond lengths that were only in\nqualitative accord with experiment. \nSubsequent models of molecular structure yielded much better agreement\nwith experiment when electron density was “allowed” to\nleave the area between the atoms and delocalize throughout the\nmolecule. As the models were further improved, bonding came to be seen\nas a whole-molecule, not sub-molecular, phenomenon. Weisberg argues\nthat such considerations should lead us to reject the structural\nconception of bonding and replace it with a molecule-wide conception.\nOne possibility is the energetic conception of bonding that\nsays that bonding is the energetic stabilization of molecules.\nStrictly speaking, according to this view, chemical bonds do not\nexist; bonding is real, bonds are not (Weisberg 2008; also see\nCoulson 1952, 1960). \nThe challenges to the structural view of bonding have engendered\nseveral responses in the philosophical and chemical literatures. The\nfirst appeals to chemical practice: Chemists engaged in synthetic and\nanalytic activities rely on the structural conception of bonding.\nThere are well over 100,000,000 compounds that have been discovered or\nsynthesized, all of which have been formally characterized. How can\nthis success be explained if a central chemical concept such as the\nstructural conception of the bond does not pick out anything real in\nnature?  Throughout his life, Linus Pauling\n(1901–1994) defended this view. \nAnother line of objection comes from Berson (2008), who discusses the\nsignificance of very weakly bonded molecules. For example, there are\nfour structural isomers of 2-methylenecyclopentane-1,3-diyl. The most\nstable of the structures does not correspond to a normal bonding\ninteraction because of an unusually stable singlet state, a state\nwhere the electron spins are parallel. Berson suggests that this is a\ncase where “the formation of a bond actually produces a\ndestabilized molecule.” In other words, the energetic conception\nbreaks down because bonding and molecule-wide stabilization come\napart. \nFinally, the “Atoms in Molecules” program (Bader 1991;\nsee Gillespie and Popelier 2001, Chs. 6 & 7 for an exposition)\nsuggests that we can hold on to the structural conception of the bond\nunderstood functionally, but reject Lewis’ ideas about how electrons\nrealize this relationship. Bader, for example, argues that we can\ndefine ‘bond paths’ in terms of topological features of\nthe molecule-wide electron density. Such bond paths have physical\nlocations, and generally correspond closely to classical covalent\nbonds. Moreover they partially vindicate the idea that bonding\ninvolves an increase in electron density between atoms: a bond path is\nan axis of maximal electron density (leaving a bond path in a\ndirection perpendicular to it involves a decrease in electron\ndensity). There are also many technical advantages to this approach.\nMolecule-wide electron density exists within the ontology of quantum\nmechanics, so no quantum-mechanical model could exclude it. Further,\nelectron density is considerably easier to calculate than other\nquantum mechanical properties, and it can be measured empirically\nusing X-ray diffraction techniques. Figure 5. Too many bonds? 60 bond paths from each carbon atom in\nC60 to a trapped Ar atom in the interior.\n \nUnfortunately, Bader’s approach does not necessarily save the day for\nthe structural conception of the bond. His critics point out that his\naccount is extremely permissive and puts bond paths in places that\nseem chemically suspect. For example, his account says that when you\ntake the soccer-ball shaped buckminster fullerene molecule\n(C60) and trap an argon atom inside it, there are 60 bonds\nbetween the carbon atoms and the argon atom as depicted in Figure 5\n(Cerpa et al. 2008). Most chemists would think this implausible\nbecause one of the most basic principles of chemical combination is\nthe fact that argon almost never forms bonds (see Bader 2009 for a\nresponse). \nA generally acknowledged problem for the delocalized account is the\nlack of what chemists call transferability. Central to the structural\nview, as we saw, is the occurrence of functional groups common to\ndifferent substances. Alcohols, for example, are characterized by\nhaving the hydroxyl OH group in common. This is reflected in the\nstrong infra red absorption at 3600cm–1 being taken\nas a tell-tale sign of the OH group. But ab initio QM treatments just\nsee different problems posed by different numbers of electrons, and\nfail to reflect that there are parts of a molecular structure, such as\nan OH group, which are transferable from one molecule to another, and\nwhich they may have in common (Woody 2000, 2012). \nA further issue is the detailed understanding of the cause of chemical\nbonding. For many years, the dominant view, based on the\nHellman-Feynman theorem, has been that it is essentially an\nelectrostatic attraction between positive nuclei and negative electron\nclouds (Feynman 1939). But an alternative, originally suggested by\nHellman and developed by Rüdenberg, has recently come into\nprominence. This emphasizes the quantum mechanical analogue of the\nkinetic energy (Needham 2014). Contemporary accounts may draw on a\nnumber of subtle quantum mechanical features. But these details\nshouldn’t obscure the overriding thermodynamic principle governing\nthe formation of stable compounds by chemical reaction. As Atkins puts\nit, \nThe difficulties faced by this and every other model of bonding have\nled a number of chemists and philosophers to argue for pluralism.\nQuantum chemist Roald Hoffmann writes “A bond will be a bond by\nsome criteria and not by others … have fun with the concept and\nspare us the hype” (Hoffmann 2009, Other Internet Resources). \nWhile most of the philosophical literature about molecular structure\nand geometry is about bonding, there are a number of important\nquestions concerning the notion of molecular structure itself. The\nfirst issue involves the correct definition of molecular structure.\nTextbooks typically describe a molecule’s structure as the equilibrium\nposition of its atoms. Water’s structure is thus characterized by\n104.5º angles between the hydrogen atoms and the oxygen atom. But\nthis is a problematic notion because molecules are not static\nentities. Atoms are constantly in motion, moving in ways that we might\ndescribe as bending, twisting, rocking, and scissoring. Bader\ntherefore argues that we should think of molecular structure as the\ntopology of bond paths, or the relationships between the atoms that\nare preserved by continuous transformations (Bader 1991). \nA second issue concerning molecular structure is even more\nfundamental: Do molecules have the kinds of shapes and directional\nfeatures that structural formulas represent? Given the history we have\ndiscussed so far it seems like the answer is obviously yes. Indeed, a\nnumber of indirect experimental techniques including x-ray\ncrystallography, spectroscopy, and product analysis provide converging\nevidence of not only the existence of shape, but specific shapes for\nspecific molecular species. \nDespite this, quantum mechanics poses a challenge to the notion of\nmolecular shape. In quantum mechanical treatments of molecular\nspecies, shape doesn’t seem to arise unless it is put in by hand.\n(Woolley 1978; Primas 1981; Sutcliffe & Woolley 2012). \nThis tension the between the familiar theories of chemical structure\nand quantum- mechanical accounts of molecules might be resolved in\nseveral ways. One might embrace eliminativism about molecular\nstructure: Quantum mechanics is a more fundamental theory, we might\nargue, and its ontology has no place for molecular structure.\nTherefore, molecular structure doesn’t exist. No philosopher or\nchemist that we are aware of has endorsed this option. Another\npossible response makes a different appeal to the underlying physics.\nSomething must be breaking the wavefunction symmetries and giving\natoms locations in molecules. This might be interactions with other\nmolecules or interactions with measuring devices. Thus, molecular\nshape is partially constituted by interaction and is a relational, not\nintrinsic property (Ramsey 1997). \nA related option is a kind of pragmatism. Hans Primas argues that,\nstrictly speaking, a quantum mechanical description of a molecule has\nto be a whole-universe description. No matter how we draw the\nboundaries of interest around some target molecular system, in\nreality, the system is open and interacting with everything else in\nthe universe. Thus the shape of any particular molecule could be the\nresult of its interactions with anything else in the universe. We only\nget the paradox of molecules having no shape when we treat systems as\nclosed—say a single methane molecule alone in the universe. It\nis fine to treat open systems as closed for pragmatic purposes, but we\nshould always understand that this is an idealization. We shouldn’t\ntreat our idealizations, such as open systems being closed, as\nveridical. Hence there is no incompatibility between quantum mechanics\nand molecular shape (Primas 1981). \nSo despite the ubiquity of structural representations of molecules, it\nturns out that even the notion of molecular shape is not ambiguous.\nPrimas’ approach, which points to the idealization in many quantum\nmechanical models, is accepted by many chemists. But there is nothing\nlike a consensus in the philosophical literature about how to\nunderstand molecular shape. \nIn the final part of this section about structure, we consider a\nfavorite example of philosophers: the thesis that “Water is\nH2O.” This thesis is often taken to be\nuncontroversially true and is used as evidence for semantic\nexternalism and for essentialism about natural kinds (Kripke 1980;\nPutnam 1975,  1990). Since general theses about the theory of reference\nand semantic externalism are beyond the scope of this article, we\nfocus narrowly on chemical essentialism. Is having a common essential\nmicrostructure sufficient to individuate chemical kinds and explain\ntheir general features? And if so, is “being\nH2O” sufficient to individuate water? \nThe essentialist thesis is often stylized by writing “water =\nH2O” or “(all and only) water is\nH2O”. Ignoring the issue of whether the identity\nmakes sense (Needham 2000) and of understanding what the predicates\napply to in the second formulation (Needham 2010a), it is not clear\nthat either formulation expresses the kind of thesis that\nessentialists intend. “H2O” is not a\ndescription of any microstructure. Rather,\n“H2O” is a compositional formula, describing\nthe combining proportions of hydrogen and oxygen to make water. \nA reasonable paraphrase of the standard formulation would be\n“Water is a collection of H2O molecules.”\nHowever, although the expression “H2O molecule”\ndescribes a particular microentity, it by no means exhausts the kinds\nof microparticles in water, and says nothing of the\nmicrostructure by which they are related in water. Describing\nthe microstructure of water completely involves elaborating the\ndetails of this interconnected structure, as well as detailing how\nthey depend on temperature and pressure, and how they change over time\n(Finney 2004). \nLike many other substances, water cannot simply be described as a\ncollection of individual molecules. Here are just a few examples of\nthe complexities of its microstructure: water self-ionizes, which\nmeans that hydrogen and hydroxide ions co-exist with H2O\nmolecules in liquid water, continually recombining to form\nH2O molecules. At the same time, the H2O\nmolecules associate into larger polymeric species. Mentioning these\ncomplexities isn’t just pedantic because they are often what give rise\nto the most striking characteristics of substances. For example, the\nelectrical conductivity of water is due to a mechanism in which a\npositive charge (hydrogen ion) attaches at one point of a polymeric\ncluster, inducing a co-ordinated transfer of charge across the\ncluster, releasing a hydrogen ion at some distant point. The effect is\nthat charge is transferred from one point to another without a\ntransfer of matter to carry it. The hydrogen bonding underlying the\nformation of clusters is also at the root of many other distinctive\nproperties of water including its high melting and boiling points and\nits increase in density upon melting. As van Brakel has argued (1986,\n2000), water is practically the poster child for such\n“non-molecular” substances. \nMaybe water isn’t simply a collection of H2O molecules, but\nit certainly has a microstructure and perhaps the essentialist thesis\ncould be recast along the lines of “Water is whatever has its\nmicrostructure,” writing in the information that would save this\nfrom tautology. But this thesis still endorses the idea that\n“water” is a predicate characterized by what Putnam calls\nstereotypical features. This neglects the importance of macroscopic,\nyet scientifically important, properties such as boiling points,\nspecific heats, latent heats, and so on, from which much of the\nmicrostructure is actually inferred. Indeed, many of the criteria that\nchemists use to determine the sameness and purity of substances are\nmacroscopic, not microscopic. In fact, international standards for\ndetermining the purity of substances like water depend on the careful\ndetermination of macroscopic properties such as the triple-point, the\ntemperature and pressure where liquid, gaseous, and solid phases exist\nsimultaneously (Needham 2011). \nSo is water H2O? In the end, the answer to this question\ncomes down to how one interprets this sentence. Many chemists would be\nsurprised to find out that water wasn’t H2O, but perhaps\nthis is because they read “H2O” as a shorthand\n(Weisberg 2005) or as a compositional formula in the manner we\ndiscussed in the opening of this section. Water is actually\ncharacterized by making reference to both its microstructural and\nmacroscopic features, so this can’t on its own provide a justification\nfor microessentialism. \nFor these reasons, microessentialist claims would need to be grounded\nin chemical classification and explanation: the systems of\nnomenclature developed by IUPAC are based entirely on microstructure,\nas are theoretical explanations of the chemical and spectroscopic\nbehaviour of substances (see Hendry 2016). On the other hand,\nH2O content fails to track usage of the term\n“water” by ordinary-language speakers, who seem to have\ndifferent interests to chemists (Malt 1994). Pluralism is one\nresponse to these tensions: Hasok Chang (2012) urges that even within\nscience, water’s identity with H2O should be left open;\nJulia Bursten (2014) tries to reconcile the special role of\nmicrostructure in chemistry with the failure of microessentialism; and\nJoyce Havstad (2018) argues that chemists’ use of substance concepts\nis just as messy and disunified as biologists’ use of various species\nconcepts. \nOur discussion so far has focused on “static” chemistry:\naccounts of the nature of matter and its structure. But much of\nchemistry involves the transformation of matter from one form to\nanother. This section describes the philosophical issues surrounding\nthe synthesis of one substance from another, as well as chemical\nmechanisms, the explanatory framework chemists use to describe these\ntransformations. \nThere has been a profusion of discussion in the literatures of\nphilosophy of biology and philosophy of neuroscience about the notion\nof mechanisms and mechanistic explanations (e.g., Machamer, Darden,\n& Craver 2000). Yet the production of mechanisms as explanatory\nschemes finds its original home in chemistry, especially organic\nchemistry. Chemical mechanisms are used to classify reactions into\ntypes, to explain chemical behavior, and to make predictions about\nnovel reactions or reactions taking place in novel circumstances\n(Weininger 2014). \nGoodwin (2012) identifies two notions of chemical mechanism at play in\nchemistry. The first or thick notion of mechanism is like a\nmotion picture of a chemical reaction. Such a mechanism traces out the\npositions of all of the electrons and atomic cores of some set of\nmolecules during the course of a reaction, and correlates these\npositions to the potential energy or free energy of the system. One\nmight think of this as an ideal reaction mechanism, as it would\ncontain all information about the time course of a chemical\nreaction. \nIn contrast, the thin notion of a reaction mechanism focuses\non a discrete set of steps. In each step, a set of reactive\nintermediates are generated. These intermediates are quasi-stable\nmolecular species that will ultimately yield the products of the\nreaction. For example, the much studied biomolecular nucleophilic\nsubstitution (SN2) reaction is said to have a single\nreactive intermediate with the incoming nucleophile and outgoing\nleaving group both partially bonded to the reactive carbon center (see\nFigure 6). Such a description of the reaction mechanism is not only\nabstract in that it leaves out much detail, but it is also highly\nidealized. Reactions do not take actually place as a series of\ndiscrete steps, each of which generates a quasi-stable reaction\nintermediate. Figure 6. Thin reaction mechanism for the SN2 reaction.\n \nWhile most textbook treatments of reaction mechanisms begin by\nmentioning the thick notion, the details nearly always turn to thin\nnotions of mechanism. At the same time, formal theoretical treatments\nof reaction mechanisms deal exclusively with the thick notion. Such\ntreatments often attempt to calculate the potential energy function\nfor the reaction from quantum mechanics. Given the importance of the\nthick notion to formal chemical theorizing, why does the thin notion\ndominate the practice of chemistry and find expression in textbooks\nand research articles? \nPart of the reason that thin reaction mechanisms are widely used is\nthat determining thick reaction mechanisms is essentially impossible\nexperimentally, and extremely difficult theoretically. But this cannot\nbe the whole story because when necessary, chemists have been able to\nproduce relevant portions of the thick mechanism. \nAlternatively, Goodwin (2012, p. 311) has argued that, given the\nexplanatory and predictive goals of chemists, not all of the thick\nmechanism is needed. In fact, only a characterization of specific\nstructures, the transition state and stable reactive intermediates,\nare necessary to produce chemical explanations and predictions.\nConstructing mechanisms as a discrete series of steps between stable\nand reactive structures allows the chemist: \nSo chemists’ explanatory goals require that specific features of\nreaction mechanisms can be identified. The rest of the thick mechanism\nwouldn’t necessarily add any explanatorily relevant detail to the\nexplanation. \nChemists typically do not engage in philosophical discussions about\ntheir work. Yet, when discussing the confirmation of reaction\nmechanisms, it is not uncommon to see mention of philosophical issues\nsurrounding confirmation. So why does the study of reaction mechanisms\nmake chemists more philosophically reflective? \nFor one thing, almost all studies aimed at elucidating reaction\nmechanisms rely on indirect techniques. Ideally, elucidating a\nreaction mechanism would be like doing experiments in biomechanics.\nSlow motion video could give direct information about the movement of\nparts and how these movements give rise to the motion of the whole.\nBut we have nothing like a video camera for chemical reactions.\nInstead, after an experimental determination of the reaction products\nand possible isolation of stable intermediate species, chemists rely\non measurements of reaction rates in differing conditions,\nspectroscopy, and isotopic labeling, among other techniques. These\ntechniques help eliminate candidate reaction mechanisms, but do not\nthemselves directly suggest new ones. This emphasis on eliminating\npossibilities has led some chemists to endorse a Popperian,\nfalsificationist analysis of reaction mechanism elucidation (e.g.,\nCarpenter 1984). \nAlthough some chemists have been attracted to a falsificationist\nanalysis, a better analysis of reaction mechanism elucidation is the\naccount of confirmation known as eliminative induction. This\naccount shares falsification’s emphasis on trying to reject\nhypotheses, but argues that the hypotheses not rejected receive some\ndegree of confirmation. So in the case of reaction mechanisms, we\nmight see eliminative induction as a processes whereby chemists: \nIn following this procedure, chemists do more than simply falsify:\nthey add confirmatory power to the mechanisms that haven’t been\neliminated. Indeed, in discussing similar issues, biochemist John\nPlatt (1964) argued that good scientific inference is strong\ninference, whereby the goal in an experiment is to eliminate one\nor more hypotheses. Several contemporary philosophers have endorsed\nthe role of eliminative induction in science (e.g., Bird 2010, Dorling\n1995, Kitcher 1993, Norton 1995). It is easy to see how it can be\nmodeled in Bayesian and other quantitative frameworks for\nconfirmation. Specifically, as particular candidate reaction\nmechanisms are eliminated, the probability that one of the remaining\nmechanisms is correct goes up (see Earman 1992 for details). \nOne difficulty with eliminative induction is the source of the\nrelevant alternative hypotheses, in this case reaction mechanisms.\nThere is no algorithmic procedure for generating these mechanisms, and\nthere is always the possibility that the correct mechanism has not\nbeen considered at all. This is a genuine problem, and we believe that\nit is the very issue that motivates chemists to turn towards\nfalsification when thinking about mechanisms; all they can do is\nevaluate the plausible mechanisms that they have thought of. However,\nwe see eliminative induction as a more plausible reflection of the\nepistemic situation of mechanistic chemists. This problem is not\nuncertainty about mechanisms compatible with\nexperiments—chemists have evidence that weighs in favor of\nthose. Rather, the problem is with unconceived alternatives. Structure\noffers one way to delineate such mechanistic possibilities: Hoffmann\n(1997, Chapter 29) provides a beautiful example of explicitly\neliminative reasoning in his discussion of how H. Okabe and J. R.\nMcNesby used isotopic labelling to eliminate two out of three possible\nmechanisms for the photolysis of ethane to ethylene. But this is an\nissue in all parts of science, not just mechanistic chemistry, and\neliminative induction has always played a role in chemists’ reasoning\nabout structure. How did van ’t Hoff argue for the tetrahedral\ncarbon atom? He argued first that it was possible to account for the\nobserved number and variety of the isomers of certain organic\nsubstances only by taking into account the arrangement of atoms in\nspace. He then defended a tetrahedral geometry for the carbon atom by\nrejecting a square planar arrangement: if carbon’s geometry were\nsquare planar, there would be more isomers of substituted methane than\nare observed. Thus, for instance, disubstituted methane (of the form\nCH2X2) should have two separable isomers if it\nis square planar, whereas only one can be found. Assuming a\ntetrahedral arrangement, in contrast, would be in accord with the\nobserved number of isomers (Brock 1992). \nIn his classic discussion, Hans Reichenbach distinguished between the\ncontext of discovery and the context of\njustification. His distinction was intended to highlight the fact\nthat we could have a logical analysis of scientific justification in\nthe form of confirmation theory, but there could never be a logical\nprocedure for generating hypotheses. Hypothesis generation is the\ncreative part of science, while confirmation is the logical part. This\ndistinction has been challenged in recent years by those that see the\npath of discovery contributing to justification. But chemistry\nprovides a more interesting challenge to Reichenbach: It apparently\ngives us logics of discovery. \nThere are two subfields in which chemists sometimes speak of logics or\nprocedures for discovery. The first is synthetic chemistry. E.J. Corey\n(Corey & Cheng 1989) has proposed that the synthesis of organic\nmolecules can be rationally planned according to the logic of\nretrosynthetic analysis. Systematizing a long tradition in\nsynthetic organic chemistry, Corey shows how one can reason backwards\nfrom a target molecule by finding a series of\n“disconnections,” bonds which one knows how to make. The\nresulting tree of disconnections gives potential pathways for\nsynthesis that can then be evaluated by plausibility, or simply tried\nout in the laboratory. \nAnother area where chemists have developed a logic for discovery is in\nthe area of drug design. Murray Goodman (Goodman & Ro 1995), for\nexample, proposed a four-step procedure for developing candidate\nmolecules for new medication. Say that you were interested in making a\ndrug that would more effectively target one of the morphine receptors\nin the brain. You start by making a molecular analogue of morphine,\nperhaps with a more constrained structure. After successful synthesis,\nyou study the molecule’s three-dimensional structure by spectroscopy\nand computer simulation. You then test your molecule in a biological\nassay to see if you have successfully targeted the receptor and to\nwhat extent. Then based on the information you get, you modify the\nstructure, hopefully improving in each iteration. \nThese examples from chemistry put pressure on Reichenbach’s claim that\nthere cannot be a logic of discovery. Moreover, they illustrate how,\nwhen a science is concerned with creating new things, procedures for\ndiscovery may become essential. \nOne of the perennial topics in philosophy of science concerns\ninter-theoretic relations. In the course of debating whether biology\nis reducible to the physical sciences or whether psychology is\nreducible to biology, many philosophers assume that chemistry has\nalready been reduced to physics. In the past, this assumption was so\npervasive that it was common to read about\n“physico/chemical” laws and explanations, as if the\nreduction of chemistry to physics was complete. Although most\nphilosophers of chemistry would accept that there is no conflict\nbetween the sciences of chemistry and physics (Needham 2010b), many\nwould reject a stronger unity thesis. Most believe that chemistry has\nnot been reduced to physics nor is it likely to be (see Le Poidevin\n2005, for the opposite view, and Hendry & Needham 2007, for a\nrejoinder). \nWhen thinking about the question of reducibility in chemistry, it is\nuseful to break this question into two parts: The first, and more\nfamiliar one to philosophers, concerns the relationship between\nelements, atoms, molecules, and the fundamental particles of physics.\nWe might ask, “Are atomic and molecular species reducible to\nsystems of fundamental particles interacting according to quantum\nmechanics?” A second, less familiar question concerns the\nrelationship between the macroscopic and microscopic descriptions of\nchemical substances. “Are chemical substances reducible to\nmolecular species?” Here, the main question is whether all\nchemical properties that have been defined macroscopically can be\nredefined in terms of the properties of atoms, molecules, and their\ninteractions. \nBogaard (1978), Scerri (1991, 1994) and Hendry (1998) have all\nquestioned the possibility of fully reducing chemical theories about\natoms and molecules to quantum mechanics. Bogaard argues that many key\nchemical concepts such as valence and bonding do not find a natural\nhome in quantum mechanics. In a similar spirit, Scerri points out that\nthe quantum mechanical calculations of atomic spectra standardly\npresented in chemistry textbooks make highly idealized assumptions\nabout the structure of many-electron systems. These approximations are\nwell-motivated on pragmatic grounds. However, they do not allow\nquantum mechanics to “approximately reduce” chemical\nfacts, because the errors introduced by these approximations cannot be\nestimated (Scerri 1991, 1994). Further, one of the most important\nchemical trends, the length of periods in the Periodic Table, cannot\nbe derived from quantum mechanics, unless experimentally derived\nchemical information is specifically introduced (Scerri 1997).\nDrawing on the work of Woolley (1978) and Primas (1981), Hendry (1998)\nargues that there are principled difficulties in accommodating\nmolecular shape within quantum mechanics: the Born-Oppenheimer\napproximation effectively adds structure by hand. Although quantum\nchemistry can be extremely illuminating, these authors argue that it\nhas not reduced chemistry to physics. \nIf one thinks that reduction means deriving the phenomenon of the\nhigher level exclusively from the lower level, then these arguments\nshould settle the question of reduction. More than 80 years after the\ndiscovery of quantum mechanics, chemistry has not been reduced to it.\nBut there are two possible reductionist responses to this\nargument. \nFirst, reductionists can argue that there are no principled reasons\nthat chemical phenomena have not been derived from quantum mechanics.\nThe problem is a lack of computational power and appropriate\napproximation schemes, not anything fundamental. Schwarz (2007) has\nmade this argument against Scerri, claiming that the electronic\nstructure of atoms, and hence the Periodic Table, is in principle\nderivable from quantum mechanics. He believes that quantum chemistry’s\ninability to reduce chemical properties is simply a manifestation of\nthe problems shared by all of the computationally complex sciences.\nDebate then turns to the plausibility of such “reducibility in\nprinciple” claims. \nThere are also arguments that focus, at least implicitly, on\nchemistry’s ontology. A well-known strand of contemporary metaphysics\ndefends physicalism, the doctrine that everything in the\nuniverse is physical (see the entry on\n physicalism).\n According to the physicalist, chemistry is “nothing but”\nphysics, even though chemical explanations and theories are not\nderivable from physics. The physical world is simply composed of the\nfundamental particles of physics. Chemical entities and their\nproperties have no independent reality. \nThe status of arguments for physicalism and the supervenience of\neverything on the physical are contentious within metaphysics proper,\nbut beyond the scope of this entry. Yet we think that the failure of\nchemical theory to be fully derivable from physics raises interesting\nquestions about the doctrine of physicalism. Minimally, it points to\nlongstanding worries that the domain of the physical is not\nwell-defined. If chemical entities such as molecules and ions end up\nbeing part of the physical ontology, one might argue that this was not\na case of the reduction of chemistry to physics at all but simply the\nexpansion of the ontology of physics to encompass the ontology of\nchemistry. \nIndependent studies of the ontology of chemistry on the basis of\nmereology have been undertaken by several authors (Earley 2005,\nHarré and Llored 2011, Needham 2010a). In disputing Scerri’s\n(2000, 2001) argument against claims (Zuo et al. 1999) that orbitals\nhave been observed, Mulder (2010) appeals to a general ontological\ndistinction between entities, which can appropriately be said to\nexist, and states which don’t exist independently but are features of\nentities that exist. Ostrovsky (2005) and Schwarz (2006) take issue\nwith the role of approximations in Scerri’s argument. \nMore controversially, some philosophers of chemistry have argued that\nchemical properties may constrain the behavior of physical systems,\nsomething akin to what philosophers of mind call strong emergence, or\ndownwards causation (Kim 1999). While acknowledging the central role\nof quantum mechanics in understanding structure, Hendry argues that in\nsome cases, molecular structure is an unexplained explainer. The issue\narises when we consider the quantum-mechanical description of\nstructural isomers, molecules with the same atoms, but with different\nmolecular structures. For example, dimethyl ether and ethanol share a\nHamiltonian, the quantum mechanical description of their physical\nstates. Nevertheless, they are very different molecules. Ethanol is\nextremely soluble in water, whereas dimethyl ether is only partially\nsoluble in water. Ethanol boils at 78.4°C, while dimethyl ether\nboils at 34.6°C. Drinking ethanol leads to intoxication, while\ndimethyl ether is toxic in quite different ways. Quantum mechanics can\nshow how each of these structures is energetically stable, and\nilluminate how they interact with other molecules and radiation to\nexplain the chemical and spectroscopic behaviour of ethanol and\ndimethyl ether, but the different structures are introduced as\nunexplained initial conditions. While he acknowledges that these facts\nare not incompatible with the claim that structure is reducible,\nHendry argues that strong emergence is just as plausible an\ninterpretation as reduction of the explanatory relationship between\nchemistry and quantum mechanics (2006b, 2010a). \nSo far we have considered intertheoretic relationships between\nchemistry and physics. What about within chemistry itself? Do the\nmacroscopic and microscopic theories of chemistry align perfectly? Are\nall macroscopic properties of substances ultimately reducible to\nmicroscopic properties? In other words, if we have a macroscopic\ndescription of matter and a thermodynamic theory about how it behaves,\ncan all of this be reduced to a molecular description? The answer has\nseemed to be “yes” to many philosophers and chemists, but\nphilosophers of chemistry have urged caution here. \nConsider first the relatively simple case of gas temperature, which\nhas often been supposed reducible to the average kinetic energy of the\ngas’s molecules (cf. Nagel 1961, p. 343). A particular average\nkinetic energy of the molecules is only a necessary condition for\nhaving a given temperature, however. Only gases at equilibrium have a\ndefinite temperature, when all the spatial parts have the same\ntemperature as the whole (reflecting the fact that temperature is an\nintensive property). A sufficient condition would need to complement\nthe average kinetic energy with a microscopic correlate of the\nmacroscopic condition of being at thermodynamic equilibrium.\nStatistical mechanics specifies the relevant correlative condition as\nthat of the energy being distributed over the gas molecules in\naccordance with the Boltzmann distribution. But the Boltzmann\ndistribution is expressed as a function of the temperature, and its\nderivation from Boltzmann’s microscopic construal of entropy appeals\nto the thermodynamic law connecting entropy with temperature.\nAccordingly, the necessary and sufficient microscopic condition for\ngas temperature becomes circular when construed as a reduction of the\nconcept of temperature (Needham 2009b; Bishop 2010) \nAlthough the reduction of temperature to microscopic properties is\nproblematic, it is a relatively easy candidate for reduction.\nProperties concerned with chemical changes such as phase transitions,\nsolubility, and reactivity, are considerably more complex. As we\ndiscussed in Section 4.5, a purely microscopic description of matter\nis not coextensive with all chemical properties. Solubility, for\nexample, is not fully explained by microscopic properties. While we\ncan explain in rough qualitative fashion that substances dissolve when\ntheir ions or molecules have more affinity for the solvent than they\ndo for each other, this doesn’t recover the subtle, quantitative\nfeatures of solubility. It also leaves the solubility of nonionic\nsubstances untouched. Predicting these features requires appeals to\nthermodynamics, and the alleged reduction of thermodynamics to\nstatistical mechanics is considered highly contentious (Sklar\n1993). \nAs we have seen in this case, even very fruitful applications of\nphysical and chemical theory at the microscopic level are often\ninsufficient to reduce chemically important properties. Whether the\ngeneral notion of chemical substance, or the property of being a\nparticular substance for each of the millions of known substances, can\nbe reduced to microstructure needs to be demonstrated and not merely\nassumed. While there is no in-principle argument that reductions will\nalways be impossible, essential reference is made back to some\nmacroscopically observable chemical property in every formal attempt\nof reduction that we are aware of. In the absence of definite\narguments to the contrary, it seems reasonable to suppose that\nchemistry employs both macroscopic and microscopic concepts in\ndetailed theories which it strives to integrate into a unified view.\nAlthough plenty of chemistry is conducted at the microscopic level\nalone, macroscopic chemical properties continue to play important\nexperimental and theoretical roles throughout chemistry. \nIn the background of all of these debates about chemical reduction are\nissues concerning the criteria for successful reduction. All of the\nliterature that we have discussed make explicit or implicit reference\nto Nagel’s influential account of reduction. Beyond the philosophy of\nchemistry literature, this account has also been presupposed by\ncritics of particular reductionist theses (e.g. Davidson 1970), even\nwhen making points about the inapplicability of Nagel’s account to\nparticular sciences (Kitcher 1984). But Nagel’s account of reduction\nis thought by many to be unrealistic and inapplicable to actual\nscience because of the logical requirements it assumes. \nPerhaps part of the anti-reductionist consensus in the philosophy of\nchemistry literature is driven by the stringent demands of Nagel’s\naccount. But even if Nagel’s account is weakened to allow\napproximative arguments (as Hempel modified his DN model of\nexplanation), as some advocates of reductionism have urged (e.g.,\nSchaffner 1967; Churchland 1985), this still doesn’t circumvent the\nproblem of the appeal to macroscopic properties in the explanation of\nmicroscopic properties. Current chemical theory makes essential\nreference to both microscopic and macroscopic chemical concepts with\nboth chemical and quantum mechanical origins. We know of no convincing\nsubstantial examples where either of these aspects have been entirely\nexcised. \nAlmost all contemporary chemical theorizing involves modeling, the\nindirect description and analysis of real chemical phenomena by way of\nmodels. From the 19th century onwards, chemistry was\ncommonly taught and studied with physical models of molecular\nstructure. Beginning in the 20th century, mathematical\nmodels based on classical and quantum mechanics were successfully\napplied to chemical systems. This section discusses some of the\nphilosophical questions that arise when we consider modeling in\nchemistry more directly. \nChemistry’s modeling tradition began with physical models of atoms and\nmolecules. In contemporary chemical education, much emphasis is placed\non the construction and manipulation of such models. Students in\norganic chemistry classes are often required to purchase plastic\nmolecular modeling kits, and it is not uncommon to see complex\nmolecular structures built from such kits in professional laboratory\nsettings. \nThe use of molecular models gained special prominence in the middle of\nthe 19th, helping chemists to understand the significance\nof molecular shape (Brock 2000). While such structures could be\nrepresented on paper, physical models gave an immediacy and an ease of\nvisualization that sketches alone did not provide. In the middle of\nthe twentieth century, the discovery of the double helical structure\nof DNA was aided by the manipulation of physical models (Watson\n1968). \nWhile physical modeling has been important historically, and is still\na central part of chemical education and some investigations in\nstereochemistry, contemporary chemical models are almost always\nmathematical. Families of partially overlapping, partially\nincompatible models such as the valence bond, molecular\norbital, and semi-empirical models are used to explain\nand predict molecular structure and reactivity. Molecular\nmechanical models are used to explain some aspects of reaction\nkinetics and transport processes. And lattice models are used\nto explain thermodynamic properties such as phase. These and other\nmathematical models are ubiquitous in chemistry textbooks and\narticles, and chemists see them as central to chemical theory. \nChemists are very permissive about which kinds of mathematical\nstructures can serve as models. But while just about any kind of\nmathematical structure can serve as a chemical model, different types\nof systems lend themselves to particular kinds of mathematical\nstructures used in modeling. For example, the most common kinds of\nmathematical structures employed in quantum chemistry are state\nspaces, which typically correlate sub-molecular particle distances\nwith the total energy of chemical systems. Other parts of chemical\nmodeling are dynamic, hence they employ trajectory spaces, which can\nrepresent the course of a reaction over time. Still other kinds of\nmathematical structures such as graphs and groups can be employed to\nmodel molecular structure and symmetry. \nThe purpose of many exercises in chemical modeling is to learn about\nreal systems. In these cases, the model must bear certain\nrelationships to real-world systems. But these relationships needn’t\nalways be of extremely high fidelity. For example, Linus Pauling\n(1939) and early proponents of the simple valence bond model believed\nthat this model captured the essential physical interactions that give\nrise to chemical bonding. This method is closely related to Lewis’\nconception of bonding, treating molecules as composed of atomic cores\n(nuclei together with inner-shell electrons) and valence electrons\nwhich give rise to localized bonds. It stands in contrast to the\nmolecular orbital method, which doesn’t localize the bonding electrons\nto any particular part of the molecule. Modern quantum chemists think\nof the valence bond model as a template for building models of greater\ncomplexity. Thus if a modern quantum chemist deploys the simple\nvalence bond model to study a real molecule, she does so with a much\nlower standard of fidelity than Pauling would have. Her use of the\nmodel is only intended to give a first approximation to the most\nimportant features of the system. \nMuch of contemporary theoretical research in chemistry involves the\napplication of quantum mechanics to chemistry. While exact solutions\nto the quantum mechanical descriptions of chemical phenomena have not\nbeen achieved, advances in theoretical physics, applied mathematics,\nand computation have made it possible to calculate the chemical\nproperties of many molecules very accurately and with few\nidealizations. The approach of striving for ever more accurate\ncalculations with decreasing levels of idealization is endorsed by\nmany quantum chemists. For example, the development team of Gaussian,\none of the leading packages for doing quantum chemical calculations,\nexplicitly endorses this position. While they admit that there are\nmany considerations that enter into the choice of the degree of\napproximation or “level of theory” for any calculation,\nthe goal is to de-idealize the models as much as possible. They argue\nthat quantum chemical calculations which are arbitrarily close to the\nexact solutions are the “limit to which all approximate methods\nstrive” (Foresman & Frisch 1996). \nThis method of developing chemical theory relies on a systematic\nrefinement of theories, attempting to bring them closer to the truth.\nPhilosophers of science have called this process Galilean\nidealization, because as in Galileo’s work, idealizations are\nintroduced for reasons of tractability and are removed as soon as\npossible (McMullin 1985; Weisberg 2007b). But not all chemists have\nshared this focus on ever more accurate calculations. Reflecting on\nwhy he didn’t choose this path in his own career, theorist Roald\nHoffmann wrote: \nElsewhere in this article, Hoffmann admits that quantum chemistry is\nenormously successful in its predictive power, and continues to give\nus better approximations to the fundamental theory. Yet the attitude\nexpressed in this paragraph seems to be that simple, idealized models\nare needed for chemical theorizing. Thus, the central philosophical\nquestion arises: Given the availability of models that are closer to\nthe truth, why work with idealized ones? \nOne answer is given by Felix Carroll, a physical organic chemist: \nCarroll does not elaborate on these issues, but this passage contains\nthe central message: Simple models prevent our theories from having a\n“black-box” character, meaning that they will not simply\nbe a recipe for calculating without giving any physical insight.\nCarroll claims that simple models are necessary in order to expose the\nmechanisms by which chemical phenomena come about. High-level\ntheoretical calculations are not capable of showing us these\nmechanistic relationships, even though they are based on the quantum\nmechanical principles that describe the fundamental physics of the\nsystem. Or, as Hoffmann puts the point: “[I]f understanding is\nsought, simpler models, not necessarily the best in predicting all\nobservables in detail, will have value. Such models may highlight the\nimportant causes and channels” (Hoffmann, Minkin, &\nCarpenter 1996). \nWhy should it be the case that simple models have less black-box\ncharacter than others? One explanation appeals to our cognitive\nlimitations. We can only hold a couple of steps of an argument in our\nmind at once. Modern, high-level calculations can take hours or days\nto compute using fast computers. Even if every step was made explicit\nby the computer, it would be impossible to hold the calculational\nsteps in mind and hence hard to understand the reason for the result,\neven if one was convinced that the answer was correct. Paul Humphreys\nhas called this the epistemic opacity of simulations\n(2004). \nThere is a second reason for employing simple, more highly idealized\nmodels in chemistry, which stems from the explanatory traditions of\nchemistry. In developing this point, Hoffmann argues that there are\ntwo modes of explanation that can be directed at chemical systems:\nhorizontal and vertical (Hoffmann 1997). Vertical\nexplanations are what philosophers of science call deductive\nnomological explanations. These explain a chemical phenomenon by\nderiving its occurrence from quantum mechanics. Calculations in\nquantum chemistry are often used to make predictions, but insofar as\nthey are taken to explain chemical phenomena, they follow this\npattern. By showing that a molecular structure is stable, the quantum\nchemist is reasoning that this structure was to be expected given the\nunderlying physics. \nIn contrast with vertical mode, the horizontal mode of explanation\nattempts to explain chemical phenomena with chemical concepts. For\nexample, all first year organic chemistry students learn about the\nrelative reaction rates of different substrates undergoing the\nSN2 reaction. An organic chemist might ask “Why does\nmethyl bromide undergo the SN2 reaction faster than methyl\nchloride?” One answer is that “the leaving group\nBr− is a weaker base than Cl−, and\nall things being equal, weaker bases are better leaving groups.”\nThis explains a chemical reaction by appealing to a chemical property,\nin this case, the weakness of bases. \nHoffmann doesn’t say much about the differing value of the horizontal\nand vertical explanations, but one important difference is that they\ngive us different kinds of explanatory information. Vertical\nexplanations demonstrate that chemical phenomena can be derived from\nquantum mechanics. They show that, given the (approximate) truth of\nquantum mechanics, the phenomenon observed had to have happened.\nHorizontal explanations are especially good for making\ncontrastive explanations, which allows the explanation of\ntrends. Consider again our example of the rate of an SN2\nreaction. By appealing to the weakness of Br− as a\nbase, the chemist invokes a chemical property, shared across other\nmolecules. This allows her to explain methyl bromide’s reactivity as\ncompared to methyl chloride, and also methyl fluoride, methyl iodide,\netc. Insofar as chemists want to explain trends, they make contrastive\nexplanations using chemical concepts. \nReflecting on the nature of chemical theorizing, the eminent chemical\ntheorist Charles Coulson (1910–1974) makes a similar point. He\nwrote: \nAlthough Coulson, Carroll, and Hoffmann defend the use of simple,\nidealized models to generate horizontal explanations, it is not clear\nthat quantum calculations can never generate contrastive explanations.\nAlthough single vertical explanations are not contrastive, a theorist\ncan conduct multiple calculations and in so doing, generate the\ninformation needed to make contrastive explanations. Many of the best\nexamples of quantum chemistry have this character: a series of closely\nrelated calculations, attempting to get at chemically relevant\ntrends.","contact.mail":"paul.needham@philosophy.su.se","contact.domain":"philosophy.su.se"},{"date.published":"2011-03-14","date.changed":"2019-01-16","url":"https://plato.stanford.edu/entries/chemistry/","author1":"Michael Weisberg","author1.info":"http://www.phil.upenn.edu/~weisberg","author2.info":"http://people.su.se/~pneedham/PNEng.html","entry":"chemistry","body.text":"\n\n\nChemistry is the study of the structure and transformation of matter.\nWhen Aristotle wrote the first systematic treatises on chemistry in\nthe 4th century BCE, his conceptual grasp of the nature of\nmatter was tailored to accommodate a relatively simple range of\nobservable phenomena. In the 21st century, chemistry has\nbecome the largest scientific discipline, producing over half a\nmillion publications a year ranging from direct empirical\ninvestigations to substantial theoretical work. However, the\nspecialized interest in the conceptual issues arising in chemistry,\nhereafter Philosophy of Chemistry, is a relatively recent\naddition to philosophy of science.\n\n\nPhilosophy of chemistry has two major parts. In the first, conceptual\nissues arising within chemistry are carefully articulated and\nanalyzed. Such questions which are internal to chemistry include the\nnature of substance, atomism, the chemical bond, and synthesis. In the\nsecond, traditional topics in philosophy of science such as realism,\nreduction, explanation, confirmation, and modeling are taken up within\nthe context of chemistry.\n\nOur contemporary understanding of chemical substances is elemental and\natomic: All substances are composed of atoms of elements such as\nhydrogen and oxygen. These atoms are the building blocks of the\nmicrostructures of compounds and hence are the fundamental units of\nchemical analysis. However, the reality of chemical atoms was\ncontroversial until the beginning of the 20th century and\nthe phrase “fundamental building blocks” has always\nrequired careful interpretation. So even today, the claim that all\nsubstances are composed of elements does not give us sufficient\nguidance about the ontological status of elements and how the elements\nare to be individuated. \nIn this section, we will begin with the issue of elements.\nHistorically, chemists have offered two answers to the question\n“What is it for something to be an element?” \nThese two theses describe elements in different ways. In the first,\nelements are explicitly identified by a procedure. Elements are simply\nthe ingredients in a mixture that can be separated no further. The\nsecond conception is more theoretical, positing elements as\nconstituents of composite bodies. In the pre-modern Aristotelian\nsystem, the end of analysis thesis was the favored option. Aristotle\nbelieved that elements were the building blocks of chemical\nsubstances, only potentially present in these substances. The modern\nconception of elements asserts that they are actual components,\nalthough, as we will see, aspects of the end of analysis thesis\nlinger. This section will explain the conceptual background behind\nchemistry’s progression from one conception to the other. Along the\nway, we will discuss the persistence of elements in chemical\ncombination, the connection between element individuation and\nclassification, and criteria for determining pure substances. \nThe earliest conceptual analyses concerning matter and its\ntransformations come in the Aristotelian tradition. As in modern\nchemistry, the focus of Aristotle’s theories was the nature of\nsubstances and their transformations. He offered the first systematic\ntreatises of chemical theory in On Generation and Corruption\n(De Generatione et Corruptione), Meteorology, and\nparts of Physics and On the Heavens (De\nCaelo). \nAristotle recognized that most ordinary, material things are composed\nof multiple substances, although he thought that some of them could be\ncomposed of a single, pure substance. Thus, he needed to give a\ncriterion of purity that would individuate a single substance. His\ncriterion was that pure substances are homoeomerous: they are\ncomposed of like parts at every level. “[I]f combination has\ntaken place, the compound must be uniform—any part of\nsuch a compound is the same as the whole, just as any part of water is\nwater” (De Generatione et Corruptione,\nhenceforth DG, I.10,\n328a10ff).[1] So\nwhen we encounter diamond in rock, oil in water, or smoke in air,\nAristotelian chemistry tells us that there is more than one substance\npresent. \nLike some of his predecessors, Aristotle held that the elements Fire,\nWater, Air, and Earth were the building blocks of all substances. But\nunlike his predecessors, Aristotle established this list from\nfundamental principles. He argued that “it is impossible for the\nsame thing to be hot and cold, or moist and dry … Fire is hot\nand dry, whereas Air is hot and moist …; and Water is cold and\nmoist, while Earth is cold and dry” (DG II.3,\n330a30–330b5). Aristotle supposed hot and moist to be maximal\ndegrees of heat and humidity, and cold and dry to be minimal degrees.\nNon-elemental substances are characterized by intermediate degrees of\nthe primary qualities of warmth and humidity. \nAristotle used this elemental theory to account for many properties of\nsubstances. For example he distinguished between liquids and solids by\nnoting the different properties imposed by two characteristic\nproperties of elements, moist and dry. “[M]oist is that which,\nbeing readily adaptable in shape, is not determinable by any limit of\nits own; while dry is that which is readily determinable by its own\nlimit, but not readily adaptable in shape” (DG II.2,\n329b30f.). Solid bodies have a shape and volume of their own, liquids\nonly have a volume of their own. He further distinguished liquids from\ngases, which don’t even have their own volume. He reasoned that while\nwater and air are both fluid because they are moist, cold renders\nwater liquid and hot makes air gas. On the other hand, dry together\nwith cold makes earth solid, but together with hot we get fire. \nChemistry focuses on more than just the building blocks of substances:\nIt attempts to account for the transformations that change substances\ninto other kinds of substances. Aristotle also contributed the first\nimportant analyses of this process, distinguishing between\ntransmutation, where one substance overwhelms and eliminates\nanother and proper mixing. The former is closest to what we\nwould now call change of phase and the latter to what we would now\ncall chemical combination. \nAristotle thought that proper mixing could occur when substances of\ncomparable amounts are brought together to yield other substances\ncalled\n ‘compounds.’[2]\n Accordingly, the substances we typically encounter are compounds, and\nall compounds have the feature that there are some ingredients from\nwhich they could be made. \nWhat happens to the original ingredients when they are mixed together\nto form a compound? Like modern chemists, Aristotle argued that the\noriginal ingredients can, at least in principle, be obtained by\nfurther transformations. He presumably knew that salt and water can be\nobtained from sea water and metals can be obtained from alloys. But he\nexplains this with a conceptual argument, not a detailed list of\nobservations. \nAristotle first argues that heterogeneous mixtures can be\ndecomposed: \nHe then goes on to offer an explicit definition of the concept of an\nelement in terms of simple bodies, specifically mentioning recovery in\nanalysis.  \nThe notion of simplicity implicit here is introduced late in\nDG where in book II Aristotle claims that “All the\ncompound bodies … are composed of all the simple bodies”\n(334b31). But if all simple bodies (elements) are present in all\ncompounds, how are the various compounds distinguished? With an eye to\nmore recent chemistry, it is natural to think that the differing\ndegrees of the primary qualities of warmth and humidity that\ncharacterize different substances arise from mixing different\nproportions of the elements. Perhaps Aristotle makes a fleeting\nreference to this idea when he expresses the uniformity of a product\nof mixing by saying that “the part exhibit[s] the same ratio\nbetween its constituents as the whole” (DG I.10,\n328a8–9 and again at DG II.7, 334b15). \nBut what does “proportions of the elements” mean? The\ncontemporary laws of constant and multiple proportions deal with a\nconcept of elemental proportions understood on the basis of the\nconcept of mass. No such concept was available to Aristotle. The\nextant texts give little indication of how Aristotle might have\nunderstood the idea of elemental proportions, and we have to resort to\nspeculation (Needham 2009a). \nRegardless of how he understood elemental proportions, Aristotle was\nquite explicit that while recoverable, elements were not actually\npresent in compounds. In DG I.10 he argues that the original\ningredients are only potentially, and not actually, present in the\nresulting compounds of a mixing process. \nThere are two reasons why in Aristotle’s theory the elements are not\nactually present in compounds. The first concerns the manner in which\nmixing occurs. Mixing only occurs because of the primary powers and\nsusceptibilities of substances to affect and be affected by other\nsubstances. This implies that all of the original matter is\nchanged when a new compound is formed. Aristotle tells us\nthat compounds are formed when the opposing contraries are neutralized\nand an intermediate state results: \nThe second reason has to do with the homogeneity requirement of pure\nsubstances. Aristotle tells us that “if combination has taken\nplace, the compound must be uniform—any part of such a\ncompound is the same as the whole, just as any part of water is\nwater” (DG I.10, 328a10f.). Since the elements are\ndefined in terms of the extremes of warmth and humidity, what has\nintermediate degrees of these qualities is not an element. Being\nhomogeneous, every part of a compound has the same intermediate\ndegrees of these qualities. Thus, there are no parts with extremal\nqualities, and hence no elements actually present. His theory of the\nappearance of new substances therefore implies that the elements are\nnot actually present in compounds. \nSo we reach an interesting theoretical impasse. Aristotle defined the\nelements by conditions they exhibit in isolation and argued that all\ncompounds are composed of the elements. However, the properties\nelements have in isolation are nothing that any part of an actually\nexisting compound could have. So how is it possible to recover the\nelements? \nIt is certainly not easy to understand what would induce a compound to\ndissociate into its elements on Aristotle’s theory, which seems\nentirely geared to showing how a stable equilibrium results from\nmixing. The overwhelming kind of mixing process doesn’t seem to be\napplicable. How, for example, could it explain the separation of salt\nand water from sea water? But the problem for the advocates of the\nactual presence of elements is to characterize them in terms of\nproperties exhibited in both isolated and combined states.\nThe general problem of adequately meeting this challenge, either in\ndefense of the potential presence or actual presence view, is the\nproblem of mixture (Cooper 2004; Fine 1995, Wood &\nWeisberg 2004). \nIn summary, Aristotle laid the philosophical groundwork for all\nsubsequent discussions of elements, pure substances, and chemical\ncombination. He asserted that all pure substances were homoeomerous\nand composed of the elements air, earth, fire, and water. These\nelements were not actually present in these substances; rather, the\nfour elements were potentially present. Their potential presence could\nbe revealed by further analysis and transformation. \nAntoine Lavoisier (1743–1794) is often called the father of\nmodern chemistry, and by 1789 he had produced a list of the elements\nthat a modern chemist would recognize. Lavoisier’s list, however, was\nnot identical to our modern one. Some items such as hydrogen and\noxygen gases were regarded as compounds by Lavoisier, although we now\nknow regard hydrogen and oxygen as elements and their gases as\nmolecules. \nOther items on his list were remnants of the Aristotelian system which\nhave no place at all in the modern system. For example, fire remained\non his list, although in the somewhat altered form of caloric. Air is\nanalyzed into several components: the respirable part called oxygen\nand the remainder called azote or nitrogen. Four types of earth found\na place on his list: lime, magnesia, barytes, and argill. The\ncomposition of these earths are “totally unknown, and, until by\nnew discoveries their constituent elements are ascertained, we are\ncertainly authorized to consider them as simple bodies” (1789,\np. 157), although Lavoisier goes on to speculate that “all the\nsubstances we call earths may be only metallic oxyds” (1789, p.\n159). \nWhat is especially important about Lavoisier’s system is his\ndiscussion of how the elemental basis of particular compounds is\ndetermined. For example, he describes how water can be shown to be a\ncompound of hydrogen and oxygen (1789, pp. 83–96). He\nwrites: \nThe metaphysical principle of the conservation of matter—that\nmatter can be neither created nor destroyed in chemical\nprocesses—called upon here is at least as old as Aristotle\n(Weisheipl 1963). What the present passage illustrates is the\nemployment of a criterion of conservation: the preservation of mass.\nThe total mass of the products must come from the mass of the\nreactants, and if this is not to be found in the easily visible ones,\nthen there must be other, less readily visible reactants. \nThis principle enabled Lavoisier to put what was essentially\nAristotle’s notion of simple substances (302a15ff., quoted in section\n1.1) to much more effective experimental use. Directly after rejecting\natomic theories, he says “if we apply the term\nelements, or principles of bodies, to express our\nidea of the last point which analysis is capable of reaching, we must\nadmit, as elements, all the substances into which we are capable, by\nany means, to reduce bodies by decomposition” (1789, p. xxiv).\nIn other words, elements are identified as the smallest components of\nsubstances that we can produce experimentally. The principle of the\nconservation of mass provided for a criterion of when a chemical\nchange was a decomposition into simpler substances, which was decisive\nin disposing of the phlogiston theory. The increase in weight on\ncalcination meant, in the light of this principle, that calcination\nwas not a decomposition, as the phlogiston theorists would have it,\nbut the formation of a more complex compound. \nDespite the pragmatic character of this definition, Lavoisier felt\nfree to speculate about the compound nature of the earths, as well as\nthe formation of metal oxides which required the decomposition of\noxygen gas. Thus, Lavoisier also developed the notion of an element as\na theoretical, last point of analysis concept. While this last point\nof analysis conception remained an important notion for Lavoisier as\nit was for Aristotle, his notion was a significant advance over\nAristotle’s and provided the basis for further theoretical advance in\nthe 19th century (Hendry 2005). \nLavoisier’s list of elements was corrected and elaborated with the\ndiscovery of many new elements in the 19th century. For example,\nHumphrey Davy (1778–1829) isolated sodium and potassium by\nelectrolysis, demonstrating that Lavoisier’s earths were actually\ncompounds. In addition, caloric disappeared from the list of accepted\nelements with the discovery of the first law of thermodynamics in the\n1840s. Thus with this changing, but growing, number of elements,\nchemists increasingly recognized the need for a systematization. Many\nattempts were made, but an early influential account was given by John\nNewlands (1837–98) who prepared the first periodic table showing\nthat 62 of the 63 then known elements follow an “octave”\nrule according to which every eighth element has similar\nproperties. \nLater, Lothar Meyer (1830–95) and Dmitrij Mendeleev\n(1834–1907) independently presented periodic tables covering all\n63 elements known in 1869. In 1871, Mendeleev published his periodic\ntable in the form it was subsequently acclaimed. This table was\norganized on the idea of periodically recurring general features as\nthe elements are followed when sequentially ordered by relative atomic\nweight. The periodically recurring similarities of chemical behavior\nprovided the basis of organizing elements into groups. He identified 8\nsuch groups across 12 horizontal periods, which, given that he was\nworking with just 63 elements, meant there were several holes. Figure 1.\nThe International Union of Pure and Applied Chemistry’s Periodic\nTable of the Elements. \nThe modern Periodic Table depicted in Figure 1 is based on Mendeleev’s\ntable, but now includes 92 naturally occurring elements and some dozen\nartificial elements (see Scerri 2006). The lightest element,\nhydrogen, is difficult to place, but is generally placed at the top of\nthe first group. Next comes helium, the lightest of the noble gases,\nwhich were not discovered until the end of the 19th\ncentury. Then the second period begins with lithium, the first of the\ngroup 1 (alkali metal) elements. As we cross the second period,\nsuccessively heavier elements are first members of other groups until\nwe reach neon, which is a noble gas like helium. Then with the next\nheaviest element sodium we return to the group 1 alkali metals and\nbegin the third period, and so on. \nOn the basis of his systematization, Mendeleev was able to correct the\nvalues of the atomic weights of certain known elements and also to\npredict the existence of then unknown elements corresponding to gaps\nin his Periodic Table. His system first began to seriously attract\nattention in 1875 when he was able to point out that gallium, the\nnewly discovered element by Lecoq de Boisbaudran (1838–1912),\nwas the same as the element he predicted under the name eka-aluminium,\nbut that its density should be considerably greater than the value\nLecoq de Boisbaudran reported. Repeating the measurement proved\nMendeleev to be right. The discovery of scandium in 1879 and germanium\nin 1886 with the properties Mendeleev predicted for what he called\n“eka-bor” and “eka-silicon” were further\ntriumphs (Scerri 2006). \nIn addition to providing the systematization of the elements used in\nmodern chemistry, Mendeleev also gave an account of the nature of\nelements which informs contemporary philosophical understanding. He\nexplicitly distinguished between the end of analysis and actual\ncomponents conceptions of elements and while he thought that both\nnotions have chemical importance, he relied on the actual components\nthesis when constructing the Periodic Table. He assumed that the\nelements remained present in compounds and that the weights of\ncompounds is the sum of the weights of their constituent atoms. He was\nthus able to use atomic weights as the primary ordering property of\nthe Periodic\n Table.[3] \nNowadays, chemical nomenclature, including the definition of the\nelement, is regulated by The International Union of Pure and Applied\nChemistry (IUPAC). In 1923, IUPAC followed Mendeleev and standardized\nthe individuation criteria for the elements by explicitly endorsing\nthe actual components thesis. Where they differed from Mendeleev is in\nwhat property they thought could best individuate the elements. Rather\nthan using atomic weights, they ordered elements according to\natomic number, the number of protons and of electrons of\nneutral elemental atoms, allowing for the occurrence of\nisotopes with the same atomic number but different atomic\nweights. They chose to order elements by atomic number because of the\ngrowing recognition that electronic structure was the atomic feature\nresponsible for governing how atoms combine to form molecules, and the\nnumber of electrons is governed by the requirement of overall\nelectrical neutrality (Kragh 2000). \nMendeleev’s periodic system was briefly called into question with the\ndiscovery of the inert gas argon in 1894, which had to be placed\noutside the existing system after chlorine. But William Ramsay\n(1852–1916) suspected there might be a whole group of chemically\ninert substances separating the electronegative halogen group 17 (to\nwhich chlorine belongs) and the electropositive alkali metals, and by\n1898 he had discovered the other noble gases, which became group 18 on\nthe modern Table. \nA more serious challenge arose when the English radiochemist Frederick\nSoddy (1877–1956) established in 1913 that according to the\natomic weight criterion of sameness, positions in the periodic table\nwere occupied by several elements. Adopting Margaret Todd’s\n(1859–1918) suggestion, Soddy called these elements\n‘isotopes,’ meaning “same place.” At the same\ntime, Bohr’s conception of the atom as comprising a positively charged\nnucleus around which much lighter electrons circulated was gaining\nacceptance. After some discussion about criteria (van der Vet 1979),\ndelegates to the 1923 IUPAC meeting saved the Periodic Table by\ndecreeing that positions should be correlated with atomic number\n(number of protons in the nucleus) rather than atomic weight. \nCorrelating positions in the Periodic Table with whole numbers finally\nprovided a criterion determining whether any gaps remained in the\ntable below the position corresponding to the highest known atomic\nnumber. The variation in atomic weight for fixed atomic number was\nexplained in 1932 when James Chadwick (1891–1974) discovered the\nneutron—a neutral particle occurring alongside the proton in\natomic nuclei with approximately the same mass as the proton. \nContemporary philosophical discussion about the nature of the elements\nbegins with the work of Friedrich Paneth (1887–1958), whose work\nheavily influenced IUPAC standards and definitions. He was among the\nfirst chemists in modern times to make explicit the distinction\nbetween the last point of analysis and actual components analyses, and\nargued that the last point in analysis thesis could not be the proper\nbasis for the chemical explanation of the nature of compounds.\nSomething that wasn’t actually present in a substance couldn’t be\ninvoked to explain the properties in a real substance. He went on to\nsay that the chemically important notion of element was\n“transcendental,” which we interpret to mean “an\nabstraction over the properties in compounds” (Paneth\n1962). \nAnother strand of the philosophical discussion probes at the\ncontemporary IUPAC definition of elements. According to IUPAC, to be\ngold is to have atomic number 79, regardless of atomic weight. A\nlogical and intended consequence of this definition is that all\nisotopes sharing an atomic number count as the same element. Needham\n(2008) has recently challenged this identification by pointing to\nchemically salient differences among the isotopes. These differences\nare best illustrated by the three isotopes of hydrogen: protium,\ndeuterium and tritium. The most striking chemical difference among the\nisotopes of hydrogen is their different rate of chemical reactions.\nBecause of the sensitivity of biochemical processes to rates of\nreaction, heavy water (deuterium oxide) is poisonous whereas ordinary\nwater (principally protium oxide) is not. With the development of more\nsensitive measuring techniques, it has become clear that this is a\ngeneral phenomenon. Isotopic variation affects the rate of chemical\nreactions, although these effects are less marked with increasing\natomic number. In view of the way chemists understand these\ndifferences in behavior, Needham argues that they can reasonably be\nsaid to underlie differences in chemical substance. He further argues\nthat the criteria of sameness and difference provided by\nthermodynamics also suggest that the isotopes should be considered\ndifferent substances. However, notwithstanding his own view, the\nplaces in Mendeleev’s periodic table were determined by atomic number\n(or nuclear charge), so a concentration on atomic weight would be\nhighly revisionary of chemical classification (Hendry 2006a). It can\nalso be argued that the thermodynamic criteria underlying the view\nthat isotopes are different substances distinguish among substances\nmore finely than is appropriate for chemistry (Hendry 2010c). \nContemporary theories of chemical combination arose from a fusion of\nancient theories of proper mixing and hundreds of years of\nexperimental work, which refined those theories. Yet even by the time\nthat Lavoisier inaugurated modern chemistry, chemists had little in\nthe way of rules or principles that govern how elements combine to\nform compounds. In this section, we discuss theoretical efforts to\nprovide such criteria. \nA first step towards a theory of chemical combination was implicit in\nLavoisier’s careful experimental work on water. In his Elements of\nChemistry, Lavoisier established the mass proportions of hydrogen\nand oxygen obtained by the complete reduction of water to its\nelements. The fact that his results were based on multiple repetitions\nof this experiment suggests that he assumed compounds like water are\nalways composed of the same elements in the same proportions. This\nwidely shared view about the constant proportions of elements in\ncompounds was first explicitly proclaimed as the law of constant\nproportions by Joseph Louis Proust (1754–1826) in the first\nyears of the 19th century. Proust did so in response to Claude Louis\nBerthollet (1748–1822), one of Lavoisier’s colleagues and\nsupporters, who argued that compounds could vary in their elemental\ncomposition. \nAlthough primarily a theoretical and conceptual posit, the law of\nconstant proportions became an important tool for chemical analysis.\nFor example, chemists had come to understand that atmospheric air is\ncomposed of both nitrogen and oxygen and is not an element. But was\nair a genuine compound of these elements or some looser mixture of\nnitrogen and oxygen, that could vary at different times and in\ndifferent places? The law of constant proportions gave a criterion for\ndistinguishing compounds from genuine mixtures. If air was a compound,\nthen it would always have the same proportion of nitrogen and oxygen\nand it should further be distinguishable from other compounds of\nnitrogen and oxygen such as nitrous oxide. If air was not a genuine\ncompound, then it would be an example of a solution, a\nhomogenous mixture of oxygen and nitrogen that could vary in\nproportions. \nBerthollet didn’t accept this rigid distinction between solutions and\ncompounds. He believed that whenever a substance is brought into\ncontact with another, it forms a homogeneous union until further\naddition of the substance leaves the union in excess. For example,\nwhen water and sugar are combined, they initially form a homogenous\nunion. At a certain point, the affinities of water and sugar for one\nanother are saturated, and a second phase of solid sugar will form\nupon the addition of more sugar. This point of saturation will vary\nwith the pressure and temperature of the solution. Berthollet\nmaintained that just as the amount of sugar in a saturated solution\nvaries with temperature and pressure, the proportions of elements in\ncompounds are sensitive to ambient conditions. Thus, he argued, it is\nnot true that substances are always composed of the same proportions\nof the element and this undermines the law of constant proportions.\nBut after a lengthy debate, chemists came to accept that the evidence\nProust adduced established the law of constant proportions for\ncompounds, which were thereby distinguished from solutions. \nChemists’ attention was largely directed towards the investigation of\ncompounds in the first half of the 19th century, initially with a view\nto broadening the evidential basis which Proust had provided. For a\ntime, the law of constant proportions seemed a satisfactory criterion\nof the occurrence of chemical combination. But towards the end of the\n19th century, chemists turned their attention to solutions.\nTheir investigation of solutions drew on the new science of\nthermodynamics, which said that changes of state undergone by\nsubstances when they are brought into contact were subjected to its\nlaws governing energy and entropy. \nAlthough thermodynamics provided no sharp distinction between\ncompounds and solutions, it did allow the formulation of a concept for\na special case called an ideal solution. An ideal solution\nforms because its increased stability compared with the separated\ncomponents is entirely due to the entropy of mixing. This can be\nunderstood as a precisification of the idea of a purely mechanical\nmixture. In contrast, compounds were stabilized by interactions\nbetween their constituent components over and above the entropy of\nmixing. For example, solid sodium chloride is stabilized by the\ninteractions of sodium and chlorine, which react to form sodium\nchloride. The behavior of real solutions could be compared with that\nof an ideal solution, and it turned out that non-ideality was the rule\nrather than the exception. Ideality is approached only in certain\ndilute binary solutions. More often, solutions exhibited behavior\nwhich could only be understood in terms of significant chemical\ninteractions between the components, of the sort characteristic of\nchemical combination. \nLong after his death, in the first decades of the 20th century,\nBerthollet was partially vindicated with the careful characterization\nof a class of substances that we now call Berthollides. These are\ncompounds whose proportions of elements do not stand in simple\nrelations to one another. Their elemental proportions are not fixed,\nbut vary with temperature and pressure. For example, the mineral\nwüstite, or ferrous oxide, has an approximate compositional\nformula of FeO, but typically has somewhat less iron than oxygen. \nFrom a purely macroscopic, thermodynamic perspective, Berthollides can\nbe understood in terms of the minimization of the thermodynamic\nfunction called the Gibbs free energy, which accommodates the\ninterplay of energy and entropy as functions of temperature and\npressure. Stable substances are ones with minimal Gibbs free energy.\nOn the microscopic scale, the basic microstructure of ferrous oxide is\na three-dimensional lattice of ferrous (Fe2+) and oxide\n(O2-) ions. However, some of the ferrous ions are replaced\nby holes randomly distributed in the crystal lattice, which generates\nan increase in entropy compared with a uniform crystal structure. An\noverall imbalance of electrical charge would be created by the missing\nions. But this is countered in ferrous oxide by twice that number of\nions from those remaining being converted to ferric (Fe3+)\nions. This removal of electrons requires an input of\nenergy, which would make for a less stable structure were it not for\nthe increased entropy afforded by the holes in the crystal structure.\nThe optimal balance between these forces depends on the temperature\nand pressure, and this is described by the Gibbs free energy function.\n \nAlthough the law of constant proportions has not survived the\ndiscovery of Berthollides and more careful analyses of solutions\nshowed that chemical combination or affinity is not confined to\ncompounds, it gave chemists a principled way of studying how elements\ncombine to form compounds through the 19th century. This\naccount of Berthollides also illustrates the interplay between\nmacroscopic and microscopic theory which is a regular feature of\nmodern chemistry, and which we turn to in the next section. \nChemistry has traditionally distinguished itself from classical\nphysics by its interest in the division of matter into different\nsubstances and in chemical combination, the process whereby substances\nare held together in compounds and solutions. In this section, we have\ndescribed how chemists came to understand that all substances were\ncomposed of the Periodic Table’s elements, and that these elements are\nactual components of substances. Even with this knowledge,\ndistinguishing pure substances from heterogeneous mixtures and\nsolutions remained a very difficult chemical challenge. And despite\nchemists’ acceptance of the law of definite proportions as a criterion\nfor substancehood, chemical complexities such as the discovery of the\nBerthollides muddied the waters. \nModern chemistry is thoroughly atomistic. All substances are thought\nto be composed of small particles, or atoms, of the Periodic Table’s\nelements. Yet until the beginning of the 20th century, much\ndebate surrounded the status of atoms and other microscopic\nconstituents of matter. As with many other issues in philosophy of\nchemistry, the discussion of atomism begins with Aristotle, who\nattacked the coherence of the notion and disputed explanations\nsupposedly built on the idea of indivisible constituents of matter\ncapable only of change in respect of position and motion, but not\nintrinsic qualities. We will discuss Aristotle’s critiques of atomism\nand Boyle’s response as well as the development of atomism in the 19th\nand 20th centuries. \nIn Aristotle’s time, atomists held that matter was fundamentally\nconstructed out of atoms. These atoms were indivisible and uniform, of\nvarious sizes and shapes, and capable only of change in respect of\nposition and motion, but not intrinsic qualities. Aristotle rejected\nthis doctrine, beginning his critique of it with a simple question:\nWhat are atoms made of? Atomists argue that they are all made of\nuniform matter. But why should uniform matter split into portions not\nthemselves further divisible? What makes atoms different from\nmacroscopic substances which are also uniform, but can be divided into\nsmaller portions? Atomism, he argued, posits a particular size as the\nfinal point of division in completely ad hoc fashion, without giving\nany account of this smallest size or why atoms are this smallest\nsize. \nApart from questions of coherence, Aristotle argued that it was\nunclear and certainly unwarranted to assume that atoms have or lack\nparticular properties. Why shouldn’t atoms have some degree of warmth\nand humidity like any observable body? But if they do, why shouldn’t\nthe degree of warmth of a cold atom be susceptible to change by the\napproach of a warm atom, in contradiction with the postulate that\natoms only change their position and motion? On the other hand, if\natoms don’t possess warmth and humidity, how can changes in degrees of\nwarmth and humidity between macroscopic substances be explained purely\non the basis of change in position and motion? \nThese and similar considerations led Aristotle to question whether the\natomists had a concept of substance at all. There are a large variety\nof substances discernible in the world—the flesh, blood and bone\nof animal bodies; the water, rock, sand and vegetable matter by the\ncoast, etc. Atomism apparently makes no provision for accommodating\nthe differing properties of these substances, and their\ninterchangeability, when for example white solid salt and tasteless\nliquid water are mixed to form brine or bronze statues slowly become\ngreen. Aristotle recognized the need to accommodate the creation of\nnew substances with the destruction of old by combination involving\nthe mutual interaction and consequent modification of the primary\nfeatures of bodies brought into contact. In spite of the weaknesses of\nhis own theory, he displays a grasp of the issue entirely lacking on\nthe part of the atomists. His conception of elements as being few in\nnumber and of such a character that all the other substances are\ncompounds derived from them by combination and reducible to them by\nanalysis provided the seeds of chemical theory. Ancient atomism\nprovided none. \nRobert Boyle (1627–1691) is often credited with first breaking\nwith ancient and medieval traditions and inaugurating modern chemistry\nby fusing an experimental approach with mechanical philosophy. Boyle’s\nchemical theory attempts to explain the diversity of substances,\nincluding the elements, in terms of variations of shape and size and\nmechanical arrangements of what would now be called sub-atomic atoms\nor corpuscles. Although Boyle’s celebrated experimental work attempted\nto respond to Aristotelian orthodoxy, his theorizing about atoms had\nlittle impact on his experimental work. Chalmers (1993, 2002)\ndocuments the total absence of any connection between Boyle’s atomic\nspeculations and his experimental work on the effects of pressure on\ngases. This analysis applies equally to Boyle’s chemical experiments\nand chemical theorizing, which was primarily driven by a desire to\ngive a mechanical philosophy of chemical combination (Chalmers 2009,\nCh. 6). No less a commentator than Antoine Lavoisier (1743–1794)\nwas quite clear that Boyle’s corpuscular theories did nothing to\nadvance chemistry. As he noted towards the end of the next century,\n“… if, by the term elements, we mean to express\nthose simple and indivisible atoms of which matter is composed, it is\nextremely probable we know nothing at all about them” (1789, p.\nxxiv). Many commentators thus regard Boyle’s empirically-based\ncriticisms of the Aristotelian chemists more important than his own\natomic theories. \nContemporary textbooks typically locate discussions of chemical\natomism in the 19th century work of John Dalton\n(1766–1844). Boyle’s ambitions of reducing elemental minima to\nstructured constellations of mechanical atoms had been abandoned by\nthis time, and Dalton’s theory simply assumes that each element has\nsmallest parts of characteristic size and mass which have the property\nof being of that elemental kind. Lavoisier’s elements\nare considered to be collections of such characteristic atoms. Dalton\nargued that this atomic hypothesis explained the law of constant\nproportions (see section 1.5). \nDalton’s theory gives expression to the idea of the real presence of\nelements in compounds. He believed that atoms survive chemical change,\nwhich underwrites the claim that elements are actually present in\ncompounds. He assumed that atoms of the same element are alike in\ntheir weight. On the assumption that atoms combine with the atoms of\nother elements in fixed ratios, Dalton claimed to explain why, when\nelements combine, they do so with fixed proportions between their\nweights. He also introduced the law of multiple proportions,\naccording to which the elements in distinct compounds of the same\nelements stand in simple proportions. He argued that this principle\nwas also explained by his atomic theory. \nDalton’s theory divided the chemical community and while he had many\nsupporters, a considerable number of chemists remained anti-atomistic.\nPart of the reason for this was controversy surrounding the empirical\napplication of Dalton’s atomic theory: How should one estimate atomic\nweights since atoms were such small quantities of matter? Daltonians\nargued that although such tiny quantities could not be measured\nabsolutely, they could be measured relative to a reference atom (the\nnatural choice being hydrogen as 1). This still left a problem in\nsetting the ratio between the weights of different atoms in compounds.\nDalton assumed that, if only one compound of two elements is known, it\nshould be assumed that they combine in equal proportions. Thus, he\nunderstood water, for instance, as though it would have been\nrepresented by HO in terms of the formulas that Berzelius was to\nintroduce (Berzelius, 1813). But Dalton’s response to this problem\nseemed arbitrary. Finding a more natural solution became pressing\nduring the first half of the nineteenth century as more and more\nelements were being discovered, and the elemental compositions of more\nand more chemical substances were being determined qualitatively\n(Duhem 2002; Needham 2004; Chalmers 2005a, 2005b, and 2008). \nDalton’s contemporaries raised other objections as well. Jacob\nBerzelius (1779–1848) argued that Daltonian atomism provided no\nexplanation of chemical combination, how elements hold together to\nform compounds (Berzelius, 1815). Since his atoms are intrinsically\nunchanging, they can suffer no modification of the kind Aristotle\nthought necessary for combination to occur. Lacking anything like the\nmodern idea of a molecule, Dalton was forced to explain chemical\ncombination in terms of atomic packing. He endowed his atoms with\natmospheres of caloric whose mutual repulsion was supposed to explain\nhow atoms pack together efficiently. But few were persuaded by this\nidea, and what came later to be known as Daltonian atomism abandoned\nthe idea of caloric shells altogether. \nThe situation was made more complex when chemists realized that\nelemental composition was not in general sufficient to distinguish\nsubstances. Dalton was aware that the same elements sometimes give\nrise to several compounds; there are several oxides of nitrogen, for\nexample. But given the law of constant proportions, these can be\ndistinguished by specifying the combining proportions, which is what\nis represented by distinct chemical formulas, for example\nN2O, NO and N2O3 for different oxides\nof nitrogen. However, as more organic compounds were isolated and\nanalyzed, it became clear that elemental composition doesn’t uniquely\ndistinguish substances. Distinct compounds with the same elemental\ncomposition are called isomers. The term was coined by\nBerzelius in 1832 when organic compounds with the same composition,\nbut different properties, were first recognized. It was later\ndiscovered that isomerism is ubiquitous, and not confined to organic\ncompounds. \nIsomers may differ radically in “physical” properties such\nas melting points and boiling points as well as patterns of chemical\nreactivity. This is the case with dimethyl ether and ethyl alcohol,\nwhich have the compositional formula\nC2H6O in common, but are represented by two\ndistinct structural formulas: (CH3)2O\nand C2H5OH. These formulas identify different\nfunctional groups, which govern patterns of chemical\nreactivity. The notion of a structural formula was developed to\naccommodate other isomers that are even more similar. This was the\ncase with a subgroup of stereoisomers called optical\nisomers, which are alike in many of their physical properties such as\nmelting points and boiling points and (when first discovered) seemed\nto be alike in chemical reactivity too. Pasteur famously separated\nenantiomers (stereoisomers of one another) of tartaric acid\nby preparing a solution of the sodium ammonium salt and allowing\nrelatively large crystals to form by slow evaporation. Using tweezers,\nhe assembled the crystals into two piles, members of the one having\nshapes which are mirror images of the shapes of those in the other\npile. Optical isomers are so called because they have the\ndistinguishing feature of rotating the plane of plane polarized light\nin opposite directions, a phenomenon first observed in quartz crystals\nat the beginning of the 19th century. These isomers are\nrepresented by three-dimensional structural formulas which are mirror\nimages of one another as we show in Figure 2. Figure 2.\nThe enantiomers of tartaric acid. D-tartaric acid is on the\nleft and L-tartaric acid is on the right. The dotted vertical line\nrepresents a mirror plane. The solid wedges represent bonds coming out\nof the plane, while the dashed wedges represent bonds going behind the\nplane. These molecular structures are mirror images of one another. \nAlthough these discoveries are often presented as having been\nexplained by the atomic or molecular hypothesis, skepticism about the\nstatus of atomism persisted throughout the 19th century. Late 19th\ncentury skeptics such as Ernst Mach, Georg Helm, Wilhelm Ostwald, and\nPierre Duhem did not see atomism as an adequate explanation of these\nphenomena, nor did they believe that there was sufficient evidence to\naccept the existence of atoms. Instead, they advocated non-atomistic\ntheories of chemical change grounded in thermodynamics (on Helm and\nOstwald, see the introduction to Deltete 2000). \nDuhem’s objections to atomism are particularly instructive. Despite\nbeing represented as a positivist in some literature (e.g. Fox 1971),\nhis objections to atomism in chemistry made no appeal to the\nunobservability of atoms. Instead, he argued that a molecule was a\ntheoretical impossibility according to 19th century physics, which\ncould say nothing about how atoms can hold together but could give\nmany reasons why they couldn’t be stable entities over reasonable\nperiods of time. He also argued that the notion of valency attributed\nto atoms to explain their combining power was simply a macroscopic\ncharacterization projected into the microscopic level. He showed that\nchemical formulae could be interpreted without resorting to atoms and\nthe notion of valency could be defined on this basis (Duhem 1892,\n1902; for an exposition, see Needham 1996). Atomists failed to meet\nthis challenge, and he criticized them for not saying what the\nfeatures of their atoms were beyond simply reading into them\nproperties defined on a macroscopic basis (Needham 2004). Duhem did\nrecognize that an atomic theory was developed in the 19th\ncentury, the vortex theory (Kragh 2002), but rejected it as\ninadequate for explaining chemical phenomena. \nSkeptics about atomism finally became convinced at the beginning of\nthe 20th century by careful experimental and theoretical\nwork on Brownian motion, the fluctuation of particles in an\nemulsion. With the development of kinetic theory it was suspected that\nthis motion was due to invisible particles within the emulsion pushing\nthe visible particles. But it wasn’t until the first decade of the\n20th century that Einstein’s theoretical analysis and Perrin’s\nexperimental work gave substance to those suspicions and provided an\nestimate of Avogadro’s number, which Perrin famously argued was\nsubstantially correct because it agreed with determinations made by\nseveral other, independent, methods. This was the decisive argument\nfor the existence of microentities which led most of those still\nskeptical of the atomic hypotheses to change their views (Einstein\n1905; Perrin 1913; Nye 1972; Maiocchi 1990). \nIt is important to appreciate, however, that establishing the\nexistence of atoms in this way left many of the questions raised by\nthe skeptics unanswered. A theory of the nature of atoms which would\nexplain how they can combine to form molecules was yet to be\nformulated. And it remains to this day an open question whether a\npurely microscopic theory is available which is adequate to explain\nthe whole range of chemical phenomena. This issue is pursued in\nSection 6 where we discuss reduction. \nAs we discussed in Section 1, by the end of the 18th century the\nmodern conception of chemical substances began to take form in\nLavoisier’s work. Contemporary looking lists of elements were being\ndrawn up and also the notion of mass was introduced into chemistry.\nDespite these advances, chemists continued to develop theories about\ntwo substances which we no longer accept: caloric and phlogiston.\nLavoisier famously rejected phlogiston, but he accepted caloric. It\nwould be another 60 years until the notion of caloric was finally\nabandoned with the development of thermodynamics. \nIn 1761, Joseph Black discovered that heating a body doesn’t always\nraise its temperature. In particular, he noticed that heating ice at\n0°C converts it to liquid at the same temperature. Similarly,\nthere is a latent heat of vaporization which must be supplied for the\nconversion of liquid water into steam at the boiling point without\nraising the temperature. It was some time before the modern\ninterpretation of Black’s ground-breaking discovery was fully\ndeveloped. He had shown that heat must be distinguished from the state\nof warmth of a body and even from the changes in that state. But it\nwasn’t until the development of thermodynamics that heating was\ndistinguished as a process from the property or quality of being warm\nwithout reference to a transferred substance. \nBlack himself was apparently wary of engaging in hypothetical\nexplanations of heat phenomena (Fox 1971), but he does suggest an\ninterpretation of the latent heat of fusion of water as a chemical\nreaction involving the combination of the heat fluid with ice to yield\nthe new substance water. Lavoisier incorporated Black’s conception of\nlatent heat into his caloric theory of heat, understanding latent heat\ntransferred to a body without raising its temperature as caloric fluid\nbound in chemical combination with that body and not contributing to\nthe body’s degree of warmth or temperature. Lavoisier’s theory thus\nretains something of Aristotle’s, understanding what we would call a\nphase change of the same substance as a transformation of one\nsubstance into another. \nCaloric figures in Lavoisier’s list of elements as the “element\nof heat or fire” (Lavoisier 1789, p. 175), “becom[ing]\nfixed in bodies … [and] act[ing] upon them with a repulsive\nforce, from which, or from its accumulation in bodies to a greater or\nlesser degree, the transformation of solids into fluids, and of fluids\nto aeriform elasticity, is entirely owing” (1789, p. 183). He\ngoes on to define ‘gas’ as “this aeriform state of\nbodies produced by a sufficient accumulation of caloric.” Under\nthe list of binary compounds formed with hydrogen, caloric is said to\nyield hydrogen gas (1789, p. 198). Similarly, under the list of binary\ncompounds formed with phosphorus, caloric yields phosphorus gas (1789,\np. 204). The Lavoisian element base of oxygen combines with the\nLavoisian element caloric to form the compound oxygen gas. The\ncompound of base of oxygen with a smaller amount of caloric is oxygen\nliquid (known only in principle to Lavoisier). What we would call the\nphase change of liquid to gaseous oxygen is thus for him a change of\nsubstance. Light also figures in his list of elements, and is said\n“to have a great affinity with oxygen, … and contributes\nalong with caloric to change it into the state of gas” (1789, p.\n185). \nAnother substance concept from roughly the same period is phlogiston,\nwhich served as the basis for 18th century theories of processes that\ncame to be called oxidation and reduction. Georg Ernst Stahl\n(1660–1734) introduced the theory, drawing on older theoretical\nideas. Alchemists thought that metals lose the mercury principle under\ncalcination and that when substances are converted to slag, rust, or\nash by heating, they lose the sulphur principle. Johann Joackim Becher\n(1635–82) modified these ideas at the end of the 17th century,\narguing that the calcination of metals is a kind of combustion\ninvolving the loss of what he called the principle of flammability.\nStahl subsequently renamed this principle ‘phlogiston’ and\nfurther modified the theory, maintaining that phlogiston could be\ntransferred from one substance to another in chemical reactions, but\nthat it could never be isolated. \nFor example, metals were thought to be compounds of the metal’s calx\nand phlogiston, sulphur was thought to be a compound of sulphuric acid\nand phlogiston, and phosphorus was thought to be a compound of\nphosphoric acid and phlogiston. Substances such as carbon which left\nlittle or no ash after burning were taken to be rich in phlogiston.\nThe preparation of metals from their calxes with the aid of wood\ncharcoal was understood as the transfer of phlogiston from carbon to\nthe metal. \nRegarding carbon as a source of phlogiston and no longer merely as a\nsource of warmth was a step forward in understanding chemical\nreactions (which Ladyman 2011 emphasizes in support of his structural\nrealist interpretation of phlogiston chemistry). The phlogiston theory\nsuggested that reactions could involve the replacement of one part of\na substance with another, where previously all reactions were thought\nto be simple associations or dissociations. \nPhlogiston theory was developed further by Henry Cavendish\n(1731–1810) and Joseph Priestley (1733–1804), who both\nattempted to better characterize the properties of phlogiston itself.\nAfter 1760, phlogiston was commonly identified with what they called\n‘inflammable air’ (hydrogen), which they successfully\ncaptured by reacting metals with muriatic (hydrochloric) acid. Upon\nfurther experimental work on the production and characterizations of\nthese “airs,” Cavendish and Priestley identified what we\nnow call oxygen as ‘dephlogisticated air’ and nitrogen as\n‘phlogiston-saturated air.’ \nAs reactants and products came to be routinely weighed, it became\nclear that metals gain weight when they become a calx. But according\nto the phlogiston theory, the calx involves the loss of phlogiston.\nAlthough the idea that a process involving the loss of a substance\ncould involve the gain of weight seems strange to us, phlogiston\ntheorists were not immediately worried. Some phlogiston theorists\nproposed explanations based on the ‘levitation’ properties\nof phlogiston, what Priestly later referred to as phlogiston’s\n‘negative weight.’ Another explanation of the phenomenon\nwas that the nearly weightless phlogiston drove out heavy, condensed\nair from the pores of the calx. The net result was a lighter product.\nSince the concept of mass did not yet play a central role in\nchemistry, these explanations were thought to be quite reasonable. \nHowever, by the end of the 1770s, Torbern Olaf Bergman\n(1735–1784) made a series of careful measurements of the weights\nof metals and their calxes. He showed that the calcination of metals\nled to a gain in their weight equal to the weight of oxygen lost by\nthe surrounding air. This ruled out the two explanations given above,\nbut interestingly, he took this in his stride, arguing that, as metals\nwere being transformed into their calxes, they lost weightless\nphlogiston. This phlogiston combines with the air’s oxygen to form\nponderable warmth, which in turn combines with what remains of the\nmetal after loss of phlogiston to form the calx. Lavoisier simplified\nthis explanation by removing the phlogiston from this scheme. This\nmoment is what many call the Chemical Revolution. \nModern chemistry primarily deals with microstructure, not elemental\ncomposition. This section will explore the history and consequences of\nchemistry’s focus on structure. The first half of this section\ndescribes chemistry’s transition from a science concerned with\nelemental composition to a science concerned with structure. The\nsecond half will focus on the conceptual puzzles raised by\ncontemporary accounts of bonding and molecular structure. \nIn the 18th and early 19th centuries, chemical\nanalyses of substances consisted in the decomposition of substances\ninto their elemental components. Careful weighing combined with an\napplication of the law of constant proportions allowed chemists to\ncharacterize substances in terms of the mass ratios of their\nconstituent elements, which is what chemists mean by the composition\nof a compound. During this period, Berzelius developed a notation of\ncompositional formulas for these mass ratios where letters stand for\nelements and subscripts stand for proportions on a scale which\nfacilitates comparison of different substances. Although these\nproportions reflect the proportion by weight in grams, the simple\nnumbers are a result of reexpressing gravimetric proportions in terms\nof chemical equivalents. For example, the formulas\n‘H2O’ and ‘H2S’ say that\nthere is just as much oxygen in combination with hydrogen in water as\nthere is sulphur in combination with hydrogen in hydrogen sulphide.\nHowever, when measured in weight, ‘H2O’\ncorresponds to combining proportions of 8 grams of oxygen to 1 gram of\nhydrogen and ‘H2S’ corresponds to 16 grams of\nsulphur to 1 of hydrogen in weight. \nBy the first decades of the 19th century, the nascent\nsub-discipline of organic chemistry began identifying and synthesizing\never increasing numbers of compounds (Klein 2003). As indicated in\nsection 2.2, it was during this period that the phenomenon of\nisomerism was recognized, and structural formulas were introduced to\ndistinguish substances with the same compositional formula that differ\nin their macroscopic properties. Although some chemists thought\nstructural formulas could be understood on a macroscopic basis, others\nsought to interpret them as representations of microscopic entities\ncalled molecules, corresponding to the smallest unit of a compound as\nan atom was held to be the smallest unit of an element. \nIn the first half of the nineteenth century there was no general\nagreement about how the notion of molecular structure could be\ndeployed in understanding isomerism. But during the second half of the\ncentury, consensus built around the structural theories of August\nKekulé (1829–1896). Kekulé noted that carbon\ntended to combine with univalent elements in a 1:4 ratio. He argued\nthat this was because each carbon atom could form bonds to four other\natoms, even other carbon atoms (1858 [1963], 127). In later papers,\nKekulé dealt with apparent exceptions to carbon’s valency of\nfour by introducing the concept of double bonds between carbon atoms.\nHe extended his treatment to aromatic compounds, producing the famous\nhexagonal structure for benzene (see Rocke 2010), although this was to\ncreate a lasting problem for the universality of carbon’s valency of 4\n(Brush 1999a, 1999b). \nKekulé’s ideas about bonding between atoms were important steps\ntoward understanding isomerism. Yet his presentations of structure\ntheory lacked a clear system of diagrammatic representation so most\nmodern systems of structural representation originate with Alexander\nCrum Brown’s (1838–1932) paper about isomerism among organic\nacids (1864 [1865]). Here, structure was shown as linkages between\natoms (see Figure 3). Figure 3. Depictions of ethane and formic acid in Crum Brown’s graphic\nnotation. (1864 [1865], 232)\n \nEdward Frankland (1825–1899) simplified and popularized Crum\nBrown’s notation in successive editions of his Lecture Notes for\nChemical Students (Russell 1971; Ritter 2001). Frankland was\nalso the first to introduce the term ‘bond’ for the\nlinkages between atoms (Ramberg 2003). \nThe next step in the development of structural theory came when James\nDewar (1842–1943) and August Hofmann (1818–1892) developed\nphysical models corresponding closely to Crum Brown’s formulae\n(Meinel 2004). Dewar’s molecules were built from carbon atoms\nrepresented by black discs placed at the centre of pairs of copper\nbands. In Hofmann’s models, atoms were colored billiard balls (black\nfor carbon, white for hydrogen, red for oxygen etc.) linked by bonds.\nEven though they were realized by concrete three-dimensional\nstructures of croquet balls and connecting arms, the\nthree-dimensionality of these models was artificial. The medium itself\nforced the representations of atoms to be spread out in space. But did\nthis correspond to chemical reality? \nKekulé, Crum Brown, and Frankland were extremely cautious when\nanswering this question. Kekulé distinguished between the\napparent atomic arrangement which could be deduced from chemical\nproperties, which he called “chemical structure,” and the\ntrue spatial arrangement of the atoms (Rocke 1984, 2010). Crum Brown\nmade a similar distinction, cautioning that in his graphical formulae\nhe did not “mean to indicate the physical, but merely the\nchemical position of the atoms” (Crum Brown, 1864, 232).\nFrankland noted that “It must carefully be borne in mind that\nthese graphic formulae are intended to represent neither the shape of\nthe molecules, nor the relative position of the constituent\natoms” (Biggs et al. 1976, 59). \nOne way to interpret these comments is that they reflect a kind of\nanti-realism: Structural formulae are merely theoretical tools for\nsummarizing a compound’s chemical behavior. Or perhaps they are simply\nagnostic, avoiding definite commitment to a microscopic realm about\nwhich little can be said. However, other comments suggest a realist\ninterpretation, but one in which structural formulae represent only\nthe topological structure of the spatial arrangement: \nThe move towards a fully spatial interpretation was advanced by the\nsimultaneous postulation in 1874 of a tetrahedral structure for the\norientation of carbon’s four bonds by Jacobus van ’t Hoff\n(1852–1911) and Joseph Achille Le Bel (1847–1930) to\naccount for optical isomerism (see Figure 4 and section 2.2). When\ncarbon atoms are bonded to four different constituents, they cannot be\nsuperimposed on their mirror images, just as your left and right hands\ncannot be. This gives rise to two possible configurations of\nchiral molecules, thus providing for a distinction between\ndistinct substances whose physical and chemical properties are the\nsame except for their ability to rotate plane polarized light in\ndifferent directions.  \nvan ’t Hoff and Le Bel provided no account of the mechanism by which\nchiral molecules affect the rotation of plane polarized light\n(Needham 2004). But by the end of the century, spatial structure was\nbeing put to use in explaining the aspects of the reactivity and\nstability of organic compounds with Viktor Meyer’s (1848–1897)\nconception of steric hindrance and Adolf von Baeyer’s\n(1835–1917) conception of internal molecular strain (Ramberg\n2003). Figure 4. A schematic representation of the tetrahedral arrangement of\nsubstituents around the carbon atom. Compare the positions of\nsubstituents Y and Z.\n \nGiven that these theories were intrinsically spatial, traditional\nquestions about chemical combination and valency took a new direction:\nWhat is it that holds the atoms together in a particular spatial\narrangement? The answer, of course, is the chemical bond. \nAs structural theory gained widespread acceptance at the end of the\n19th century, chemists began focusing their attention on\nwhat connects the atoms together, constraining the spatial\nrelationships between these atoms. In other words, they began\ninvestigating the chemical bond. Modern theoretical accounts of\nchemical bonding are quantum mechanical, but even contemporary\nconceptions of bonds owe a huge amount to the classical conception of\nbonds developed by G.N. Lewis at the very beginning of the\n20th century. \nG.N. Lewis (1875–1946) was responsible for the first influential\ntheory of the chemical bond (Lewis 1923; see Kohler 1971, 1975 for\nbackground). His theory said that chemical bonds are pairs of\nelectrons shared between atoms. Lewis also distinguished between what\ncame to be called ionic and covalent compounds,\nwhich has proved to be remarkably resilient in modern chemistry. \nIonic compounds are composed of electrically charged ions, usually\narranged in a neutral crystal lattice. Neutrality is achieved when the\npositively charged ions (cations) are of exactly the right number to\nbalance the negatively charged ions (anions). Crystals of common salt,\nfor example, comprise as many sodium cations (Na+) as there\nare chlorine anions (Cl−). Compared to the isolated\natoms, the sodium cation has lost an electron and the chlorine anion\nhas gained an electron. \nCovalent compounds, on the other hand, are either individual molecules\nor indefinitely repeating structures. In either case, Lewis thought\nthat they are formed from atoms bound together by shared pairs of\nelectrons. Hydrogen gas is said to consist of molecules composed of\ntwo hydrogen atoms held together by a single, covalent bond; oxygen\ngas, of molecules composed of two oxygen atoms and a double bond;\nmethane, of molecules composed of four equivalent carbon-hydrogen\nsingle bonds, and silicon dioxide (sand) crystals of indefinitely\nrepeating covalently bonded arrays of SiO2 units. \nAn important part of Lewis’ account of molecular structure concerns\ndirectionality of bonding. In ionic compounds, bonding is\nelectrostatic and therefore radially symmetrical. Hence an individual\nion bears no special relationship to any one of its neighbors. On the\nother hand, in covalent or non-polar bonding, bonds have a definite\ndirection; they are located between atomic centers. \nThe nature of the covalent bond has been the subject of considerable\ndiscussion in the recent philosophy of chemistry literature (Berson\n2008; Hendry 2008; Weisberg 2008). While the chemical bond plays a\ncentral role in chemical predictions, interventions, and explanations,\nit is a difficult concept to define precisely. Fundamental\ndisagreements exist between classical and quantum mechanical\nconceptions of the chemical bond, and even between different quantum\nmechanical models. Once one moves beyond introductory textbooks to\nadvanced treatments, one finds many theoretical approaches to bonding,\nbut few if any definitions or direct characterizations of the bond\nitself. While some might attribute this lack of definitional clarity\nto common background knowledge shared among all chemists, we believe\nthis reflects uncertainty or maybe even ambivalence about the status\nof the chemical bond itself. \nThe new philosophical literature about the chemical bond begins with\nthe structural conception of chemical bonding (Hendry 2008).\nOn the structural conception, chemical bonds are sub-molecular,\nmaterial parts of molecules, which are localized between individual\natomic centers and are responsible for holding the molecule together.\nThis is the notion of the chemical bond that arose at the end of the\n19th century, which continues to inform the practice of\nsynthetic and analytical chemistry. But is the structural conception\nof bonding correct? Several distinct challenges have been raised in\nthe philosophical literature. \nThe first challenge comes from the incompatibility between the\nontology of quantum mechanics and the apparent ontology of the\nchemical bonds. Electrons cannot be distinguished in principle\n (Identity and Individuality in Quantum Theory)\n and hence quantum mechanical descriptions of bonds cannot depend on\nthe identity of particular electrons. If we interpret the structural\nconception of bonding in a Lewis-like fashion, where bonds are\ncomposed of specific pairs of electrons donated by particular atoms,\nwe can see that this picture is incompatible with quantum mechanics. A\nrelated objection notes that both experimental and theoretical\nevidence suggest that electrons are delocalized,\n“smeared out” over whole molecules. Quantum mechanics\ntells us not to expect pairs of electrons to be localized between\nbonded atoms. Furthermore, Mulliken argued that pairing was\nunnecessary for covalent bond formation. Electrons in a hydrogen\nmolecule “are more firmly bound when they have two hydrogen\nnuclei to run around than when each has only one. The fact that two\nelectrons become paired … seems to be largely incidental”\n(1931, p. 360). Later authors point to the stability of the\nH2+ ion in support of this contention. \nDefenders of the structural conception of bonding respond to these\nchallenges by noting that G.N. Lewis’ particular structural account\nisn’t the only possible one. While bonds on the structural conception\nmust be sub-molecular and directional, they need not be electron\npairs. Responding specifically to the challenge from quantum ontology,\nthey argue that bonds should be individuated by the atomic centers\nthey link, not by the electrons. Insofar as electrons participate\nphysically in the bond, they do so not as individuals. All of the\nelectrons are associated with the whole molecule, but portions of the\nelectron density can be localized. To the objection from\ndelocalization, they argue that all the structural account requires is\nthat some part of the total electron density of the molecule\nis responsible for the features associated with the bond and there\nneed be no assumption that it is localized directly between the atoms\nas in Lewis’ model (Hendry 2008, 2010b). \nA second challenge to the structural conception of bonding comes from\ncomputational chemistry, the application of quantum mechanics to make\npredictions about chemical phenomena. Drawing on the work of quantum\nchemist Charles Coulson (1910–1974), Weisberg (2008) has argued\nthat the structural conception of chemical bonding is not robust in\nquantum chemistry. This argument looks to the history of quantum\nmechanical models of molecular structure. In the earliest quantum\nmechanical models, something very much like the structural conception\nof bonding was preserved; electron density was, for the most part,\nlocalized between atomic centers and was responsible for holding\nmolecules together. However, these early models made empirical\npredictions about bond energies and bond lengths that were only in\nqualitative accord with experiment. \nSubsequent models of molecular structure yielded much better agreement\nwith experiment when electron density was “allowed” to\nleave the area between the atoms and delocalize throughout the\nmolecule. As the models were further improved, bonding came to be seen\nas a whole-molecule, not sub-molecular, phenomenon. Weisberg argues\nthat such considerations should lead us to reject the structural\nconception of bonding and replace it with a molecule-wide conception.\nOne possibility is the energetic conception of bonding that\nsays that bonding is the energetic stabilization of molecules.\nStrictly speaking, according to this view, chemical bonds do not\nexist; bonding is real, bonds are not (Weisberg 2008; also see\nCoulson 1952, 1960). \nThe challenges to the structural view of bonding have engendered\nseveral responses in the philosophical and chemical literatures. The\nfirst appeals to chemical practice: Chemists engaged in synthetic and\nanalytic activities rely on the structural conception of bonding.\nThere are well over 100,000,000 compounds that have been discovered or\nsynthesized, all of which have been formally characterized. How can\nthis success be explained if a central chemical concept such as the\nstructural conception of the bond does not pick out anything real in\nnature?  Throughout his life, Linus Pauling\n(1901–1994) defended this view. \nAnother line of objection comes from Berson (2008), who discusses the\nsignificance of very weakly bonded molecules. For example, there are\nfour structural isomers of 2-methylenecyclopentane-1,3-diyl. The most\nstable of the structures does not correspond to a normal bonding\ninteraction because of an unusually stable singlet state, a state\nwhere the electron spins are parallel. Berson suggests that this is a\ncase where “the formation of a bond actually produces a\ndestabilized molecule.” In other words, the energetic conception\nbreaks down because bonding and molecule-wide stabilization come\napart. \nFinally, the “Atoms in Molecules” program (Bader 1991;\nsee Gillespie and Popelier 2001, Chs. 6 & 7 for an exposition)\nsuggests that we can hold on to the structural conception of the bond\nunderstood functionally, but reject Lewis’ ideas about how electrons\nrealize this relationship. Bader, for example, argues that we can\ndefine ‘bond paths’ in terms of topological features of\nthe molecule-wide electron density. Such bond paths have physical\nlocations, and generally correspond closely to classical covalent\nbonds. Moreover they partially vindicate the idea that bonding\ninvolves an increase in electron density between atoms: a bond path is\nan axis of maximal electron density (leaving a bond path in a\ndirection perpendicular to it involves a decrease in electron\ndensity). There are also many technical advantages to this approach.\nMolecule-wide electron density exists within the ontology of quantum\nmechanics, so no quantum-mechanical model could exclude it. Further,\nelectron density is considerably easier to calculate than other\nquantum mechanical properties, and it can be measured empirically\nusing X-ray diffraction techniques. Figure 5. Too many bonds? 60 bond paths from each carbon atom in\nC60 to a trapped Ar atom in the interior.\n \nUnfortunately, Bader’s approach does not necessarily save the day for\nthe structural conception of the bond. His critics point out that his\naccount is extremely permissive and puts bond paths in places that\nseem chemically suspect. For example, his account says that when you\ntake the soccer-ball shaped buckminster fullerene molecule\n(C60) and trap an argon atom inside it, there are 60 bonds\nbetween the carbon atoms and the argon atom as depicted in Figure 5\n(Cerpa et al. 2008). Most chemists would think this implausible\nbecause one of the most basic principles of chemical combination is\nthe fact that argon almost never forms bonds (see Bader 2009 for a\nresponse). \nA generally acknowledged problem for the delocalized account is the\nlack of what chemists call transferability. Central to the structural\nview, as we saw, is the occurrence of functional groups common to\ndifferent substances. Alcohols, for example, are characterized by\nhaving the hydroxyl OH group in common. This is reflected in the\nstrong infra red absorption at 3600cm–1 being taken\nas a tell-tale sign of the OH group. But ab initio QM treatments just\nsee different problems posed by different numbers of electrons, and\nfail to reflect that there are parts of a molecular structure, such as\nan OH group, which are transferable from one molecule to another, and\nwhich they may have in common (Woody 2000, 2012). \nA further issue is the detailed understanding of the cause of chemical\nbonding. For many years, the dominant view, based on the\nHellman-Feynman theorem, has been that it is essentially an\nelectrostatic attraction between positive nuclei and negative electron\nclouds (Feynman 1939). But an alternative, originally suggested by\nHellman and developed by Rüdenberg, has recently come into\nprominence. This emphasizes the quantum mechanical analogue of the\nkinetic energy (Needham 2014). Contemporary accounts may draw on a\nnumber of subtle quantum mechanical features. But these details\nshouldn’t obscure the overriding thermodynamic principle governing\nthe formation of stable compounds by chemical reaction. As Atkins puts\nit, \nThe difficulties faced by this and every other model of bonding have\nled a number of chemists and philosophers to argue for pluralism.\nQuantum chemist Roald Hoffmann writes “A bond will be a bond by\nsome criteria and not by others … have fun with the concept and\nspare us the hype” (Hoffmann 2009, Other Internet Resources). \nWhile most of the philosophical literature about molecular structure\nand geometry is about bonding, there are a number of important\nquestions concerning the notion of molecular structure itself. The\nfirst issue involves the correct definition of molecular structure.\nTextbooks typically describe a molecule’s structure as the equilibrium\nposition of its atoms. Water’s structure is thus characterized by\n104.5º angles between the hydrogen atoms and the oxygen atom. But\nthis is a problematic notion because molecules are not static\nentities. Atoms are constantly in motion, moving in ways that we might\ndescribe as bending, twisting, rocking, and scissoring. Bader\ntherefore argues that we should think of molecular structure as the\ntopology of bond paths, or the relationships between the atoms that\nare preserved by continuous transformations (Bader 1991). \nA second issue concerning molecular structure is even more\nfundamental: Do molecules have the kinds of shapes and directional\nfeatures that structural formulas represent? Given the history we have\ndiscussed so far it seems like the answer is obviously yes. Indeed, a\nnumber of indirect experimental techniques including x-ray\ncrystallography, spectroscopy, and product analysis provide converging\nevidence of not only the existence of shape, but specific shapes for\nspecific molecular species. \nDespite this, quantum mechanics poses a challenge to the notion of\nmolecular shape. In quantum mechanical treatments of molecular\nspecies, shape doesn’t seem to arise unless it is put in by hand.\n(Woolley 1978; Primas 1981; Sutcliffe & Woolley 2012). \nThis tension the between the familiar theories of chemical structure\nand quantum- mechanical accounts of molecules might be resolved in\nseveral ways. One might embrace eliminativism about molecular\nstructure: Quantum mechanics is a more fundamental theory, we might\nargue, and its ontology has no place for molecular structure.\nTherefore, molecular structure doesn’t exist. No philosopher or\nchemist that we are aware of has endorsed this option. Another\npossible response makes a different appeal to the underlying physics.\nSomething must be breaking the wavefunction symmetries and giving\natoms locations in molecules. This might be interactions with other\nmolecules or interactions with measuring devices. Thus, molecular\nshape is partially constituted by interaction and is a relational, not\nintrinsic property (Ramsey 1997). \nA related option is a kind of pragmatism. Hans Primas argues that,\nstrictly speaking, a quantum mechanical description of a molecule has\nto be a whole-universe description. No matter how we draw the\nboundaries of interest around some target molecular system, in\nreality, the system is open and interacting with everything else in\nthe universe. Thus the shape of any particular molecule could be the\nresult of its interactions with anything else in the universe. We only\nget the paradox of molecules having no shape when we treat systems as\nclosed—say a single methane molecule alone in the universe. It\nis fine to treat open systems as closed for pragmatic purposes, but we\nshould always understand that this is an idealization. We shouldn’t\ntreat our idealizations, such as open systems being closed, as\nveridical. Hence there is no incompatibility between quantum mechanics\nand molecular shape (Primas 1981). \nSo despite the ubiquity of structural representations of molecules, it\nturns out that even the notion of molecular shape is not ambiguous.\nPrimas’ approach, which points to the idealization in many quantum\nmechanical models, is accepted by many chemists. But there is nothing\nlike a consensus in the philosophical literature about how to\nunderstand molecular shape. \nIn the final part of this section about structure, we consider a\nfavorite example of philosophers: the thesis that “Water is\nH2O.” This thesis is often taken to be\nuncontroversially true and is used as evidence for semantic\nexternalism and for essentialism about natural kinds (Kripke 1980;\nPutnam 1975,  1990). Since general theses about the theory of reference\nand semantic externalism are beyond the scope of this article, we\nfocus narrowly on chemical essentialism. Is having a common essential\nmicrostructure sufficient to individuate chemical kinds and explain\ntheir general features? And if so, is “being\nH2O” sufficient to individuate water? \nThe essentialist thesis is often stylized by writing “water =\nH2O” or “(all and only) water is\nH2O”. Ignoring the issue of whether the identity\nmakes sense (Needham 2000) and of understanding what the predicates\napply to in the second formulation (Needham 2010a), it is not clear\nthat either formulation expresses the kind of thesis that\nessentialists intend. “H2O” is not a\ndescription of any microstructure. Rather,\n“H2O” is a compositional formula, describing\nthe combining proportions of hydrogen and oxygen to make water. \nA reasonable paraphrase of the standard formulation would be\n“Water is a collection of H2O molecules.”\nHowever, although the expression “H2O molecule”\ndescribes a particular microentity, it by no means exhausts the kinds\nof microparticles in water, and says nothing of the\nmicrostructure by which they are related in water. Describing\nthe microstructure of water completely involves elaborating the\ndetails of this interconnected structure, as well as detailing how\nthey depend on temperature and pressure, and how they change over time\n(Finney 2004). \nLike many other substances, water cannot simply be described as a\ncollection of individual molecules. Here are just a few examples of\nthe complexities of its microstructure: water self-ionizes, which\nmeans that hydrogen and hydroxide ions co-exist with H2O\nmolecules in liquid water, continually recombining to form\nH2O molecules. At the same time, the H2O\nmolecules associate into larger polymeric species. Mentioning these\ncomplexities isn’t just pedantic because they are often what give rise\nto the most striking characteristics of substances. For example, the\nelectrical conductivity of water is due to a mechanism in which a\npositive charge (hydrogen ion) attaches at one point of a polymeric\ncluster, inducing a co-ordinated transfer of charge across the\ncluster, releasing a hydrogen ion at some distant point. The effect is\nthat charge is transferred from one point to another without a\ntransfer of matter to carry it. The hydrogen bonding underlying the\nformation of clusters is also at the root of many other distinctive\nproperties of water including its high melting and boiling points and\nits increase in density upon melting. As van Brakel has argued (1986,\n2000), water is practically the poster child for such\n“non-molecular” substances. \nMaybe water isn’t simply a collection of H2O molecules, but\nit certainly has a microstructure and perhaps the essentialist thesis\ncould be recast along the lines of “Water is whatever has its\nmicrostructure,” writing in the information that would save this\nfrom tautology. But this thesis still endorses the idea that\n“water” is a predicate characterized by what Putnam calls\nstereotypical features. This neglects the importance of macroscopic,\nyet scientifically important, properties such as boiling points,\nspecific heats, latent heats, and so on, from which much of the\nmicrostructure is actually inferred. Indeed, many of the criteria that\nchemists use to determine the sameness and purity of substances are\nmacroscopic, not microscopic. In fact, international standards for\ndetermining the purity of substances like water depend on the careful\ndetermination of macroscopic properties such as the triple-point, the\ntemperature and pressure where liquid, gaseous, and solid phases exist\nsimultaneously (Needham 2011). \nSo is water H2O? In the end, the answer to this question\ncomes down to how one interprets this sentence. Many chemists would be\nsurprised to find out that water wasn’t H2O, but perhaps\nthis is because they read “H2O” as a shorthand\n(Weisberg 2005) or as a compositional formula in the manner we\ndiscussed in the opening of this section. Water is actually\ncharacterized by making reference to both its microstructural and\nmacroscopic features, so this can’t on its own provide a justification\nfor microessentialism. \nFor these reasons, microessentialist claims would need to be grounded\nin chemical classification and explanation: the systems of\nnomenclature developed by IUPAC are based entirely on microstructure,\nas are theoretical explanations of the chemical and spectroscopic\nbehaviour of substances (see Hendry 2016). On the other hand,\nH2O content fails to track usage of the term\n“water” by ordinary-language speakers, who seem to have\ndifferent interests to chemists (Malt 1994). Pluralism is one\nresponse to these tensions: Hasok Chang (2012) urges that even within\nscience, water’s identity with H2O should be left open;\nJulia Bursten (2014) tries to reconcile the special role of\nmicrostructure in chemistry with the failure of microessentialism; and\nJoyce Havstad (2018) argues that chemists’ use of substance concepts\nis just as messy and disunified as biologists’ use of various species\nconcepts. \nOur discussion so far has focused on “static” chemistry:\naccounts of the nature of matter and its structure. But much of\nchemistry involves the transformation of matter from one form to\nanother. This section describes the philosophical issues surrounding\nthe synthesis of one substance from another, as well as chemical\nmechanisms, the explanatory framework chemists use to describe these\ntransformations. \nThere has been a profusion of discussion in the literatures of\nphilosophy of biology and philosophy of neuroscience about the notion\nof mechanisms and mechanistic explanations (e.g., Machamer, Darden,\n& Craver 2000). Yet the production of mechanisms as explanatory\nschemes finds its original home in chemistry, especially organic\nchemistry. Chemical mechanisms are used to classify reactions into\ntypes, to explain chemical behavior, and to make predictions about\nnovel reactions or reactions taking place in novel circumstances\n(Weininger 2014). \nGoodwin (2012) identifies two notions of chemical mechanism at play in\nchemistry. The first or thick notion of mechanism is like a\nmotion picture of a chemical reaction. Such a mechanism traces out the\npositions of all of the electrons and atomic cores of some set of\nmolecules during the course of a reaction, and correlates these\npositions to the potential energy or free energy of the system. One\nmight think of this as an ideal reaction mechanism, as it would\ncontain all information about the time course of a chemical\nreaction. \nIn contrast, the thin notion of a reaction mechanism focuses\non a discrete set of steps. In each step, a set of reactive\nintermediates are generated. These intermediates are quasi-stable\nmolecular species that will ultimately yield the products of the\nreaction. For example, the much studied biomolecular nucleophilic\nsubstitution (SN2) reaction is said to have a single\nreactive intermediate with the incoming nucleophile and outgoing\nleaving group both partially bonded to the reactive carbon center (see\nFigure 6). Such a description of the reaction mechanism is not only\nabstract in that it leaves out much detail, but it is also highly\nidealized. Reactions do not take actually place as a series of\ndiscrete steps, each of which generates a quasi-stable reaction\nintermediate. Figure 6. Thin reaction mechanism for the SN2 reaction.\n \nWhile most textbook treatments of reaction mechanisms begin by\nmentioning the thick notion, the details nearly always turn to thin\nnotions of mechanism. At the same time, formal theoretical treatments\nof reaction mechanisms deal exclusively with the thick notion. Such\ntreatments often attempt to calculate the potential energy function\nfor the reaction from quantum mechanics. Given the importance of the\nthick notion to formal chemical theorizing, why does the thin notion\ndominate the practice of chemistry and find expression in textbooks\nand research articles? \nPart of the reason that thin reaction mechanisms are widely used is\nthat determining thick reaction mechanisms is essentially impossible\nexperimentally, and extremely difficult theoretically. But this cannot\nbe the whole story because when necessary, chemists have been able to\nproduce relevant portions of the thick mechanism. \nAlternatively, Goodwin (2012, p. 311) has argued that, given the\nexplanatory and predictive goals of chemists, not all of the thick\nmechanism is needed. In fact, only a characterization of specific\nstructures, the transition state and stable reactive intermediates,\nare necessary to produce chemical explanations and predictions.\nConstructing mechanisms as a discrete series of steps between stable\nand reactive structures allows the chemist: \nSo chemists’ explanatory goals require that specific features of\nreaction mechanisms can be identified. The rest of the thick mechanism\nwouldn’t necessarily add any explanatorily relevant detail to the\nexplanation. \nChemists typically do not engage in philosophical discussions about\ntheir work. Yet, when discussing the confirmation of reaction\nmechanisms, it is not uncommon to see mention of philosophical issues\nsurrounding confirmation. So why does the study of reaction mechanisms\nmake chemists more philosophically reflective? \nFor one thing, almost all studies aimed at elucidating reaction\nmechanisms rely on indirect techniques. Ideally, elucidating a\nreaction mechanism would be like doing experiments in biomechanics.\nSlow motion video could give direct information about the movement of\nparts and how these movements give rise to the motion of the whole.\nBut we have nothing like a video camera for chemical reactions.\nInstead, after an experimental determination of the reaction products\nand possible isolation of stable intermediate species, chemists rely\non measurements of reaction rates in differing conditions,\nspectroscopy, and isotopic labeling, among other techniques. These\ntechniques help eliminate candidate reaction mechanisms, but do not\nthemselves directly suggest new ones. This emphasis on eliminating\npossibilities has led some chemists to endorse a Popperian,\nfalsificationist analysis of reaction mechanism elucidation (e.g.,\nCarpenter 1984). \nAlthough some chemists have been attracted to a falsificationist\nanalysis, a better analysis of reaction mechanism elucidation is the\naccount of confirmation known as eliminative induction. This\naccount shares falsification’s emphasis on trying to reject\nhypotheses, but argues that the hypotheses not rejected receive some\ndegree of confirmation. So in the case of reaction mechanisms, we\nmight see eliminative induction as a processes whereby chemists: \nIn following this procedure, chemists do more than simply falsify:\nthey add confirmatory power to the mechanisms that haven’t been\neliminated. Indeed, in discussing similar issues, biochemist John\nPlatt (1964) argued that good scientific inference is strong\ninference, whereby the goal in an experiment is to eliminate one\nor more hypotheses. Several contemporary philosophers have endorsed\nthe role of eliminative induction in science (e.g., Bird 2010, Dorling\n1995, Kitcher 1993, Norton 1995). It is easy to see how it can be\nmodeled in Bayesian and other quantitative frameworks for\nconfirmation. Specifically, as particular candidate reaction\nmechanisms are eliminated, the probability that one of the remaining\nmechanisms is correct goes up (see Earman 1992 for details). \nOne difficulty with eliminative induction is the source of the\nrelevant alternative hypotheses, in this case reaction mechanisms.\nThere is no algorithmic procedure for generating these mechanisms, and\nthere is always the possibility that the correct mechanism has not\nbeen considered at all. This is a genuine problem, and we believe that\nit is the very issue that motivates chemists to turn towards\nfalsification when thinking about mechanisms; all they can do is\nevaluate the plausible mechanisms that they have thought of. However,\nwe see eliminative induction as a more plausible reflection of the\nepistemic situation of mechanistic chemists. This problem is not\nuncertainty about mechanisms compatible with\nexperiments—chemists have evidence that weighs in favor of\nthose. Rather, the problem is with unconceived alternatives. Structure\noffers one way to delineate such mechanistic possibilities: Hoffmann\n(1997, Chapter 29) provides a beautiful example of explicitly\neliminative reasoning in his discussion of how H. Okabe and J. R.\nMcNesby used isotopic labelling to eliminate two out of three possible\nmechanisms for the photolysis of ethane to ethylene. But this is an\nissue in all parts of science, not just mechanistic chemistry, and\neliminative induction has always played a role in chemists’ reasoning\nabout structure. How did van ’t Hoff argue for the tetrahedral\ncarbon atom? He argued first that it was possible to account for the\nobserved number and variety of the isomers of certain organic\nsubstances only by taking into account the arrangement of atoms in\nspace. He then defended a tetrahedral geometry for the carbon atom by\nrejecting a square planar arrangement: if carbon’s geometry were\nsquare planar, there would be more isomers of substituted methane than\nare observed. Thus, for instance, disubstituted methane (of the form\nCH2X2) should have two separable isomers if it\nis square planar, whereas only one can be found. Assuming a\ntetrahedral arrangement, in contrast, would be in accord with the\nobserved number of isomers (Brock 1992). \nIn his classic discussion, Hans Reichenbach distinguished between the\ncontext of discovery and the context of\njustification. His distinction was intended to highlight the fact\nthat we could have a logical analysis of scientific justification in\nthe form of confirmation theory, but there could never be a logical\nprocedure for generating hypotheses. Hypothesis generation is the\ncreative part of science, while confirmation is the logical part. This\ndistinction has been challenged in recent years by those that see the\npath of discovery contributing to justification. But chemistry\nprovides a more interesting challenge to Reichenbach: It apparently\ngives us logics of discovery. \nThere are two subfields in which chemists sometimes speak of logics or\nprocedures for discovery. The first is synthetic chemistry. E.J. Corey\n(Corey & Cheng 1989) has proposed that the synthesis of organic\nmolecules can be rationally planned according to the logic of\nretrosynthetic analysis. Systematizing a long tradition in\nsynthetic organic chemistry, Corey shows how one can reason backwards\nfrom a target molecule by finding a series of\n“disconnections,” bonds which one knows how to make. The\nresulting tree of disconnections gives potential pathways for\nsynthesis that can then be evaluated by plausibility, or simply tried\nout in the laboratory. \nAnother area where chemists have developed a logic for discovery is in\nthe area of drug design. Murray Goodman (Goodman & Ro 1995), for\nexample, proposed a four-step procedure for developing candidate\nmolecules for new medication. Say that you were interested in making a\ndrug that would more effectively target one of the morphine receptors\nin the brain. You start by making a molecular analogue of morphine,\nperhaps with a more constrained structure. After successful synthesis,\nyou study the molecule’s three-dimensional structure by spectroscopy\nand computer simulation. You then test your molecule in a biological\nassay to see if you have successfully targeted the receptor and to\nwhat extent. Then based on the information you get, you modify the\nstructure, hopefully improving in each iteration. \nThese examples from chemistry put pressure on Reichenbach’s claim that\nthere cannot be a logic of discovery. Moreover, they illustrate how,\nwhen a science is concerned with creating new things, procedures for\ndiscovery may become essential. \nOne of the perennial topics in philosophy of science concerns\ninter-theoretic relations. In the course of debating whether biology\nis reducible to the physical sciences or whether psychology is\nreducible to biology, many philosophers assume that chemistry has\nalready been reduced to physics. In the past, this assumption was so\npervasive that it was common to read about\n“physico/chemical” laws and explanations, as if the\nreduction of chemistry to physics was complete. Although most\nphilosophers of chemistry would accept that there is no conflict\nbetween the sciences of chemistry and physics (Needham 2010b), many\nwould reject a stronger unity thesis. Most believe that chemistry has\nnot been reduced to physics nor is it likely to be (see Le Poidevin\n2005, for the opposite view, and Hendry & Needham 2007, for a\nrejoinder). \nWhen thinking about the question of reducibility in chemistry, it is\nuseful to break this question into two parts: The first, and more\nfamiliar one to philosophers, concerns the relationship between\nelements, atoms, molecules, and the fundamental particles of physics.\nWe might ask, “Are atomic and molecular species reducible to\nsystems of fundamental particles interacting according to quantum\nmechanics?” A second, less familiar question concerns the\nrelationship between the macroscopic and microscopic descriptions of\nchemical substances. “Are chemical substances reducible to\nmolecular species?” Here, the main question is whether all\nchemical properties that have been defined macroscopically can be\nredefined in terms of the properties of atoms, molecules, and their\ninteractions. \nBogaard (1978), Scerri (1991, 1994) and Hendry (1998) have all\nquestioned the possibility of fully reducing chemical theories about\natoms and molecules to quantum mechanics. Bogaard argues that many key\nchemical concepts such as valence and bonding do not find a natural\nhome in quantum mechanics. In a similar spirit, Scerri points out that\nthe quantum mechanical calculations of atomic spectra standardly\npresented in chemistry textbooks make highly idealized assumptions\nabout the structure of many-electron systems. These approximations are\nwell-motivated on pragmatic grounds. However, they do not allow\nquantum mechanics to “approximately reduce” chemical\nfacts, because the errors introduced by these approximations cannot be\nestimated (Scerri 1991, 1994). Further, one of the most important\nchemical trends, the length of periods in the Periodic Table, cannot\nbe derived from quantum mechanics, unless experimentally derived\nchemical information is specifically introduced (Scerri 1997).\nDrawing on the work of Woolley (1978) and Primas (1981), Hendry (1998)\nargues that there are principled difficulties in accommodating\nmolecular shape within quantum mechanics: the Born-Oppenheimer\napproximation effectively adds structure by hand. Although quantum\nchemistry can be extremely illuminating, these authors argue that it\nhas not reduced chemistry to physics. \nIf one thinks that reduction means deriving the phenomenon of the\nhigher level exclusively from the lower level, then these arguments\nshould settle the question of reduction. More than 80 years after the\ndiscovery of quantum mechanics, chemistry has not been reduced to it.\nBut there are two possible reductionist responses to this\nargument. \nFirst, reductionists can argue that there are no principled reasons\nthat chemical phenomena have not been derived from quantum mechanics.\nThe problem is a lack of computational power and appropriate\napproximation schemes, not anything fundamental. Schwarz (2007) has\nmade this argument against Scerri, claiming that the electronic\nstructure of atoms, and hence the Periodic Table, is in principle\nderivable from quantum mechanics. He believes that quantum chemistry’s\ninability to reduce chemical properties is simply a manifestation of\nthe problems shared by all of the computationally complex sciences.\nDebate then turns to the plausibility of such “reducibility in\nprinciple” claims. \nThere are also arguments that focus, at least implicitly, on\nchemistry’s ontology. A well-known strand of contemporary metaphysics\ndefends physicalism, the doctrine that everything in the\nuniverse is physical (see the entry on\n physicalism).\n According to the physicalist, chemistry is “nothing but”\nphysics, even though chemical explanations and theories are not\nderivable from physics. The physical world is simply composed of the\nfundamental particles of physics. Chemical entities and their\nproperties have no independent reality. \nThe status of arguments for physicalism and the supervenience of\neverything on the physical are contentious within metaphysics proper,\nbut beyond the scope of this entry. Yet we think that the failure of\nchemical theory to be fully derivable from physics raises interesting\nquestions about the doctrine of physicalism. Minimally, it points to\nlongstanding worries that the domain of the physical is not\nwell-defined. If chemical entities such as molecules and ions end up\nbeing part of the physical ontology, one might argue that this was not\na case of the reduction of chemistry to physics at all but simply the\nexpansion of the ontology of physics to encompass the ontology of\nchemistry. \nIndependent studies of the ontology of chemistry on the basis of\nmereology have been undertaken by several authors (Earley 2005,\nHarré and Llored 2011, Needham 2010a). In disputing Scerri’s\n(2000, 2001) argument against claims (Zuo et al. 1999) that orbitals\nhave been observed, Mulder (2010) appeals to a general ontological\ndistinction between entities, which can appropriately be said to\nexist, and states which don’t exist independently but are features of\nentities that exist. Ostrovsky (2005) and Schwarz (2006) take issue\nwith the role of approximations in Scerri’s argument. \nMore controversially, some philosophers of chemistry have argued that\nchemical properties may constrain the behavior of physical systems,\nsomething akin to what philosophers of mind call strong emergence, or\ndownwards causation (Kim 1999). While acknowledging the central role\nof quantum mechanics in understanding structure, Hendry argues that in\nsome cases, molecular structure is an unexplained explainer. The issue\narises when we consider the quantum-mechanical description of\nstructural isomers, molecules with the same atoms, but with different\nmolecular structures. For example, dimethyl ether and ethanol share a\nHamiltonian, the quantum mechanical description of their physical\nstates. Nevertheless, they are very different molecules. Ethanol is\nextremely soluble in water, whereas dimethyl ether is only partially\nsoluble in water. Ethanol boils at 78.4°C, while dimethyl ether\nboils at 34.6°C. Drinking ethanol leads to intoxication, while\ndimethyl ether is toxic in quite different ways. Quantum mechanics can\nshow how each of these structures is energetically stable, and\nilluminate how they interact with other molecules and radiation to\nexplain the chemical and spectroscopic behaviour of ethanol and\ndimethyl ether, but the different structures are introduced as\nunexplained initial conditions. While he acknowledges that these facts\nare not incompatible with the claim that structure is reducible,\nHendry argues that strong emergence is just as plausible an\ninterpretation as reduction of the explanatory relationship between\nchemistry and quantum mechanics (2006b, 2010a). \nSo far we have considered intertheoretic relationships between\nchemistry and physics. What about within chemistry itself? Do the\nmacroscopic and microscopic theories of chemistry align perfectly? Are\nall macroscopic properties of substances ultimately reducible to\nmicroscopic properties? In other words, if we have a macroscopic\ndescription of matter and a thermodynamic theory about how it behaves,\ncan all of this be reduced to a molecular description? The answer has\nseemed to be “yes” to many philosophers and chemists, but\nphilosophers of chemistry have urged caution here. \nConsider first the relatively simple case of gas temperature, which\nhas often been supposed reducible to the average kinetic energy of the\ngas’s molecules (cf. Nagel 1961, p. 343). A particular average\nkinetic energy of the molecules is only a necessary condition for\nhaving a given temperature, however. Only gases at equilibrium have a\ndefinite temperature, when all the spatial parts have the same\ntemperature as the whole (reflecting the fact that temperature is an\nintensive property). A sufficient condition would need to complement\nthe average kinetic energy with a microscopic correlate of the\nmacroscopic condition of being at thermodynamic equilibrium.\nStatistical mechanics specifies the relevant correlative condition as\nthat of the energy being distributed over the gas molecules in\naccordance with the Boltzmann distribution. But the Boltzmann\ndistribution is expressed as a function of the temperature, and its\nderivation from Boltzmann’s microscopic construal of entropy appeals\nto the thermodynamic law connecting entropy with temperature.\nAccordingly, the necessary and sufficient microscopic condition for\ngas temperature becomes circular when construed as a reduction of the\nconcept of temperature (Needham 2009b; Bishop 2010) \nAlthough the reduction of temperature to microscopic properties is\nproblematic, it is a relatively easy candidate for reduction.\nProperties concerned with chemical changes such as phase transitions,\nsolubility, and reactivity, are considerably more complex. As we\ndiscussed in Section 4.5, a purely microscopic description of matter\nis not coextensive with all chemical properties. Solubility, for\nexample, is not fully explained by microscopic properties. While we\ncan explain in rough qualitative fashion that substances dissolve when\ntheir ions or molecules have more affinity for the solvent than they\ndo for each other, this doesn’t recover the subtle, quantitative\nfeatures of solubility. It also leaves the solubility of nonionic\nsubstances untouched. Predicting these features requires appeals to\nthermodynamics, and the alleged reduction of thermodynamics to\nstatistical mechanics is considered highly contentious (Sklar\n1993). \nAs we have seen in this case, even very fruitful applications of\nphysical and chemical theory at the microscopic level are often\ninsufficient to reduce chemically important properties. Whether the\ngeneral notion of chemical substance, or the property of being a\nparticular substance for each of the millions of known substances, can\nbe reduced to microstructure needs to be demonstrated and not merely\nassumed. While there is no in-principle argument that reductions will\nalways be impossible, essential reference is made back to some\nmacroscopically observable chemical property in every formal attempt\nof reduction that we are aware of. In the absence of definite\narguments to the contrary, it seems reasonable to suppose that\nchemistry employs both macroscopic and microscopic concepts in\ndetailed theories which it strives to integrate into a unified view.\nAlthough plenty of chemistry is conducted at the microscopic level\nalone, macroscopic chemical properties continue to play important\nexperimental and theoretical roles throughout chemistry. \nIn the background of all of these debates about chemical reduction are\nissues concerning the criteria for successful reduction. All of the\nliterature that we have discussed make explicit or implicit reference\nto Nagel’s influential account of reduction. Beyond the philosophy of\nchemistry literature, this account has also been presupposed by\ncritics of particular reductionist theses (e.g. Davidson 1970), even\nwhen making points about the inapplicability of Nagel’s account to\nparticular sciences (Kitcher 1984). But Nagel’s account of reduction\nis thought by many to be unrealistic and inapplicable to actual\nscience because of the logical requirements it assumes. \nPerhaps part of the anti-reductionist consensus in the philosophy of\nchemistry literature is driven by the stringent demands of Nagel’s\naccount. But even if Nagel’s account is weakened to allow\napproximative arguments (as Hempel modified his DN model of\nexplanation), as some advocates of reductionism have urged (e.g.,\nSchaffner 1967; Churchland 1985), this still doesn’t circumvent the\nproblem of the appeal to macroscopic properties in the explanation of\nmicroscopic properties. Current chemical theory makes essential\nreference to both microscopic and macroscopic chemical concepts with\nboth chemical and quantum mechanical origins. We know of no convincing\nsubstantial examples where either of these aspects have been entirely\nexcised. \nAlmost all contemporary chemical theorizing involves modeling, the\nindirect description and analysis of real chemical phenomena by way of\nmodels. From the 19th century onwards, chemistry was\ncommonly taught and studied with physical models of molecular\nstructure. Beginning in the 20th century, mathematical\nmodels based on classical and quantum mechanics were successfully\napplied to chemical systems. This section discusses some of the\nphilosophical questions that arise when we consider modeling in\nchemistry more directly. \nChemistry’s modeling tradition began with physical models of atoms and\nmolecules. In contemporary chemical education, much emphasis is placed\non the construction and manipulation of such models. Students in\norganic chemistry classes are often required to purchase plastic\nmolecular modeling kits, and it is not uncommon to see complex\nmolecular structures built from such kits in professional laboratory\nsettings. \nThe use of molecular models gained special prominence in the middle of\nthe 19th, helping chemists to understand the significance\nof molecular shape (Brock 2000). While such structures could be\nrepresented on paper, physical models gave an immediacy and an ease of\nvisualization that sketches alone did not provide. In the middle of\nthe twentieth century, the discovery of the double helical structure\nof DNA was aided by the manipulation of physical models (Watson\n1968). \nWhile physical modeling has been important historically, and is still\na central part of chemical education and some investigations in\nstereochemistry, contemporary chemical models are almost always\nmathematical. Families of partially overlapping, partially\nincompatible models such as the valence bond, molecular\norbital, and semi-empirical models are used to explain\nand predict molecular structure and reactivity. Molecular\nmechanical models are used to explain some aspects of reaction\nkinetics and transport processes. And lattice models are used\nto explain thermodynamic properties such as phase. These and other\nmathematical models are ubiquitous in chemistry textbooks and\narticles, and chemists see them as central to chemical theory. \nChemists are very permissive about which kinds of mathematical\nstructures can serve as models. But while just about any kind of\nmathematical structure can serve as a chemical model, different types\nof systems lend themselves to particular kinds of mathematical\nstructures used in modeling. For example, the most common kinds of\nmathematical structures employed in quantum chemistry are state\nspaces, which typically correlate sub-molecular particle distances\nwith the total energy of chemical systems. Other parts of chemical\nmodeling are dynamic, hence they employ trajectory spaces, which can\nrepresent the course of a reaction over time. Still other kinds of\nmathematical structures such as graphs and groups can be employed to\nmodel molecular structure and symmetry. \nThe purpose of many exercises in chemical modeling is to learn about\nreal systems. In these cases, the model must bear certain\nrelationships to real-world systems. But these relationships needn’t\nalways be of extremely high fidelity. For example, Linus Pauling\n(1939) and early proponents of the simple valence bond model believed\nthat this model captured the essential physical interactions that give\nrise to chemical bonding. This method is closely related to Lewis’\nconception of bonding, treating molecules as composed of atomic cores\n(nuclei together with inner-shell electrons) and valence electrons\nwhich give rise to localized bonds. It stands in contrast to the\nmolecular orbital method, which doesn’t localize the bonding electrons\nto any particular part of the molecule. Modern quantum chemists think\nof the valence bond model as a template for building models of greater\ncomplexity. Thus if a modern quantum chemist deploys the simple\nvalence bond model to study a real molecule, she does so with a much\nlower standard of fidelity than Pauling would have. Her use of the\nmodel is only intended to give a first approximation to the most\nimportant features of the system. \nMuch of contemporary theoretical research in chemistry involves the\napplication of quantum mechanics to chemistry. While exact solutions\nto the quantum mechanical descriptions of chemical phenomena have not\nbeen achieved, advances in theoretical physics, applied mathematics,\nand computation have made it possible to calculate the chemical\nproperties of many molecules very accurately and with few\nidealizations. The approach of striving for ever more accurate\ncalculations with decreasing levels of idealization is endorsed by\nmany quantum chemists. For example, the development team of Gaussian,\none of the leading packages for doing quantum chemical calculations,\nexplicitly endorses this position. While they admit that there are\nmany considerations that enter into the choice of the degree of\napproximation or “level of theory” for any calculation,\nthe goal is to de-idealize the models as much as possible. They argue\nthat quantum chemical calculations which are arbitrarily close to the\nexact solutions are the “limit to which all approximate methods\nstrive” (Foresman & Frisch 1996). \nThis method of developing chemical theory relies on a systematic\nrefinement of theories, attempting to bring them closer to the truth.\nPhilosophers of science have called this process Galilean\nidealization, because as in Galileo’s work, idealizations are\nintroduced for reasons of tractability and are removed as soon as\npossible (McMullin 1985; Weisberg 2007b). But not all chemists have\nshared this focus on ever more accurate calculations. Reflecting on\nwhy he didn’t choose this path in his own career, theorist Roald\nHoffmann wrote: \nElsewhere in this article, Hoffmann admits that quantum chemistry is\nenormously successful in its predictive power, and continues to give\nus better approximations to the fundamental theory. Yet the attitude\nexpressed in this paragraph seems to be that simple, idealized models\nare needed for chemical theorizing. Thus, the central philosophical\nquestion arises: Given the availability of models that are closer to\nthe truth, why work with idealized ones? \nOne answer is given by Felix Carroll, a physical organic chemist: \nCarroll does not elaborate on these issues, but this passage contains\nthe central message: Simple models prevent our theories from having a\n“black-box” character, meaning that they will not simply\nbe a recipe for calculating without giving any physical insight.\nCarroll claims that simple models are necessary in order to expose the\nmechanisms by which chemical phenomena come about. High-level\ntheoretical calculations are not capable of showing us these\nmechanistic relationships, even though they are based on the quantum\nmechanical principles that describe the fundamental physics of the\nsystem. Or, as Hoffmann puts the point: “[I]f understanding is\nsought, simpler models, not necessarily the best in predicting all\nobservables in detail, will have value. Such models may highlight the\nimportant causes and channels” (Hoffmann, Minkin, &\nCarpenter 1996). \nWhy should it be the case that simple models have less black-box\ncharacter than others? One explanation appeals to our cognitive\nlimitations. We can only hold a couple of steps of an argument in our\nmind at once. Modern, high-level calculations can take hours or days\nto compute using fast computers. Even if every step was made explicit\nby the computer, it would be impossible to hold the calculational\nsteps in mind and hence hard to understand the reason for the result,\neven if one was convinced that the answer was correct. Paul Humphreys\nhas called this the epistemic opacity of simulations\n(2004). \nThere is a second reason for employing simple, more highly idealized\nmodels in chemistry, which stems from the explanatory traditions of\nchemistry. In developing this point, Hoffmann argues that there are\ntwo modes of explanation that can be directed at chemical systems:\nhorizontal and vertical (Hoffmann 1997). Vertical\nexplanations are what philosophers of science call deductive\nnomological explanations. These explain a chemical phenomenon by\nderiving its occurrence from quantum mechanics. Calculations in\nquantum chemistry are often used to make predictions, but insofar as\nthey are taken to explain chemical phenomena, they follow this\npattern. By showing that a molecular structure is stable, the quantum\nchemist is reasoning that this structure was to be expected given the\nunderlying physics. \nIn contrast with vertical mode, the horizontal mode of explanation\nattempts to explain chemical phenomena with chemical concepts. For\nexample, all first year organic chemistry students learn about the\nrelative reaction rates of different substrates undergoing the\nSN2 reaction. An organic chemist might ask “Why does\nmethyl bromide undergo the SN2 reaction faster than methyl\nchloride?” One answer is that “the leaving group\nBr− is a weaker base than Cl−, and\nall things being equal, weaker bases are better leaving groups.”\nThis explains a chemical reaction by appealing to a chemical property,\nin this case, the weakness of bases. \nHoffmann doesn’t say much about the differing value of the horizontal\nand vertical explanations, but one important difference is that they\ngive us different kinds of explanatory information. Vertical\nexplanations demonstrate that chemical phenomena can be derived from\nquantum mechanics. They show that, given the (approximate) truth of\nquantum mechanics, the phenomenon observed had to have happened.\nHorizontal explanations are especially good for making\ncontrastive explanations, which allows the explanation of\ntrends. Consider again our example of the rate of an SN2\nreaction. By appealing to the weakness of Br− as a\nbase, the chemist invokes a chemical property, shared across other\nmolecules. This allows her to explain methyl bromide’s reactivity as\ncompared to methyl chloride, and also methyl fluoride, methyl iodide,\netc. Insofar as chemists want to explain trends, they make contrastive\nexplanations using chemical concepts. \nReflecting on the nature of chemical theorizing, the eminent chemical\ntheorist Charles Coulson (1910–1974) makes a similar point. He\nwrote: \nAlthough Coulson, Carroll, and Hoffmann defend the use of simple,\nidealized models to generate horizontal explanations, it is not clear\nthat quantum calculations can never generate contrastive explanations.\nAlthough single vertical explanations are not contrastive, a theorist\ncan conduct multiple calculations and in so doing, generate the\ninformation needed to make contrastive explanations. Many of the best\nexamples of quantum chemistry have this character: a series of closely\nrelated calculations, attempting to get at chemically relevant\ntrends.","contact.mail":"r.f.hendry@durham.ac.uk","contact.domain":"durham.ac.uk"}]
