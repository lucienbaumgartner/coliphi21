[{"date.published":"2012-07-18","date.changed":"2018-02-16","url":"https://plato.stanford.edu/entries/computing-responsibility/","author1":"Merel Noorman","entry":"computing-responsibility","body.text":"\n\n\n\nTraditionally philosophical discussions on moral responsibility have\nfocused on the human components in moral action. Accounts of how to\nascribe moral responsibility usually describe human agents performing\nactions that have well-defined, direct consequences. In today’s\nincreasingly technological society, however, human activity cannot be\nproperly understood without making reference to technological\nartifacts, which complicates the ascription of moral responsibility\n(Jonas 1984; Waelbers\n  2009).[1]\nAs we interact with and through these artifacts, they affect the\ndecisions that we make and how we make them (Latour 1992). They\npersuade, facilitate and enable particular human cognitive processes,\nactions or attitudes, while constraining, discouraging and inhibiting\nothers. For instance, internet search engines prioritize and present\ninformation in a particular order, thereby influencing what internet\nusers get to see. As Verbeek points out, such technological artifacts\nare “active mediators” that “actively co-shape\npeople’s being in the world: their perception and actions,\nexperience and existence” (2006, p. 364). As active mediators,\nthey change the character of human action and as a result it challenges\nconventional notions of moral responsibility\n(Jonas 1984; Johnson 2001).\n\n\n\nComputing presents a particular case for understanding the role of\ntechnology in moral responsibility. As these technologies become a\nmore integral part of daily activities, automate more decision-making\nprocesses and continue to transform the way people communicate and\nrelate to each other, they further complicate the already problematic\ntasks of attributing moral responsibility. The growing pervasiveness\nof computer technologies in everyday life, the growing complexities of\nthese technologies and the new possibilities that they provide raise\nnew kinds of questions: who is responsible for the information\npublished on the Internet? Who is responsible when a self-driving\nvehicle causes an accident? Who is accountable when electronic records\nare lost or when they contain errors? To what extent and for what\nperiod of time are developers of computer technologies accountable for\nuntoward consequences of their products? And as computer technologies\nbecome more complex and behave increasingly autonomous can or should\nhumans still be held responsible for the behavior of these\ntechnologies?\n\n\n\nThis entry will first look at the challenges that computing poses to\nconventional notions of moral responsibility. The discussion will then\nreview two different ways in which various authors have addressed these\nchallenges: 1) by reconsidering the idea of moral agency and 2) by\nrethinking the concept of moral responsibility itself.\n\n\n\nMoral responsibility is about human action and its intentions and\nconsequences (Fisher 1999, Eshleman 2016).  Generally speaking a\nperson or a group of people is morally responsible when their\nvoluntary actions have morally significant outcomes that would make it\nappropriate to blame or praise them. Thus, we may consider it a\nperson’s moral responsibility to jump in the water and try to rescue\nanother person, when she sees that person drowning.  If she manages to\npull the person from the water we are likely to praise her, whereas if\nshe refuses to help we may blame her. Ascribing moral responsibility\nestablishes a link between a person or a group of people and someone\nor something that is affected by the actions of this person or\ngroup. The person or group that performs the action and causes\nsomething to happen is often referred to as the agent. The\nperson, group or thing that is affects by the action is referred to as\nthe patient. Establishing a link in terms of moral\nresponsibility between the agent and the patient can be done both\nretrospectively as well as prospectively. That is, sometimes\nascriptions of responsibility involve giving an account of who was at\nfault for an accident and who should be punished. It can also be about\nprospectively determining the obligations and duties a person has to\nfulfill in the future and what she ought to do.  \n\nHowever, the circumstances under which it is appropriate to ascribe moral\nresponsibility are not always clear. On the one hand the concept has varying\nmeanings and debates continue on what sets moral responsibility apart\nfrom other kinds of responsibility (Hart 1968). The concept is\nintertwined and sometimes overlaps with notions of accountability,\nliability, blameworthiness, role-responsibility and causality. Opinions\nalso differ on which conditions warrant the attribution of moral\nresponsibility; whether it requires an agent with free will or not and\nwhether humans are the only entities to which moral responsibility can\nbe attributed (see the entry on\n moral responsibility). \n\nOn the other hand, it can be difficult to establish a direct link\nbetween the agent and the patient because of the complexity involved\nin human activity, in particular in today’s technological society.\nIndividuals and institutions generally act with and in\nsociotechnical systems in which tasks are distributed among\nhuman and technological components, which mutually affect each other\nin contingent ways (Bijker, Hughes and Pinch 1987). Increasingly\ncomplex technologies can exacerbate the difficulty of identifying who\nor what is ‘responsible’. When something goes wrong, a\nretrospective account of what happened is expected and the more\ncomplex the system, the more challenging is the task of ascribing\nresponsibility (Johnson and Powers 2005). Indeed, Matthias argues that\nthere is a growing ‘ responsibility gap’: the more complex\ncomputer technologies become and the less human beings can directly\ncontrol or intervene in the behavior of these technologies, the less\nwe can reasonably hold human beings responsible for these technologies\n(Matthias, 2004).  \n\nThe increasing pervasiveness of computer technologies poses various\nchallenges to figuring out what moral responsibility entails and how\nit should be properly ascribed. To explain how computing complicates\nthe ascription of responsibility we have to consider the conditions\nunder which it makes sense to hold someone responsible. Despite the\nongoing philosophical debates on the issue, most analysis of moral\nresponsibility share at least the following three conditions (Eshleman\n2016; Jonas 1984): \n\nA closer look at these three conditions shows that computing can\ncomplicate the applicability of each of these conditions. \n\nIn order for a person to be held morally responsible for a\nparticular event, she has to be able to exert some kind of influence on\nthat event. It does not make sense to blame someone for an accident if\nshe could not have avoided it by acting differently or if she had no\ncontrol over the events leading up to the accident. \n\nHowever, computer technologies can obscure the causal connections\nbetween a person’s actions and the eventual consequences. Tracing the\nsequence of events that led to a computer-related catastrophic\nincident, such as a plane crash, usually leads in many directions, as\nsuch incidents are seldom the result of a single error or\nmishap. Technological accidents are commonly the product of an\naccumulation of mistakes, misunderstanding or negligent behavior of\nvarious individuals involved in the development, use and maintenance\nof computer systems, including designers, engineers, technicians,\nregulators, managers, users, manufacturers, sellers, resellers and\neven policy makers.   The involvement of multiple actors in the development and\ndeployment of technologies gives rise to what is known as the problem\nof ‘many hands’: it is difficult to determine who was\nresponsible for what when multiple individuals contributed to the\noutcome of events (Friedman 1990; Nissenbaum 1994; Jonas 1984; van de\nPoel et al. 2015). One classic example of the problem of many hands in\ncomputing is the case of the malfunctioning radiation treatment\nmachine Therac-25 (Leveson and Turner 1993; Leveson 1995). This\ncomputer-controlled machine was designed for the radiation treatment\nof cancer patients as well as for X-rays. During a two-year period in\nthe 1980s the machine massively overdosed six patients, contributing\nto the eventual death of three of them. These incidents were the\nresult of the combination of a number of factors, including software\nerrors, inadequate testing and quality assurance, exaggerated claims\nabout the reliability, bad interface design, overconfidence in\nsoftware design, and inadequate investigation or follow-up on accident\nreports.  Nevertheless, in their analysis of the events Leveson and\nTurner conclude that it is hard to place the blame on a single\nperson. The actions or negligence of all those involved might not have\nproven fatal were it not for the other contributing events. This is\nnot to say that there is no moral responsibility in this case\n(Nissenbaum 1994; Gotterbarn 2001; Coeckelbergh 2012; Floridi 2013),\nas many actors could have acted differently, but it makes it difficult\nto retrospectively identify the appropriate person that can be called\nupon to answer and make amends for the outcome. \n\nAdding to the problem of many hands is the temporal and physical\ndistance that computing creates between a person and the consequences\nof her actions, as this distance can blur the causal connection\nbetween actions and events (Friedman 1990). Computational technologies\nextend the reach of human activity through time and space. With the\nhelp of social media and communication technologies people can\ninteract with others on the other side of the world. Satellites and\nadvanced communication technologies allow pilots to fly a\nremote-controlled drone from their ground-control station half way\nacross the world. These technologies enable people to act over greater\ndistances, but this remoteness can dissociate the original actions\nfrom its eventual consequences (Waelbers 2009; Coeckelbergh 2013). When\na person uses a technological artifact to perform an action thousands\nof miles a way, that person might not know the people that will be\naffected and she might not directly, or only partially, experience the\nconsequences.  This can reduce the sense of responsibility the person\nfeels and it may interfere with her ability to fully comprehend the\nsignificance of her actions. Similarly, the designers of an automated\ndecision-making system determine ahead of time how decisions should be\nmade, but they will rarely see how these decisions will impact the\nindividuals they affect. Their original actions in programming the\nsystem may have effects on people years later. \n\nThe problem of many hands and the distancing effects of the use of\ntechnology illustrate the mediating role of technological artifacts in\nthe confusion about moral responsibility. Technological artifacts\nbring together the various different intentions of their creators and\nusers.  People create and deploy technologies with the objective of\nproducing some effect in the world. Software developers develop an\nInternet filter, often at the request of a manager or a client, with\nthe aim of shielding particular content from its users and influencing\nwhat these users can or cannot read. The software has inscribed in its\ndesign the various intentions of the developers, managers and clients;\nit is poised to behave, given a particular input, according to their\nideas about which information is appropriate (Friedman 1997). Moral\nresponsibility can therefore not be attributed without looking at the\ncausal efficacy of these artifacts and how they constrain and enable\nparticular human activities. However, although technological artefacts\nmay influence and shape human action, they do not determine it.\n\nThey are not isolated instruments that mean and work the same\nregardless of why, by whom, and in what context they are used; they\nhave interpretive flexibility (Bijker et al. 1987) or multistability (Ihde\n 1990).[2]\nAlthough the design of the technology provides\na set of conditions for action, the form and meaning of these actions\nis the result of how human agents choose to use these technologies in\nparticular contexts. People often use technologies in ways unforeseen\nby their designers. This interpretive flexibility makes it difficult\nfor designers to anticipate all the possible outcomes of the use of\ntheir technologies. The mediating role of computer technologies\ncomplicates the effort of retrospectively tracing back the causal\nconnection between actions and outcomes, but it also complicates\nforward-looking responsibility. \n\nAs computer technologies shape how people perceive and experience\nthe world, they affect the second condition for attributing moral\nresponsibility. In order to make appropriate decisions a person has to\nbe able to consider and deliberate about the consequences of her\nactions. She has be aware of the possible risks or harms that her\nactions might cause. It is unfair to hold someone responsible for\nsomething if they could not have known that their actions might lead to\nharm.  On the one hand computer technologies can help users to think\nthrough what their actions or choices may lead to. They help the user\nto capture, store, organize and analyze data and information (Zuboff\n1982). For example, one often-named advantage of remote-controlled\nrobots used by the armed forces or rescue workers is that they enable\ntheir operators to acquire information that would not be able\navailable without them. They allow their operators to look\n“beyond the next hill” or “around the next\ncorner” and they can thus help operators to reflect on what the\nconsequences of particular tactical decisions might be (US Department\nof Defense 2009). Similarly, data analysis tools can find patterns in\nlarge volumes of data that human data analysts cannot manually process\n(Boyd and Crawford 2012).  \n\nOn the other hand the use of computers can constrain the ability of\nusers to understand or consider the outcomes of their actions. These\ncomplex technologies, which are never fully free from errors,\nincreasingly hide the automated processes behind the interface (Van\nden Hoven 2002). An example that illustrates how computer technologies\ncan limit understanding of the outcomes are the controversial risk\nassessment tools used by judges in several states in the U.S. for\nparole decisions and sentencing. In 2016 a civil society organization\nfound, based on an analysis of the risk scores of 7000 defendants\nproduced by one particular algorithm, that the scores poorly reflected\nthe actual recidivism rate and seemed to have a racial bias (Angwin et\nal. 2016). Regardless of whether its findings were correct, what is\nparticularly relevant here is that the investigation also showed that\njudges did not have a full understanding of how the probabilities were\ncalculated, because the algorithm was proprietary. The judges were\nbasing their sentencing on the suggestion of an algorithm that they\ndid not fully understand. This is the case for most computer\ntechnologies today. Users only see part of the many computations that\na computer performs and are for the most part unaware of how it\nperforms them; they usually only have a partial understanding of the\nassumptions, models and theories on which the information on their\ncomputer screen is based.   The opacity of many computer systems can get in the way of\nassessing the validity and relevance of the information and can\nprevent a user from making appropriate decisions. People have a\ntendency to either rely too much or not enough on the accuracy\nautomated systems (Cummings 2004; Parasuraman & Riley 1997). A\nperson’s ability to act responsibly, for example, can suffer when she\ndistrust the automation as result of a high rate of false alarms. In\nthe Therac 25 case, one of the machine’s operators testified that she\nhad become used to the many cryptic error messages the machine gave\nand most did not involve patient safety (Leveson and Turner 1993,\np.24). She tended ignore them and therefore failed to notice when the\nmachine was set to overdose a patient. Too much reliance on automated\nsystems can have equally disastrous consequences. In 1988 the missile\ncruiser U.S.S. Vincennes shot down an Iranian civilian jet airliner,\nkilling all 290 passengers onboard, after it mistakenly identified the\nairliner as an attacking military aircraft (Gray 1997).  The cruiser\nwas equipped with an Aegis defensive system that could automatically\ntrack and target incoming missiles and enemy aircrafts.  Analyses of\nthe events leading up to incident showed that overconfidence in the\nabilities of the Aegis system prevented others from intervening when\nthey could have. Two other warships nearby had correctly identified\nthe aircraft as civilian. Yet, they did not dispute the Vincennes’\nidentification of the aircraft as a military aircraft. In a later\nexplanation Lt. Richard Thomas of one of the nearby ships stated,\n“We called her Robocruiser… she always seemed to have a\npicture… She always seemed to be telling everybody to get on or\noff the link as though her picture was better” (as quoted in\nGray 1997, p. 34). The captains of both ships thought that the\nsophisticated Aegis system provided the crew of Vincennes with\ninformation they did not have.  Considering the possible consequences of one’s actions is further\ncomplicated as computer technologies make it possible for humans to do\nthings that they could not do before. Several decades ago, the\nphilosopher Ladd pointed out, “[C]omputer technology has created\nnew modes of conduct and new social institutions, new vices and new\nvirtues, new ways of helping and new ways of abusing other\npeople” (Ladd 1989, p. 210–11). Computer technologies of\ntoday have had a similar effect.  The social or legal conventions that\ngovern what we can do with these technologies take some time to emerge\nand the initial absence of these conventions contributes to confusion\nabout responsibilities (Taddeo and Floridi 2015). For example, the\nability for users to upload and share text, videos and images publicly\non the Internet raises a whole set of questions about who is\nresponsible for the content of the uploaded material. Such questions\nwere at the heart of the debate about the conviction of three Google\nexecutives in Italy for a violation of the data protection act (Sartor\nand Viola de Azevedo Cunha 2010). The case concerned a video on\nYouTube of four students assaulting a disabled person. In response to\na request by the Italian Postal Police, Google, as owner of YouTube,\ntook the video down two months after the students uploaded it. The\njudge, nonetheless, ruled that Google was criminally liable for\nprocessing the video without taking adequate precautionary measures to\navoid privacy violations. The judge also held Google liable for\nfailing to adequately inform the students, who uploaded the videos, of\ntheir data protection obligations (p. 367). In the ensuing debate\nabout the verdict, those critical of the ruling insisted that it\nthreatened the freedom of expression on the Internet and it sets a\ndangerous precedent that can be used by authoritarian regimes to\njustify web censorship (see also Singel 2010). Moreover, they claimed\nthat platform providers could not be held responsible for the actions\nof their users, as they could not realistically approve every upload\nand it was not their job to censure.  Yet, others instead argued that\nit would be immoral for Google to be exempt from liability for the\ndamage that others suffered due to Google’s profitable commercial\nactivity. Cases like this one show that in the confusion about the\npossibilities and limitations of new technologies it can be difficult\nto determine one’s moral obligations to others.  The lack of experience with new technological innovations can also\naffect what counts as negligent use of the technology. In order to\noperate a new computer system, users typically have to go through a\nprocess of training and familiarization with the system. It requires\nskill and experience to understand and imagine how the system will\nbehave (Coeckelbergh and Wackers 2007). Friedman describes the case of\na programmer who invented and was experimenting with a ‘computer\nworm’, a piece of code that can replicate itself. At the time\nthis was a relatively new computational entity (1990). The programmer\nreleased the worm on the Internet, but the experiment quickly got out\nof the control when the code replicated much faster than he had\nexpected (see also Denning 1989). Today we would not find this a\nsatisfactory excuse, familiar as we have become with computer worms\nand viruses. However, Friedman poses the question of whether the\nprogrammer really acted in a negligent way if the consequences were\ntruly unanticipated. Does the computer community’s lack of experience\nwith a particular type of computational entity influence what we judge\nto be negligent behavior? \n\nThe freedom to act is probably the most important condition for\nattributing moral responsibility and also one of the most\ncontested. We tend to excuse people from moral blame if they had no\nother choice but to act in the way that they did. We typically do not\nhold people responsible if they were coerced or forced to take\nparticular actions.  In moral philosophy, the freedom to act can also\nmean that a person has free will or autonomy (Fisher 1999). Someone\ncan be held morally responsible because she acts on the basis of her\nown authentic thoughts and motivations and has the capacity to control\nher behavior (Johnson 2001). Note that this conception of autonomy is\ndiffers from the way the term ‘autonomy’ is often used in\ncomputer science, where it tends to refer to the ability of a robot or\ncomputer system to independently perform complex tasks in\nunpredictable environments for extended periods of time (Noorman\n2009).  \n\nNevertheless, there is little consensus on what capacities human\nbeings have, that other entities do not have, which enables them to act\nfreely (see the entries on\n free will,\n autonomy in moral and political philosophy,\n personal autonomy\nand\n compatibilism).\nDoes it require rationality, emotion, intentionality or cognition?\nIndeed, one important debate in moral philosophy centers on the\nquestion of whether human beings really have autonomy or free will?\nAnd, if not, can moral responsibility still be attributed (Eshleman\n2016)? \n\nIn practice, attributing autonomy or free will to humans on the\nbasis of the fulfillment of a set of conditions turns out to be a less\nthan straightforward endeavor. We attribute autonomy to persons in\ndegrees. An adult is generally considered to be more autonomous than a child.\nAs individuals in a society our autonomy is thought to vary because\nwe are manipulated, controlled or influenced by forces outside of\nourselves, such as by our parents or through peer pressure. Moreover,\ninternal physical or psychological influences, such as addictions or\nmental problems, are perceived as further constraining the autonomy of\na person. \n\nComputing, like other technologies, adds an additional layer of\ncomplexity to determining whether someone is free to act, as it affects\nthe choices that humans have and how they make them. One of the biggest\napplication areas of computing is the automation of decision-making\nprocesses and control. Automation can help to centralize and increase\ncontrol over multiple processes for those in charge, while it limits\nthe discretionary power of human operators on the lower-end of the\ndecision-making chain. An example is provided by the automation of\ndecision-making in public administration (Bovens and Zouridis 2002).\nLarge public sector organizations have over the last few decades\nprogressively standardized and formalized their production processes.\nThe process of issuing decisions about student loans, speeding tickets\nor tax returns is carried out almost entirely by computer systems. This\nhas reduced the scope of the administrative discretion that many\nofficials, such as tax inspectors, welfare workers, and policy\nofficers, have in deciding how to apply formal policy rules in\nindividual cases. Citizens no longer interact with officials that have\nsignificant responsibility in applying their knowledge of the rules and\nregulations to decide what is appropriate (e.g., would it be better to\nlet someone off with a warning or is a speeding ticket required?).\nRather, decisions are pre-programmed in the algorithms that apply the\nsame measures and rules regardless of the person or the context (e.g.,\na speeding camera does not care about the context). Responsibility for\ndecisions made, in these cases, has moved from ‘street-level\nbureaucrats’ to the ‘system-level bureaucrats’, such\nas managers and computer experts, that decide on how to convert policy\nand legal frameworks into algorithms and decision-trees. \n\nThe automation of bureaucratic processes illustrates that some\ncomputer technologies are intentionally designed to limit the\ndiscretion of some human beings. Indeed the field of\nPersuasive Technology explicitly aims to develop technological\nartifacts that persuade humans to perform in ‘desirable’\nways (IJsselsteijn et al. 2006). An example is the anti-alcohol lock\nthat is already in use in a number of countries, including the USA,\nCanada, Sweden and the UK. It requires the driver to pass a breathing\ntest before she can start the car. This technology forces a particular\nkind of action and leaves the driver with hardly any choice. Other\ntechnologies might have a more subtle way of steering behavior, by\neither persuading or seducing users (Verbeek 2006). For example, the\nonboard computer devices in some cars that show, in real-time,\ninformation about fuel consumption can encourage the driver to optimize\nfuel efficiency. Such technologies are designed with the explicit aim\nof making humans behave responsibly by limiting their options or\npersuading them to choose in a certain way. \n\nVerbeek notes that critics of the idea of intentionally developing\ntechnology to enforce morally desirable behavior have argued that it\njettisons the democratic principles of our society and threatens human\ndignity. They argue that it deprives humans of their ability and rights\nto make deliberate decisions and to act voluntarily. In addition,\ncritics have claimed that if humans are not acting freely, their\nactions cannot be considered moral. These objections can be countered,\nas Verbeek argues, by pointing to the rules, norms, regulations and a\nhost of technological artifacts that already set conditions for actions\nthat humans are able or allowed to perform. Moreover, he notes,\ntechnological artifacts, as active mediators, affect the actions and\nexperiences of humans, but they do not determine them. Some people have\ncreatively circumvented the strict morality of the alcohol lock by\nhaving an air pump in the car (Vidal 2004). Nevertheless, these\ncritiques underline the issues at stake in automating decision-making\nprocesses: computing can set constraints on the freedom a person has to\nact and thus affects the extent to which she can be held morally\nresponsible. \n\nThe challenges that computer technologies present with regard to the\nconditions for ascribing responsibility indicate the limitations of\nconventional ethical frameworks in dealing with the question of moral\nresponsibility. Traditional models of moral responsibility seem to be\ndeveloped for the kinds of actions performed by an individual that have\ndirectly visible consequences (Waelbers 2009). However, in\ntoday’s society attributions of responsibility to an individual\nor a group of individuals are intertwined with the artifacts with which\nthey interact as well as with intentions and actions of other human\nagents that these artifacts mediate. Acting with computer technologies\nmay require a different kind of analysis of who can be held responsible\nand what it means to be morally responsible. \n\nMoral responsibility is generally attributed to moral agents and, at\nleast in Western philosophical traditions, moral agency has been a\nconcept exclusively reserved for human beings (Johnson 2001; Doorn and\nvan de Poel 2012). Unlike animals or natural disasters, human beings\nin these traditions can be the originators of morally significant\nactions, as they can freely choose to act in one way rather than\nanother way and deliberate about the consequences of this choice. And,\nalthough some people are inclined to anthropomorphize computers and\ntreat them as if they were moral agents (Reeves and Nass 1996; Nass\nand Moon 2000; Rosenthal-von der Pütten 2013), most philosophers agree\nthat current computer technologies should not be called moral agents,\nif that would mean that they could be held morally\nresponsible. However, the limitations of traditional ethical\nvocabularies in thinking about the moral dimensions of computing have\nled some authors to rethink the concept of moral agency. \n\nThe increasing complexity of computer technology and the advances in\nArtificial Intelligence (AI), challenge the idea that human beings are\nthe only entities to which moral responsibility can or should be\nascribed (Bechtel 1985; Kroes and Verbeek 2014). Dennett, for example, suggests that holding a\ncomputer morally responsible is possible if it concerned a higher-order\nintentional computer system (1997). An intentional system, according to\nhim, is one that can be predicted and explained by attributing beliefs\nand desires to it, as well as rationality. In other words, its behavior\ncan be described by assuming the systems has mental states and that it\nacts according what it thinks it ought to do, given its beliefs and\ndesires. Many computers today, according to Dennett, are already\nintentional systems, but they lack the higher-order ability to reflect\non and reason about their mental states. They do not have beliefs about\ntheir beliefs or thoughts about desires. Dennett suggests that the\nfictional HAL 9000 that featured in the movie 2001: A Space\nOdyssey would qualify as a higher-order intentional system that\ncan be held morally responsible. Although current advances in AI might\nnot lead to HAL, he does see the development of computers systems with\nhigher-order intentionality as a real possibility.  Sullins argues in line with Dennett that moral agency is not\nrestricted to human beings (2006). He proposes that computers systems\nor, more specifically, robots are moral agents when they have a\nsignificant level of autonomy and they can be regarded at an\nappropriate level of abstraction as exhibiting intentional behavior. A\nrobot, according to Sullins, would be significantly autonomous if it\nwas not under the direct control of other agents in performing its\ntasks. Note that Sullins interprets autonomy in a narrow sense in\ncomparison to the conception of autonomy in moral philosophy as\nproperty of human beings. He adds as a third condition that a robot\nalso has to be in a position of responsibility to be a moral\nagent. That is, the robot performs some social role that carries with\nit some responsibilities and in performing this role the robot appears\nto have ‘beliefs’ about and an understanding of its duties\ntowards other moral agents (p.  28). To illustrate what kind of\ncapabilities are required for “full moral agency”, he\ndraws an analogy with a human nurse. He argues that if a robot was\nautonomous enough to carry out the same duties as a human nurse and\nhad an understanding of its role and responsibilities in the health\ncare systems, then it would be a “full moral\nagent”. Sullins maintains that it will be some time before\nmachines with these kinds of capabilities will be available, but\n“even the modest robots of today can be seen to be moral agents\nof a sort under certain, but not all, levels of abstraction and are\ndeserving of moral consideration” (p. 29). \n\nEchoing objections to the early project of (strong) AI\n(Sack  1997),[3]\ncritics of analyses such\nas presented by Dennett and Sullins, have objected to the idea that\ncomputer technologies can have capacities that make human beings moral\nagents, such as mental states, intentionality, common sense or emotion\n(Johnson 2006; Kuflik 1999). They, for instance, point out that it\nmakes no sense to treat computer system as moral agents that can be\nheld responsible, for they cannot suffer and thus cannot be punished\n(Sparrow 2007; Asaro 2011). Or they argue, as Stahl does, that\ncomputers are not capable of moral reasoning, because they do not have\nthe capacity to understand the meaning of the information that they\nprocess (2006). In order to comprehend the meaning of moral statements\nan agent has to be part of the form of life in which the statement is\nmeaningful; it has to be able to take part in moral discourses. Similar\nto the debates about AI, critics continue to draw a distinction between\nhumans and computers by noting various capacities that computers do\nnot, and cannot, have that would justify the attribution of moral\nagency. \n\nIn the absence of any definitive arguments for or against the\npossibility of future computer systems being morally responsible,\nresearchers within the field of machine ethics aim to further develop\nthe discussion by focusing instead on creating computer system that can\nbehave as if they are moral agents (Moor 2006). Research\nwithin this field has been concerned with the design and development of\ncomputer systems that can independently determine what the right thing\nto do would be in a given situation. According to Allen and Wallach,\nsuch autonomous moral agents (AMAs) would have to be capable\nof reasoning about the moral and social significance of their behavior\nand use their assessment of the effects their behavior has on sentient\nbeings to make appropriate choices (2012; see also Wallach and Allen\n2009 and Allen et al. 2000). Such abilities are needed, they argue,\nbecause computers are becoming more and more complex and capable of\noperating without direct human control in different contexts and\nenvironments. Progressively autonomous technologies already in\ndevelopment, such as military robots, driverless cars or trains and\nservice robots in the home and for healthcare, will be involved in\nmoral situations that directly affect the safety and well-being of\nhumans. An autonomous bomb disposal robot might in the future be faced\nwith the decision which bomb it should defuse first, in order to\nminimize casualties. Similarly, a moral decision that a driverless car\nmight have to make is whether to break for a crossing dog or avoid the\nrisk of causing injury to the driver behind him. Such decisions require\njudgment. Currently operators make such moral decisions, or the\ndecision is already inscribed in the design of the computer system.\nMachine ethics, Wallach and Allen argue, goes one step beyond making\nengineers aware of the values they build into the design of their\nproducts, as it seeks to build ethical decision-making into the\nmachines. \n\nTo further specify what it means for computers to make ethical\ndecisions or to put ‘ethics in the machine’, Moor\ndistinguishes between three different kinds of ethical agents: implicit\nethical agents, explicit ethical agents, and full ethical agents\n(2006). The first kind of agent is a computer that has the ethics of\nits developers inscribed in their design. These agents are constructed\nto adhere to the norms and values of the contexts in which they are\ndeveloped or will be used. Thus, ATM tellers are designed to have a\nhigh level of security to prevent unauthorized people from drawing\nmoney from accounts. An explicit ethical agent is a computer that can\n‘do ethics’. In other words, it can on the basis of an\nethical model determine what would be the right thing to do, given\ncertain inputs. The ethical model can be based on traditional ethical\ntheories, such as Kantian or utilitarian ethics—depending on the\npreferences of its creators. These agents would ‘make ethical\ndecisions’ on behalf of its human users (and developers). Such\nagents are akin to the autonomous moral agents described by Allen and\nWallach. Finally, Moor defines full ethical agents as entities that can\nmake ethical judgments and can justify them, much like human beings\ncan. He claims that although there are no computer technologies today\nthat can be called fully ethical, it is an empirical question whether\nor not it would be possible in the future. \n\nThe effort to build AMAs raises the question of how this effort\naffects the ascription of moral responsibility. If these technologies\nare not moral agents like human beings are, can they be held morally\nresponsible? As human beings would design these artificial agents to\nbehave within pre-specified formalized ethical frameworks, it is\nlikely that responsibility will still be ascribed to these human\nactors and those that deploy these technologies. However, as Allen and\nWallach acknowledge, the danger of exclusively focusing on equipping\nrobots with moral decision-making abilities, rather than also looking\nat the sociotechnical systems in which these robots are embedded, is\nthat it may cause further confusion about the distribution of\nresponsibility (2012). Robots with moral decision-making capabilities\nmay present similar challenges to ascribing responsibility as other\ntechnologies, when they introduce new complexities that further\nobfuscate causal connections that lead back to their creators and\nusers. \n\nThe prospect of increasingly autonomous and intelligent computer\ntechnologies and the growing difficulty of finding responsible human\nagents lead Floridi and Sanders to take a different approach (2004).\nThey propose to extend the class of moral agents to include artificial\nagents, while disconnecting moral agency and moral accountability from\nthe notion of moral responsibility. They contend that “the\ninsurmountable difficulties for the traditional and now rather outdated\nview that a human can be found accountable for certain kinds of\nsoftware and even hardware” demands a different approach (p.\n372). Instead, they suggest that artificial agents should be\nacknowledged as moral agents that can be held accountable, but not\nresponsible. To illustrate they draw a comparison between artificial\nagents and dogs as sources of moral actions. Dogs can be the cause of a\nmorally charged action, like damaging property or helping to save a\nperson’s life, as in the case of search-and-rescue dogs. We can\nidentify them as moral agents even though we generally do not hold them\nmorally responsible, according to Floridi and Sanders: they are the\nsource of a moral action and can be held morally accountable by\ncorrecting or punishing them. \n\nJust like animals, Floridi and Sanders argue, artificial agents can\nbe seen as sources of moral actions and thus can be held morally\naccountable when they can be conceived of as behaving like a moral\nagent from an appropriate level of abstraction. The notion of\nlevels of abstraction refers to the stance one adopts towards and\nentity to predict and explain its behavior. At a low level of\nabstraction we would explain the behavior of a system in terms of its\nmechanical or biological processes. At a higher level of abstraction it\ncan help to describe the behavior of a system in terms of beliefs,\ndesires and thoughts. If at a high enough level a computational system\ncan effectively be described as being interactive, autonomous and\nadaptive, then it can be held accountable according to Floridi and\nSanders (p. 352). It, thus, does not require personhood or free will\nfor an agent to be morally accountable; rather the agent has to act as\nif it had intentions and was able to make choices. \n\nThe advantage of disconnecting accountability from responsibility,\naccording to Floridi and Sanders, is that it places the focus on moral\nagenthood, accountability and censure, instead of on figuring out which\nhuman agents are responsible. “We are less likely to assign\nresponsibility at any cost, forced by the necessity to identify a human\nmoral agent. We can liberate technological development of AAs\n[Artificial Agents] from being bound by the standard limiting\nview” (p. 376). When artificial agents ‘behave badly’\nthey can be dealt with directly, when their autonomous behavior and\ncomplexity makes it too difficult to distribute responsibility among\nhuman agents. Immoral agents can be modified or deleted. It is then\npossible to attribute moral accountability even when moral\nresponsibility cannot be determined. \n\nCritics of Floridi’s and Sanders’ view on accountability and moral\nagency argue that placing the focus of analysis on computational\nartifacts by treating them as moral agents will draw attention away\nfrom the humans that deploy and develop them. Johnson, for instance,\nmakes the case that computer technologies remain connected to the\nintentionality of their creators and users (2006). She argues that\nalthough computational artifacts are a part of the moral world and\nshould be recognized as entities that have moral relevance, they are\nnot moral agents, for they are not intentional. They are not\nintentional, because they do not have mental states or a purpose that\ncomes from the freedom to act. She emphasizes that although these\nartifacts are not intentional, they do have intentionality, but their\nintentionality is related to their functionality. They are human-made\nartifacts and their design and use reflect the intentions of designers\nand users. Human users, in turn, use their intentionality to interact\nwith and through the software. In interacting with the artifacts they\nactivate the inscribed intentions of the designers and developers. It\nis through human activity that computer technology is designed,\ndeveloped, tested, installed, initiated and provided with input and\ninstructions to perform specified tasks. Without this human activity,\ncomputers would do nothing. Attributing independent moral agency to\ncomputers, Johnson claims, disconnects them from the human behavior\nthat creates, deploys and uses them. It turns the attention away from\nthe forces that shape technological development and limits the\npossibility for intervention. For instance, it leaves the issue of\nsorting out who is responsible for dealing with malfunctioning or\nimmoral artificial agents or who should make amends for the harmful\nevents they may cause. It postpones the question of who has to account\nfor the conditions under which artificial agents are allowed to\noperate (Noorman 2009). \n\nYet, to say that technologies are not moral agents is not to say\nthat they are not part of moral action. Several philosophers have\nstressed that moral responsibility cannot be properly understood\nwithout recognizing the active role of technology in shaping human\naction (Jonas 1984; Verbeek 2006; Johnson and Powers 2005; Waelbers\n2009). Johnson, for instance, claims that although computers are not\nmoral agents, the artifact designer, the artifact, and the artifact\nuser should all be the focus of moral evaluation as they are all at\nwork in an action (Johnson 2006). Humans create these\nartifacts and inscribe in them their particular values and intentions\nto achieve particular effects in the world and in turn these\ntechnological artifacts influence what human beings can and cannot do\nand affect how they perceive and interpret the world.  Similarly, Verbeek maintains that technological artifacts alone do\nnot have moral agency, but moral agency is hardly ever\n‘purely’ human. Moral agency generally involves a\nmediating artifact that shapes human behavior, often in way not\nanticipated by the designer (2008). Moral decisions and actions are\nco-shaped by technological artifacts. He suggests that in all forms of\nhuman action there are three forms of agency at work: 1) the agency of\nthe human performing the action; 2) the agency of the designer who\nhelped shaped the mediating role of the artifacts and 3) the artifact\nmediating human action. The agency of artifacts is inextricably linked\nto the agency of its designers and users, but it cannot be reduced to\neither of them. For him, then, a subject that acts or makes moral\ndecisions is a composite of human and technological components. Moral\nagency is not merely located in a human being, but in a complex blend\nof humans and technologies. \nIn later papers, Floridi explores the concept of distributed moral\nactions (2013, 2016). He argues that some moral significant outcomes\ncannot be reduced to the moral significant actions of some\nindividuals. Morally neutral actions of several individuals can still\nresult in morally significant events. Individuals might not have\nintended to cause harm, but nevertheless their combined actions may\nstill result in moral harm to someone or something. In order to deal\nwith the problem of subsequently assigning moral responsibility for\nsuch distributed moral actions, he argues that the focus of analysis\nshould shift from the agents to the patients of moral actions. A moral\naction can then be evaluated in terms of the harm to the patient,\nregardless of the intentions of the agents involved. Assigning\nresponsibility then focuses on whether or not an agent is causally\naccountable for the outcome and on adjusting their behavior to prevent\nharm. If the agents causally accountable - be they artificial or\nbiological - are autonomous, can interact with each other and their\nenvironments and can learn from their interactions they can be held\nresponsible for distributed moral actions, according to Floridi\n(2016). \n\nIn light of the noted difficulties in ascribing moral\nresponsibility, several authors have critiqued the way in which the\nconcept is used and interpreted in relation to computing. They claim\nthat the traditional models or frameworks for dealing with moral\nresponsibility fall short and propose different perspectives or\ninterpretations to address some of the difficulties. \nOne approach is to rethink how moral responsibility is assigned\n(Gotterbarn 2001; Waelbers 2009). When it comes to computing\npractitioners, Gotterbarn identifies a potential to side-step or avoid\nresponsibility by looking for someone else to blame. He attributes this\npotential to two pervasive misconceptions about responsibility. The\nfirst misconception is that computing is an ethically neutral practice.\nAccording to Gotterbarn this misplaced belief that technological\nartifacts and the practices of building them are ethically neutral is\noften used to justify a narrow technology-centered focus on the\ndevelopment of computer system without taking the broader context in\nwhich these technologies operate into account. This narrow focus can\nhave detrimental consequences. Gotterbarn gives the example of a\nprogrammer who was given the assignment to write a program that could\nlower or raise an X-ray device on a pole, after an X-ray technician set\nthe required height. The programmer focused on solving the given\npuzzle, but failed to take account of the circumstances in which the\ndevice would be used and the contingencies that might occur. He, thus,\ndid not consider the possibility that a patient could accidentally be in\nthe way of the device moving up and down the pole. This oversight\neventually resulted in a tragic accident. A patient was crushed by the device, when a\ntechnician set the device to tabletop height, not realizing that the\npatient was still underneath it. According to Gotterbarn, computer\npractitioners have a moral responsibility to consider such\ncontingencies, even though they may not be legally required to do so.\nThe design and use of technological artifacts is a moral activity and\nthe choice for one particular design solution over another has real and\nmaterial consequences. \n\nThe second misconception is that responsibility is only about\ndetermining blame when something goes wrong. Computer practitioners,\naccording to Gotterbarn, have conventionally adopted a malpractice\nmodel of responsibility that focuses on determining the appropriate\nperson to blame for harmful incidents (2001). This malpractice model leads to\nall sorts of excuses to shirk responsibility. In particular, the\ncomplexities that computer technologies introduce allow computer\npractitioners to side-step responsibility. The distance between\ndevelopers and the effects of the use of the technologies they create\ncan, for instance, be used to claim that there is no direct and\nimmediate causal link that would tie developers to a malfunction.\nDevelopers can argue that their contribution to the chain of events was\nnegligible, as they are part of a team or larger organization and they\nhad limited opportunity to do otherwise. The malpractice model,\naccording to Gotterbarn, entices computer practitioners to distance\nthemselves from accountability and blame. \n\nThe two misconceptions are based on a particular view of\nresponsibility that places the focus on that which exempts one from\nblame and liability. In reference to Ladd, Gotterbarn calls this\nnegative responsibility and distinguishes it from positive\nresponsibility (see also Ladd 1989). Positive responsibility\nemphasizes “the virtue of having or being obliged to have regard\nfor the consequences that his or her actions have on others”\n(Gotterbarn 2001, p. 227). Positive responsibility entails that part\nof the professionalism of computer experts is that they strive to\nminimize foreseeable undesirable events. It focuses on what ought to be\ndone rather than on blaming or punishing others for irresponsible\nbehavior. Gotterbarn argues that the computing professions should adopt\na positive concept of responsibility, as it emphasizes the obligations\nand duties of computer practitioners to have regard for the\nconsequences of one’s actions and to minimize the possibility of\ncausing harm. Computer practitioners have a moral responsibility to\navoid harm and to deliver a properly working product, according to him,\nregardless of whether they will be held accountable if things turn out\ndifferently. \n\nThe emphasis on the prospective moral responsibility of computer\npractitioners raises the question of how far this responsibility\nreaches, in particular in light of systems that many hands help create\nand the difficulties involved in anticipating contingencies that might\ncause a system to malfunction (Stieb 2008; Miller 2008). To what\nextent can developers and manufacturers be expected to exert themselves\nto anticipate or prevent the consequences of the use of their\ntechnologies or possible ‘bugs’ in their code? These\nsystems are generally incomprehensible to any single programmer and it\nseems unlikely that complex computer systems can be completely error\nfree. Moreover, designers and engineers cannot foresee all the possible\nconditions under which their products will eventually operate. Should\nmanufacturers of mobile phones have anticipated that their products\nwould be used in roadside bombs? A more fundamental question is whether\ncomputer programmers have a broader responsibility to the welfare of\nthe public or just to their employer? \n\nNevertheless, the distinction between positive and negative\nresponsibility underlines that holding someone morally responsible has\na social function, which provides yet another perspective on the issue\n(Stahl 2006; Eshleman 2016). Both prospectively and retrospectively,\nresponsibility works to organize social relations between people and\nbetween people and institutions. It sets expectations between people\nfor the fulfillment of certain obligations and duties and provides the\nmeans to correct or encourage certain behavior. For instance, a\nrobotics company is expected to build in safeguards that prevent\nrobots from harming humans. If the company fails to live up to this\nexpectation, it will be held accountable and in some cases it will\nhave to pay for damages or undergo some other kind of punishment. The\npunishment or prospect of punishment can encourage the company to have\nmore regard for system safety, reliability, sound design and the risks\ninvolved in their production of robots. It might trigger the company\nto take actions to prevent future accidents. Yet, it might also\nencourage it to find ways to shift the blame. The idea that\nresponsibility is about interpersonal relationships and expectations\nabout duties and obligations places the focus on the practices of\nholding someone responsible (Eshleman 2016). \n\nThe particular practices and social structures that are in place to\nascribe responsibility and hold people accountable, have an influence\non how we relate to technologies. Nissenbaum contends that the\ndifficulties in attributing moral responsibility can, to a large\nextent, be traced back to the particular characteristics of the\norganizational and cultural context in which computer technologies are\nembedded. She argues that how we conceive of the nature, capacities and\nlimitations of computing is of influence on the answerability of those\nwho develop and use computer technologies (1997). She observes a\nsystematic erosion of accountability in our increasingly computerized\nsociety, where she conceives of accountability as a value and a\npractice that places an emphasis on preventing harm and risk.\n \nAccountability means there will be someone, or several people,\nto answer not only for the malfunctions in life-critical systems that\ncause or risk grave injuries and cause infrastructure and large\nmonetary losses, but even for the malfunction that cause individual\nlosses of time, convenience, and contentment. (1994, p. 74)   It can be used as “a powerful tool for motivating better\npractices, and consequently more reliable and trustworthy\nsystems” (1997, p.  43). Holding people accountable for the\nharms or risks caused by computer systems provides a strong incentive\nto minimize them and can provide a starting point for assigning just\npunishment.  Cultural and organizational practices however do the opposite, due\nto “the conditions under which computer technologies are\ncommonly developed and deployed, coupled with popular conceptions\nabout the nature, capacities and limitations of computing”\n(p. 43). Nissenbaum identifies four barriers to accountability in\ntoday’s society: (1) the problem of many hands, (2) the acceptance of\ncomputer bugs as an inherent element of large software systems, (3)\nusing the computer as scapegoat and (4) ownership without\nliability. According to Nissenbaum people have a tendency to shirk\nresponsibility and to shift the blame to others when accidents\noccur. The problem of many hands and the idea that software bugs are\nan inevitable by-product of complex computer systems are too easily\naccepted as excuses for not answering for harmful outcomes. People are\nalso inclined to point the finger at the complexity of the computer\nand argue that “it was the computer’s fault” when things\ngo wrong. Finally, she perceives a tendency of companies to claim\nownership of the software they develop, but to dismiss the\nresponsibilities that come with ownership. Current day computer\nprograms come with extended license agreements that assert the\nmanufacturer’s ownership of the software, but disclaim any\naccountability for the quality or performance of the product. They\nalso dismiss any liability for the consequential damages resulting\nfrom defects in the software. \n\nThese four barriers, Nissenbaum holds, stand in the way of a\n“culture of accountability” that is aimed at maintaining\nclear lines of accountability. Such a culture fosters a strong sense\nof responsibility as a virtue to be encouraged and everyone connected\nto an outcome of particular actions is answerable for it.\nAccountability, according to Nissenbaum, is different from liability.\nLiability is about looking for a person to blame and to compensate for\ndamages suffered after the event. Once that person has been found,\nothers can be let ‘off the hook’, which may encourage\npeople to look for excuses, such as blaming the\ncomputer. Accountability, however, applies to all those involved. It\nrequires a particular kind of organizational context, one in which\nanswerability works to entice people to pay greater attention to\nsystem safety, reliability and sound design, in order to establish a\nculture of accountability. An organization that places less value on\naccountability and that has little regards for responsibilities in\norganizing their production processes is more likely to allow their\ntechnological products to become incomprehensible. Nissenbaum’s\nanalysis illustrates that our practices of holding someone responsible\n- the established ways of holding people to account and of conveying\nexpectations about duties and obligations - are continuously changing\nand negotiated, partly as a response to the introduction of new\ntechnologies (see also Noorman 2012).\n \n\nNissenbaum argues that the context in which technologies are developed\nand used has a significant influence on the ascription of moral\nresponsibility, but several authors have stressed that moral\nresponsibility cannot be properly understood without recognizing the\nactive role of technology in shaping human action (Jonas 1984; Verbeek\n2006; Johnson and Powers 2005; Waelbers 2009).  According to Johnson\nand Powers it is not enough to just look at what humans intend and\ndo. “Ascribing more responsibility to persons who act with\ntechnology requires coming to grips with the behavior of the\ntechnology” (p. 107). One has to consider the various ways in\nwhich technological artifacts mediate human actions. Moral\nresponsibility is, thus, not only about how the actions of a person or\na group of people affect others in a morally significant way; it is\nalso about how their actions are shaped by technology. Moral\nresponsibility from this perspective is not located in an individual\nor an interpersonal relationship, but is distributed among humans and\ntechnologies.  \nComputer technologies have challenged conventional conceptions of\nmoral responsibility and have raised questions about how to\ndistributed responsibility appropriately. Can human beings still be\nheld responsible for the behavior of complex computer technologies\nthat they have limited control over or understanding of? Are human\nbeings the only agents that can be held morally responsible or can the\nconcept of moral agent be extended to include artificial computational\nentities? In response to such questions philosophers have reexamined\nthe concepts of moral agency and moral responsibility. Although there\nis no clear consensus on what these concepts should entail in an\nincreasingly digital society, what is clear from the discussions is\nthat any reflection on these concepts will need to address how these\ntechnologies affect human action and the boundaries between computer\ntechnologies and human beings.","contact.mail":"merelnoorman@gmail.com","contact.domain":"gmail.com"}]
