[{"date.published":"2004-10-26","date.changed":"2015-02-19","url":"https://plato.stanford.edu/entries/gene/","author1":"Hans-Jörg Rheinberger","author2":"Staffan Müller-Wille","author1.info":"http://www.mpiwg-berlin.mpg.de/en/staff/members/rheinbg","author2.info":"http://huss.exeter.ac.uk/sociology/staff/mueller-wille/","entry":"gene","body.text":"\n\n “There can be little doubt,” philosopher and\nbiochemist Lenny Moss claimed in 2003, “that the idea of\n‘the gene’ has been the central organizing theme of\ntwentieth century biology” (Moss 2003, xiii; cf. Keller 2000,\n9). And yet it is clear that the science of genetics never provided\none generally accepted definition of the gene. More than a hundred\nyears of genetic research have rather resulted in the proliferation of\na variety of gene concepts, which sometimes complement, sometimes\ncontradict each other. Some philosophers and scientists have tried to\nremedy this situation by reducing this variety of gene concepts,\neither “vertically” to a fundamental unit, or\n“horizontally” by subsuming them under a general\nterm. Others have opted for more pluralist stances. As a consequence,\n“the gene” has become a hot topic in philosophy of science\naround which questions of reduction, emergence, or supervenience of\nconcepts and theories (along with the epistemic entities they refer\nto) are lively debated.  So far, however, all attempts to reach a\nconsensus regarding these questions have been unsuccessful. Today,\nsince the completion of the human genome sequence and the beginning of\nwhat is being called the era of postgenomics, genetics is again\nexperiencing a time of conceptual change. The concept of the gene,\nemerging out of a century of genetic research, has been and continues\nto be, as Raphael Falk has reminded us not so long ago, a\n“concept in tension” (Falk 2000).\n\n\n\nThe layout of the following article will therefore be largely\nhistorical. There exist several accounts of the historical development\nand diversification of the gene concept, written from the perspective\nof a history of ideas (Dunn 1965; Stubbe 1965; Carlson 1966, 2004;\nSchwartz 2008).  While we will largely follow the conventional time\nline of events established by this literature, we will take a slightly\ndifferent perspective by looking at genes as epistemic objects,\ni.e., as objects subjected to on-going research.  This means that we\nwill not simply relate established concepts of “the” gene,\nbut rather analyze how changing experimental practices and\nexperimental systems determined and modified such concepts\n (see also the entry on\n  experiment in biology). After\nhaving thus established a rich historical “panorama” of\nthe gene as a “concept in flux”, to pick up a suggestive\nterm introduced by Yehuda Elkana (1970; cf. Falk 1986), some more\ngeneral philosophical themes will be briefly addressed, for which the\ngene has served as a convenient “handle” in\ndiscussion. These revolve around the topic of reduction, but also\ninvolve questions about causality in living systems (for fuller\naccounts see entries on\n molecular biology,\n molecular genetics,\n biological information, and\n reductionism in biology;\n for a recent monograph length treatment of philosophical questions\n regarding genetics, see Griffiths and Stotz 2013).\n\n\n\nBefore dealing with the historical stages of the gene concept's\ntangled development, we will have to see how it came into being. It\nwas only in the nineteenth century that heredity became a major\nproblem to be dealt with in biology (López Beltrán 2004;\nMüller-Wille and Rheinberger 2007 and 2012).  With the rise of heredity as\na biological research area the question of its material basis and of\nits mechanism took shape. In the second half of the nineteenth\ncentury, two alternative frameworks were proposed to deal with this\nquestion. The first one conceived of heredity as a force whose\nstrength was accumulated over the generations, and which, as a\nmeasurable magnitude, could be subjected to statistical analysis. This\nconcept was particularly widespread among nineteenth-century breeders\n(Gayon and Zallen 1998) and influenced Francis Galton and the\nso-called “biometrical school” (Gayon 1998, 105-146).  The\nsecond framework saw heredity as residing in matter that was\ntransmitted from one generation to the next. Two major trends are to\nbe differentiated here. One of them regarded hereditary matter as\nparticulate and amenable to breeding analysis. Charles Darwin, for\nexample, called the presumed hereditary particles\n“gemmules”; Hugo de Vries, “pangenes”. None of\nthese nineteenth- century authors, however, thought of associating\nthese particles with a particular hereditary substance. They all\nbelieved that they consisted of the very same stuff that the rest of\nthe organism was made of, so that their mere growth, recombination and\naccumulation en masse would make visible the particular\ntraits for which they were responsible. A second category of\nbiologists in the second half of the nineteeenth century, to whom Carl\nNaegeli and August Weismann belonged, distinguished the body\nsubstance, the “trophoplasm” or “soma”, from a\nspecific hereditary substance, the “idioplasm” or\n“germ plasm”, which was assumed to be responsible for\nintergenerational hereditary continuity. However, they took this\nidioplasmic substance as being, not particulate, but highly\norganized. In the case of Weismann it remained intact in the germ\ncells, but irreversibly differentiated in the body cells during\ndevelopment. In the case of Naegeli it extended even from cell to cell\nand throughout the whole body, a capillary hereditary system analogous\nto the nervous system (Robinson 1979; Churchill 1987, Rheinberger 2008). \nMendel stands out among these biologists, although he worked within a\nwell-defined botanical tradition of hybrid research. He is generally\nconsidered as the precursor to twentieth-century genetics (see,\nhowever, Olby 1979 and, for a more recent discussion, Orel and Hartl\n1997). As Jean Gayon has argued, Mendel's 1865 paper attacked heredity\nfrom a wholly new angle, interpreting it not as a measurable\nmagnitude, as the biometrical school did at a later stage, but as\n“a certain level of organization,” a “structure in a\ngiven generation to be expressed in the context of specific\ncrosses.” This is why Mendel applied a “calculus of\ndifferences,” i.e., combinatorial mathematics to the resolution\nof hereditary phenomena (Gayon 2000, 77-78). With that, he introduced\na new formal tool for an analysis of hybridization experiments that\nwas at the same time based on a new experimental regime: the selection\nof pairs of alternative and “constant” (i.e., heritable)\ntraits. Mendel believed that these traits were related by a\n“constant law of development” to certain\n“elements” or “factors” in the reproductive\ncells from which organisms developed. An analysis of the distribution\nof alternative traits in the progeny of hybrids could therefore reveal\nsomething about the relationship that the underlying\n“factors” entered when united in the hybrid parent\norganism (Müller-Wille and Orel 2007). \n\nThe year 1900 can be seen as the annus mirabilis that gave\nbirth to a new discipline soon to be called genetics. During that\nyear, three botanists, Hugo de Vries, Carl Correns, and Erich\nTschermak, reported on their breeding experiments of the late 1890s\nand claimed to have confirmed the regularities in the transmission of\ncharacters from parents to offspring that Mendel had already presented\nin his seminal paper of 1865 (Olby 1985, 109-37).  Basically, in their\nexperimental crosses with Zea mays, Pisum,\nand Phaseolus, they observed that the elements responsible for\npairs of alternative traits, “allelomorphs” in the later\nterminology of William Bateson (1902), which soon came into general\nuse under the abbreviation of “alleles” segregated\nrandomly in the second filial generation (Mendel's law of\nsegregation), and that these elements were transmitted independently\nfrom each other (Mendel's law of independent assortment). The\nadditional observation that sometimes several elements behaved as if\nthey were linked, contributed to the assumption soon promoted by\nWalter Sutton and by Theodor Boveri that these elements were located\nin groups on different chromosomes in the nucleus. Thus the chromosome\ntheory of inheritance assumed that the regularities of character\ntransmission were grounded in cytomorphology, in particular the\nnuclear morphology with its individual chromosomes keeping their\nidentity over the generations (Coleman 1965; Martins 1999). \n\nDespite initial resistance from the biometrical school (Provine 1971;\nMackenzie and Barnes 1979), awareness rapidly grew that the\npossibility of an independent assortment of discrete hereditary\nfactors according to the laws of probability was to be seen as the\nvery cornerstone of a new “paradigm” of inheritance (Kim\n1994). This went together after an initial period of conflation by\nwhat Elof Carlson called the “unit-character fallacy”\n(Carlson 1966, ch. 4) with the establishment of a categorical\ndistinction between genetic factors on the one hand\nand traits or\ncharacters on the other hand. The masking effect of dominant traits over\nrecessive ones and the subsequent reappearance of recessive traits were\nparticularly instrumental in stabilizing this distinction (Falk 2001).\nFurthermore, it resonated with the earlier concept of two material regimes,\none germinal and one bodily, already promoted by Naegeli and Weismann. \n\nYet if—as Correns stated in his first review on the new\nMendelian literature in 1901—“we cannot uphold the idea\nof a permanent fixation [of the hereditary factors] in the germ plasm,\nbut have to assume, because of their miscibility, some mobility at\nleast at certain times,” and if chromosomal coupling was a\npossible, but not a necessary and general mechanism of conveying\nstructure to inheritance, how was one to explain the successive and\nregular physiological deployment of the dispositions (Anlagen)\nin the orderly development of the organism? To resolve this\ndifficulty, Correns came up with the following, as he called it,\n“heresy”: \nIn this way, Correns, at the beginning of the first decade of the\ntwentieth century, distinguished a hereditary space with an\nindependent logic and metrics from another, physiological and\ndevelopmental space represented by the cytoplasm. Toward the end of\nthe first decade of the twentieth century, after Bateson had coined\nthe term genetics for the emerging new field of transmission\nstudies in 1906, Wilhelm Johannsen codified this distinction by\nintroducing the notions of genotype and phenotype,\nrespectively, for these two spaces. In contrast to Correns, Johannsen\nconsidered genotyope and phenotype as abstract entities, not confining\nthem to certain cellular spaces and remaining skeptical about the\nchromosome theory of inheritance throughout his life. In addition, for\nthe elements of the genotype, Johannsen proposed the notion\nof gene, which for him was a concept “completely free of\nany hypothesis” regarding localization and material constitution\n(Johannsen 1909, 124). \nJohannsen's codification, which was based on microbiology's\n“pure culture” approach, breeders' practices of separating\n“pure lines” as well as Richard Woltereck's notion of an\ninnate “norm of reaction”, was gradually taken up by the\ngenetics community and has profoundly marked all of twentieth century\nbiology (Allen 2002, Müller-Wille 2007). We can safely say that\nit instituted the gene as an epistemic object to be studied within\nits proper space, and with that an “exact, experimental\ndoctrine of heredity” (Johannsen 1909, 1) that concentrated on\ntransmission only and not on the development of the organism in its\nenvironment. Some historians have spoken of a “divorce” of\ngenetical from embryological concerns with regard to this separation\n(Allen 1986; Bowler 1989). Others hold that this separation was itself\nan expression of the embryological interests of early geneticists in\ntheir search for “developmental invariants” (Gilbert 1978;\nGriesemer 2000). Be that as it may, the result was that the relations\nbetween the two spaces, once separated by abstraction, were now\nelucidated experimentally in their own right (Falk 1995). Michel\nMorange observed that this “division was logically\nabsurd”—from hindsight but—“historically and\nscientifically necessary” (Morange 2001, 9). \nJohannsen himself stressed that the genotype had to be treated as\nindependent of any life history and thus, at least within the bounds\nof time in which research operated, as an “ahistoric”\nentity amenable to scientific scrutiny like the objects of physics and\nchemistry (Johannsen 1911, 139; cf. Churchill 1974; Roll-Hansen\n1978a). “The personal qualities of any individual\norganism do not at all cause the qualities of its offspring; but the\nqualities of both ancestor and descendant are in quite the same manner\ndetermined by the nature of the sexual substances,” Johannsen\nclaimed (Johannsen 1911, 130). Unlike most Mendelians, however, he\nremained convinced that the genotype would possess an overall\narchitecture—as expressed in the notion of\n“type”. He therefore had reservations with respect to its\nparticulate nature, and especially warned that the notion of\n“genes for a particular character” should always be used\ncautiously if not altogether be omitted (Johannsen 1911,\n147). Johannsen also remained consciously agnostic with respect to the\nmaterial constitution of the genotype and its elements. He clearly\nrecognized that the experimental regime of Mendelian genetics,\nalthough scientific in its character like physics or chemistry, did\nneither require nor allow for any definite supposition about the\nmaterial structure of the genetic elements. “Personally,”\nhe wrote as late as 1923, “I believe in a great central\nsomething as yet not divisible into separate factors,”\nidentifying this “something” with the specific nature of\nthe organism. “The pomace-flies in Morgan's splendid\nexperiments,” he explained, “continue to be pomace-flies\neven if they loose all good genes necessary for a normal fly-life, or\nif they be possessed with all the bad genes, detrimental to the\nwelfare of this little friend of the geneticist” (Johannsen\n1923, 137). On this account, genes were taken as abstract elements of an\nequally abstract space, whose structure, however, could be explored\nthrough the visible and quantifiable outcome of breeding experiments\nbased on model organisms and their mutants. This became the research\nprogram of Thomas Hunt Morgan and his group. From the early 1910s\nright into the 1930s, the growing community of researchers around\nMorgan and their followers used mutants of the fruit fly Drosophila\nmelanogaster, constructed in ever more sophisticated ways, in\norder to produce a map of the fruit flys genotype in which genes, and\nalleles thereof, figured as genetic markers occupying a particular\nlocus on one of the four homologous chromosome pairs of the fly\n(Kohler 1994). The basic assumptions that allowed the program to\noperate were that genes were located in a linear order along the\ndifferent chromosomes (like \"beads on a string\" as Morgan put it in\n1926, 24), and that the frequency of recombination events between\nhomologous chromosomes, that is, the frequency of crossovers during\nreduction division, gave a measure of the distance between the genes,\nat the same time defining them as units of recombination\n(Morgan et al. 1915).  In this practice, identifiable aspects of the phenotype, assumed to\nbe determined directly by genes in a consciously black-boxed manner,\nwere used as indicators or windows for an outlook on the formal\nstructure of the genotype. This is what Moss has termed the\n“Gene-P” (P standing for phenotype, but also for\npreformationist; Moss 2003, 45 – for the counterpart, the\n“Gene-D”, see below). Throughout his career, Morgan\nremained aware of the formal character of his program. As late as\n1933, on the occasion of his Nobel address, he declared: “At the\nlevel at which the genetic experiments lie it does not make the\nslightest difference whether the gene is a hypothetical unit, or\nwhether the gene is a material particle” (Morgan 1935, 3). In\nparticular, it did not matter if one-to-one, or more complicated\nrelationships reigned between genes and traits (Waters 1994). Morgan\nand his school were well aware that, as a rule, many genes were\ninvolved in the development of a particular trait as, e.g., eye-color,\nand that one gene could affect several characters. To accommodate this\ndifficulty and in line with their experimental regime, they embraced a\ndifferential concept of the gene. What mattered to them was the\nrelationship between a change in a gene and a change in a trait,\nrather than the nature of these entities themselves. Thus the\nalteration of a trait could be causally related to a change in (or a\nloss of) a single genetic factor, even if it was plausible in general\nthat a trait like eye-color was, in fact, determined by a whole group\nof variously interacting genes (Roll-Hansen 1978b; Schwartz\n2000). The fascination of this gene concept consisted in the fact that it\nworked, if properly applied, like a precision instrument in\ndevelopmental and evolutionary studies. On the one hand, the classical\ngene allowed for the identification of developmental processes across\ngenerations.  As a consequence, procedures of classical genetics were\nsoon integrated with the panoply of methods that embryologists had\ndeveloped since the end of the nineteenth to “track”\ndevelopment. (Griesemer 2007). On the other hand, mathematical\npopulation geneticists like Ronald A. Fisher, J. B. S. Haldane, and\nSewall Wright could make use of the classical gene with equal rigor\nand precision to elaborate testable mathematical models describing the\neffects of evolutionary factors like selection and mutation on the\ngenetic composition of populations (Provine 1971). As a consequence,\nevolution became re-defined as a change of gene frequencies in the\ngene pool of a population in what is commonly called the\n“evolutionary,” “neo-Darwinian” or simply\n“modern synthesis” of the late 1930s and early 1940s (Mayr\n& Provine 1980, Gayon 1998). Considered as a “developmental\ninvariant” in reproduction, solely obeying the Mendelian laws in\nits transmission from one generation to the next, the classical gene\nprovided a kind of inertia principle against which the effects of both\ndevelopmental (epistasis, inhibition, position effects etc.) and\nevolutionary factors (selection, mutation, isolation, recombination\netc.) could be measured with utmost accuracy (Gayon 1995, 74). We will\nrevisit the evolutionary synthesis in the third section; for the\nremainder of this section, we would like to turn to the early history\nof developmental genetics, which played an important role in the\neventual “reification” of the gene. Despite the formal character of the classical gene, it became the\nconviction of many geneticians in the 1920s, among them Morgans\nstudent Herman J. Muller, that genes had to be material\nparticles. Muller saw genes as fundamentally endowed with two\nproperties: that of autocatalysis and that\nof heterocatalysis. Their autocatalytic function allowed them\nto reproduce as units of transmission and thus to connect the genotype\nof one generation to that of the next. Their concomitant capability of\nreproducing mutations faithfully once they had occurred gave rise, on\nthis account, to the possibility of evolution. Their heterocatalytic\ncapabilities connected them to the phenotype, as units of function\ninvolved in the expression of a particular character. With his own\nexperimental work, Muller added a significant argument for the\nmateriality of the gene, pertaining to the third aspect of the gene as\na unit of mutation. In 1927, he reported on the induction of Mendelian\nmutations in Drosophila by using X-rays. He was not the first\nto use radiation to induce mutations, but stands out for his\nconclusion that X-rays caused mutations by altering some molecular\nstructure in a permanent fashion, thus giving rise to a whole\n“industry” of radiation genetics in the 1930s and\n1940s.  But the experimental practice of X-raying alone could not open the\npath to a material characterization of genes as units of heredity. On\nthe occasion of the fiftieth anniversary of the rediscovery of\nMendel's work in 1950, Muller thus had to confess: “[T]he real\ncore of gene theory still appears to lie in the deep unknown. That is,\nwe have as yet no actual knowledge of the mechanism underlying that\nunique property which makes a gene a gene—its ability to cause\nthe synthesis of another structure like itself, [in] which even the\nmutations of the original gene are copied. [We] do not know of such\nthings yet in chemistry” (Muller 1951, 95-96). Meanwhile, cytological work had also added credence to the\nmateriality of genes-on-chromosomes. At the same time, however, it\nfurther complicated the notion of the classical gene. During the\n1930s, the cytogeneticist Theophilus Painter correlated formal\npatterns of displacement of genetic loci on Morganian chromosome maps\nwith corresponding visible changes in the banding pattern of giant\nsalivary gland chromosomes of Drosophila.  Barbara McClintock\nwas able to follow with her microscope the\nchanges—translocations, inversions and deletions—induced\nby X-rays in the chromosomes of Zea mays\n(maize). Simultaneously, Alfred Sturtevant, in his experimental work\non the Bar-eye-effect in Drosophila at the end of the 1920s,\nhad shown what came to be called a position effect: the\nexpression of a mutation was dependent on the position which the\ncorresponding gene occupied in the chromosome. This finding stirred\nwide-ranging discussions about what Muller had called the\nheterocatalytic aspect of a gene, namely, its functional association\nwith the expression of a particular phenotypic trait. If a genes\nfunction depended on its position on the chromosome, it became\nquestionable whether that function was stably connected to that gene\nat all, or as Richard Goldschmidt later assumed, whether physiological\nfunction was not altogether a question of the organization of the\ngenetic material as a whole rather than of particulate genes\n(Goldschmidt 1940; cf. Dietrich 2000 and Richmond 2007). Thus far, all experimental approaches to the new field of genetics\nand its presumed elements, the genes, had remained silent with respect\nto the two basic Mullerian aspects of the gene: its autocatalytic and\nits heterocatalytic function. Toward the end of the 1930s, Max\nDelbrück had the intuition that the question of autocatalysis,\nthat is, replication, could be attacked through the study of phage,\ni.e., viruses replicating in bacteria. It has, however, been noted that\nthe phage system, which he established throughout the 1940s, largely\nremained as formal as that of classical Drosophila\ngenetics. Seymour Benzer, for example, used this system in an entirely\n“classical” manner to increase the resolving power of\ngenetic mapping techniques down to distances of a few nucleotide\npairs, thus preparing the ground for Francis Cricks sequence\nhypothesis. Interestingly, Benzer came to the conclusion that\n“gene” was a “dirty word,” as the inferred\nmolecular dimensions of the gene as a unit of function, recombination,\nand mutation clearly differed. Consequently, he suggested referring to\ngenetic elements as cistrons, recons and mutons respectively (Holmes\n2006). Around the same time, Alfred Kühn and his group, as well as\nBoris Ephrussi with George Beadle, were able to open a window on the\nspace between the gene and its presumed physiological function by\ntransplanting organs between mutant and wild type insects. Studying\nthe pigmentation of insect eyes, they realized that genes did not\ndirectly give rise to physiological substances, but that they\nobviously first initiated what Kühn termed a “primary\nreaction” leading to ferments or enzymes, which in turn\ncatalyzed particular steps in metabolic reaction cascades. In 1941,\nKühn summarized the perspective of this kind of\n“developmental-physiological genetics,” as he called\nit: Kühn viewed his experiments as the beginning of a\nreorientation away from what he perceived as the new preformationism\nof transmission genetics (Rheinberger 2000a). He pleaded for an\nepigenetics that would combine genetic, developmental and\nphysiological analyses to define heterocatalysis, that is, the\nexpression of a gene, as the result of an interaction of two reaction\nchains, one leading from genes to particular ferments and the other\nleading from one metabolic intermediate to the next by the\nintervention of these ferments, thus resulting in complex epigenetic\nnetworks. But his own experimental practice throughout the 1940s led\nhim to stay with the completion of the pathway of eye pigment\nformation in Ephestia kühniella (the\nflour-moth). He did not try to develop experimental instruments to\nattack the gene-enzyme relations themselves implicated in the\nprocess. On the other side of the Atlantic, George Beadle and Edward\nTatum, working with cultures of Neurospora crassa, codified the\nlatter connection into the one gene-one enzyme hypothesis. But to\nthem, too, the material character of genes and the way these putative\nentities gave rise to primary products remained elusive and beyond the\nreach of their own biochemical analysis. Thus by the 1940s, the gene in classical genetics was already far\nfrom being a simple notion corresponding to a simple entity.\nConceiving of the gene as a unit of transmission, recombination,\nmutation, and function, classical geneticists combined various aspects\nof hereditary phenomena whose interrelations, as a rule, turned out\nnot to be simple one-to-one relationships. Due to the lack of\nknowledge about the material nature of the gene, however, the\nclassical gene remained a largely formal and operational concept,\ni.e., had to be substantiated indirectly by the successes achieved in\nexplaining and predicting experimental results. This lack\nnotwithstanding, however, the mounting successes of the various\nresearch strands associated with classical genetics led to a\n“hardening” of the belief in the gene as a discrete,\nmaterial entity (Falk 2000, 323-26). The enzyme view of gene function, as envisaged by Kühn and by\nBeadle and Tatum, though with cautious reservation, gave the idea of\ngenetic specificity a new twist and helped to pave the way to the\nmolecularization of the gene to which this section will be devoted\n(see also Kay 1993). The same can be said about the findings of Oswald\nAvery and his colleagues in the early 1940s. They purified the\ndeoxyribonuleic acid of one strain of bacteria, and demonstrated that\nit was able to transmit the infectious characteristics of that strain\nto another, harmless one. Yet the historical path that led to an\nunderstanding of the nature of the molecular gene was not a direct\nfollow-up of classical genetics (cf. Olby 1974 and Morange 2000a).  It\nwas rather embedded in an over-all molecularization of biology driven\nby the application of newly developed physical and chemical methods\nand instruments to problems of biology, including those of\ngenetics. Among these methods were ultracentrifugation, X-ray\ncrystallography, electron microscopy, electrophoresis, macromolecular\nsequencing, and radioactive tracing. At the biological end, it relied\non the transition to new, comparatively simple model organisms like\nunicellular fungi, bacteria, viruses, and phage. A new culture of\nphysically and chemically instructed in vitro biology ensued\nthat in large parts did no longer rest on the presence of intact\norganisms in a particular experimental system (Rheinberger 1997;\nLandecker 2007). For the development of molecular genetics in the narrower sense,\nthree lines of experimental inquiry proved to be crucial. They were\nnot connected to each other when they gained momentum in the late\n1940s, but they happened to merge at the beginning of the 1960s,\ngiving rise to a grand new picture. The first of these developments\nwas the elucidation of the structure of deoxyribonucleic acid (DNA) as\na macromolecular double helix by Francis Crick and James D. Watson in\n1953. This work was based on chemical information about base\ncomposition of the molecule provided by Erwin Chargaff, on data from\nX-ray crystallography produced by Rosalind Franklin and Maurice\nWilkins, and on mechanical model building as developed by Linus\nPauling. The result was a picture of a nucleic acid double strand\nwhose four bases (Adenine, Thymine,\nGuanine, Cytosine) formed complementary pairs (A-T, G-C) that\ncould be arranged in all possible combinations into long linear sequences. At\nthe same time, that molecular model suggested an elegant mechanism for the\nduplication of the molecule. Opening the strands and synthesizing two new strands\ncomplementary to each of the separated threads respectively would suffice to\ncreate two identical helices from one. This indeed turned out to be the case,\nalthough the duplication process would come to be seen as relying on a\ncomplicated molecular replication machinery. Thus, the structure of the DNA\ndouble helix had all the characteristics that were to be expected from a\nmolecule serving as an autocatalytic hereditary entity (Chadarevian 2002). The second line of experiment that formed molecular genetics was\nthe in vitro characterization of the process of protein\nbiosynthesis to which many biochemically working researchers\ncontributed, among them Paul Zamecnik, Mahlon Hoagland, Paul Berg,\nFritz Lipmann, Marshall Nirenberg and Heinrich Matthaei. It started in\nthe 1940s largely as an effort to understand the growth of malignant\ntumors. During the 1950s, it became evident that the process required\nan RNA template that was originally thought to be part of the\nmicrosomes on which the assembly of amino acids took place. In\naddition it turned out that the process of amino acid condensation was\nmediated by a transfer molecule with the characteristics of a nucleic\nacid and the capacity to carry an amino acid. The ensuing idea\nthat it was a linear sequence of ribonucleic acid derived from one of\nthe DNA strands that directed the synthesis of a linear sequence of\namino acids, or a polypeptide, and that this process was mediated by\nan adaptor molecule, was soon corroborated experimentally (Rheinberger\n1997). The relation between these two classes of molecules was\neventually found to be ruled by a nucleic acid triplet code,\nwhich consisted in three bases at a time specifying one amino acid\n(Kay 2000, ch. 6); hence, the sequence hypothesis and\nthe central dogma of molecular biology, which Francis Crick\nformulated at the end of the 1950s: With these two fundamental assumptions, a new view of biological\nspecificity came into play. It was centered on the transfer of\nmolecular order from one macromolecule to the other. In one molecule\nthe order is preserved structurally; in the other it becomes expressed\nand provides the basis for a biological function. This transfer\nprocess became characterized as molecular information\ntransfer. Henceforth, genes could be seen as stretches of\ndeoxyribonucleic acid (or ribonucleic acid in certain viruses)\ncarrying the information for the assembly of a particular\nprotein. Both molecules were thus thought to be colinear, and this\nindeed turned out to be the case for many bacterial genes. In the end,\nboth fundamental properties that Muller had required of genes, namely\nautocatalysis and heterocatalysis, were perceived as relying on one\nand the same stereochemical principle respectively: The base\ncomplementarity between nucleic acid building blocks C/G and A/T (U in\nthe case of RNA) was both responsible for the faithful duplication of\ngenetic information in the process of replication, and, via the\ngenetic code, for the transformation of genetic information into\nbiological function through transcription to RNA\nand translation to proteins. The code turned out to be nearly universal for all classes of\nliving beings, as were the mechanisms of transcription and\ntranslation. The genotype was thus reconfigured as a universal\nrepository of genetic information, sometimes also addressed as\na genetic program. Talk of DNA as embodying genetic\n“information,” as being the “blueprint of\nlife” which governs public discourse until today, emerged from a\npeculiar conjunction of the physical and the life sciences during\nWorld War II, with Erwin Schrödinger's What is Life? as a\nsource of inspiration (Schrödinger 1944), and cybernetics as the\nthen-leading discipline in the study of complex systems. It needs to\nbe stressed, however, that initial attempts to “crack” the\nDNA code by purely cryptographic means soon ran into a dead\nend. Finally it was biochemists who unraveled the genetic code by the\nadvanced tools of their discipline (Judson 1996; Kay 2000). For the further development of the notion of DNA as a\n“program,” we have to consider an additional third line of\nexperiment, aside from the elucidation of DNA structure and the\nmechanisms of protein synthesis.  This line of experiment came out of\na fusion of bacterial genetics with the biochemical characterization\nof an inducible system of sugar metabolizing enzymes. It was largely\nthe work of François Jacob and Jacques Monod and led, at the\nbeginning of the 1960s, to the identification of messenger RNA as the\nmediator between genes and proteins, and to the description of a\nregulatory model of gene activation, the so called operon-model, in\nwhich two classes of genes became distinguished: One class was that\nof structural genes. They were presumed to carry the\n“structural information” for the production of particular\npolypeptides. The other class was that of regulatory\ngenes. They were assumed to be involved in the regulation of the\nexpression of structural information (how this distinction became\nchallenged recently is discussed in Piro 2011). A third element of DNA\ninvolved in the regulatory loop of an operon was a binding site,\nor signal sequence that was not transcribed at all. These three elements, structural genes, regulatory genes, and\nsignal sequences provided the framework for viewing the genotype\nitself as an ordered, hierarchical system, as a “genetic\nprogram,” as Jacob contended, not without immediately adding\nthat it was a very peculiar program, namely one that needed its own\nproducts for being executed: “There is only the incessant\nexecution of a program that is inseparable from its realization. For\nthe only elements being able to interpret the genetic message are the\nproducts of that message” (Jacob 1976, 297). If we take this\nview seriously, although the whole conception looks like a circle and\nhas been criticized as such (Keller 2000), it is in the end the\norganism which interprets or “recruits” the structural\ngenes by activating or inhibiting the regulatory genes that control\ntheir expression. The operon model of Jacob and Monod marked thus the precipitous end\nof the simple, informational concept of the molecular gene.  Since the\nbeginning of the 1960s, the picture of gene expression has become\nvastly more complicated (for the following, compare Rheinberger\n2000b). Moreover, most genomes of higher organisms appear to comprise\nhuge DNA stretches to which no function can as yet be\nassigned. “Non-coding,” but functionally specific,\nregulatory DNA-elements have proliferated: There exist promoter and\nterminator sequences; upstream and downstream activating elements in\ntranscribed or non-transcribed, translated or untranslated regions;\nleader sequences; externally and internally transcribed spacers\nbefore, between, and after structural genes; interspersed repetitive\nelements and tandemly repeated sequences such as satellites, LINEs\n(long interspersed sequences) and SINEs (short interspersed sequences)\nof various classes and sizes. Given all the bewildering details of\nthese elements, it comes as no surprise that their molecular function\nis still far from being fully understood (for an overview see Fischer\n1995). As far as transcription, i.e., the synthesis of an RNA copy from a\nsequence of DNA, is concerned, overlapping reading frames have been\nfound on one and the same strand of DNA, and protein coding stretches\nhave been found to derive from both strands of the double helix in an\noverlapping fashion. On the level of modification after transcription,\nthe picture has become equally complicated. Already in the 1960s it\nwas realized that DNA transcripts such as transfer RNA and ribosomal\nRNA had to be trimmed and matured in a complex enzymatic manner to\nbecome functional molecules, and that messenger RNAs of eukaryotes\nunderwent extensive posttranscriptional modification both at their\n5′-ends (capping) and their 3′-ends (polyadenylation)\nbefore they were ready to go into the translation machinery.  In the\n1970s, to the surprise of everybody, Phillip Allen Sharp and Richard\nJ. Roberts independently found that eukaryotic genes were composed of\nmodules, and that, after transcription, introns were cut out\nand exons spliced together in order to yield a functional\nmessage.  The “gene-in-pieces” (Gilbert 1978) was one of the\nfirst major scientific offshoots of recombinant DNA technology, and\nthis technology has since continued to be good for unanticipated\nvistas on the genome and the processing of its units. A spliced\nmessenger sometimes may comprise a fraction as little as ten percent\nor less of the primary transcript. Since the late 1970s, molecular\nbiologists have become familiar with various kinds of RNA\nsplicing autocatalytic self-splicing, alternative splicing of one\nsingle transcript to yield different messages and even trans-splicing\nof different primary transcripts to yield one hybrid message. In the\ncase of the egg-laying hormone of Aplysia, to take just one\nexample, one and the same stretch of DNA gives rise to eleven protein\nproducts involved in the reproductive behavior of this snail. Finally,\nyet another mechanism, or rather, class of mechanisms has been found\nto operate on the level of RNA transcripts. It is called messenger\nRNA editing. In this case-which in the meanwhile has turned out\nnot just to be an exotic curiosity of some trypanosomes-the original\ntranscript is not only cut and pasted, but its nucleotide sequence is\nsystematically altered after transcription. The nucleotide replacement\nhappens before translation starts, and is mediated by various guide\nRNAs and enzymes that excise old and insert new nucleotides in a\nvariety of ways to yield a product that is no longer complementary to\nthe DNA stretch from which it was originally derived, and a protein\nthat is no longer co-linear with the DNA sequence in the classical\nmolecular biological sense. The complications with the molecular biological gene continue on\nthe level of translation, i.e., the synthesis of a polypeptide\naccording to the sequence of triplets of the mRNA molecule. There are\nfindings such as translational starts at different start codons on one\nand the same messenger RNA; instances of obligatory frameshifting\nwithin a given message without which a nonfunctional polypeptide would\nresult; and post-translational protein modification such as removing\namino acids from the amino terminus of the translated\npolypeptide. There is another observation called protein\nsplicing, instances of which have been reported since the early\n1990s.  Here, portions of the original translation product have to be\ncleaved out (inteins) and others joined together (exteins) before\nyielding a functional protein. And finally, a recent development from\nthe translational field is that a ribosome can manage to translate two\ndifferent messenger RNAs into one single polypeptide. François\nGros, after a life in molecular biology, has come to the rather\nparadoxically sounding conclusion that in view of this perplexing\ncomplexity, the “exploded gene” le gène\néclaté could be specified, if at all, only by\n“the products that result from its activity,” that is, the\nfunctional molecules to which it gives rise (Gros 1991, 297). But it\nappears difficult, if thought through, to follow Gros' advice of such\na reverse definition, as the phenotype would come to define the\ngenotype. The most recent debates concerning the structure and function of\nthe genome are centered around the Encyclopedia of DNA Elements\n(ENCODE) project. The project aimed at identifying all functional\nelements in the human genome. The results of the consortium´s work so\nfar make the already known deviations from the classic model of the\nmolecular gene as a continuous protein coding region flanked by\nregulatory regions appear as the rule rather than the exception. To a\nlarge extent ENCODE researchers found overlap of transcripts, products\nderived from widely separated pieces of DNA sequence and widely\ndispersed regulatory sequences for a given gene. The findings also\nconfirm that most of the genome is transcribed and emphasize the\nimportance and pervasiveness of functional non-protein-coding RNA\ntranscripts that has emerged during the last decade suggesting a\n“vast hidden layer of RNA regulatory transactions”\n(Mattick 2007). In the light of these findings a definition of the\ngene has been proposed, according to which “The gene is a union\nof genomic sequences encoding a coherent set of potentially\noverlapping functional products.” (Gerstein et al 2007,\n677). Such definitions mainly serve the purpose of solving the\nannotation problem (Baetu 2012), which becomes particularly important\nin the context of the increasing importance of bioinformatics and the\nuse of databases that requires a consistent ontology (Leonelli\n2008). More controversial is the notion of function involved\nhere. According to the ENCODE Consortium their data enabled them\n“to assign biochemical functions for 80% of the\ngenome.” (ENCODE Project Consortium 2012, 57), despite the fact\nthat according to conservative estimates only 3–8% of bases\nare under purifying selection, which is usually taken to indicate\nsequence function. Critics have argued that an etiological notion of\nfunction, according to which function is a selected effect, is more\nappropriate in the context of functional genomics (Doolittle et\nal. 2014), whereas others maintain that any causal role of a strand of\nDNA might be relevant, especially in biomedical research (see Germain\net al. 2014 for a philosophical take on the discussion). As we have\nnoticed for previous twists and turns in the history of the gene\nconcept, these developments have been driven by technological\nadvances, in particular in deep RNA sequencing and in identifying\nprotein-DNA interactions. In conclusion, it can be said with Falk (2000, 327) that, on the\none hand, the autocatalytic property once attributed to the gene as an\nelementary unit has been relegated to the DNA at large. Replication\ncan no longer be taken as being specific to the gene as such. After\nall, the process of DNA replication is not punctuated by the\nboundaries of coding regions. On the other hand, as many observers of\nthe scene have remarked (Kitcher 1982; Gros 1991; Morange 2001; Portin\n1993; Fogle 2000), it has become ever harder to define clear-cut\nproperties of a gene as a functional unit with heterocatalytic\nproperties. It has become a matter of choice under contextual\nconstraints as to which sequence elements are to be included and which\nones to be excluded in the functional characterization of a gene. Some\nhave therefore adopted a pluralist attitude towards gene\nconcepts. (Burian 2004). There have been different reactions to this situation. Scientists\nlike Thomas Fogle and Michel Morange concede that there is no longer a\nprecise definition of what could count as a gene. But they do not\nworry much about this situation and are ready to continue to talk\nabout genes in a pluralist, contextual, and pragmatic manner (Fogle\n1990, 2000; Morange 2000b). Elof Carlson and Petter Portin have as well\nconcluded that the present gene concept is abstract, general, and\nopen, despite or just because present knowledge of the structure and\norganization of the genetic material has become so comprehensive and\nso detailed. But they, like Richard Burian (1985), take open concepts\nwith a large reference potential not only as a deficit to live with,\nbut as a potentially productive tool in science. Such concepts offer\noptions and leave choices open (Carlson 1991, Portin\n1993). Philosopher Philip Kitcher, as a consequence of all the\nmolecular input concerning the gene, already some 25 years ago praised\nthe “heterogenous reference potential” of the gene as a\nvirtue and drew the ultraliberal conclusion that “there is no\nmolecular biology of the gene. There is only molecular biology of the\ngenetic material” (Kitcher 1982, 357). From the perspective of the autocatalytic and evolutionary\ndimension of the genetic material, the reproductive function ascribed\nto genes has turned out to be a function of the whole genome. The\nreplication process, that is, the transmission aspect of genetics as\nsuch has revealed itself to be a complicated molecular process whose\nversatility, far from being restricted to gene shuffling during\nmeiotic recombination, constitutes a reservoir for evolution and is\nrun by a highly complex molecular machinery including polymerases,\ngyrases, DNA binding proteins, repair mechanisms, and more. Genomic\ndifferences, targeted by selection, then can, but must not become\n“compartmented into genes” during evolution, as Peter\nBeurton has put it (Beurton 2000, 303). On the other hand, there are those who take the heterocatalytic\nvariability of the gene as an argument to treat the genetic material\nas a whole, hence genes as well, no longer as fundamental in its own\nright, but rather as a developmental resource that needs to be\ncontextualized.  They claim that time has come, if not to dissolve,\nthen at least to embed genetics in development and even development in\nreproduction—as James Griesemer suggests (Griesemer\n2000)—and thus to pick up the thread where Kühn and others\nleft it more than half a century ago. Consequently, Moss defines\n“Gene-D” (the counterpart of the previously mentioned\nphenotypically defined Gene-P) as a “developmental resource\n(hence the D), which in itself is indeterminate with respect to\nphenotype. To be a Gene-D is to be a transcriptional unit on a\nchromosome, within which are contained molecular template\nresources” (Moss 2003, 46; cf. Moss 2008). On this view, these\ntemplates constitute only one reservoir on which the developmental\nprocess draws and are not ontologically privileged as hereditary\nmolecules. With molecular biology the classical gene “went\nmolecular” (Waters 1994). Ironically, the initial idea of genes\nas simple stretches of DNA coding for a protein became dissolved in\nthis process. As soon as the gene of classical genetics had acquired\nmaterial structure through molecular biology, the biochemical and\nphysiological mechanisms that accounted for its transmission and\nexpression proliferated. The development of molecular biology\nitself—that enterprise which is so often described as an utterly\nreductionist conquest—has made it impossible to think of the\ngenome simply as a set of pieces of contiguous DNA co-linear with the\nproteins derived from it. At the beginning of the twenty-first\ncentury, when the results of the \n Human Genome Project were timely\npresented on the fiftieth anniversary of the double helix, molecular\ngenetics seems to have accomplished a full circle, readdressing\nreproduction and inheritance no longer from a purely genetic, but from\nan evolutionary-developmental perspective. At the same time, the gene\nhas become a central category in medicine in the course of the 20th\ncentury (Lindee 2005) and dominates discourses of health and disease\nin the postgenomic era (Rose 2007).  One of the more spectacular events in the history of\ntwentieth-century biology as a discipline, triggered by the rise of\ngenetics (mathematical population genetics in particular), was the\nso-called “modern evolutionary synthesis.” In a whole\nseries of textbooks, published by evolutionary biologists like\nTheodosius Dobzhansky, Ernst Mayr and Julian S.  Huxley, the results\nof population genetics were used to re-establish Darwinian,\nselectionist evolution. After the “eclipse of Darwinism”,\nwhich had reigned around 1900 (Bowler 1983), neo-Darwinism once again\nprovided a unifying, explanatory framework for biology that also\nincluded the more descriptive, naturalist disciplines like\nsystematics, biogeography, and paleontology (Provine 1971; Mayr &\nProvine 1980; Smocoovitis 1996). Scott Gilbert (2000) has singled out six aspects of the notion of\nthe gene as it had been used in population genetics up to the modern\nevolutionary synthesis. First, it was an abstraction, an entity that\nhad to fulfill formal requirements, but that did not need to be and\nindeed was not materially specified. Second, the evolutionary gene had\nto result in or had to be correlated with some phenotypic difference\nthat could be “seen” or targeted by selection. Third, and\nby the same token, the gene of the evolutionary synthesis was the\nentity that was ultimately responsible for selection to occur and last\nacross generations. Fourth, the gene of the evolutionary synthesis was\nlargely equated with what molecular biologists came to call\n“structural genes.” Fifth, it was a gene expressed in an\norganism competing for reproductive advantage. And finally, it was\nseen as a largely independent unit. Richard Dawkins has taken this\nlast argument to its extreme by defining the gene as a\n“selfish” replicator with a life of its own, competing\nwith its fellow genes and using the organism as an instrument for its\nown survival (Dawkins 1976; cf.  Sterelny and Kitcher 1988). Molecular biology, with higher organisms moving center-stage during\nthe past three decades, has made a caricature of this kind of\nevolutionary gene, and has moved before our eyes genes and whole\ngenomes as complex systems not only allowing for evolution to occur,\nbut being themselves subjected to a vigorous process of evolution. The\ngenome in its entirety has taken on a more and more flexible and\ndynamic configuration. Evelyn Fox Keller speaks of “reactive\ngenomes” (Keller 2014). Not only have the mobile genetic\nelements, characterized by McClintock more than half a century ago\nin Zea mays, gained currency in the form of transposons\nthat regularly and irregularly can become excised and inserted all\nover bacterial and eukaryotic genomes, there are also other forms of\nshuffling that occur at the DNA level. A gigantic amount of somatic\ngene tinkering and DNA splicing, for instance, is involved in\norganizing the immune response. It gives rise to the production of\npotentially millions of different antibodies. No genome would be large\nenough to cope with such a task if not the parceling out of genes and\na sophisticated permutation of their parts had not been invented\nduring evolution.  Gene families have arisen from duplication over\ntime, containing silenced genes (sometimes called pseudogenes). Genes\nthemselves appear to have largely arisen from modules by\ncombination. We find jumping genes and multiple genes of one sort\ncoding for different protein isoforms. In short, there appears to be a\nwhole battery of mechanisms and entities that constitute what has been\ncalled “hereditary respiration” (Gros 1991, 337). Molecular evolutionary biologists have scarcely scratched the\nsurface and barely started to understand this flexible genetic\napparatus, although Jacob already put forward a view of the genome as\na dynamic body of ancestrally iterated and tinkered pieces more than\nthirty years ago (Jacob 1977). Genome sequencing combined with\nintelligent sequence data comparison is currently bringing out more\nand more of this structure (on the history of these developments, see\nGarcía-Sancho 2012, on data-driven biology, see Stevens\n2013). One of the surprising results of the Human Genome Project has\nbeen that there are only 21,000 genes. If there is a chance to\nunderstand evolution beyond the classical, itself largely formal,\nevolutionary synthesis, it is from such perspectives of learning more\nabout the genome as a dynamic and modular\nconfiguration. The purported elementary events on the basis of which\nthe complex machinery of genome expression and reproduction\noperates—such as point mutations, nucleotide deletions,\nadditions, and oligonucleotide inversions—are no longer the only\nelements of the evolutionary process, but solely one component in a\nmuch wider arsenal of DNA-tinkering. Beurton concludes from all\nthis that the gene is no longer to be seen as the unit of evolution,\nbut rather as its late product, the eventual result of a long history\nof genomic condensation (Beurton 2000). Others have argued that\ngenomic analysis is not concerned with learning about genes as\nfunctionally or structurally defined units, but with\n“identifying causal relationships between parts of genomes and\nmolecular products and identifying different” (Perini 2011). Finally, recent years have seen a steady increase in evidence for\nepigenetic inheritance systems (Jablonka and Raz 2009, see also the\nentry on inheritance systems).\n This development has not only been promoted as a\nrevolution in molecular biology, defining the post-genomic era (see\nMeloni and Testa 2014 for a discussion of the sociology of hype and\nexpectation in this respect), but has led to yet another change in the\nconcept of the gene, in so far as it can no longer be seen as the only\nunit of inheritance and selection and the primary cause in\ndevelopment. While “epigenetics” refers more generally to\nprocesses of cell determination and differentiation — so called\n“epigenetic control systems,” — epigenetic\ninheritance “occurs when phenotypic variations that do not stem\nfrom variations in DNA base sequences are transmitted to subsequent\ngenerations of cells or organisms” (Jablonka and Raz 2009,\n132). While this can include developmental interactions between mother\nand offspring, social learning, symbolic communication, there is also\na more narrow concept of cellular epigenetic inheritance. It refers to\n“the transmission from mother cell to daughter cell of\nvariations that are not the result of differences in DNA base sequence\nand/or the present environment” (Jablonka and Raz 2009,\n132). Cellular epigenetic inheritance systems discussed in the\nliterature are the transmission of chromatin marks, especially DNA\nmethylation, and RNAs, the inheritance of protein conformations, such\nas in prions, and self-sustaining loops and chromatin inheritance in\nbacteria. In multicellular organisms, especially the first type of\nmechanisms can explain how differentiated cells give rise to identical\ndaughter cells even if the signal that initiated differentiation is\ngone. But more important for the concept of heredity is cellular\ntransgenerational epigenetic inheritance. In such cases “the\nenvironment may induce epigenetic variation by directly affecting the\ngermline or by affecting germ cells through the mediation of the soma,\nbut, in either case, subsequent transmission is through the\ngermline” (Jablonka and Raz 2009, 133). This clearly implies\nthat “the epigenetic body brings the Weismannian body to an\nend,” as Meloni and Testa (2014, 19) put it. Epigenetic\nvariation can have phenotypic effects in the generation exposed to the\nstimulus, or in its offspring, which can persist for several\ngenerations. This possibility opened a new field of interaction\nbetween biology and the social sciences, because factors in the human\nenvironment, from exposure to toxic compounds, via nutrition to\neducation, can have epigenetic effects that span several\ngenerations. The idiom of epigenetics serves to biologize once more\nsocial and ethnic difference, and redefines individual vulnerability\nas well as responsibility and transgenerational accountability\nconcerning effects of lifestyles on health and disease (Meloni and\nTesta 2014). Furthermore, epigenetic inheritance in its broad as well as in its\nnarrow meaning has significant consequences for the understanding of\nevolution and development (Jablonka & Lamb 2005). On the one hand,\nwhen combined with the idea of genetic assimilation (Waddington 1957),\naccording to which genes are selected to fixate previous adaptive, but\nnon-genetic variation, epiegenetic inheritance helps to explain how\nadaptive phenotypic responses become genetically fixed, suggesting\nneo-Lamarckian views of evolution (West-Eberhard 2003). On the other\nhand, the investigation of epigenetic mechanisms casts doubt on the\ncausal or informational primacy of genes, or DNA. As a consequence\ngenetic elements are treated on a par with other developmental\nresources necessary for the formation of a phenotypic trait. From this\nperspective, “genes” appear as processes resulting in\nphenotypic outcomes that involve a great number of other resources\nalongside DNA (Griffiths and Neumann-Held 1999). This view is defended\nin accounts of Developmental Systems Theory (Oyama et al. 2001;\nNeumann-Held and Rehmann-Sutter 2006), although this theory has come\nunder attack from philosophers for offering nothing “that\naspiring researchers can put to work” (Kitcher 2001, 408;\ncf. Hall 2001). And, as Meloni and Testa have pointed out, while\nepigenetics counters gene-centered reductionism, it leads to a\nreduction of environmental influences to molecular agents. When\nscientists today speak of the exposome, the suffix ‐ome\nis indeed supposed to reflect the “digitization of all forms of\nenvironmental exposure, from motherly love to toxins, from food to\nclass inequalities, into a single unifying category and syntax”\n(Meloni and Testa 2014, 18). We have come a long way with molecular biology from genes to\ngenomes to developmental systems. But there is still a longer way to\ngo from genomes to organisms. The developmental gene as it acquired\ncontours over the last twenty years from the early work of Ed Lewis\nand Antonio Garcia-Bellido, and from later work by Walter Gehring,\nChristiane Nüsslein-Volhard, Eric Wieschaus, Peter Gruss, Denis\nDuboule, and others, allows us possibly to go a step along on this\nway. As Gilbert (2000) argues, it is the exact counterpart to the gene\nof the evolutionary synthesis. But we need to be more specific and\ndirect attention to what have been termed “developmental\ngenes” proper. As it turned out, largely from an exhaustive\nexploitation of mutation saturation and genetic engineering\ntechnologies, fundamental processes in development such as\nsegmentation or eye formation in such widely different organisms as\ninsects and mammals are decisively influenced by the activation and\ninhibition of a class of regulatory genes that to some extent resemble\nthe regulator genes of the operon model. But in distinction to these\nlong-known regulatory genes, whose function rests on their ability to\nbeing reversibly switched on and off according to the requirements of\nactual metabolic and environmental situations, developmental genes\ninitiate irreversible processes. They code for so-called transcription\nfactors which can bind to control regions of DNA and thus influence\nthe rate of transcription of a particular gene or a whole set of genes\nat a particular stage of development. Among them are what may be\ncalled developmental genes of a second order which appear to control\nand modulate the units gated by developmental genes of the first\norder. They act as a veritable kind of “master switch” and have been\nfound to be highly conserved throughout evolution. An example is a\nmember of the pax-gene family that can switch on a whole\ncomplex process such as eye formation from insects to\nvertebrates. Most surprisingly, the homologous gene isolated from the\nmouse can replace the one present in Drosophila, and when\nplaced in the fruit fly, switch on, not mammalian eye formation, but\ninsect eye formation. Many of these genes or gene families, like\nthe homeobox-family, are thought to be involved in the\ngeneration of spatial patterning during embryogenesis as well as in\nits temporal patterning. Morange (2000b) distinguishes two central “hard facts”\nthat can be retained from this actually highly fluid and contested\nresearch field.  The first is that the regulatory genes appear to play\na central role in development as judged from the often drastic effects\nresulting from their inactivation. And second, it appears that not\nonly particular homeotic genes have been highly conserved between\ndistantly related organisms, but that they tend to come in complexes\nwhich have themselves been structurally conserved throughout\nevolution, thus once more testifying to genomic higher\norder-structures. Another class of such highly conserved genes and\ngene complexes is involved in the formation of components of pathways\nthat bring about intracellular and cell-to-cell signaling. These\nprocesses are of obvious importance for cellular differentiation and\nfor embryonic development of multi-cellular organisms. One of the big surprises of the extensive use of the technology of\ntargeted gene knockout has been that genes thought to be indispensable\nfor a particular function, when knocked out, did not alter or at least\ndid not significantly alter the organisms performance. This made\ndevelopmental molecular biologists aware that the networks of\ndevelopment appear to be largely redundant, and that certain parts can\ncompensate for eventually missing parts (Mitchell 2009, Ch. 4).\n These networks are obviously\nhighly buffered and thus robust to a considerable extent with respect\nto changing external and internal conditions. Gene products are of\ncourse involved in these networks and their complex functions, but\nthese functions are by no means defined by the genes alone. Another\nresult, coming from embryonic gene expression studies with recently\ndeveloped chip technologies, was that one and the same gene product\ncan be expressed at different stages of development and in different\ntissues, and that it can be implicated in quite different metabolic\nand cellular functions.  Again, this multi-functionality of genes may\nhelp to rethink what genetic and biological determination means\n (see also the discussion of biological determinism in the entry on\n feminist philosophy of biology). These recent results seriously call into question the further\napplicability of straightforward “gene-for” talk. The\ndiscovery of developmental genes throws light on the way in which the\ngenome as a whole is organized as a dynamic, modular, and robust\nentity. Unlike the pieces of DNA with a determinate function as\noriginally envisioned by molecular genetics, developmental genes\nappear as highly conserved in evolution, yet highly variable and\nredundant in function. They rather look like molecular building blocks\nwith which evolution tinkers in constructing organisms—or with\nwhich organisms tinker in evolving. In recent years, evolutionary theories have\ncome to acknowledge this active role of the organism in making use of,\nand even shaping highly conserved genetic mechanisms (West-Eberhard\n2003; Kirschner & Gerhart 2005). As we argued in the preceding sections, the history of\ntwentieth-century genetics is characterized by a proliferation of\nmethods for the individuation of genetic components, and, accordingly,\nby a proliferation of gene definitions. These definitions appear to be\nlargely technology-dependent. Major conceptual changes did not precede\nbut followed experimental breakthroughs. Especially the contrast of\nthe “classical” and the “molecular” gene, the\nlatter succeeding the former chronologically, has raised issues of how\nsuch alternative concepts relate semantically, ontologically, and\nepistemologically. Understanding these relations might offer a chance\nto convey some order to the bewildering variety of meanings inscribed\nin the concept of the gene in the course of a long century. In a now classical paper, Kenneth Schaffner argued that molecular\nbiology—the Watson-Crick model of DNA in\nparticular—effected a reduction of the laws of (classical)\ngenetics to physical and chemical laws (Schaffner 1969, 342). The\nsuccesses of molecular biology in identifying DNA as the genetic\nmaterial—as Watson's and Crick's discovery of the DNA structure\nor the Meselson-Stahl experiment—lend empirical support,\naccording to Schaffner, “for reduction functions involved in the\nreduction of biology as: gene1 = DNA\nsequence1.” Schaffner's account was severely\ncriticized by David Hull, who pointed out that relations between\nMendelian and molecular terms are “many-many, and ” not\n“one-one” or “many-one” relations as assumed\nby Schaffner, because “phenomena characterized by a single\nMendelian predicate term can be reproduced by several types of\nmolecular mechanisms [... and] conversely, the same type of molecular\nmechanism can produce phenomena that must be characterized by\ndifferent Mendelian predicate terms” (Hull 1974, 39).  “To\nconvert these many-many relations,” Hull concluded, “into\nthe necessary one-one or many-one relations leading from molecular to\nMendelian terms, Mendelian genetics must be modified extensively. Two\nproblems then arise —the justification for terming these\nmodifications ‘corrections’ and the transition from\nMendelian to molecular genetics ‘reduction’ rather than\n‘replacement’” (Hull 1974, 43).  To account for this difficulty and accommodate the intuition (which\nHull shared) that there should be at least some way in which it makes\nsense to speak of a reduction of classical to molecular genetics,\nAlexander Rosenberg adopted the notion of supervenience (coined by\nDonald Davidson and going back to George Edward Moore) to describe the\nrelation of classical to molecular genetics. Supervenience implies\nthat any two items that share the same properties in molecular terms,\nalso have the same properties in Mendelian terms, without, however,\nentailing a commitment that Mendelian laws must be deducible from the\nlaws of biochemistry (Rosenberg 1978). This recalls the way in which\nclassical geneticists related gene differences and trait differences\nin the differential gene concept, where trait differences were used as\nmarkers for genetic differences without implying a deducibility of\ntrait behavior, the dominance or recessivity of traits in particular,\nfrom Mendelian laws (Schwartz 2000; Falk 2001). Interestingly, Kenneth\nWaters has argued on this basis, and against Hull, that the complexity\nthat was revealed by molecular genetics was simply the complexity\nalready posited, albeit in an abstract manner, by classical\ngeneticists (Waters 1994, 2000). While relations between molecular\nand classical genetics have proven to be non-deductive, they exist and\nconnect the two fields in epistemically productive ways on a\ncase-by-case basis (Kitcher 1984; Schaffner 1993; Darden 2005; Weber\n2007). The literature on genetics and reductionism has meanwhile become as\nvariegated and complex as the field of scientific activities it\nattends to illuminate. In his book-length, critical assessment of that\nliterature, Sahotra Sarkar made an interesting move by distinguishing\nfive different concepts of reduction, of which he considers three to\nbe particularly relevant to genetics: “weak reduction,”\nexemplified by the notion of heritability; “abstract\nhierarchical reduction,” exemplified by classical genetics; and\n“approximate strong reduction,” exemplified by the use of\n“information”-based explanation in molecular genetics. The\nperhaps not so surprising result with which Sarkar comes up is that\n“reduction—in its various types—is scientifically\ninteresting beyond, especially, the formal concerns of most\nphilosophers of sciences” in that it constitutes a\n“valuable, sometimes exciting, and occasionally indispensable\nstrategy in science” and thus needs to be acknowledged as being\nultimately “related to the actual practice of genetics”\n(Sarkar 1998, 190). In a similar vein, Jean Gayon has expounded a\n“philosophical scheme” for the history of genetics which\ntreats phenomenalism, instrumentalism, and realism not as alternative\nsystems that philosophers have to decide between, but as actual,\nhistorically consecutive strategies employed by geneticists in their\nwork (Gayon 2000). We would finally like to address briefly two issues that are\nrelated to the problem of reduction and have occasioned repeated\ndiscussion in the philosophical literature. The first point concerns\nthe notion of “information” in molecular genetics. The\ninflationary early molecular use of the terms “genetic\ninformation” and “genetic program” has been widely\ncriticized by philosophers and historians of science alike (Sarkar\n1996, Kay 2000, Keller 2001). No one less than Gunther Stent, one of\nthe strongest proponents of what has been termed the\n“informational school” of molecular biology, warned long\nago that talk about “genetic information” is best confined\nto its explicit and explicable meaning of sequence specification, that\nis, that it is best to keep it in the local confines of\n“coding” instead of scaling it up to a global talk of\ngenetic “programming.” “It goes without saying,\n” he contends, “that the principles of chemical catalysis\n[of an enzyme] are not represented in the DNA nucleotide base\nsequences,” and he concludes: However, it appears to us that one should remain aware of the fact\nthat the molecular biological notion of a flow of information, in the\ndouble sense of storage as well as expression in the interaction\nbetween two classes of macromolecules, has added a dimension of\ntalking about living systems that helps to distinguish them\nspecifically from chemical and physical systems characterized solely\nby flows of matter and flows of energy (Crick 1958; Maynard Smith\n2000). Molecular biology, seen by many historians and philosophers of\nbiology as a paragon of reductionism, did not only introduce physics\nand chemistry into biology, or even reduce the latter to the\nformer. Paradoxically, the achievements of molecular biology also\nhelped to find a new way of conceiving of organisms in a fundamentally\nnon-reductive manner. In a broader vision, this implies\n“epigenetic” mechanisms of intracellular and intercellular\nmolecular signaling and communication in which genetic information and\nits differential expression is embedded and through which it is\ncontextualized. Upon this view, it appears not only legitimate, but\nheuristically productive to conceive of the functional networks of\nliving beings in a biosemiotic terminology instead of a simply\nmechanistic or energetic idiom (Emmeche 1999). The second point concerns the already mentioned\n“gene-for” talk. Why has talk about genes coding for this\nand that become so entrenched?  Why do genes still appear as the\nultimate determinants and executers of life?  As we have seen in the\npreceding two sections, the advances in conceptualizing processes of\norganismic development and evolution have thoroughly deconstructed the\nview of genes that dominated classical genetics and the early phases\nof molecular genetics. Why is it, to talk with Moss, that genetics is\nstill “understood not as a practice of instrumental reductionism\nbut rather in the constitutive reductionist vein” implying the\n“ability to account for the production of the phenotype on the\nbasis of the genes” (Moss 2003, 50)? A recent empirical study by\nPaul Griffiths and Karola Stotz on how biologists conceptualize genes\ncomes indeed to the conclusion “that the classical molecular\ngene concept continues to function as something like a stereotype for\nbiologists, despite the many cases in which that conception does not\ngive a principled answer to the question of whether a particular\nsequence is a gene” (Stotz, Griffiths and Knight 2004,\n671). Waters provides a surprising but altogether plausible\nepistemological answer to this apparent conundrum. He reminds us\nforcefully that in the context of scientific work and research, genes\nare first and foremost handled as entities of investigative rather\nthan explanatory value (Waters 2004; cf. Weber 2004, 223). It is on\nthe grounds of their epistemic function in research that they appear\nso privileged. Waters deliberately goes beyond the question of\nreductionism or anti-reductionism that has structured so much\nphilosophical work on modern biology, especially on genetics and\nmolecular biology over the past decades, and ties it into the\nphilosophical literature on the relationship between causation and\nmanipulability that has more recently gained in prominence (Waters\n2007). He stresses that the successes of a gene-centered view on the\norganism are not due to the fact that genes are the major determinants\nof the main processes in living beings. Rather, they figure so\nprominently because they provide highly successful entry points for\nthe investigation of these processes. The success of\ngene-centrism, according to this view, is not ontologically, but first\nand foremost epistemologically and pragmatically grounded (cf. Gannett\n1999).  From this, two major philosophical claims result: First, that it is\nthe structure of investigation rather than an all-encompassing system\nof explanation that has grounded the scientific success of genetics;\nand second, that the essential incompleteness of genetic explanations,\nwhenever they are meant to be located at the ontological level, calls\nfor the promotion of a scientific pluralism (Waters 2004b;\nDupré 2004; Burian 2004; Griffiths and Stotz 2006). The message\nis that complex objects of investigation such as organisms cannot be\nsuccessfully understood by a single best account or description, and\nthat any experimentally proceeding science is basically advancing\nthrough the construction of successful, but always partial models.\nWhether and how long these models will continue to be gene-based,\nremains an open question. Any answers to that question will be\ncontingent on future research results, not on an ontology of life.","contact.mail":"robert.meunier@ici-berlin.org","contact.domain":"ici-berlin.org"}]
