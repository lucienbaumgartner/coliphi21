[{"date.published":"2003-11-03","date.changed":"2020-04-21","url":"https://plato.stanford.edu/entries/qm-decoherence/","author1":"Guido Bacciagaluppi","author1.info":"http://www.uu.nl/staff/GBacciagaluppi/0","entry":"qm-decoherence","body.text":"\n\n\nInterference phenomena are a well-known and crucial aspect of\n quantum mechanics,\n famously exemplified by the two-slit experiment. There are many\nsituations, however, in which interference effects are artificially or\nspontaneously suppressed. The theory of decoherence is\nprecisely the study of such situations. It is is relevant (or is\nclaimed to be relevant) to a variety of questions ranging from the\nmeasurement problem to the arrow of time, and in particular to the\nquestion of whether and how the ‘classical world’ may\nemerge from quantum mechanics. (See also the entry on\n philosophical issues in quantum theory.)\n \n\nIn\n Section 1\n we discuss the concept of suppression of interference and give a\nsimplified survey of the theory, emphasising features that will be\nrelevant later. In fact, the term decoherence refers to two largely\noverlapping areas of research. The characteristic feature of the first\n(often called ‘environmental’ or ‘dynamical’\ndecoherence) is the study of concrete models of (spontaneous)\ninteractions between a system and its environment that lead to\nsuppression of interference effects. The second (the theory of\n‘decoherent histories’ or ‘consistent\nhistories’) is an abstract and more general formalism capturing\nessential features of decoherence. The two are obviously closely\nrelated, and will be reviewed in turn.\n Section 2\n then criticises the claim that decoherence solves the measurement\nproblem of quantum mechanics, and discusses the exacerbation of the\nproblem through the inclusion of environmental interactions. It is\nthus important to consider not decoherence by itself, but the\ninterplay between decoherence and the various approaches to the\nfoundations of quantum mechanics that provide possible solutions to\nthe measurement problem and related puzzles.\n Section 3\n deals with the role of decoherence in relation to a number of such\napproaches, including mainstream foundational approaches such as\nEverett, Bohm and GRW, traditional approaches such as those by von\nNeumann, Heisenberg and Bohr, and a few more. Finally, in\n Section 4\n we describe the overall picture of the emergent structures that\nresult from this use of decoherence, as well as a few more speculative\n applications.[1]\n \n\nSuppression of interference has featured in many papers since the\nbeginning of quantum mechanics, such as Mott’s (1929) analysis\nof \\(\\alpha\\)-particle tracks. The modern foundation of decoherence as a\nsubject in its own right was laid by H.-D. Zeh in the early 1970s (Zeh\n1970, 1973). Equally influential were the papers by W. Zurek from the\nearly 1980s (Zurek 1981, 1982). Some of these earlier examples of\ndecoherence (e.g., suppression of interference between left-handed and\nright-handed states of a molecule) are mathematically more accessible\nthan more recent ones. A concise and readable introduction to the\ntheory is provided by Zurek in Physics Today (1991). This\narticle was followed by publication of several letters with\nZurek’s replies (1993), which highlight controversial issues.\nMore recent surveys are given in Zeh (2003a), Zurek (2003), and in the\nbooks by Giulini et al. (1996, second edition Joos et\nal. 2003), and by Schlosshauer (2007).\n\nThe two-slit experiment is a paradigm example of an\ninterference experiment. One repeatedly sends electrons or\nother particles through a screen with two narrow slits, the electrons\nimpinge upon a second screen, and we ask for the probability\ndistribution of detections over the surface of the screen. One might\nnaively try to calculate them by summing over the probabilities of\ndetection at the slits multiplied by the probabilities for detection\nat the screen conditional on detection at the slits. But these are the\ncorrect probabilities for a different experiment, with\ndetections at the slits, whether or not we believe that measurements\nare related to a ‘true’ collapse of the wave function\n(i.e. that only one of the components survives the\nmeasurement and proceeds to hit the\n screen[2]).\n If there are no such detections, in general there is an additional\nso-called interference term in the correct expression for the\nprobability, and this term depends on both the wave\ncomponents that pass through the\n slits.[3] \nThere are, however, situations in which this interference term does\nnot appear or is negligible, and the naive formula applies. This is\nthe case if some other systems interact with the electron between the\nslits and the screen, leading to enough\n entanglement\n with the components of the wave going through the two slits. Then,\nthe probabilities of detection at the screen are as if we had\nperformed a detection at the slits. \nIt is not difficult to see why this must be so. If Alice and Bob share\na pair of systems that are entangled, then the probabilities for the\nresults of any measurements Bob might make do not depend on\nwhether or not Alice also makes any measurements (this is the quantum\nmechanical no-signalling theorem). In exactly the same way, the\npattern of detections at the screen cannot distinguish mere\nentanglement with some other systems from the actual use of those\nsystems for detection at the slits. \nSo, for example, there could be sufficiently many stray particles that\nscatter off the\n electron.[4]\n The phase relation between the two components of the wave function,\nwhich is responsible for interference, is now well-defined only at the\nlevel of the larger system composed of electron and stray particles,\nand can produce interference only in a suitable experiment including\nthe larger system. Such a phenomenon of suppression of interference is\nwhat is called decoherence. \n‘Environmental’ decoherence is decoherence that arises\nthrough suitable interaction of a system with its environment. The\nstudy of environmental decoherence consists to a large extent in the\nconstruction and investigation of concrete models of such\ninteractions. We have already mentioned taking an environment of\nrelatively light particles that scatter off a relatively heavy\nparticle. Such a model can be used to study e.g. chiral molecules. Or\none can take an atom in interaction with the electromagnetic field, or\na harmonic oscillator in a thermal bath of oscillators, and many more.\nVarious features of interest typically arise in such models: some are\nin common to most models, others are highly model-dependent. \nOne feature of these environmental interactions is that they suppress\ninterference between states from some preferred set\n(‘eigenstates of the decohering variable’). This can be a\ndiscrete set of states, e.g. the upper and lower component of the wave\nfunction in our simple example of the two-slit experiment, or left-\nand right-handed states in models of chiral molecules; when an atom\ninteracts with the electromagnetic field, the preferred states will be\nthe stationary states (which are the states we observe in\nspectroscopy). Or it could be some continuous set, e.g. the\n‘coherent states’ of a harmonic oscillator (in which case\nthe terminology of ‘eigenstates’ or\n‘eigenbasis’ of a preferred observable is not quite\naccurate). The intuitive picture is one in which the environment\nmonitors the system of interest by spontanesouly and continuously\n‘measuring’ some quantity characterised by the set of\npreferred states (i.e. the environment interacts with the system in\nsuch a way that it could in principle be used as a measuring\napparatus). \nSuch a ‘measurement-like’ interaction intuitively does not\ndisturb the eigenstates of the monitored observable. Thus these\npreferred states can in fact be characterised in terms of their\nrobustness or stability with respect to the interaction with the\nenvironment. The system gets entangled with the environment, but the\nstates between which interference is suppressed are the ones that\nwould themselves get least entangled with the environment\nunder this interaction. In this connection, one also says that\ndecoherence induces ‘effective superselection rules’,\nmeaning the following. A strict superselection rule applies when there\nare some observables – in technical terminology they are called\nclassical – that commute with all observables (for a review, see\nWightman 1995). Intuitively, these observables are infinitely robust,\nsince no possible interaction can disturb them (at least as long as\nthe interaction Hamiltonian is considered to be an observable). By an\neffective superselection rule one means, analogously, that\ncertain observables (e.g. chirality) will not be disturbed by the\ninteractions that actually take place. \nIn many models of decoherence, the preferred states are robust in an\neven stronger sense, because information about them is stored in a\nredundant way in the environment (say, because a Schrödinger cat\nhas interacted with so many stray particles: photons, air molecules,\ndust). This information can later be acquired by an observer without\nfurther disturbing the system (we observe – however that may be\ninterpreted – whether the cat is alive or dead by intercepting\non our retina a small fraction of the light that has interacted with\nthe cat).  \nWhat states are preferred will depend on the details of the\ninteraction, but in many cases, interactions are characterised by\npotentials that are functions of position, so preferred states are\noften related to position. For the chiral molecule, the left- and\nright-handed states are indeed characterised by different spatial\nconfigurations of the atoms in the molecule. For the harmonic\noscillator, one should think of the environment\n‘measuring’ approximate eigenstates of position, or rather\napproximate joint eigenstates of position and momentum, so-called\ncoherent states (since information about the time of flight is also\nrecorded in the environment).  \nThe resulting localisation can be on a very short length scale, i.e.\nthe characteristic length above which coherence is dispersed\n(‘coherence length’) can be very short. A speck of dust of\nradius \\(a = 10^{-5}\\)cm floating in the air will have\ninterference suppressed between spatially localised components with a\nwidth of \\(10^{-13}\\)cm. Even more strikingly, the time scales for\nthis process are often minute. This coherence length is reached after\na microsecond of exposure to air, and suppression of interference on a\nlength scale of \\(10^{-12}\\)cm is achieved already after a\n nanosecond.[5] \nWithin the environmental decoherence literature, models tend to be\nformulated in terms of master equations for the evolution of the\ndensity operator describing the system. As a consequence of\ndecoherence this very quickly becomes (at least approximately)\ndiagonal in the basis of preferred states (whether discrete or\ncontinuous). Thus, the master equation for the density operator for\nthe system is essentially equivalent to an evolution equation for the\nprobability distribution over the preferred states. In models\nwhere coherent states are preferred, one can then compare this to the\nLiouville evolution of probability distributions over classical phase\nspace, and in fact one obtains extremely good quantitative agreement.\n \nThese features are not claimed to obtain in all cases of interaction\nwith some environment. It is a matter of detailed physical\ninvestigation to assess which systems exhibit which features, and how\ngeneral the lessons are that we might learn from studying specific\nmodels. One should thus beware of common overgeneralisations. For\ninstance, decoherence does not affect only and all\n‘macroscopic systems’. It is true that middle-sized\nobjects, say, on the Earth’s surface will be very effectively\ndecohered by the air in the atmosphere, and this is an excellent\nexample of decoherence at work. On the other hand, there are also very\ngood examples of decoherence-like interactions affecting microscopic\nsystems, such as in the interaction of \\(\\alpha\\)-particles with the gas\nin a bubble chamber. (Note, however, that this also relies on the\n\\(\\alpha\\)-particles being emitted in states that are superpositions of\nstrongly outward directed wavepackets.) Further, there are arguably\nmacroscopic systems for which interference effects are not suppressed.\nFor instance, it has been shown to be possible to sufficiently shield\nSQUIDS (a type of superconducting devices) from decoherence for the\npurpose of observing superpositions of different macroscopic currents\n– contrary to what one had expected (see e.g. Leggett 1984, and\nesp. 2002, Section 5.4). Anglin, Paz and Zurek (1997) examine some\nless well-behaved models of environmental decoherence and provide a\nuseful corrective as to its scope. \nAs mentioned above, when interference is suppressed in a two-slit\nexperiment, the naive probability formula applies, and we can\ncalculate the detection probabilities at the screen by adding\nprobabilities for what are formally the ‘trajectories’\nfollowed by individual electrons. The decoherent histories or\nconsistent histories formalism (originating with Griffiths 1984;\nOmnès 1988, 1989; and Gell-Mann and Hartle 1990) takes this as\nthe defining feature of decoherence. (See also the entry on the\n consistent histories approach to quantum mechanics.\n There are some differences between the various authors, but we shall\ngloss them\n over.[6]) \nIn a nutshell, the formalism is as follows. Take a sequence of times\n\\(t_1 ,\\ldots ,t_n\\), and take\northogonal families of (Heisenberg-picture) projections at those\n times,[7]\n with \nOne defines histories as time-ordered sequences of projections\nat the given times, choosing one projection from each family,\nrespectively. Such histories form a so-called alternative and\nexhaustive set of histories. \nTake a state \\(\\varrho\\). We wish to define probabilities for the set of\nhistories. If one takes the usual probability formula based on\nrepeated application of the Born rule, one obtains \nWe shall take (2) as defining ‘candidate probabilities’.\nIn general these probabilities exhibit interference, in the sense that\nsumming over them is not equivalent to omitting the intermediate\nprojections in (2) (‘coarse-graining’ the histories). In\nthe special cases in which the interference terms vanish for any pair\nof distinct histories, we say that the set of histories satisfies the\nconsistency or (weak) decoherence condition. It is easy\nto see that this condition takes the form \nfor any pair of distinct histories (the real part of the ‘decoherence\nfunctional’ vanishes). \nIf this is satisfied, we can view (2) as defining the distribution\nfunctions for a stochastic process with the histories as trajectories.\nDecoherence in the sense of this abstract formalism is thus defined\nsimply by the condition that the quantum probabilities for later\nevents can be calculated as if the state had collapsed at the\nintermediate times. Qualitatively one recovers classical behaviour, in\nthe sense that the histories are assigned quantum probabilities that\nnevertheless satisfy the classical formula of total probability.  \nA stronger form of the decoherence condition, namely the vanishing of\nboth the real and imaginary part of the decoherence functional, can be\nused to prove theorems on the existence of (later) ‘permanent\nrecords’ of (earlier) events in a history, which is a\ngeneralisation of the idea of ‘environmental\n monitoring’.[8]\n For instance, if the state \\(\\varrho\\) is a pure state \\(\\lvert \\psi \\rangle\\langle\\psi\\rvert\\)\nthis (strong) decoherence condition is equivalent, for all \\(n\\),\nto the orthogonality of the vectors \nand this in turn is equivalent to the existence of a set of orthogonal\nprojections\n\\(R_{\\alpha_1 \\ldots\\alpha_i}(t_i)\\)\n(for any \\(t_i \\le t_n\\))\nthat extend consistently the given set of histories and are perfectly\ncorrelated with the histories of the original set (Gell-Mann and\nHartle 1990). Note, however, that these ‘generalised\nrecords’ need not be stored in separate degrees of freedom, such\nas an environment or measuring\n apparatus.[9] \nVarious authors have taken the theory of decoherent histories as\nproviding an interpretation of quantum mechanics. For instance,\nGell-Mann and Hartle sometimes talk of decoherent histories as a\nneo-Everettian approach, while Omnès appears to think of\nhistories along neo-Copenhagen lines (perhaps as an experimental\ncontext creating a ‘quantum phenomenon’ that can stretch\nback into the\n past).[10]\n Griffiths (2002) has probably developed the most detailed of these\ninterpretational approaches (also addressing various earlier\ncriticisms, e.g. by Dowker and Kent (1995, 1996)). In itself, however,\nthe formalism is interpretationally neutral and has the particular\nmerit of bringing out that when interference is suppressed, one can\nreidentify different components of the state over time, making this\nformalism especially appropriate for discussing temporal evolution at\nthe level of the non-interfering components. \nWork on environmental decoherence and that on decoherent histories tend to\nbe unfortunately rather separate. In comparing the two, we shall need\nto look both at cases that can be described by both formalisms (and\nask whether or not the two descriptions are equivalent), and at cases\nwhere only the more abstract formalism of decoherent histories\napplies. \nWith regard to the latter, there are of course cases in which the\ndecoherence functional vanishes just by numerical coincidence. But\nthere are also systematic cases of vanishing of interference\neven without environmental monitoring, namely in the presence of\n‘conservation-induced’ decoherence (see e.g. Halliwell\n2010). As an example, take an isolated system (say, with discrete\nenergy levels), and consider histories composed of projections onto\nits energy states at arbitrary times. Because energy is conserved, in\nthe energy basis each individual component is following the\nSchrödinger equation without interfering with the other\ncomponents, and the corresponding histories decohere. While some\nauthors in the decoherent histories literature take\nconservation-induced decoherence to be a significant novelty of the\ntheory, it should be noted that it lacks the robustness of\nenvironment-induced decoherence, since it lacks a mechanism that\nactively suppresses interference. \nWith regard to the former case, environmental decoherence can be\neasily described also in terms of decoherent histories. One needs to\ntake times that are separated by intervals larger than the decoherence\ntime scale, and projections onto the preferred states. Then the\nenvironmental monitoring ensures that the resulting histories\ndecohere. (In case of a continuous set of preferred states, one might\nneed to generalise the histories formalism slightly, using\n‘effects’ rather than projections; see e.g. Kent 1998.) In\nthis sense, environmental decoherence can be seen as a special case of\ndecoherent histories, but the descriptions given by the two formalisms\nare somewhat different. While decoherent histories define\nmulti-time distributions over the preferred states (at discrete\ntimes), models of environmental decoherence essentially describe\nsingle-time distributions over the preferred states. While they\nhave the advantage of being well-defined at all times, these\nsingle-time distributions do not explicitly describe any temporal\nevolution at the level of the individual components.  \nIn a number of models of environmental decoherence, however, it is\nobvious what the dynamical behaviour should be even at the level of\nindividual components. Specifically, in models where the preferred\nstates are coherent states, comparison of the master equation for the\nreduced state of the system with the evolution of a classical\nLiouville distribution suggests that the trajectories of individual\ncomponents in fact approximate surprisingly well the corresponding\nNewtonian trajectories. Intuitively, one can explain this by noting\nthat the preferred states (which are wave packets that are narrow in\nposition and remain so because they are also narrow in momentum) are\nthe states that tend to get least entangled with the environment.\nTherefore they will tend to follow the Schrödinger equation more\nor less undisturbed. But, as a matter of fact, narrow wave packets\nfollow approximately Newtonian trajectories, at least if the external\npotentials in which they move are uniform enough across the width of\nthe packets (results of this kind are known as ‘Ehrenfest\ntheorems’). Thus, the resulting trajectories will be close to\nNewtonian ones (on the relevant\n scales).[11] \nThis picture cannot be exact, because as soon as a localised\nwave packet has spread enough, it will be decohered into new more\nlocalised packets, so that intuitively one will get some kind of\n‘fanning out’ of trajectories. In fact, such deviations\nfrom Newtonian behaviour are due both to the tendency of the\nindividual components to spread and to the localising effect of the\ninteraction with the environment, which further enhances the\ncollective spreading of the components (because a narrowing in\nposition corresponds to a widening in momentum). See Rosaler (2016)\nfor a very nice treatment (that uses an ‘open systems’\nversion of Ehrenfest). A vivid example are the observed trajectories\nof \\(\\alpha\\)-particles in a cloud chamber, which are indeed extremely\nclose to Newtonian ones, except for additional tiny\n ‘kinks’.[12] \nIn other models, e.g. when the electromagnetic field privileges the\nstationary states of an atom, there is no such comparison with\nclassical equations, and the lack of multi-time distributions becomes\na limitation of the model. Such limitations might be overcome by\ncombining models of environmental decoherence with more\nphenomenological models of ‘continuous measurement’ (as\ndone in a different example by Bhattacharya, Habib and Jacobs 2000).\nAs shown by Brun (2002), the dynamics of stationary states (quantum\njumps!) can be obtained from first principles in the decoherent\nhistories formalism.  \nOne often hears the claim that decoherence solves the measurement\nproblem of quantum mechanics (see the entry on\n philosophical issues in quantum theory).\n Physicists who work on decoherence generally know better, but it is\nimportant to see why even in the presence of decoherence phenomena,\nthe measurement problem remains or in fact gets even worse. \nThe measurement problem is really a complex of problems, revolving\naround the question of whether one can apply quantum mechanics itself\nto the description of quantum measurements. One can just deny this, if\none takes quantum mechanics to be a phenomenological theory. But if\nquantum mechanics is not the fundamental theory that explains the\nphenomenology of quantum measurements, the question arises how we can\nexplain what ‘measurements’ and ‘results’ are.\nThis is the measurement problem in the wide sense of the term. \nIf instead we assume that quantum mechanics is itself applicable to\nthe description of measurements, then the question becomes one of how\none should model a measurement within quantum theory, specifically as\nsome appropriate interaction between a ‘system’ and an\n‘apparatus’, and of whether by so doing one can\nderive from the unitary evolution for the total system of\nsystem and apparatus the three phenomenological aspects of quantum\nmeasurements: that measurements have results, that these results\nobtain with some characteristic probabilities, and that depending on\nthe result of a measurement the state of the system is generally\ntransformed in a characteristic way (for this subdivision of the\nproblem, see Maudlin 1995). This derivation, however, appears to be\nimpossible. \nIndeed, as pointed out already by von Neumann (1932, Section VI.3),\none cannot reproduce the correct probabilities by assuming that they\narise because we are ignorant of the exact state of a macroscopic\napparatus. But whatever the exact initial state of the apparatus, if\nthe system (say, an electron) is described by a superposition of two\ngiven states, say, spin in \\(x\\)-direction equal \\(+\\frac{1}{2}\\) and spin in\n\\(x\\)-direction equal \\(-\\frac{1}{2}\\), and we let it interact\nwith a measuring apparatus that couples to these states, the final\nquantum state of the composite will be a sum of two components, one in\nwhich the apparatus has coupled to (has registered) \\(x\\)-spin \\(= +\\frac{1}{2}\\), and one in which the apparatus has coupled to (has registered)\n\\(x\\)-spin \\(= -\\frac{1}{2}\\).[13]\n This is the measurement problem in the narrow sense of the term.  \nThe fact that interference is typically very well suppressed between\nlocalised states of macroscopic objects suggests that it is at least\nrelevant to why macroscopic objects in fact appear to us to be in\nlocalised states. In the special case of measuring apparatuses, it\nwould then be relevant to why we never observe an apparatus pointing,\nsay, to two different results. Does modelling measurements\nincluding the decoherence interactions with the environment\nallow one to derive that measurements always have results? This is\nsomewhat part of the ‘folklore’ of decoherence, but as\npointed out by many physicists and philosophers alike (e.g. Pearle\n1997; Bub 1997, Chapter 8; Adler 2003; Zeh 2003a, pp. 14–15), it\nis not the case: while decoherence does explain why we do\nnot observe superpositions of measurement results, it does\nnot explain why we do observe measurement results in the\nfirst place. \nIndeed, what happens if we include decoherence in the description?\nDecoherence tells us, among other things, that plenty of interactions\nare taking place all the time in which differently localised states of\nthe apparatus registering, say, different \\(x\\)-spin values of an\nelectron couple to different states of the environment. But now, by\nthe same arguments as above, the composite of electron,\napparatus and environment will be a superposition of (i) a state\ncorresponding to the environment coupling to the apparatus coupling in\nturn to the value \\(+\\frac{1}{2}\\) for the spin, and of (ii) a state corresponding\nto the environment coupling to the apparatus coupling in turn to the\nvalue \\(-\\frac{1}{2}\\) for the spin. We are thus left with the following choice,\nwhether or not we include decoherence: either the composite\nsystem is not described by such a superposition, because the\nSchrödinger equation actually breaks down and needs to be\nmodified, or it is described by such a superposition, but then we need\nto either to supplement quantum mechanics with appropriate hidden\nvariables, or to give an appropriate interpretation of the\nsuperposition. \nTherefore, decoherence as such does not provide a solution to the\nmeasurement problem, at least not unless it is combined with an\nappropriate foundational approach to the theory – whether this\nbe one that attempts to solve the measurement problem, such as\nBohm, Everett or GRW; or one that attempts to dissolve it, such\nas various versions of the Copenhagen interpretation. (See also\nWallace 2012b.) \nDecoherence is clearly neither a dynamical evolution contradicting the\nSchrödinger equation, nor a new supplementation or interpretation\nof the theory. As we shall discuss, however, it both reveals important\ndynamical effects within the Schrödinger evolution, and\nmay be suggestive of possible interpretational moves. As such\nit has much to offer to the philosophy of quantum mechanics. At first,\nhowever, it seems that discussion of environmental interactions should\nactually exacerbate the existing problems. Intuitively, if the\nenvironment is carrying out lots of spontaneous measurements even\nwithout our intervention, then the measurement problem ought to apply\nmore widely, also to these spontaneously occurring\nmeasurements. \nIndeed, while it is well-known that localised states of macroscopic\nobjects spread very slowly with time under the free Schrödinger\nevolution (i.e., if there are no interactions), the situation turns\nout to be different if they are in interaction with the environment.\nAlthough the different components that couple to the environment will\nbe individually incredibly localised, collectively they can have a\nspread that is many orders of magnitude larger. That is, the state of\nthe object and the environment could be a superposition of zillions of\nvery well localised terms, each with slightly different positions, and\nthat are collectively spread over a macroscopic distance,\neven in the case of everyday\n objects.[14]\n Given that everyday macroscopic objects are particularly subject to\ndecoherence interactions, this raises the question of whether quantum\nmechanics can account for the appearance of the everyday world even\napart from the measurement problem. \nThere is thus an even wider problem, which we can call the problem of\nthe classical regime of quantum mechanics, and quite analogous\nto the measurement problem. Can quantum mechanics be applied to the\ndescription of classical systems? We can deny it (as orthodox\napproaches do), but then what are classical systems in the first\nplace? And if we apply quantum mechanics also to the systems that seem\nto populate our everyday world, can we derive from quantum\nmechanics the behaviour that is characteristic of such\n‘classical’ systems? But such a derivation appears\nimpossible. To put it crudely: if everything is in interaction with\neverything else, everything is generically entangled with everything\nelse, and that is a worse problem than measuring apparatuses being\nentangled with measured systems. \nDespite the fact that decoherence interactions extend the measurement\nproblem to the wider problem of the classical regime, decoherence\nis relevant to the solution of both problems because at the\nlevel of components of the wave function the quantum description\nof decoherence phenomena (tantalisingly!) includes both measurement\nresults and other quantum phenomena (such as quantum jumps) as well as\nclassical behaviour. This suggests that to a large extent decoherence\nprovides an interpretation-neutral strategy for tackling the\nmeasurement problem and the problem of the classical regime (a thesis\ndeveloped in greater detail by Rosaler 2016), and that the solution to\nthese problems lies in combining decoherence with the main\nfoundational approaches to quantum mechanics.  \nThere are a wide range of approaches to the foundations of quantum\nmechanics, however (see also the entry on\n philosophical issues in quantum theory).\n In some cases, one just needs to point out how an approach fits into\nthe overall picture suggested by decoherence, other approaches are in\nfact less able to exploit the results of decoherence. (The term\n‘approach’ here is more appropriate than the term\n‘interpretation’, because several of these are in fact\nmodifications of or additions to the theory.) We\nshall thus discuss in turn a number of approaches and how they relate\nto decoherence. These will be: the three most widespread approaches in\nthe philosophy of physics (Everett, Bohm and GRW), followed by the\nmore ‘orthodox’ approaches of von Neumann, Heisenberg and\nBohr, and a few others. \nWe shall start with the Everett theory (or many-worlds interpretation)\nin some of its main variants. This is in fact most closely related to\ndecoherence, since the latter can be used to naturally identify stable\n(if branching) structures within the universal wave function that can\ninstantiate the multiplicity of worlds or measurement records or\nconscious experiences characteristic of Everettian views. Another\napproach that arguably makes crucial use of decoherence is pilot-wave\ntheory (or de Broglie–Bohm theory, or Bohmian mechanics), where\nparticle positions (or other suitable ‘beables’) are\nguided in their temporal evolution by the universal wave function. The\nbranching structure of the latter will clearly have an effect on the\ncharacter of the evolution of the variables it guides. Instead,\nspontaneous collapse theories intuitively have less to do with\ndecoherence because they seek to suppress unwanted superpositions.\nStill, they are also arguably able to make use of decoherence,\nperhaps with some qualifications. \nMore traditional approaches to quantum mechanics that somehow\nprivilege the notion of measurement or observation also may have\nless-than-obvious connections with decoherence and in fact fit less\nwell with it, but we shall look at von Neumann’s,\nHeisenberg’s and Bohr’s views. Finally, we shall briefly\nmention other approaches and remark on their various relations to\ndecoherence. These will be Nelson’s stochastic mechanics, modal\ninterpretations, and QBism.  \nThe Everett theory (see the entries on\n Everett’s relative-state interpretation\n and on the\n many-worlds interpretation)\n was originally developed in 1957, before the theory of decoherence\n(Everett 1957). As we shall see, in recent years decoherence has\nbecome a defining notion of the theory, but it arguably fits rather\nwell also with Everett’s original formulation. \nThe central technical notion in Everett’s own formulation of the\ntheory is a relative state: e.g. the electron is in a state of\nspin up relative to the corresponding read-out state of the apparatus\nand in a state of spin down relative to the other read-out state. But Everett is interested in the emergence of\nstable structures in the universal wavefunction in terms of\nrelative states. His paradigm example is that of a hydrogen atom: put\na proton and an electron in a box, both spread out over the entire\nvolume. After a while, the proton and electron will have relaxed. The\nposition of the proton will still be spread out over the entire box,\nbut relative to each position state of the proton, the electron wil\nnow be in the usual ground state of the hydrogen atom. According to\nEverett, this is what we mean by a stable atom forming. Everett\nthinks of classical systems (a cannonball!) along the same lines, and\nuses these arguments as justifying the assumption that classical\nsystems exist, in particular ones that are complex enough to store\n(and perhaps act upon) records of measurement-like interactions they\nhave had with their environments. Everett’s aim is to recover\nthe usual predictions of quantum mechanics for the memory registers of\nsuch\n‘servomechanisms’.[15][16] \nIt should be clear that the theory of decoherence is an ideal\ntechnical tool if (like Everett) one wishes to identify stable\nstructures within the universal wave function. And, indeed, some of\nthe main workers in the field such as Zeh (2000) and (perhaps more\nguardedly) Zurek (1998) and Gell-Mann and Hartle (e.g. 1990) suggest\nthat decoherence is most naturally understood in terms of Everett-like\n interpretations.[17]\n This role of decoherence has been emphasised most prominently by\nSaunders (e.g. 1993) and by Wallace (e.g. 2003), and is in fact\nresponsible for the extraordinary renaissance of the Everett theory\nwithin the philosophy of physics since the\n mid-1990s.[18] \nUntil then, Everett was thought to be suffering from a problem of the\n‘preferred\n basis’:[19]\n it was thought that without putting in by hand what should count as\n‘worlds’, there were too many possible ways of defining\nsuch worlds, or too many ways of defining relative states. But looking\nfor potentially relevant structures that are already present in the\nwave function allows one to identify worlds (or other relevant\nstructures) without having to postulate the existence of some\npreferred states (whether or not they form an orthonormal basis).  \nA justification for this identification can be variously\ngiven by suggesting that a ‘world’ should be a\ntemporally extended structure and thus reidentification over\ntime will be a necessary condition for defining worlds; or similarly\nby suggesting that in order for observers to have evolved\nthere must be stable records of past events (Saunders 1993,\nand the unpublished Gell-Mann and Hartle 1994 – see the\n Other Internet Resources\n section below); or that observers must be able to access robust\nstates, preferably through the existence of redundant information\nin the environment (Zurek’s ‘existential\ninterpretation’,\n 1998).[20]\n But the most comprehensive justification of the use of decoherence in\nterms of how Everett can be understood using structures in the\nuniversal wave function has been given by Wallace, starting with his\n(2003) and given its final form in his book (2012a). Wallace places\nhis discussion in the wider context of an approach to emergence based\non Dennett’s notion of ‘real patterns’. Higher-level\ntheories are functionally instantiated by lower-level (more\nfundamental) ones if there exist relatively simple mappings from\nsolutions of the lower-level theory over a certain domain to solutions\nof the higher-level theory. Higher-level structures are thus reduced\nto patterns at the more fundamental level, which are real in the\n(quasi-)Dennettian sense that they are objectively useful in terms of\nboth predicting and explaining phenomena at the higher level. At the\nsame time they are emergent, because they could be multiply realised,\nand because finding the relevant mapping may be possible only in a\ntop-down perspective. Everettian worlds are such real patterns,\nbecause decoherence ensures their dynamical independence of each\nother.  \nAlternatively to some global notion of a world, one can look at the\ncomponents of the (mixed) state of a (local) system, either from the\npoint of view that the different components defined by decoherence\nwill separately affect (different components of the state of) another\nsystem, or from the point of view that they will separately underlie\nthe conscious experience (if any) of the system. The former sits well\nwith the relational interpretation of Everett as put forward in the\n1990s by Saunders (e.g. 1993), possibly with Zurek’s (1998)\nviews, and arguably with Everett’s (1957) original notion of\nrelative\n state.[21]\n The latter leads directly to the idea of ‘many-minds’ in\nthe sense used by Zeh (2000; also 2003a, p. 24). As Zeh puts it, \nthe ‘psycho-physical parallelism’ invoked by von Neumann \n(cf. below Section 3.4.1) is to be understood as the\nrequirement of supervenience of the mental on the physical: only one\nmental state is experienced, so there should be only one corresponding\ncomponent in the physical state. In a decohering no-collapse universe\none can instead introduce a new psycho-physical parallelism,\nin which individual minds supervene on each non-interfering component\nin the physical state. (This is different from the many-minds\ninterpretation of Albert and Loewer (1988), where the mental\ndoes not supervene on the physical, because individual minds\nhave trans-temporal identity of their\nown.[22]) Zeh\nindeed suggests that, given decoherence, this is the most natural\ninterpretation of quantum\n mechanics.[23] \n‘Hidden variables’ approaches seek to explain quantum\nphenomena as equilibrium statistical effects arising from a\ndeeper-level theory, rather strongly in analogy with attempts at\nunderstanding thermodynamics in terms of statistical mechanics (see\nthe entry on\n philosophy of statistical mechanics).\n Of these, the most developed are the so-called pilot-wave theories,\nin particular the theory by de Broglie and Bohm (see also the entry on\n Bohmian mechanics).\n Pilot-wave theories are no-collapse formulations of quantum mechanics\nthat assign to the wave function the role of determining the evolution\nof (‘piloting’, ‘guiding’) the variables\ncharacterising the system, say particle configurations, as in de\nBroglie’s (1928) and Bohm’s (1952) theory, or fermion\nnumber density, as in Bell’s (1987, Chapter 19)\n‘beable’ quantum field theory, or again field\nconfigurations, as in various proposals for pilot-wave quantum field\ntheories (for a recent survey, see Struyve 2011). \nDe Broglie’s idea was to modify classical Hamiltonian mechanics\nin such a way as to make it analogous to classical wave optics, by\nsubstituting for Hamilton and Jacobi’s action function the phase\n\\(S\\) of a physical wave. Such a ‘wave mechanics’ of\ncourse yields non-classical motions, but in order to understand how de\nBroglie’s dynamics relates to typical quantum phenomena, we must\ninclude Bohm’s (1952, Part II) analysis of the appearance of\ncollapse. In the case of measurements, Bohm argued that the wave\nfunction evolves into a superposition of components that are and\nremain separated in the total configuration space of measured system\nand apparatus, so that the total configuration is\n‘trapped’ inside a single component of the wave\nfunction, which will guide its further evolution, as if the wave had\ncollapsed (‘effective’ wave function). This analysis\nallows one to recover the apparent collapse upon measurement (and the\nquantum probabilities are further recovered via statistical\nconsiderations).  \nIt is natural to extend this analysis from the case of measurements\ninduced by an apparatus to that of ‘spontaneous\nmeasurements’ as performed by the environment in the theory of\ndecoherence, thus applying the same strategy to recover both quantum\nand classical phenomena. The resulting picture is one in which de\nBroglie–Bohm theory, in cases of decoherence, describes the\nmotion of particles that are trapped inside one of the extremely well\nlocalised components selected by the decoherence interaction. Thus, de\nBroglie–Bohm trajectories will partake of the classical motions\non the level defined by decoherence (the width of the components).\nThis use of decoherence would arguably resolve the puzzles discussed,\ne.g., by Holland (1996) with regard to the possibility of a\n‘classical limit’ of de Broglie’s theory. One\nbaffling problem, for instance, is that trajectories with different\ninitial conditions cannot cross in de Broglie–Bohm theory,\nbecause the wave guides the particles by way of a first-order\nequation, while, as is well known, Newton’s equations are\nsecond-order and possible trajectories in Newton’s theory do\ncross. Now, however, the non-interfering components produced by\ndecoherence can indeed cross, and so will the trajectories of\nparticles trapped inside them. \nIf the main instances of decoherence are indeed coextensive with\ninstances of separation in configuration, de Broglie–Bohm theory\ncan thus use the results of decoherence relating to the\nformation of classical structures, while providing an interpretation\nof quantum mechanics that explains why these structures are indeed\nobservationally\n relevant.[24]\n This picture is natural, but not self-evident. De Broglie–Bohm\ntheory and decoherence contemplate two a priori distinct\nmechanisms connected to apparent collapse: respectively, separation of\ncomponents in configuration space and suppression of interference.\nWhile the former obviously implies the latter, it is equally obvious\nthat decoherence need not imply separation in configuration space. One\ncan expect, however, that decoherence interactions of the form of\napproximate position measurements will. \nA discussion of the role of decoherence in pilot-wave theory in the\nform suggested above has been given by Rosaler (2015, 2016). An\ninformal discussion is given in Bohm and Hiley (1993, Chapter 8),\npartial results are given by Appleby\n (1999),[25]\n and some simulations have been realised by Sanz and co-workers (e.g.\nSanz and Borondo\n 2009).[26]\n Relevant results have also been derived by Toroš, Donadi and\nBassi (2016) who show quantitative correspondence with a spontaneous\ncollapse model (see also Romano 2016). A rather different approach is\ninstead suggested by Allori (2001; see also Allori and Zanghì\n 2009).[27] \nWhile, as argued above, it appears plausible that decoherence might be\ninstrumental in recovering the classicality of pilot-wave trajectories\nin the case of the non-relativistic particle theory, it is less clear\nwhether this strategy might work equally well in the case of field\ntheory. Doubts to this effect have been raised, e.g., by Saunders\n(1999) and by Wallace (2008, 2012b). Essentially, these authors doubt\nwhether the configuration-space variables, or some coarse-grainings\nthereof, are, indeed, decohering\n variables.[28] \nSpontaneous collapse theories\n seek to modify the Schrödinger equation, so that superpositions\nof different ‘everyday’ states do not arise or are very\nunstable. The best known such theory is the so-called GRW theory\n(Ghirardi Rimini and Weber 1986), in which a material particle\nspontaneously undergoes localisation in the sense that at\nrandom times it experiences a collapse of the form used to describe\napproximate position\n measurements.[29]\n In the original model, the collapse occurs independently for each\nparticle (a large number of particles thus ‘triggering’\ncollapse much more frequently); in later models the frequency for each\nparticle is weighted by its mass, and the overall frequency for\ncollapse is thus tied to mass\n density.[30] \nCan decoherence be put to use in GRW? Such approaches may have\nintuitively little to do with decoherence since they seek to suppress\nprecisely those superpositions that are created by decoherence.\nNevertheless their relation to decoherence is interesting (and, as we\nshall see in the next section, interestingly different from the role\nthat decoherence at least implicitly plays in von Neumann’s\ncollapse postulate). \nQualitatively at least, since spontaneous collapse produces\nlocalisation, the effect appears formally similar as in some of the\nmodels of decoherence. But we have ‘true’ collapse instead\nof suppression of interference, and spontaneous collapse occurs\nwithout there being any interaction between the system and\nanything else. In cases in which the decoherence interaction indeed\nalso takes the form of approximate position measurements, the relation\nbetweeen spontaneous collapse and decoherence presumably boils down to\na quantitative comparison. If collapse happens faster than\ndecoherence, then the superposition of components relevant to\ndecoherence will not have time to arise, and insofar as the collapse\ntheory is successful in recovering classical phenomena, decoherence\nplays no role in this recovery. Instead, if decoherence takes place\nfaster than collapse, then the collapse mechanism can find\n‘ready-made’ structures onto which to truly collapse the\nwave function.  \nNot much explicit work has been done on modelling decoherence in the\nsetting of spontaneous collapse theories, however. Simple comparison\nof the relevant rates in models of decoherence and in spontaneous\ncollapse theories suggests that decoherence is generally faster (Tegmark\n1993, esp. Table 2). The more detailed model by Toroš, Donadi and\nBassi (2016, esp. Section V) indicates that the effect of the collapse\nis amplified through the presence of the environment, i.e. the\ncollapse rate is increased. The situation may be more complex when the\ndecoherence interaction does not approximately privilege position\n(e.g. when instead it selects for currents in a SQUID), because\ncollapse and decoherence might actually ‘pull’ in\ndifferent\n directions.[31] \nA further aspect of the relation between decoherence and spontaneous\ncollapse theories relates to the experimental testability of\nspontaneous collapse theories. Indeed, if we assume that collapse is a\nreal physical process, decoherence will make it extremely difficult in\npractice to detect empirically when and where exactly spontaneous\ncollapse takes place: on the one hand, decoherence makes it look as if\ncollapse has taken place already, while on the other it makes it\ndifficult to perform interference experiments to check whether\ncollapse has not yet taken place. (See the nice discussion of this\nissue in Chapter 5 of Albert (1992)). \nEven worse, what might be interpreted as evidence for collapse could\nbe reinterpreted as ‘mere’ suppression of interference\nwithin an Everett or pilot-wave approach, and only those cases in\nwhich the collapse theory predicts collapse but the system is shielded\nfrom decoherence (or perhaps in which the two pull in different\ndirections) could be used to test collapse theories\nexperimentally. \nOne particularly bad scenario for experimental testability is related\nto the speculation (in the context of the ‘mass density’\nversion) that the cause of spontaneous collapse may be connected with\ngravitation. Tegmark (1993, Table 2) quotes some admittedly uncertain\nestimates for the suppression of interference due to a putative\nquantum gravity, but they are quantitatively very close to the rate of\ndestruction of interference due to the GRW collapse (at least outside\nof the microscopic domain). Similar conclusions are arrived at in the\nmore detailed work by Kay (1998). If there is indeed such a\nquantitative similarity between these possible effects, then it would\nbecome extremely difficult to distinguish between the two. In the\npresence of gravitation, any positive effect could be interpreted as\nsupport for either collapse or decoherence. And in those cases in\nwhich the system is effectively shielded from decoherence (say, if the\nexperiment is performed in free fall), then if the collapse mechanism\nis indeed triggered by gravitational effects, no collapse should be\nexpected\n either.[32] \nIn the final Chapter VI of his book (von Neumann 1932), von Neumann\nprovided a systematic discussion of quantum mechanics with collapse\nupon measurement (described by what he calls an intervention of type\n\\(\\mathbf{1})\\), as distinct from the Schrödinger equation\n(intervention of type \\(\\mathbf{2})\\), and traditionally associated with a\nrole for conscious observation. (The two types of interventions are\nintroduced already in Section V.1, but von Neumann postpones their\nconceptual discussion to the final chapter.) \nIn actual fact, von Neumann starts his discussion by pointing out that\nmeasurements are different from other physical processes both\nphenomenologically and by presupposing conscious observation. But he\ninsists on preserving what he calls ‘psycho-physical\nparallelism’, requiring that the process of observation be\ndescribable also in purely physical terms. He thus requires that a\nboundary be drawn between the ‘observed’ and the\n‘observer’, but also crucially that this boundary be\nmovable arbitrarily far towards the observer end. (Note that\nvon Neumann stops short of at least explicitly attributing to\nconsciousness a causal role in collapsing the quantum state.) \nVon Neumann thus needs to show that the final predictions for what we\nconsciously observe are insensitive to how far along such a\n‘measurement chain’ one chooses to continue applying\nintervention \\(\\mathbf{2}\\), thus ensuring that every step in the process of\nobservation can be described purely in physical terms. In von\nNeumann’s example of a measurement of temperature, we need not\napply intervention \\(\\mathbf{1}\\) to the system itself, but may apply it to\nthe thermometer, or to the retina in the eye, or to the optic nerve,\nor anywhere else within the physical realm between the system and the\n‘abstract ego’ of the observer. By the same token,\nhowever, we can (much more practically!) apply it also directly to the\nmeasured system.  \nAs a preliminary, von Neumann discusses the relation between states of\nsystems and subsystems, in particular the notion of partial trace and\nthe biorthogonal decomposition theorem (i.e. the theorem stating that\nan entangled quantum state can always be written in terms of perfect\ncorrelations between two special bases for the subsystems). He also\nshows (as mentioned above) that the usual statistics of measurements\ncannot be recovered by assuming that the ‘observer’ is\ninitially in a mixed state. He then proves that it is always possible\nto define an interaction Hamiltonian that will correlate perfectly the\neigenstates of any given observable of an ‘observed’\nsystem with the eigenstates of some other suitable observable of an\n‘observer’, leaving as an exercise for the reader to show\nthat predictions are independent of where one places the boundary\nbetween the two. \nWhat the reader is supposed to do is to imagine a series of such\ninteractions, between the system and the thermometer, between the\nthermometer and the light, between the light and the retina, etc., and\nrely on the absence of interference at each step to argue that,\neven if we describe a number of systems using intervention \\(\\mathbf{2}\\),\nthey behave for the purpose of the application of intervention\n\\(\\mathbf{1}\\) as if they had collapsed already. In this sense, even\nthough he is quite clearly not thinking in terms of mechanisms for\nsuppressing interference, he is relying on decoherence. A fuller\ntreatment (e.g. a detailed model of how the thermometer interacts with\nlight, and some of the light is then sampled by the eye) would\nresemble more closely an analysis in terms of environmental\ndecoherence.  \nSimilar considerations may be made about Heisenberg’s views on\nquantum mechanics, even though Heisenberg’s conceptual framework\nis arguably rather different from von Neumann’s. \nFor Heisenberg, the application of quantum mechanics requires a\n‘cut’ between the system to be described quantum\nmechanically, and what is to be considered external to the system and\nis to be treated classically. Indeed, if one were to apply quantum\nmechanics to the entire universe, one would have a perfectly closed\nsystem in which nothing would ever happen. But Heisenberg places\nspecial emphasis on the idea that any special system must be\ndescribable using quantum mechanics (indeed, that such a system is in\nprinciple always able to display interference effects if placed under\nthe appropriate\n conditions[33]).\n Self-consistency of the theory then requires the arbitrary\nmovability of the cut away from the system. (The most detailed\npresentation of these ideas is in Heisenberg’s draft reply to\nthe\n Einstein–Podolsky–Rosen argument\n – see Crull and Bacciagaluppi (2011) in the\n Other Internet Resources.) \nIf one thinks about some of the examples that Heisenberg considers to\nbe measurements, it is even clearer than in von Neumann’s case\nthat the movability of the Heisenberg cut in fact requires\ndecoherence. In particular, his discussion of \\(\\alpha\\)-particle tracks\ninvolves successive measurements whenever the \\(\\alpha\\)-particle\nionises an atom in a cloud chamber. If we require that the Heisenberg\ncut be movable to the level of the entire cloud chamber, we shift\ndirectly to a Mott-type analysis of the \\(\\alpha\\)-particle tracks. \nOne further aspect that is characteristic for Heisenberg and that\nprima facie does not fit with the theory of decoherence, is\nthat Heisenberg does not take quantum states as fundamental.\nFor him, Schrödinger’s notion of a ‘state’ was\njust a mathematical artifact that is convenient for calculating\ntransition probabilities between values of (measured) observables.\nThis can also be seen as underpinning the movability of the cut: there\nis no matter of fact about when the collapse takes place, and all that\nmatters physically are the transition probabilities between values of\nobservables. This view is still compatible with decoherence, however,\nas long as one sees the role of the quantum state there as again just\na convenient tool for calculating transition probabilities (say, in a\ndecoherent histories\n framework).[34] \nBohr shared with von Neumann and with Heisenberg the idea that that\nquantum mechanics is in principle applicable to any physical system\n(as shown e.g. by his willingness in the course of his debates with\nEinstein to apply the uncertainty relations to parts of the\nexperimental apparatus when not used as an apparatus), while\ndenying that it is meaningful to apply it to the entire universe. What\nis central to Bohr’s views, however, is not so much the\nmovability of the cut within a given experimental arrangement, but the\nfact that different experimental arrangements will generally select\ncomplementary aspects of the description of a physical system,\ncorresponding to different equally necessary classical pictures that\nhowever cannot be combined. In this sense, for Bohr classical concepts\nare conceptually prior to quantum mechanics. In a terminology\nreminiscent of Kant, the quantum state is not an anschaulich\n(‘intuitive’) representation of a quantum object, but only\na symbolic representation, a shorthand for the quantum\nphenomena that are constituted by applying the various complementary\nclassical pictures. (See also the entry on the\n Copenhagen interpretation.) \nThus, if we understand the theory of decoherence as pointing to how\nclassical concepts might in fact emerge from quantum mechanics, we see\na tension with Bohr’s basic position. According to decoherence,\neven though classical concepts are autonomous in the sense of being\nemergent, they are not fundamentally prior to quantum\nmechanics. In another sense, however, decoherence does support\nBohr’s point of view, because we can see decoherence (in\nparticular environmental decoherence) as suggesting that there are no\nquantum phenomena without classical records: it is the\nsuppression of interference that creates the conditions for restoring\nthe objectivity that gets lost through what Bohr sees as the loss of\nindependent reality attaching to both the system and the measuring\n apparatus.[35] \nBoth of these aspects can be seen in the reception of Everett’s\nideas by Bohr and his circle. While Everett saw his own theory as\ndirectly opposed to von Neumann’s approach, he believed that he\ncould provide a justification for Bohr’s idea of\ncomplementarity. Bohr, however, rejected the attempt to apply the\nnotion of quantum state to a description of the whole universe. (The\nrejection of Everett’s ideas in Copenhagen in fact rather\ntragically contributed to Everett leaving physics in favour of\nmilitary operations\n research.[36])\n  \nNelson’s (1966, 1985) stochastic mechanics is a proposal to\nrecover the wave function and the Schrödinger equation as\neffective elements in the description of a fundamental diffusion\nprocess in configuration space. Insofar as the proposal is\n successful,[37]\n it shares many features with de Broglie–Bohm theory. Indeed,\nthe current velocity for the particles in Nelson’s theory turns\nout to be equal to the de Broglie–Bohm velocity, and the\nparticle distribution in Nelson’s theory is equal to that in de\nBroglie–Bohm theory (in equilibrium). \nIt follows that many results from pilot-wave theories can be imported\ninto Nelson’s stochastic mechanics, including those based on\ndecoherence. In particular, the strategies used in pilot-wave\ntheories to recover the appearance of collapse and the emergence of a\nclassical regime can be applied also to the case of stochastic\nmechanics, even though so far very little has been done along these\nlines. Doing so will arguably also resolve some conceptual puzzles\nspecific to Nelson’s theory, such as the problem of two-time\ncorrelations raised in Nelson\n (2006).[38] \nThe first ‘modal interpretation’ of quantum mechanics was\nproposed by Van Fraassen (1973, 1991), and was strictly an\ninterpretation of the theory (while other later versions came more to\nresemble pilot-wave theories; see the entry on\n modal interpretations).\n Van Fraassen’s basic intuition was that the quantum state of a\nsystem should be understood as describing a collection of\npossibilities, represented by components in the (mixed) quantum state.\nHis proposal considers only decompositions at single instants, and is\nagnostic about re-identification over time. Thus, it can directly\nexploit only the fact that decoherence produces descriptions in terms\nof classical-like states, which will count as possibilities in Van\nFraassen’s sense. This ensures ‘empirical adequacy’\nof the quantum description (crucial in Van Fraassen’s\n constructive empiricism).\n The dynamical aspects of decoherence can be exploited indirectly, in\nthat single-time components will exhibit records of the past,\nwhich ensure adequacy with respect to observations, but about whose\nveridicity Van Fraassen remains agnostic. \nA different strand of modal interpretations is loosely associated with\nthe (distinct) views of Kochen (1985), Healey (1989) and Dieks (see\ne.g. Dieks and Vermaas 1998). We focus on the last of these to fix\nideas. Van Fraassen’s possible decompositions are restricted to\none singled out by a mathematical criterion (related to the\nbiorthogonal decomposition theorem mentioned above in\n Section 3.4.1),\n and a dynamical picture is explicitly sought (and was later\ndeveloped). In the case of an ideal (non-approximate) quantum\nmeasurement, this special decomposition coincides with that defined by\nthe eigenstates of the measured observable and the corresponding\npointer states, and the interpretation thus appears to solve the\nmeasurement problem (for this case at least). In\nDieks’s original intentions, the approach was meant to provide\nan attractive interpretation of quantum mechanics also in the case of\ndecoherence interactions, since at least in simple models of\ndecoherence the same kind of decomposition singles out more or less\nalso those states between which interference is suppressed (with a\nproviso about very degenerate states). \nInterestingly, this approach fails when applied to other models of\ndecoherence, e.g., that in Joos and Zeh (1985, Section III.2). Indeed,\nit appears that in more general models of decoherence the components\nsingled out by this version of the modal interpretation are given by\ndelocalised states, and are unrelated to the localised\ncomponents naturally privileged by decoherence (Donald 1998;\nBacciagaluppi 2000). Thus the relation with decoherence has been the\ntouchstone for these versions of the modal interpretation. Note that\nVan Fraassen’s original interpretation is untouched by this\nproblem, and so are possibly some more recent modal or modal-like\ninterpretations by Spekkens and Sipe (2001), Bene and Dieks (2002),\nand Berkovitz and Hemmo (2006). \nThe general idea of modal interpretations, more or less in the spirit\nof Van Fraassen, can be applied more widely. For one thing, it is\ncognate to some of the views expressed in the decoherent histories\nliterature. Decoherent histories could be seen as alternative possible\nhistories of the world, one of which is in fact actualised. A\ndiscussion in these terms has been outlined by Hemmo (1996). Such\nviews are also possibly quite close to Everett’s own views, who\n(maybe surprisingly for the modern reader) was not a realist but an\nempiricist. A discussion of Everett with parallels to Van Fraassen is\ngiven by Barrett (2015). One final view that has some similarities\nwith Van Fraassen’s and should be equally able to exploit the\nresults of decoherence is Rovelli’s\n relational quantum mechanics\n (see also Van Fraassen 2010). \nQBism (originally short for ‘quantum Bayesianism’) is a\nview of quantum mechanics developed by Chris Fuchs and co-workers,\nwhich has made current the idea that subjective probabilities à\nla de Finetti can be used also in quantum mechanics (see the entry on\n quantum Bayesian and pragmatist views of quantum theory).\n The position is more radical than this, however, in that it does not\nonly claim that the quantum probabilities as defined by the quantum\nstate should be interpreted subjectively, but that the quantum state\nitself is merely an expression of an agent’s degrees of\n belief.[39] \nThe role of decoherence in QBism is rather downplayed. E.g. Fuchs and\nSchack (2012, Section 7) see it in light of the reflection principle\n(concerning an agent’s beliefs about their future beliefs).\nSpecifically, in the context of a von Neumann measurement chain, an\nagent can use the state of the system as decohered by some later\nelements of the chain as an expression of their beliefs about what\ntheir beliefs will be after the previous elements of the measurement\nchain have been completed. (And of course, the results of decoherence\ncan be taken into account if an agent is considering making\nmeasurements on a system that is in interaction with some\nenvironment.) \nWe have seen in the last section that not all approaches to quantum\nmechanics can make full use of decoherence. In those approaches that\ncan, however, decoherence is instrumental in yielding a wealth of\nstructures that emerge from the unitary Schrödinger (or\nHeisenberg) dynamics. How far can this programme of\ndecoherence (Zeh 2003a, p. 9) be successfully developed?  \nWhat seems very clear is that decoherence is crucial for the emergence\nof much of the macroscopic world around us, from the motions in the\nsolar system (cf. the discussion of the motion of Saturn’s moon\nHyperion – for an assessment of which see Schlosshauer (2008))\nand down to the working of enzymes (which relies on their molecular\nshapes). The detailed picture of the world that emerges from\ndecoherence, however, is full of subtleties. \nFor one thing, while the more ‘macroscopic’ a system, the\nmore pervasive the effects of decoherence and the more complex the\nstructures that emerge through it, this is only a rule of thumb. Not\nall molecules are chiral (bound ammonia groups tend to be\nin superpositions for instance), and there is no clear-cut criterion\nfor when a system should count as macroscopic. Indeed, even apart from\nexamples like superconducting systems, there might be surprising cases\nin which not all interference effects have been suppressed by\ndecoherence even at the macroscopic level. A famous proposal by\nHameroff and Penrose (1996) links the phenomenon of consciousness with\nthe possibility of quantum superpositions within microtubules (and\ntheir subsequent active suppression via collapse); other authors\ninterpret the mathematically quantum-like effects described within\n‘quantum cognition’ as actual quantum effects (for both, see the\nentry on\n quantum approaches to consciousness).\n At present, most macroscopic quantum effects remain speculative at\nbest, but plausible cases for the continuing relevance of quantum\nsuperpositions at the macroscopic level can be found in quantum\nbiology, notably the studies of possible quantum effects in the\nnavigational system of migrating birds (Cai, Guerreschi and Briegel\n2010). \nCloser to home, while the classical world is recognised as having been\nall the time a dynamical pattern emerging from quantum mechanics, it\nturns out to be less classical than we might have expected. One\ninteresting example is the description of classically chaotic systems.\nA straightforward application of the techniques allowing one to derive\nNewtonian trajectories at the level of components has been employed by\nZurek and Paz (1994) to derive chaotic trajectories in\nquantum mechanics. The problem with the quantum description of chaotic\nbehaviour is that prima facie it should be impossible. Chaos\nis characterised roughly as extreme sensitivity in the behaviour of a\nsystem on its initial conditions, in the sense that the distance\nbetween the trajectories arising from different initial conditions\nincreases exponentially in time (see the entry on\n chaos).\n Since the Schrödinger evolution is unitary, it\npreserves all scalar products and all distances between quantum state\nvectors. Thus, it would seem, close initial conditions lead to\ntrajectories that are uniformly close throughout all of time, and no\nchaotic behaviour is possible (‘problem of quantum\nchaos’). The crucial point that enables Zurek and Paz’s\nanalysis is that the relevant trajectories defined by decoherence are\nat the level of components of the state of the system.\nUnitarity is preserved because the vectors in the environment, to\nwhich these different components are coupled, are and remain\northogonal: how the components themselves more specifically evolve is\nimmaterial. Explicit modelling yields a picture of quantum chaos in\nwhich different trajectories branch (a feature absent from classical\nchaos, which is deterministic) and then indeed diverge exponentially.\n(As with the crossing of trajectories in de Broglie–Bohm theory\nin\n Section 3.2,\n one has behaviour at the level of components that is qualitatively\ndifferent from the behaviour derived for wave functions of an isolated\nsystem.) The qualitative picture is the same as we mentioned above in\n Section 1.3,\n of classical trajectories that are kicked slightly off course\n(trajectories with slight kinks). In the case of classically chaotic\nsystems, however, this has a dramatic effect. This means that systems\nlike the weather turn out to be ‘branching’ all the time\ndue to decoherence interactions, so that what we usually think of as\nclassical unpredictability is in fact quantum indeterminism! (For an\nexcellent discussion, see Wallace 2012a, Chapters 3 and 10.)  \nAnd as we have also mentioned, quantum phenomena themselves\nare a feature of the world that emerges through decoherence (Zeh\n2003a, p. 33; see also Bacciagaluppi 2002, Section 6.2): not only the\nstability of the outcomes of laboratory measurements, and thus\n‘quantum phenomena’ in the specific sense of Bohr, but\nalso quantum jumps or the appearance of \\(\\alpha\\)-particle trajectories\nare a direct consequence of decoherence. The classical world yielded\nby decoherence is thus one (or one of many!) punctuated by quantum\nphenomena. \nFurther along these lines, Zeh (2003b) argues that decoherence can\nexplain the appearance of particle detections within quantum\nfield theory (see the entry on\n quantum field theory).\n Therefore, only fields need to be included in the fundamental\nconcepts, and ‘particles’ are a derived concept, unlike\nwhat might be suggested by the customary introduction of fields\nthrough a process of ‘second quantisation’. Another\napplication to quantum field theory is the suggestion that the\njustification for the (strict) superselection rule for charge in\nquantum field theory can also be phrased in terms of decoherence. As\npointed out by Giulini, Kiefer and Zeh (1995; see also Giulini et\nal. 1996, Section 6.4), an electric charge is surrounded by a\nCoulomb field (which electrostatically is infinitely extended; the\nargument can also be carried out using the retarded field, though).\nStates of different electric charge of a particle are thus coupled to\ndifferent, presumably orthogonal, states of its electric field. One\ncan consider the far-field as an effectively uncontrollable\nenvironment that decoheres the particle (and the near-field), so that\nsuperpositions of different charges are indeed always\n suppressed.[40] \nAnother claim about the significance of decoherence relates to time\nasymmetry (see e.g. the entries on\n time asymmetry in thermodynamics\n and\n philosophy of statistical mechanics).\n Insofar as apparent collapse (branching) is indeed a time-directed\nprocess, decoherence will have direct relevance to the emergence of\nthis ‘quantum mechanical arrow of time’. This is not a\nstraightforward issue, however, and questions familiar from the\nfoundations of statistical mechanics arise also in the context of\ndecoherence. Specific issues include the fact that models of\nenvironmental decoherence tend to assume very special (unentangled)\ninitial conditions, and the fact that the definition of the\ndecoherence functional in the histories approach is itself\nasymmetrical (although symmetric versions also exist). For a spectrum\nof discussions, see Hartle (1998, and references therein), Zeh (2001,\nChapter 4), and Bacciagaluppi (2002, Section 6.1; 2007). Whether\ndecoherence is connected to the other familiar arrows of time is a\nmore specific question. Indeed, should decoherence allow us to recover\nthe time-symmetric classical description of, say, particle\ntrajectories in a gas, or does it allow us to derive time-asymmetric\nthermodynamic behaviour directly, by-passing classical attempts at\nunderstanding it? Note that insofar as classical systems such as gases\nare believed to be chaotic, the origin of the probabilities in\nclassical statistical mechanics will arguably be quantum. For various\ndiscussions, see e.g. Zurek and Paz (1994), Hemmo and Shenker (2001),\nand Wallace (2012a, Chapter 9; and 2001, 2013a, 2013b in the\n Other Internet Resources).\n  \nFinally, it has been suggested that decoherence should be a useful\ningredient in a theory of quantum gravity (see the entry on\n quantum gravity),\n as discussed e.g. by Kiefer (1994). First, because a suitable\ngeneralisation of decoherence theory to a full theory of quantum\ngravity should yield suppression of interference between different\nclassical spacetimes (see e.g. also Giulini et al. 1996,\nSection 4.2). Second, it is speculated that decoherence might solve\nthe so-called problem of time, which arises as a prominent\npuzzle in (the ‘canonical’ approach to) quantum gravity.\nThis is the problem that the candidate fundamental equation (in this\napproach) – the Wheeler–DeWitt equation – is an\nanalogue of a time-independent Schrödinger equation, and\ndoes not contain time at all, so that time needs somehow to emerge. In\nthe context of decoherence theory, one can for instance construct toy\nmodels in which the analogue of the Wheeler–DeWitt wave function\ndecomposes into non-interfering components (for a suitable sub-system)\neach satisfying a time-dependent Schrödinger equation,\nso that decoherence appears in fact as the source of time. An\naccessible introduction to and philosophical discussion of such models\nis given by Ridderbos (1999), with references to the original papers.\nSee also the more recent model by Halliwell and Thorwart\n (2002).[41]","contact.mail":"g.bacciagaluppi@uu.nl","contact.domain":"uu.nl"}]
