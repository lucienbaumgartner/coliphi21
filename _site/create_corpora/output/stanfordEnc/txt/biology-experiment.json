[{"date.published":"2012-02-14","date.changed":"2018-05-17","url":"https://plato.stanford.edu/entries/biology-experiment/","author1":"Marcel Weber","entry":"biology-experiment","body.text":"\n\n\nLike the philosophy of science in general, the philosophy of biology\nhas traditionally been mostly concerned with theories. For a variety\nof reasons, the theory that has by far attracted the most interest is\nevolutionary theory. There was a period in the philosophy of biology\nwhen this field was almost identical with the philosophy of\nevolutionary theory, especially if debates on the nature of species\nare included in this area. From the 1960s almost until the 1990s,\nbasically the only non-evolutionary topic discussed was the issue of\nthe reduction (or non-reduction) of classical genetics to molecular\ngenetics. In the 1990s, practitioners of the field finally started to\nmove into other areas of biology such as cell biology, molecular\nbiology, immunology, neuroscience, as well as addressing philosophical\nissues other than theory reduction (e.g., Darden 1991; Burian 1992,\n1997; Schaffner 1993; Bechtel and Richardson 1993; Rheinberger 1997).\nAs these are profoundly experimental disciplines, an increased\nattention to the experiment in biology was inevitable. Thus, what is\ngenerally known as the “New Experimentalism” in general\nphilosophy of science arose more or less independently in the\nphilosophy of biology, even though this fact has not been advertised\nmuch. Perhaps, the increased focus on experimentation was more due to\nthe influence of historians of biology and their practical turn than\ndue to the “New Experimentalists” in general philosophy of\nscience such as Hacking (1983), Franklin (1986) or Mayo (1996). At any\nrate, today there is a wealth of historical as well as philosophical\nscholarship that closely studies what goes on in biological\nlaboratories. Some of this literature relates to classical issues in\nphilosophy of science, but it has also explored new philosophical\nground.\n\n\nThere are issues in the philosophy of biological experimentation that\nalso arise in other experimental sciences, for example, issues that\nhave to do with causal inference, experimental testing, the\nreliability of data, the problem of experimental artifacts, and the\nrationality of scientific practice. Experimental biological knowledge\nis sufficiently different from other kinds of scientific knowledge to\nmerit a separate philosophical treatment of these issues. In addition,\nthere are issues that arise exclusively in biology, such as the role\nof model organisms. In what follows, both kinds of issues will be\nconsidered.\n\nCausal reasoning approaches try to reconstruct and sometimes justify\nthe rules that allow scientists to infer causal relationships from\ndata, including experimental data. One of the oldest such attempts is\ndue to John Stuart Mill (1996 [1843]), who presented a systematic\naccount of causal inference that consisted of five different so-called\n“methods”: The Method of Agreement, the Method of\nDifference, the Joint Method of Agreement and of Difference, the\nMethod of Residues, and the Method Concomitant Variation. While some\nof these “methods” pertain more to observation, the Method\nof Difference in particular is widely seen as encapsulating an\nimportant principle of scientific reasoning based on experiment. Mill\nhimself characterized it thus: “If an instance in which the\nphenomenon under investigation occurs, and an instance in which it\ndoes not occur, have every circumstance in common save one, that one\noccurring only in the former; the circumstance in which alone the two\ninstances differ, is the effect, or the cause, or an indispensable\npart of the cause, of the phenomenon” (Mill 1996 [1843], Ch. 8,\n§2). Thus, Mill’s method of difference asks us to look at two\nsituations: one in which the phenomenon under investigation occurs,\nand one in which it does not occur. If a factor can be identified that\nis the only other difference between the two situations, then this\nfactor must be causally relevant. \nAs Mill noted, the method of difference is particularly germane to\nexperimental inquiry because such a difference as is required by this\nmethod can often be produced by an experimental intervention. Indeed,\naccording to a position known as interventionism about causality there\nis a tight connection between the concept of cause and experimental\ninterventions (Woodward 2003). \nMill’s method of difference captures an important kind of reasoning\nthat is used frequently in biological experiments. Let’s suppose we\nwant to find out if a newly discovered compound is an antibiotic,\ni.e., inhibits the growth of certain bacteria. We start by dividing a\nbacterial cell culture into several aliquots (samples of same size\nderived from a homogeneous solution). Then, we add to one group of\naliquots the suspected antibiotic that is dissolved in phosphate\nbuffer (“treatment”). To the other group, we add only the\nphosphate buffer (“control”). Then we record bacterial\ngrowth in all the samples (e.g., by measuring the increase in optical\ndensity as the culture medium clouds up due to the bacteria). This\nexperimental setup makes sure that the treatment and control samples\ndiffer only in the presence or absence of the antibiotic, thus ruling\nout that any observed difference in growth between the treatment and\ncontrol aliquots is caused not by the suspected antibiotic but by the\nbuffer solution. Let us denote the antibiotic as “A” and\nthe growth inhibition as “W”. Biologists would thus infer\nfrom this experiment that A is an antibiotic if W is observed in the\nsamples containing A but not in the samples not containing A. \nMill construed this “method” in terms of a principle of\ninductive inference that can be justified pragmatically.\nHowever, it is interesting to note that the principle can also be\nviewed as instantiating a form of deductive inference. \nTo this end, of course, the Method of Difference must be strengthened\nwith additional premises. Here is one way how this can be done\n(adapted from Hofmann and Baumgartner 2011): \nSome of the terms used in this deduction require explication. Two test\nsituations are homogeneous on the condition that, if a factor\nis causally relevant and is present in test situation S1,\nthen it is also present in test situation S2, and vice\nversa. A confounder would be a causally relevant factor that\ndoes not belong to the set {A, W}. The presence of such a confounder\nis excluded by the assumption of causal homogeneity (1). In practical\nsituations, a confounder might be an unknown or uncontrolled (i.e.,\nnot measurable or not measured) factor that is present in only one of\nthe test situations. In our antibiotics example, this could be some\nchemical that was put into just one aliquot inadvertently or without\nknowledge of the experimenter. In fact, the risk of a confounder is\nprecisely why our biological experimenter will divide up the mother\nculture into aliquots just before putting in the substance to be\ntested. This makes it unlikely that one aliquot contains an\nuncontrolled chemical that the other does not contain. Furthermore, a\nskilled experimenter will make sure that the cultures are well\nstirred, thus preventing a physical-chemical inhomogeneity of the\nculture (e.g., some chemical or temperature gradient). Thus, there are\ntypical laboratory manipulations and procedures that reduce the risk\nof confounders. \nIn spite of these control procedures, it is clear that the derivation\nof a causal factor from a Millean difference test presupposes strong\nassumptions. In particular, it must be assumed that we are dealing\nwith a deterministic causal structure (3) and that nothing happens\nuncaused (4). In our simple example, this amounts to assuming that the\nbacteria don’t exhibit any kind of spontaneity, in other words, their\ngrowth behavior is assumed to be determined by their genetic\nconstitution and their environment (although most biologists are\nconvinced that their experimental organisms have good and bad days\njust like themselves!). \nIf we thus construe Millean causal reasoning as deductive reasoning,\nall the inferential risks are shifted from the induction rules into\nthe premises such as causal homogeneity, determinism and the principle\nof universal causality. It is a characteristic feature of inductive\ninference that this is always possible (Norton 2003). A justification\nof these premises, of course, does not exist. They may be viewed as\nbeing part and parcel of a certain kind of experimental practice that\nis vindicated as a whole by its fruitfulness for research (see\n Section 5). \nMill’s methods can be formalized and generalized into rich and\nsophisticated methodologies of causal reasoning (e.g., Ragin 1987,\nBaumgartner 2009, Graßhoff 2011, Beirlaen, Leuridan and Van De\nPutte 2018). Such accounts haven been successfully used in the\nreconstruction of historical episodes such as the discovery of the\nurea cycle (Graßhoff, Casties and Nickelsen 2000; Grasshoff and\nMay 1995). \nMill’s methods and their generalized versions may even be viewed as\nproviding some kind of a logic of discovery (see also Schaffner 1974),\nthe existence of which has long been controversial (Nickles 1980).\nHowever, it should be noted that the methods of causal reasoning do\nnot generate causal knowledge from scratch, as it were. They already\ntake causal hypotheses as inputs and refine them in an iterative way\n(Grasshoff 2011). While some formulations (including Mill’s own\nmentioned above) may suggest that the inputs to Mill’s methods are\nmere associations or regularities, it should be clear by now that the\nmethod is only reliable if some kind of causal knowledge is already at\nhand. In our example above, the causal assumption is that of causal\nhomogeneity, which obviously has causal content. This vindicates the\nslogan “no causes in, no causes out” (Cartwright 1989, Ch.\n2). \nThis discussion has focused on deterministic causal reasoning, which\nis widespread in experimental biology. It should be mentioned that, of\ncourse, there are also statistical inference methods of the kind\nformalized in Spirtes, Glymour and Scheines (2000) in use, in\nparticular regression analysis and analysis of variance, which are\nfrequently used in biological field experiments. Wet lab experiments,\nby contrast, rarely require such techniques. \nSome enthusiasts of causal inference methods believe that a\nsophisticated version of Mill’s methods (and their statistical\ncounterparts) are basically all that is needed in order to account for\nexperimental practice (e.g., Graßhoff 2011). An attraction of\nthis view is that it can be construed essentially as requiring only\ndeduction and no cumbersome inductive methods. \nExperimental methodology has traditionally been mainly concerned with\ninferring causal dependencies. However, recent research suggests that\nwe need to broaden its scope. A considerable body of scholarship\ndocuments that much biological research is best described in terms of\nthe search for mechanisms, which may be understood as collections of\nentities and activities that produce a phenomenon that biologists want\nto understand (e.g., Wimsatt 1974, Machamer, Darden and Craver 2000,\nGlennan 2005, Bechtel 2006, Craver 2007a). Mechanisms are both what\nbiological science is aiming at and a means to this end, for sketches\nor schemes of mechanisms can guide scientists in the discovery of\nmissing parts (Darden and Craver 2002, Scholl and Nickelsen 2015). \nAccording to Craver (2007b), we should distinguish between two kinds\nof relations that make up mechanisms: (1) causal and (2) constitutive\nrelations. The former may hold between different parts of a mechanism.\nFor example, in the basic mechanism of synaptic transmission at the\nterminal part of a neuron, the influx of calcium causes the release of\nneurotransmitter in the space between the synaptic terminal and the\npostsynaptic cell membrane. This causal link can be understood pretty\nmuch as discussed in the last two sections. The other kind of\nrelation, mechanistic constitutive relevance (or just mechanistic\nconstitution) holds between the parts and of a mechanism and the\nphenomenon the mechanism is for. For example, the influx of calcium\ninto an axon terminal, together with other events, constitutes the\nphenomenon of synaptic transmission. Craver (2007b) contends that this\nis not a causal relation because the relata cannot be viewed as\ndistinct and non-overlapping. \nBut what defines constitutive relevance? Inspired by interventionism\nabout causality, Craver argued that it is best defined by the kinds of\ninterventions that are used by biologists in order to find out whether\nsome entity and an associated activity are part of a mechanism: by\ncertain kinds of experiments. In particular, there are two kinds of\nso-called interlevel experiments the combination of which establishes\n(and defines) constitutive relevance. In the first kind, an\nintervention is performed on some part and an ensuing change is\nobserved in the phenomenon under study. To return to our synaptic\nexample, a calcium antagonist may be used to show that preventing the\nbinding of calcium ions to their receptors prevents the release of\nneurotransmitter. This is a bottom-up experiment. The second kind of\ninter-level experiment intervenes on the phenomenon as a whole in\norder to see some change in the parts. For example, stimulating\nsynaptic transmission by increasing the rate by which action\npotentials arrive at the terminal will lead to a measurable increase\nin calcium influx at this terminal. This can be done, for example, by\nasking a subject to perform a cognitive task (such as trying to\nmemorize something) and by observing changes in calcium concentration\nby functional magnetic resonance imaging (fMRI). Thus, mechanistic\nconstitution is defined by the mutual manipulability of the parts of\nmechanisms and the mechanism as a whole. \nRecent debate has challenged the mutual manipulability account\n(Leuridan 2012, Harinen 2018, Romero 2015). One issue is that\ninter-level experiments are necessarily “fat-handed”\n(Baumgartner and Gebharter 2016) because they change the values of at\nleast two different variables residing at different levels (e.g.,\ncalcium binding and synaptic transmission, where the former is a part\nof the latter). But this threatens to undermine the inferences to\nmechanistic constitution. A possible solution might consist in\ninferring constituents abductively, by positing constitutive relations\nas the best explanation for the presence of common causes that\nunbreakably correlate phenomena and their constituents (Baumgartner\nand Casini 2017). \nThus, the discovery of mechanisms in biology may require a set of\nexperimental reasoning principles that must supplement Mill’s\nmethods, even if there are considerable similarities between these\nprinciples and the better-known principles of causal inference\n(Harbecke 2015). \nBiological textbooks are replete with famous experiments that are\npresented as having provided crucial experimental evidence for some\nhypothesis. To name just a few examples: Oswald Avery, Colin MacLeod\nand Maclyn McCarthy (1944) are reputed to have established DNA as the\ncarrier of hereditary information by a set of ingenious transformation\nexperiments using a bacterium causing lung infection,\nPneumococcus. Matthew Meselson and Frank Stahl (1958)\nconducted an experiment in that is widely thought to have provided\nevidence that DNA replication is semi-conservative rather than\nconservative or dissipative (meaning that a newly formed DNA double\nhelix contains one intact strand from the parent molecule and one\nnewly synthesized strand). Ephraim Racker and Walter Stoeckenius\n(1974) are thought to have provided experimental evidence that settled\nthe long-standing controversy (known as the “oxidative\nphosphorylation or ox-phos controversy”) as to whether\nrespiration and the generation of the energy-rich compound ATP in\nmitochondria are coupled through a chemical intermediate or a proton\ngradient. \nMany philosophers of science have been skeptical about the possibility\nof such “crucial experiments”. Most famous are the\narguments provided by Pierre Duhem (1905), who claimed that crucial\nexperimental are impossible (in physics) because, even if they succeed\nin eliminating a false hypothesis, this does not prove that the\nremaining hypothesis is true. Scientists, unlike mathematicians, are\nnever in possession of a complete set of hypotheses one of which must\nbe true. It is always possible that the true hypothesis has not yet\nbeen conceived, thus eliminating all but one hypothesis by crucial\nexperiments will not necessarily lead to the truth. \nBased on such considerations, it could be argued that the function of\nthe alleged crucial experiments in biological textbooks is more\npedagogical than evidential. In order to assess this claim, I will\nconsider two examples in somewhat greater detail, namely the ox-phos\ncontroversy and the Meselson-Stahl case. \nThe ox-phos controversy was about the mechanism by which respiration\n(i.e., the oxidation of energy-rich compounds) is coupled to the\nphosphorylation of ADP (adenosine diphosphate) to ATP (adenosine\ntriphosphate). The reverse reaction, hydrolysis of ATP to ADP and\ninorganic phosphate, is used by cells to power numerous biochemical\nreactions such as the synthesis of proteins as well as small\nmolecules, the replication of DNA, locomotion, and many more (the\nenergy for muscle contraction is also provided by ATP hydrolysis).\nThus, ATP functions like a universal biochemical battery that is used\nto power all kinds of cellular processes. So how can the cell use the\nchemical energy contained in food to charge these batteries? A first\npathway that generates ATP from ADP to be described was glycolysis,\nthe degradation of sugar. This pathway does not require oxygen. The\nway it works is that the breakdown of sugar is used by the cell to\nmake an activated phosphate compound – a so-called high-energy\nintermediate – that subsequently transfers its phosphate group\nto ADP to make ATP.  \nIt was clear already in the 1940s that this cannot be the only process\nthat makes ATP, as it is anaerobic (doesn’t require oxygen) and not\nvery efficient. The complete breakdown of food molecules requires\noxygen. Hans Krebs showed that this occurs in a cyclic reaction\npathway today known as the Krebs cycle. The Krebs cycle generates a\ncompound called reduced NADH (nicotinamide adenine dinucleotide). The\nbig question in biochemistry from the 1940s onward was how this\ncompound was oxidized and how this oxidation was coupled to the\nphosphorylation of ADP. Naturally, biochemists searched for a\nhigh-energy intermediate as it was used in glycolysis. However, this\nintermediate proved to be extremely elusive. \nIn 1961, the British biochemist Peter Mitchell (1961) suggested a\nmechanism for oxidative phosphorylation (as this process became known)\nthat did not require a high-energy intermediate. According to\nMitchell’s scheme, NADH is oxidized step-wise on the inner membranes\nof mitochondria (membrane-bounded intracellular organelles that are\ncanonically described as the cell’s power stations). This process\noccurs asymmetrically with respect to the membrane such that a net\ntransport of protons across the membrane occurs (later it was shown\nthat the protons are actually transported across proteinaceous pores\nin the membrane). Thus, a concentration gradient of protons builds up.\nAs protons are electrically charged, this gradient also creates a\nvoltage difference across the membrane. How is this energy used?\nAccording to Mitchell, the membrane contains another enzyme that uses\nthe energy of the proton gradient to directly phosphorylate ADP. Thus,\nit is the proton gradient that couples the reduction of NADH to the\nphosphorylation of ADP, not a chemical high-energy intermediate. This\nmechanism became known as the “chemi-osmotic mechanism”. \nMitchell’s hypothesis was met with considerable skepticism, in spite\nof the fact that Mitchell and a co-worker were quickly able to produce\nexperimental evidence in its favor. Specifically, he was able to\ndemonstrate that isolated respiring mitochondria indeed expel protons\n(thus leading to a detectable acidification of the surrounding\nsolution), as his hypothesis predicted. However, this evidence was\ndismissed by most of the biochemists at that time as inconclusive. For\nit was difficult to rule out at that time that the proton expulsion by\nrespiring mitochondria was a mere side-effect of respiration, while\nthe energy coupling was still mediated by a chemical intermediate. \nThis was the beginning of one of the most epic controversies in the\nhistory of 20th Century biochemistry. It took the larger part of a\ndecade to resolve this issue, time during which several laboratories\nclaimed success in finding the elusive chemical intermediate. But none\nof these findings survived critical scrutiny. On the other side, all\nthe evidence adduced by Mitchell and his few supporters was considered\nnon-conclusive for the reasons mentioned above. Weber (2002) shows\nthat the two competing theories of oxidative phosphorylation can be\nviewed as incommensurable in the sense of T.S. Kuhn (1970), without\nthereby being incomparable (as Kuhn is often misread). \nNonetheless, the controversy was eventually closed in the mid-1970s in\nMitchell’s favor, earning him the 1978 Nobel Prize for Chemistry. The\ncause(s) for this closure are subject to interpretation. Most\nbiochemistry textbooks as well as Weber (2002, 2005) cite a famous\nexperiment by Ephraim Racker and Walter Stoeckenius (1974) as crucial\nfor establishing Mitchell’s mechanism. These biochemists worked with\nsynthetic membrane vesicles into which they had inserted purified\nenzymes, one of which being the mitochondrial ATPase (the enzyme\ndirectly responsible for phosphorylation of ADP). The other enzyme was\nnot derived from mitochondria at all, but from a photosynthetic\nbacterium. This enzyme had been shown to function as a light-driven\nproton pump. When this enzyme was inserted into artificial membrane\nvesicles together with the mitochondrial ATPase, the vesicles\nphosphorylated ADP upon illumination. \nThis experiment was viewed as crucial evidence for the chemi-osmotic\nmechanism because it was the first demonstration that the ATPase can\nbe powered by a chemiosmotic gradient alone. The involvement of a\nchemical intermediate could be ruled out because there were no\nrespiratory enzymes present to deliver an unknown intermediate to the\nATPase. \nThe Racker-Stoeckenius experiment is perhaps best viewed as a\nstraightforward case of an eliminative induction. On this\nview, the experiment decided between two causal graphs: \nThe alternative causal graph was: \nThe experiment showed a direct causal link from the protein gradient\nto phosphorylation, which was consistent with (1) but not with (2),\nthus (2) was eliminated. However, it is clear that Duhem-style\nskeptical doubts could be raised about this case. From a strictly\nlogical point of view, even if the experiment ruled out the chemical\nhypothesis, it didn’t prove Mitchell’s hypothesis. \nSo why have biochemists emphasized this experiment so much? Perhaps\nthe reason is not merely the beauty of the experiment or its\npedagogical value. From a methodological point of view, the experiment\nstands out from other experiments in the amount of control that\nbiochemists had of what was going on in their test tubes. Because it\nwas an artificially created system assembled from purified components,\nthe biochemists knew exactly what they had put in there, making it\nless likely that they were deceived by traces from the enormously\ncomplex system from which these biochemical agents were derived. On\nsuch an interpretation, the experiment was not crucial in\nDuhem’s sense of ruling out all alternative hypotheses. Rather,\nwhat distinguishes it is that, within the body of evidence that\nsupported Mitchell’s hypothesis, this one reached an\nunprecedented standard of experimental rigor, making it more difficult\nfor opponents of Mitchell’s hypothesis to challenge it (see\nWeber 2005, Chapter 4). \nAs another example, consider the models for DNA replication that were\nproposed in the 1950s after the publication of the double helix\nstructure of DNA by J.D. Watson and F.R.H. Crick (Lehninger 1975): Figure 1. \nIn purely causal terms, what all these models show is that the\nsequence of nucleotides in a newly synthesized DNA molecule depends on\nthe sequence of a pre-existing molecule. But the models also contain\nstructural information such as information the DNA double\nhelix topology and the movements undergone by a replicating DNA\nmolecule (still hypothetical at that time). According to the model on\nthe left, the so-called “conservative” model, the two\nstrands of the DNA double helix do not separate prior to replication;\nthey stick together and the daughter molecule grows along an intact\ndouble helix that acts as a template as a whole. According to the\nmodel in the middle, called the “semi-conservative”\nmechanism (believed to be correct today), the two strands do separate\nin the process. The newly synthesized strands grow along the separated\npre-existing strands that each acts as templates. On the model on the\nright, the “dispersive” model, the two strands not only\nseparate in the process, they are also cut into pieces that are then\nrejoined. While pure causal graphs (i.e., causal graphs that contain\nonly information about what variables depend on what other variables\ncausally) may be used to depict the causes of the separation and\ncutting events in semi-conservative and dispersive model, it also\ntakes a description of the structure of the DNA molecule, including\nthe topology and relative movements of the strands at different stages\nof the replication process. This is genuine mechanistic information\nthat must supplement the information contained in the causal graphs in\norder to provide a normatively adequate mechanistic explanation. \nHow can such a mechanism be tested? To see this, it is illuminating to\nconsider the famous experiment that is said to have enabled a choice\nbetween the three DNA replication mechanisms in 1957. Also known as\n“the most beautiful experiment in biology” (Holmes 2011),\nthis experiment was carried out by Matthew Meselson and Frank Stahl by\nusing an analytic ultracentrifuge (see also the discussion of\nthe Meselson-Stahl experiment in the entry\n experiment in physics). \nMeselson and Stahl grew bacteria in a growth medium that contained a\nheavy nitrogen isotope, 15N. These bacteria were then\ntransferred to a medium with normal light nitrogen, 14N.\nAfter one generation, the bacteria were harvested and their DNA was\nextracted and loaded onto a caesium chloride solution in the\nultracentrifuge chamber (Fig. 2). Caesium atoms are so heavy that they\nwill form a concentration gradient when subjected to high centrifugal\nforces. In such a gradient, DNA floats at a band in the gradient where\nthe molecular density corresponds to that of the caesium chloride\nsolution. Thus, the analytic centrifuge is an extremely sensitive\nmeasuring device for molecular density. In fact, it is sensitive\nenough to detect the difference between DNA that contains heavy\nnitrogen and DNA that contains light nitrogen. It can also distinguish\nthese two DNA species from a hybrid made of one strand containing\nheavy nitrogen and one strand containing light nitrogen. Indeed, after\none round of replication, the bacterial DNA in Meselson’s and Stahl’s\nexperiment was of intermediate density. After another round, the DNA\nhad the same density as DNA in its natural state. Figure 2.\nThe Meselson-Stahl experiment. The depiction of the centrifuge is\nmisleading; Meselson and Stahl were using a sophisticated analytic\nultracentrifuge, not a swinging bucket preparatory centrifuge as this\npicture suggests. Furthermore, this representation of the experiment\nalready contains its theoretical interpretation (Watson\n1965).[1] \nThe outcome was exactly what one would expect on the basis of the\nsemi-conservative model: After one round of replication, the DNA is\nexactly of intermediate density because it is a hybrid of a heavy and\na light strand (containing heavy or light nitrogen, respectively).\nAfter another round, a light molecule appears which contains no heavy\nnitrogen. With the exception of the heavy/light hybrid, there are no\nother molecules of intermediate density (as one would expect on the\ndispersive scheme). \nEven though this really looked like a spectacular confirmation of the\nsemi-conservative scheme, Meselson and Stahl (1958) were quite\ncautious in stating their conclusions. Avoiding theoretical\ninterpretation, all the experiment showed was that base nitrogen\ndistributes evenly during replication. While this is inconsistent with\nthe dispersive mechanism, it did not rule out the conservative\nmechanism. For it was possible that the material of intermediate\ndensity that the ultracentrifuge’s optical devices picked up did not\nconsist of hybrid heavy/light DNA molecules at all, but of some kind\nof complex of heavy and light DNA. For example, Meselson and Stahl\nspeculated that end-to-end covalent associations of old and newly\nsynthesized DNA that was produced by the conservative mechanism would\nalso have intermediate density and therefore produce the same band in\ntheir experiment. For this reason, could not completely rule out an\ninterpretation of the results in terms of the conservative\nmechanism. \nMeselson’s and Stahl’s caution was well justified, as we know now. For\nthey did not know exactly what molecular species made up the bands\nthey saw in their CsCl gradients. More recent considerations suggest\nthat their DNA must have been fragmented, due to Meselson’s and\nStahl’s use of a hypodermic needle to upload the DNA on their\ngradients. Such a treatment tears a whole genome from E. coli\ninto little pieces. This fragmentation of the DNA would have ruled out\nthe alternative interpretation with end-to-end linked molecules, had\nthe scientists been aware of it at the time. In fact, without the\nshearing of the DNA before the density measurement the whole\nexperiment wouldn’t have worked in the first place, due to the\ncircular structure of the E. coli chromosome, which affects\nits density (Hanawalt 2004). What this shows is that there were\nrelevant facts about the experimental system that Meselson and Stahl\ndid not know, which makes it difficult to view this experiment as\ncrucial. \nNonetheless, it is possible to view the experiment as performed in\n1957 as strong evidence for the semi-conservative mechanism. Weber\n(2009) argues that this mechanism is part of the best\nexplanation of Meselson’s and Stahl’s data. By\n“data”, Weber means the actual bands observed. Why is this\nexplanation better than the one involving the conservative scheme and\nend-to-end associations of heavy and light DNA? To be\nconsistent with some data is not the same as being a good\nexplanation of the data. The explanation using the\nsemi-conservative scheme was better because it showed directly how the\nnitrogen is distributed evenly. By contrast, the alternative\nexplanation requires the additional assumption that the parental and\nfilial DNA somehow remain covalently attached together after\nreplication. This assumption wasn’t part of the mechanism, therefore\nthe latter explanation was incomplete. A complete explanation is\nalways better than an incomplete one; no principle of parsimony need\nbe invoked here. \nThe case can thus be reconstructed as an inference to the best\nexplanation (Lipton 2004), or perhaps preferably as an inference to\nthe only explanation (Bird 2007). Inference to the best explanation\n(IBE), also known as abduction, is a controversial inductive inference\nscheme that selects from a set of candidate hypothesis the one that\nbest explains the phenomena. \nA variant of IBE is also used by Cresto (2008) to reconstruct the\nexperiment by Avery that showed that the transforming principle in\npneumococcus is DNA rather than protein. However, her conclusion is\nthat, in this case, IBE is inconclusive. What it does show is that\nthere was a rational disagreement, at the time, on the question of\nwhether DNA was the transforming principle. Novick and Scholl\n(forthcoming) argue that Avery et al. were not relying on IBE; they\nactually had a stronger case conforming to the traditional vera causa\nideal (see Section 5.2). \nTo sum up, we have seen that a methodological analysis of some of\nbiology’s most famous experiments reveals many intricacies and\nlacunae in their evidential force, which makes it difficult to see\nthem as “crucial experiments”. However, a rationale other\nthan their pedagogical value can often be given for why the textbooks\naccord them a special epistemic status. \nThe last two sections have treated experiments primarily as ways of\ntesting theoretical hypotheses and causal claims, which is where\ntraditional philosophy of science saw their principal role. However,\nthere exists a considerable body of scholarship in the history and\nphilosophy of biology that shows that this does not exhaust the role\nof experiments in biology. Experimentation, to echo Ian Hacking’s\nfamous slogan, “has a life of its own” (Hacking 1983).\nMuch that goes on in a biological laboratory does not have the goal of\ntesting a preconceived theory. For example, many experiments play an\nexploratory role, that is, they assist scientists in\ndiscovering new phenomena about which they may not yet have any\ntheoretical account or not even any clear ideas. Exploratory\nexperimentation has been investigated in the history and philosophy of\nphysics (Steinle 1997) as well as biology (Burian 1997, 2007; O’Malley\n2007; Waters 2007). These studies show that the development of any\ndiscipline in experimental biology cannot be understood by focusing on\ntheories and attempts to confirm or refute these theories.\nExperimental practice is simply not organized around theories,\nparticularly not in biology. If this is so, we must ask in what other\nterms this practice can be explained or reconstructed. Recent\nscholarship has focused in particular on two kinds of entities:\nmodel organisms and experimental systems. \nWhat would modern biology be without its model organisms? To give just\na few examples: Classical genetics was developed mainly by\nexperimenting with fruit flies of the species Drosophila\nmelanogaster. The same organism has recently also been at the\ncenter of much exciting work in developmental biology, together with\nthe nematode Caenorhabditis elegans and the zebrafish\nDanio rerio. Molecular biologists were initially mostly\nworking with the bacterium Escherichia coli and bacteriophage\n(viruses that infect bacteria). Neuroscientists owe much to the squid\nwith its giant axons. For immunology, the mouse Mus musculus\nhas proven invaluable. Plant molecular biology’s favorite is\nArabidopsis thaliana. Cell biology has found in baker’s yeast\nSaccharomyes cerevisiae an organism that is as easy to breed\nas E. coli, but its cells are more similar to animal and\nhuman cells. This list could go on, but not forever. Compared to field\nbiology, the number of species that are being studied in the\nexperimental life sciences is quite small; so small, in fact, that for\nmost of these model organisms complete genomic DNA sequences are\navailable today. \nIt may seem as somewhat paradoxical that it is exactly these\nexperimental disciplines of biology that limit their research to the\nsmallest part of life’s diversity that aspire to the greatest degree\nof universality. For the point of studying model organism is often to\ngain knowledge that is not only valid for one species but for many\nspecies, sometimes including humans. Before discussing this\nepistemological question, it is pertinent to review some results of\nhistorical and philosophical analyses of research on model\norganisms. \nAn important study is Robert Kohler’s Lords of the Fly\n(Kohler 1994), a history of Drosophila genetics originating\nfrom Thomas Hunt Morgan’s laboratory at Columbia University. Kohler\nattributes the success of this way of doing science to the enormous\noutput of the Drosophila system in terms of newly described\nand mapped mutations. The fruit fly breeds relatively fast and is easy\nto keep in laboratories in large numbers, which increases the chance\nof detecting new mutations. Kohler refers to Drosophila as the\n“breeder-reactor” to capture this kind of fecundity, thus,\nhis study is not unlike an ecology of the lab fruit fly. The elaborate\ntechniques of genetic mapping developed in Morgan’s laboratory, which\nassigned each mutation a physical location on one of the fly’s four\nchromosomes, can also be viewed as a way of organizing laboratory work\nand keeping track of the zillions of mutants that were cropping up\ncontinuously. A genetic map is a systematic catalogue of mutants and\nthus a research tool as much as it is a theoretical model of the fly’s\ngenetic system (cf. Weber 1998). Thus, most of what went on in\nMorgan’s laboratory is best described as breeding, maintaining,\ncharacterizing and cataloguing (by genetic mapping) fly strains. Very\nlittle of it had anything to do with testing genetic theories, even\nthough the theoretical knowledge about regularities of gene\ntransmission canonically known as “Mendel’s laws” was, of\ncourse, also considerably refined in the process (Darden 1991).\nHowever, classical genetic theory was also a means of discovering\nmutants and genes for further study, not an end in itself (Waters\n2004). This is also evident in the way in which Drosophila\nwas adapted to the age of the molecular biology (Weber 2005; Falk\n2009). \nModel organisms were often chosen for a particular field of study, but\nturned out to be invaluable for other fields as well.\nDrosophila is a case in point; Morgan was mainly interested\nin embryology when he chose Drosophila as a model system.\nThis paid off enormously, but the fly was just as important for\ngenetics itself, including evolutionary genetics (especially the work\nof one of the “architects” of the evolutionary synthesis,\nT. Dobzhansky; see Dobzhansky 1937), behavioral genetics (e.g., S.\nBenzer’s studies of the circadian clock mechanism, see Weiner 1999),\nbiochemical genetics, as well as neuroscience. \nThere is always an element of contingency in the choice of model\norganisms; no biological problem determines the choice of the best\norganism to study it. Even though considerations of convenience, easy\nbreeding and maintenance, as well as suitability for the study of\nspecific phenomena play a role (Clarke and Fujimura 1992; Burian 1992,\n1993; Lederman and Burian 1993; Clause 1993; Creager 2002; Geison and\nCreager 1999; Lederman and Tolin 1993), there are always many\ndifferent organisms that would satisfy these criteria. Thus, which\nmodel organism is chosen sometimes is a matter of chance. \nIs it possible that our biological knowledge would be altogether\ndifferent, had contingent choices been different in the past? As this\nquestion requires us to indulge in historical counterfactuals, which\nare controversial (Radick 2005), it is difficult to answer. However,\nit may not matter much one way or the other if the knowledge produced\nby model organisms can be shown to be of general validity, or at least\nas transcending the particular organism in which it was produced. This\nis an instance of the more general epistemological problem of\nextrapolating knowledge from one domain into another and can be\nfruitfully discussed as such (Burian 1992). \nLet us first consider a special case, the universality of the genetic\ncode. In molecular biology, unlike in popular science articles, the\nterm “genetic code” does not refer to an organism’s\ngenomic DNA sequence but to the assignment of amino acids to base\ntriplets on the messenger RNA. There are four such bases: A, U, G and\nC. Three bases together form a “codon” and specify an\namino acid, for example, the triplet AUG specifies the amino acid\nmethionine (it is also a start codon, i.e., it initiates an open\nreading frame for making a protein). It was remarkable to find that\nthe genetic code is completely identical in almost every organism that\nhas been studied so far. There are only a few exceptions known, for\nexample, the genetic code of mitochondria and chloroplasts or that of\nTrypanosomes (a human parasite causing sleeping sickness). Therefore,\nthe code is called “universal”, because the exceptions are\nso few. But how do biologists know that the code is universal, given\nthat it has been determined only in a small number of species? \nIn this case, a simple Bayesian argument can be used to show that\ncurrent belief in the universality of the genetic code is well\njustified. To see this, it must first be noted that the code is\narbitrary; there is no necessity in the specific mapping of amino\nacids to codons in the current standard genetic code. A second premise\nis that there is an enormously large number of different possible\ngenetic codes. Now, we may ask what the probability is that the exact\nsame genetic code evolved independently in so many different species\nas have been investigated at the molecular level to date. Surely, this\nprobability is vanishingly small. The probability of coincidence of\ngenetic codes is much, much larger on the assumption that the code did\nnot arise independently in the different species. On this hypothesis,\nthe code arose only once, and the coincidence is due to common\ndescent. However, once a specific genetic code is in place, it is very\ndifficult to change, because any mutational change in the amino\nacid-codon assignment tends to affect a larger number of different\nproteins and thus threatens to kill the organism. It is for this\nreason that Jacques Monod (1974) has described the code as a\n“frozen accident”. Its mutation rate is just very, very\nlow. These considerations together support the hypothesis of\nuniversality in the case of the genetic code. \nSuch an argument may not always be available. In fact, in the general\ncase extrapolation from a model organism is intrinsically problematic.\nThe main problem is what Daniel Steel (2008) has termed the\n“extrapolator’s circle”. The problem is this. We want to\ninfer that some system S has a mechanism M because another system T is\nalready known to have M. For this inference to be good, we must assume\nthat S and T are relevantly similar. We can’t know that the systems\nare relevantly similar unless we already know that they both have M.\nBut if we knew that, there would not be a need for extrapolating in\nthe first place. \nNote that such a circle does not arise when we extrapolate genetic\ncodes. In this case, we do know that the systems are\nrelevantly similar because all organisms share a common ancestor and\nbecause the code is hard to change. What can we do if no such argument\nis available? \nSteel (2008) has proposed a form of reasoning that he calls\n“comparative process tracing”. Steel’s proposed solution\nis that even in the absence of strong evidence as to what mechanism(s)\noperate in a target system compared to a model system, scientists can\ncompare the two systems with respect to certain crucial nodes in the\nrespective causal structures, namely those where the two systems are\nmost likely to differ. This can break the regress by providing\nevidence that the two systems are similar enough to warrant\nextrapolations, yet it is not necessary to already have an\nunderstanding of the target systems that matches the knowledge about\nthe model system. \nWhile this reasoning strategy seems plausible, it may not be so\npertinent for extrapolating from model organisms. Steel’s reasoning\nstrategy pertains to cases where we want to infer from one system\nwhere we know a certain mechanism operates to a different kind of\nsystem about we do not (yet) know that the same mechanism\noperates. However, in biology there is often evidence to the effect\nthat, in fact, very similar mechanisms operate in the model as well as\nin the target system. A particularly important kind of evidence is the\nexistence of so-called sequence homologies between different\norganisms. These are usually DNA sequences that show a more or less\nstrong similarity, in some cases even identity. As it is (like in the\ncase of the genetic code discussed above) not likely that this\nsequence similarity is due to chance, it is normally attributed to\ncommon ancestry (hence the term sequence “homology”; in\nbiology, this term in its modern usage indicates relations of shared\nancestry). Sequence homology has proven to be a reliable indicator of\nsimilarity of mechanisms. To give an example, a highly conserved\nsequence element called the “homeobox”, which was first\ndescribed in Drosophila, is today believed to play a similar\nrole in development in an extremely broad range of organisms (Gehring\n1998). When there are reasons to think that the model and the target\nshare similar causal mechanisms due to common descent, the\nextrapolator’s circle does not arise. \nHowever, the extent in which extrapolations from model organisms to\nother organisms can serve as evidence in the context of justification\nor merely as a heuristic in the context of discovery remains\ncontroversial. Baetu (1916) argues for a middle ground according to\nwhich extrapolations can be lines of evidence, but only in the context\nof a larger body of evidence where certain assumptions made in\nextrapolations are subjected to empirical scrutiny. \nThere has also been some debate to what extent there is a strong\nparallel between model organisms and theoretical models in science,\nfor example, the Lotka-Volterra predator-prey model in population\necology. A strong parallel is suggested by the fact that there is a\nwidespread agreement in the literature on scientific modeling (see\nentry Models in Science) that scientific models need not be abstract\n(i.e., consist of mathematical equations or something of this sort);\nthey can also be concrete and physical. Watson’s and\nCrick’s 1953 DNA model made of cardboard and wire qualifies as a\nmodel in this sense. Could a fruit fly bred in a laboratory to do\ngenetic experiments also be considered as a scientific model in the\nstandard sense? Ankeny and Leonelli (2011) answer this question in the\naffirmative, on the grounds that (1) model organisms are used to\nrepresent a whole range of other organisms and (2) that they are used\nto represent specific phenomena. \nLevy and Currie (2015) have criticized this idea. On their view,\nscientific models such as Lotka’s and Volterra’s (and all\nthe other models discussed in the burgeoning modeling literature) play\na completely different role in science than model organisms. In\nparticular, they differ in the way they are used in scientific\ninferences. In a model such as the mathematical predator-prey model,\nscientists draw analogical inferences from model to target, inferences\nthat are underwritten by theoretical assumptions. By contrast, model\norganisms serve as a basis for extrapolations from individual members\nto a whole class (often but not necessarily on the basis of shared\nancestry). Thus, they want to exclude what they call “empirical\nextrapolation” (a form of induction), which they take to be\ncharacteristic for model organism-target inferences from the kind of\ntheoretical inferences that are used to gain knowledge about a target\nvia scientific models in general. \nHowever, this does not mean that living organisms or populations of\norganisms cannot be used as models. Levy and Currie accept that\ncertain model systems in experimental ecology and experimental\nevolution are scientific models that are akin to the Lotka-Volterra\nmodel. A case in point are Thomas Park’s (1948) famous\ncompetition experiments using the flour beetle Tribolium. What\ncharacterizes such experimental models, in contrast to model\norganisms, is that they can be used to represent generalized\nbiologically phenomena (e.g., interspecific competition in\nPark’s case) that can be applied to radically different and\ntaxonomically unrelated target systems (see also Weber 2014). \nNo matter whether model organisms are considered as scientific models\nor as something else, it is widely agreed that they serve many\ndifferent purposes, only some of which are representational. The\ndevelopment of model organisms such as Drosophila (fruit\nfly), Arabidopsis (Thale cress), Caenorhabditis\n(roundworm) and the associated databases such as FlyBase, Wormbase\netc. has had a profound impact on the way in which biological research\nis organized (e.g., Geison and Creager 1999, Clarke and Fujimura 1992,\nCreager 2002, Leonelli and Ankeny 2012). Furthermore, model organisms\nare also important sources of research materials such as genetically\nwell-defined laboratory strains or genetically modified organisms\nwithout which cutting-edge biological experimentation is no longer\npossible today. Weber (2005) has introduced the notion of preparative\nexperimentation to capture this role of model organisms. \nAs we have seen, model organisms have greatly shaped the development\nof experimental biology, and continue to do so. Another important\nentity are so-called experimental systems. These are not to be\nconfused with model organisms: The latter is a biological species that\nis being bred in the laboratory for experimental work. An experimental\nsystem may involve one or several model organisms, and most model\norganisms are used in several experimental system. An experimental\nsystem typically consists of certain research materials (which may be\nobtained from a model organism), preparatory procedures, measurement\ninstruments, and data analysis procedures that are mutually adapted to\neach other. \nExperimental systems are the core of Hans-Jörg Rheinberger’s\n(1997) account of mid-20th Century research on protein\nsynthesis. Rheinberger characterizes experimental systems as\n“systems of manipulation designed to give unknown answers to\nquestions that the experimenters themselves are not yet clearly to\nask” and also as “the smallest integral working units of\nresearch” (Rheinberger 1997, 28). He argues that research in\nexperimental biology always “begins with the choice of a system\nrather than with the choice of a theoretical framework” (p. 25).\nHe then follows a certain experimental system through time and shows\nhow it exerted a strong effect on the development of biology. The\nsystem in question is a so-called in-vitro system for protein\nsynthesis that was developed in the laboratory of Paul Zamecnik at\nMassachusetts General Hospital in the 1950s. “In vitro”\nmeans that the system doesn’t contain any living cells. It is rather\nbased on a cell extract, originally from bacteria, later also from\nother organisms including rabbits and yeast. The cell extract is\nprepared in a way that preserves functionality of the protein\nsynthesis machinery. Instead of the RNAs that naturally occur in the\nsource organism, the in-vitro system may be “programmed”\nwith exogenous or even artificial RNA. Furthermore, the experimental\nsystem contains measurement procedures (sometimes called\n“assays” in experimental biology) for protein synthesis\nactivity. One such method is to measure incorporation of radioactivity\nintroduced by amino acids containing a radioisotope such as sulfur-35,\nbut there were other methods as well. Probably the most famous\nexperiment done with such a system is the experiment by Marshall\nNirenberg and Heinrich Matthaei (1961). These researchers added an\nartificial RNA, namely poly-U (U or uracil is one of the four bases of\nRNA) to the in-vitro system and showed that it produced a polypeptide\n(=protein) containing only the amino acid phenylalanine. Thus, they\n“cracked” the first “word” of the genetic\ncode, namely UUU for phenylalanine. \nOne of the central points of Rheinberger’s analysis is that the\nin-vitro system was never designed to do this task. Its original\npurpose was to study protein synthesis in cancer cells. In addition to\nunraveling the genetic code and the mechanisms of protein synthesis,\nit was later used for various other purposes in biochemistry and cell\nbiology (for example, for studying the transport of proteins across\nendoplasmic reticulum and mitochondrial membranes). The system was\nthus not even confined to a single discipline, in fact, it to some\nextent lead to a re-organization of scientific disciplines.\nFurthermore, the system was also fused with other experimental system\nor it bifurcated into different systems. Experimental systems, like\nmodel organisms are at least as important as focal points that\norganize research as theories. \nWhile Rheinberger’s account of experimental research in biology is\nright to put its emphasis on experimental systems rather than on\ntheories, it may be criticized for the ontology of experimental\nsystems that it contains. On this account, experimental systems are\nconstrued as very inclusive entities. They contain not only the\nspecific materials and apparatus necessary to do research, but also\nthe preparatory procedures, lab protocols, storage devices,\nmeasurement techniques, and so on. Thus, it is clear that such systems\nare not only composed of material entities; at least some of the\nconstituents of experimental systems are conceptual in\nnature. For example, a lab protocol that describes how to prepare a\ncell extract that will be able to carry out protein synthesis in vitro\nis really a conceptual entity. A researcher who is following such a\nprotocol is not merely manipulating materials, lab equipment or\nsymbols. Rather, she is acting under the guidance of concepts. (Action\nis always behavior guided by concepts). Among these concepts there may\nbe what used to be known as “observation concepts” such as\n“supernatant”, “pellet”, “band”,\nand so on. Furthermore, the protocol will contain terms such\n“microsomal fraction”, which is supposed to designate a\ncertain sediment produced by centrifugation that contains microsomes\n(vesicles of endoplasmic reticulum membrane). Thus, the protocol also\ncontains theoretical terms. \nRheinberger refers to theoretical entities as “epistemic\nthings”. This concept signifies the “material entities or\nprocesses—physical structures, chemical reactions, biological\nfunctions—that constitute the objects of inquiry”\n(Rheinberger 1997, 28). Epistemic things can unexpectedly appear and\ndisappear, or be re-constituted in new guises as experimental systems\nare developed. Sufficiently stabilized epistemic things can\n“turn into the technical repertoire of the experimental\narrangement” (1997, 29) and thus become “technical\nobjects.” One of Rheinberger’s examples of an epistemic thing is\nthe ribosome (a comparatively large molecular complex of protein and\nRNA that catalyzes some of the crucial steps in protein synthesis).\nHere, it can be argued that whether or not some experimental setup\ncontained ribosomes is a theoretical judgment. Thus, an\nexperimental system contains materials and apparatus as well as\nobservation concepts and theoretical concepts. They are rather\nheterogeneous ontologically. \nIf jumbles are considered undesirable from an ontological point of\nview, it is also possible to construe experimental systems as\npurely conceptual entities. They are ways of acting in a\nlaboratory, and action is always guided by concepts. Material things,\ninsofar as they appear in experimental practice, are only parts of an\nexperimental system to the extent in which they are subject to\npurposeful experimental action. They can only be subject to such\naction to the extent to which they are recognized by an experimenter,\nfor example, as a microsomal fraction or an antiserum with a certain\nantigen specificity, and so on. A biological sample without its label\nand some researcher who knows what this label means is of no value\nwhatsoever; in fact, it isn’t even a biological sample. \nIt can thus be argued that it’s really thought and concepts that give\nan experimental system its identity and persistence conditions;\nwithout concepts it’s really just a loose assemblage of plastic,\nglass, metal, and dead stuff coming out of a blender. On such a view,\nexperimental systems are not “out there” as\nmind-independent parts of a material reality; they exist only within\nsome concept-guided practice. \nAs the preceding sections should have made clear, there is ample\nevidence that biological research does not fit a Popperian image of\nscience according to which “The theoretician puts certain\ndefinite questions to the experimenter, and the latter, by his\nexperiments, tries to elicit a decisive answer to these questions, and\nto no others. All other questions he tries hard to exclude”\n(Popper 1959, 107). According to Rheinberger, much experimental\nresearch in biology does not aim at testing pre-conceived theories.\nHowever, sometimes theories or theoretical frameworks are, in fact,\nadopted by the scientific community, while others are abandoned. Even\nif it is true that most research is not aimed at testing theories,\nresearch can still undermine some theoretical ideas and support\nothers, to the point that one framework is chosen while another is\nrejected. How are such choices made? What principles guide them? And\ndo the choices actually exhibit some kind of epistemic rationality, as\nmost philosophers of science think, or do they merely reflect the\ninterests or larger cultural changes in society, as many sociologists\nand historians of science think? \nQuestions such as these are notoriously difficult to answer. Those who\nprefer a rational view of scientific change must demonstrate that\ntheir preferred epistemic norms actually inform the choices made by\nscientists. This has proven to be difficult. If we re-consider our\ncase from Section 3, the oxidative phosphorylation controversy, there\nexists a sociological account (Gilbert and Mulkay 1984) as well as\ndifferent philosophical accounts that do not even agree in how to\nexplain scientific change (Allchin 1992, 1994, 1996; Weber 2005, Ch.\n4–5). \nBy the same token, those who think that scientific change is\nhistorically contingent, in theory, must be able to justify historical\ncounterfactuals of the form “had the social/cultural context at\nsome given time been different, scientists would have adopted other\ntheories (or other experimental systems, model organisms, etc)”.\nIt is controversial whether such claims are justifiable (see Radick\n2005 for a recent attempt). \nPerhaps there is a way of reconciling the two perspectives. In recent\nyears, a new kind of epistemology has emerged that sees no\ncontradiction in viewing science both as a profoundly social activity\nand at the same time as rational. This, of course, is social\nepistemology (e.g., Goldman 1999; Longino 2002; Solomon 2001). Social\nepistemologists try to show that social interactions in scientific\ngroups or communities can give rise to a practice that is rational,\nalthough perhaps not in exactly the same way as epistemological\nindividualists have imagined it. \nCould there be a social account of scientific change for experimental\nbiology? Of course, such an account should not fall back into a\ntheory-first view of science but rather view theories as embedded in\nsome practice. To see how this is possible, it’s best to consider the\ncase of classical genetics. \nClassical genetics is said to derive from Mendel’s experiments with\npea plants, however, the story is more complex than this. At the\nbeginning of the 20th Century, there existed several\nschools in genetics, on both sides of the Atlantic. In England, the\nBiometric school of Karl Pearson and Raphael Weldon were developing a\nquantitative approach that took Francis Galton “Law of ancestral\nheredity” as its point of departure. The Mendelian William\nBateson was leading a rival movement. In the United States, Thomas\nHunt Morgan and his associates were constructing the first genetic\nmaps of the fruit fly. For a while, William Castle was defending a\ndifferent approach to genetic mapping, as well as a different theory\nof the gene. Various approaches were also developed in Continental\nEurope, for example, in the Netherlands (Hugo de Vries), Germany (Carl\nCorrens) or Austria (Erich Tschermak). The American school of Thomas\nHunt Morgan pretty much carried the day. Its approach to mapping genes\nas well the (ever changing) theory of the gene became widely accepted\nin the 1930s, and became officially incorporated in the Evolutionary\nSynthesis of the 1930s and 40s (Mayr 1982; Provine 1971). \nA traditional way for a philosopher of science of looking at this case\nwould be to consider the salient theories and to ask: What\nevidence compelled scientists to adopt Morgan’s theory? However, this\nmay not be the right question to ask. For this question makes the\nsilent supposition that theories and experimental evidence can be\nseparated from each other in genetics. But it can be argued that the\ntwo depend too strongly on each other to be considered separate\nentities. Classical genetics—in all its forms—was first\nand foremost a way of doing crossing experiments with various model\norganisms, and to arrange the results of these experiments in a\ncertain way, for example, linear genetic maps. To be sure, there were\nalso certain ideas about the nature of the gene, some of which highly\nspeculative, others well established by experiments (for example, the\nlinear arrangement on the chromosome, see Weber 1998b). But what is\nsometimes referred to as “the laws of transmission\ngenetics” or simply “Mendel´s laws” was part\nand parcel of a certain way of doing genetics. Thus, theory is not\nseparable from experimental practice (Waters 2008). This, of course,\ncuts both ways: Experimental data in genetics are as\n“theory-laden” as they are in other sciences, in other\nwords, the analysis and interpretation of experimental data require\ntheories. \nFor our current issue of scientific change, what this means is that\ntheory and experimental evidence were selected together, not\nthe former on the basis of the latter (as a naive empiricist account\nwould have it). \nOn what grounds was this selection made? And who made it? A simple,\nbut perhaps incomplete explanation is that the Morgan approach to\ngenetics was selected because it was the most fruitful. A skilled\ngeneticist who adopted the Morgan way was almost guaranteed to come up\nwith publishable results, mostly in the form of genetic maps, and\nperhaps also some interesting observations such as mutations that\nbehaved oddly in genetic crosses. By contrast, the alternative\napproaches that were still around in the early 20th Century\ndid not prove to be as fruitful. That is, they did not produce a\ncomparable number of results that could be deemed a success according\nto these sciences’ own standards. What is more, the\nalternative genetic approaches were not adaptable to other scientific\ndisciplines such as evolutionary biology. Classical genetics thus\nout-competed the other approaches by its sheer productivity, which was\nmanifested by published research papers, successful grant\napplications, successful students, and so on. It was therefore the\nscientific community as a whole that selected the new genetics due to\nits fruitfulness. At no point was there any weighing of evidence for\nor against the theoretical framework in question on the part of\nindividual scientists, or at least this played no role (of course,\nthere was weighing of evidence for more specific claims made within\nthe same framework). Theories in experimental biology are thus\nselected as parts of experimental practices (or experimental systems,\nsee\n §4.2),\n and always at the community level. This is how a social\nepistemological account of scientific change in experimental biology\nmight look like (Weber 2011). \nThe image of the rise of classical genetics just drawn is reminiscent\nof Thomas Kuhn’s philosophy of science (Kuhn 1970), or of David Hull’s\naccount of the rise of cladistic taxonomy (Hull 1988). It is a\ncontroversial issue whether such an image is compatible with a broadly\nrational construal of scientific practice. Kuhn thought that there\nexists no external standard of rationality that we could appeal to in\norder to answer this question, but he also held that science is the\nvery epitome of rationality (Kuhn 1971, 143f.). According to Kuhn,\nthere is an emergent social rationality inherent in modern science\nthat cannot be construed as the exercise of individual rational\nfaculties. Hull (1988) seems to be taking a similar view. \nUnder what conditions should scientists trust experimental data that\nsupport some theoretical conclusion such as, for example, that DNA has\na helical structure or that mitochondria are bounded by two layers of\nmembrane? This is known as the problem of “data\nreliability”. Unreliable data are commonly referred to as\n“experimental artifacts” in biology, indicating their\nstatus as being “made” by the experimental procedure. This\nis somewhat misleading, as all data—whether they are reliable or\nartifacts—are made by the experimental procedure. The difference\nis that reliable data are correct representations of an underlying\nreality, whereas so-called artifacts are incorrect representations.\nThis characterization assumes that data have some sort of\nrepresentational content; they represent an object as\ninstantiating some property or properties; (see Van Fraassen 2008, Ch.\n6–7). The question of what it means for data to represent their\nobject correctly has not been much discussed in this context. The\nfocus so far has been mostly on various strategies for checking data\nfor reliability, which will be discussed below. As for scientists\nthemselves, they tend to place this problem under the rubric of\nreplicability. While this is also an important topic, the fact that\nsome data are replicable is not sufficient for their reliability.\nThere are lots of well-known examples of perfectly replicable data\nthat turned out to be not reliable, e.g., the case of the mesosome to\nbe discussed below. \nOne of the more influential ideas in philosophy of science is that of\nrobustness. Originally, the term was introduced to denote a certain\nproperty of theoretical models, namely insensitivity to modeling\nassumptions. A robust result in theoretical modeling is a result that\nis invariant with respect to variations in modeling assumptions\n(Levins 1966; Wimsatt 1981; Weisberg 2006). In experimental science,\nthe term means something else, namely that a result is invariant with\nrespect to the use of different experimental methods. Robust\nexperimental results are results that somehow agree even although they\nhave been produced by independent methods. The independence of methods\ncan mean at least two things: First, the methods use different\nphysical processes. For example, a light microscope and a\n(transmission) electron microscope are independent methods in this\nsense; the former uses visible light, the latter an electron beam to\ngather information about biological structures. The second sense of\nindependence concerns the theoretical assumptions used to analyze and\ninterpret the data (most data are theory-dependent). If these\nassumptions are different for the two methods, they are also referred\nto as independent, or as independently theory-dependent (Culp 1995).\nThese two senses of “independent” usually coincide,\nalthough there could be exceptions. \nA much-discussed example of a robust experimental result from the\nphysical sciences is the numerical value obtained for Avogadro’s\nnumber (6.022 x 1023) at the beginning of the 20th Century\nby using no less than thirteen independent methods according to the\nphysicist Jean Perrin (Nye 1972). Salmon (1984) has argued that this\nprovided a strong justification for belief in the existence of atoms\nby way of a common-cause argument. On Salmon’s view, the existence of\natoms can be inferred as the common cause of Perrin’s thirteen\ndifferent determinations of Avogadro’s number. Others have construed\nrobustness arguments more along the lines of Putnam’s “no\nmiracle” argument for scientific realism, according to which the\nassumption of the reality of some theoretical entities is the best\nexplanation for the predictive success of theories or, in the case of\nrobustness, for the agreement of experimental results (Weber 2005, Ch.\n8; Stegenga 2009). \nVarious authors have tried to show that robustness-based reasoning\nplays an important role in judgments about data reliability in\nexperimental biology. For example, Weber (1998b) argues that the\ncoincidence of genetic maps of Drosophila produced by\ndifferent mapping techniques provided an important argument for the\nfidelity of these maps in the 1930s. Culp (1995) argued that\nrobustness provides a way out of a methodological conundrum know as\n“data-technique circles”, which is a more apt designation\nfor Collins’ “experimenter’s regress”. According to\nCollins (1985), scientists must trust data on the grounds of their\nbeing produced by a reliable instrument, but they can only judge an\ninstrument to be reliable on the basis of it’s producing the right\nkind of data. According to Culp (1995), robustness provides a way out\nof this circle. If the data obtained by independent methods agree,\nthen this supports both the reliability of these data and of the\ninstrument that produced these data. She attempts to demonstrate this\non the example of different methods of DNA sequencing. \nStegenga (2009) criticizes the approach from robustness. On his view,\nwhat he calls “multi-modal evidence”, i.e., evidence\nobtained by different techniques, is often discordant. If it is\nconcordant, it is difficult to judge if the different methods are\nreally independent. However, Stegenga (2009) does not discuss Culp’s\n(1995) example of DNA sequencing, which seems to be a case where\nevidence obtained by different sequencing techniques is often\nconcordant. Furthermore, Culp shows that the techniques are\nindependent in the sense that they are based both on different\nbiochemical processes and different theoretical assumptions. \nA historical case that has been extensively discussed is the case of\nthe hapless mesosome. For more than 20 years (ca. 1960–1983),\nthis was thought to be a membrane-bound structure inside bacterial\ncells, not unlike the nucleus or the mitochondria of the cells of\nhigher organisms. However, it could only be seen in electron\nmicroscopy (bacterial cells being too small for light microscopy).\nBecause electron microscopy requires a vacuum, biological samples\nrequire a lot of preparation. Cells must be chemically fixed, which\nwas done mostly by using Osmium tetroxide at the time. Furthermore,\nthe samples must be protected from the effects of freezing water. For\nthis purpose, cryoprotectants such as glycol must be used. Finally,\nthe material must be cut into extremely thin slices, as an electron\nbeam does not have much penetrating power. Electron microscopists used\ntwo different techniques: The first involved embedding the cells in a\nraisin before cutting with very sharp knives (so-called microtomes).\nThe other technique used fast freezing followed by fracturing the\ncells along their membranes. At first, mesosomes were seen in electron\nmicrographs under a variety of conditions, but then there were\nincreasing findings were they failed to show. Thus, suspicion started\nto rise that mesosomes were really a microscopic artifact. Ultimately,\nthis is what microbiologists and electron microscopists ended up\nconcluding by the mid-1980s, after more than two decades of research\non these structures. \nThere are different interpretations of this episode to be found in the\nliterature. One of the oldest is Rasmussen (1993), who basically\noffered a sociological explanation for the rise and fall of the\nmesosome. This account has been widely criticized (see Rasmussen 2001\nfor a more recent defense). Others have tried to show that this case\nexhibits some methodological standard in action. On Culp’s (1994)\naccount, the case demonstrates the relevance of the standard of\nrobustness in actual scientific practice. According to Culp, the\nmesosome was accepted as a real entity so long as a robust (i.e.,\ndiverse and independently theory-dependent) body of data provided\nsigns of their existence. When the winds changed and the evidence\nagainst the mesosome became more robust than the evidence in its\nfavor, it was dropped by biologists. \nAs this presentation should make clear, Culp’s account requires that\nrobustness comes in degrees and that there should, therefore, be some\nway of comparing degrees of robustness. However, no proponents of\nrobustness to date have been able to come up with such a measure.\nUltimately, what the case of the mesosome shows is that, in realistic\ncases, the evidence for some theoretical entity is discordant\n(Stegenga 2009), meaning that some findings supported the existence of\nmesosomes whereas others did not. But this means that robustness as a\ncriterion cannot give unequivocal results, unless there is some way of\nsaying which body of data is more robust: the body that supported the\ntheoretical entity or the body which did not. Thus, when the evidence\nis discordant the criterion of robustness fails to provide a solution\nto the problem of data reliability. \nAn alternative account of the mesosome story has been offered by\nHudson (1999). Hudson argues that considerations of robustness did not\nplay a role in the scientists’ decisions concerning the reality\nof the mesosome. Had they, in fact, used such reasoning, they would\nnever have believed in mesosomes. Instead, they used a kind of\nreasoning that Hudson refers to as “reliable process\nreasoning”. By this, he means basically that scientists trust\nexperimental data to the extent in which they have reasons to believe\nthat the processes that generated these data provide faithful\nrepresentations of reality. In the case of the mesosome, the evidence\nfrom electron microscopes was first judged reliable. But later, it was\nshown empirically that mesosomes were actually produced by the\npreparation procedure. Briefly, it was shown by various experiments\nthat the chemical fixants used directly damage biological membranes.\nThe mesosomes were thus shown to be artifacts by empirical\nmethods. Considerations of robustness thus played no role in the\nelectron microscopists’ data reliability judgments. \nSimilar to Hudson, Weber (2005, Ch. 9) argues that the demonstration\nthat mesosomes were experimental artifacts was nothing but an ordinary\nexperimental demonstration of the existence of a causal relationship,\nnamely between mesosome appearances and the fixation agents. Thus,\nthere may not be any special kind of reasoning other than ordinary\ncausal reasoning (see\n §2.1)\n involved in data reliability judgments. \nAn aspect that has so far been largely neglected in this debate is a\ndifferent kind of robustness, namely physical robustness. If\nwe compare the case of the mesosomes with that of other intracellular\norganelles, for example, the eukaryotic nucleus or mitochondria, it is\nnotable that it was never possible to isolate mesosomes, whereas the\nlatter organelles can be recovered in more or less intact form from\ncell extracts, normally by the use of centrifugation. Although it is,\nof course, entirely possible that the structures so recovered are\nartifacts of the preparation procedure, there may be additional\nevidence for their reality. For example, there could be telltale\nenzyme activities that can be measured in isolated cell organelles\nthat correspond to the organelle’s function. It might be possible to\nsubsume this kind of reasoning under Hacking’s (1983) strategies of\nusing the ability to intervene with theoretical entities as a ground\nfor their real existence. \nRecent scholarship has defended the traditional ideal of vera causa\nboth as a normatively adequate methodological standard for\nexperimental biology as well as a standard that biologists are\nactually committed to. The ideal requires that scientific explanations\nof a phenomenon cite true causes or verae causae. To qualify as a true\ncause, an entity must satisfy the following criteria (e.g., Hodge\n1992): First, it must have been shown to exist. Second, it must be\ncausally competent to produce the phenomenon in question. Third, it\nmust be causally responsible for this phenomenon. Importantly, the\nevidence for existence must be independent from the evidence for\ncausal competence. Thus, the mere fact that an entity’s\npurported causal capacities are able to explain a phenomenon dies not\nalone constitute sufficient evidence for its existence. What this\nmeans, obviously, is that an inference to the best explanation (IBE)\nof a phenomenon cannot alone justify acceptance of a postulated\nentity. It must always be accompanied by independent evidence for\nexistence. \nProponents of the vera causa ideal thus oppose the idea that IBE might\nbe something like a common methodological standard for metaphysics and\nfor science. If metaphysicians rely on IBE to justify the entities\nthey postulate, for examples, tropes or substances (as argued by Paul\n2012), they do not hold these beliefs to the same methodological\nstandard as scientists do (Novick forthcoming). For scientists are not\ncontent with IBE-type arguments; they require something more before\nthey accept theoretical entities. \nAn example for the vera causa ideal at work can be found in the\nhistory of genetics. In the 19th Century, there was much speculation\nabout hereditary particles. For example, Darwin’s\n“provisional hypothesis of pangenesis” postulated freely\ncirculating particles or “gemmules” that represented the\nparts and organs of an organism and transmitted this information to\nsubsequent generations. While Darwin was himself an advocate of the\nvera causa ideal, which he meticulously applied to his theory of\nnatural selection, he held this “provisional hypothesis”\nto less exacting standards (Stanford 2006). However, of course others\ncriticized the pangenesis theory for its failure to meet the vera\ncausa standard, because the gemmules were only inferred on the grounds\nthat they could explain certain hereditary phenomena. \nAs Novick and Scholl (forthcoming) show, a science of genetics that\nmet the vera causa standard eventually emerged in the 20th century, in\nthe wake of the re-discovery of Mendel’s laws as well as\nimproved microscopic and cytological techniques. The latter were\nimportant, because they allowed independent checks of the structures\nthat were invoked in order to explain the observed experimental\nregularities. A case in point is Thomas Hunt Morgan’s and his\nco-workers’ “proof” of the hypothesis that genes are\nparts of chromosomes (Bridges 2014).  \nMorgan et al.’s argument made use of a phenomenon that is called\nchromosomal non-disjunction. This term designates the failure of\nsister chromosomes to segregate during the process of meiosis. A\npossible result of this failure in Drosophila is a fly that has two\nfemale sex chromosomes in addition to a male one (XXY). The\ngeneticists noticed that this would explain why some flies failed to\nexhibit an inheritance pattern normally exhibited by traits that are\ndetermined by sex-linked factors (i.e., factors that lie on one of the\nsex chromosomes X or Y), namely “criss-cross” inheritance.\nIn this pattern, a recessive trait such as white eye color is\ntransmitted from the females of the first generation to the males in\nthe second generation, while the females revert to the original state\nof red eyes, hence the expression “criss-cross”. This\nnormal pattern is explained by the fact that a recessive X-linked\ntrait is always expressed in a male because there is only copy of the\ncorresponding gene in a male. The female offspring, by contrast,\nreceive a normal copy of the gene from their father, which is why the\nrecessive trait will be hidden. \nIn non-disjunction, some of the offspring carry an extra X in addition\nto the male Y chromosome. This obliterates the criss-cross\nmechanism. \nMorgan and his group were adamant that neither the phenomenon of\nsex-linked inheritance nor the fact that non-disjunction would explain\nthe observed deviations from this inheritance pattern were sufficient\nto prove the chromosome theory. A necessary element in their case was\nthe cytology: The excessive X-chromosome was microscopically\nobservable, and it was shown to be present exactly in those flies that\nfailed to exhibit the criss-cross pattern. This is how the vera causa\nideal was met: The chromosomes as carriers of the genes not only\nexplained the phenomena, their existence was also supported by\nevidence other than the fact that they explained the phenomena.\nThat’s how they earned their place in biology’s\nontology. \nThe vera causa ideal may also have been effective in the historical\nepisodes presented in previous sections. In the oxidative\nphosphorylation case, the hypothetical chemical intermediate was never\naccepted because biochemists failed to isolate and detect it.\nMitchell’s chemi-osmotic gradient, by contrast, was readily\naccepted. However, it’s causal efficacy in powering\nphosphorylation was doubted. This is where the Racker-Stoeckenius came\nin: By demonstrating this causal efficacy, it completed the case for\nchemiosmotic coupling in compliance with the vera causa ideal. \nIn a similar way, it can be argued that Meselson’s and\nStahl’s evidence for semi-conservative replication was\nincomplete. The distribution of DNA in the density gradient supported\nsemi-conservative replication by IBE, but before the identity of the\nthe DNA molecules was confirmed the case was inconclusive, which is\nwhy Meselson and Stahl were so cautious.","contact.mail":"marcel.weber@unige.ch","contact.domain":"unige.ch"}]
