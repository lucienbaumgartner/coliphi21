[{"date.published":"2014-02-06","date.changed":"2014-02-26","url":"https://plato.stanford.edu/entries/computational-linguistics/","author1":"Lenhart Schubert","author1.info":"http://www.cs.rochester.edu/~schubert/","entry":"computational-linguistics","body.text":"\n\n\n“Human knowledge is expressed in language. So computational\nlinguistics is very important.”\n\n–Mark Steedman, ACL Presidential Address (2007)\n\n\n\n\nComputational linguistics is the scientific and engineering\ndiscipline concerned with understanding written and spoken language\nfrom a computational perspective, and building artifacts that usefully\nprocess and produce language, either in bulk or in a dialogue setting.\nTo the extent that language is a mirror of mind, a computational\nunderstanding of language also provides insight into thinking and\nintelligence. And since language is our most natural and most versatile\nmeans of communication, linguistically competent computers would\ngreatly facilitate our interaction with machines and software of all\nsorts, and put at our fingertips, in ways that truly meet our needs,\nthe vast textual and other resources of the internet.\n\n\nThe following article outlines the goals and methods of\ncomputational linguistics (in historical perspective), and then delves\nin some detail into the essential concepts of  linguistic\nstructure and analysis (section 2),  interpretation (sections\n3–5), and language use (sections 6–7), as well as acquisition of\nknowledge for language (section 8), statistical and machine learning\ntechniques in natural language processing (section 9), and\nmiscellaneous applications (section 10).\n\n\n\nThe theoretical goals of computational linguistics include the\nformulation of grammatical and semantic frameworks for characterizing\nlanguages in ways enabling computationally tractable implementations of\nsyntactic and semantic analysis; the discovery of processing techniques\nand learning principles that exploit both the structural and\ndistributional (statistical) properties of language; and the\ndevelopment of cognitively and neuroscientifically plausible\ncomputational models of how language processing and learning might\noccur in the brain. The practical goals of the field are broad and varied. Some of the most\nprominent are: efficient text retrieval on some desired topic;\neffective machine translation (MT); question answering (QA), ranging\nfrom simple factual questions to ones requiring inference and\ndescriptive or discursive answers (perhaps with justifications); text\nsummarization; analysis of texts or spoken language for topic,\nsentiment, or other psychological attributes; dialogue agents for\naccomplishing particular tasks (purchases, technical trouble shooting,\ntrip planning, schedule maintenance, medical advising, etc.); and\nultimately, creation of computational systems with human-like\ncompetency in dialogue, in acquiring language, and in gaining knowledge\nfrom text. \nThe methods employed in theoretical and practical research in\ncomputational linguistics have often drawn upon theories and findings\nin theoretical linguistics, philosophical logic, cognitive science\n(especially psycholinguistics), and of course computer science.\nHowever, early work from the mid-1950s to around 1970 tended to be\nrather theory-neutral, the primary concern being the development of\npractical techniques for such applications as MT and simple QA. In MT,\ncentral issues were lexical structure and content, the characterization\nof “sublanguages” for particular domains (for example, weather\nreports), and the transduction from one language to another (for\nexample, using rather ad hoc graph transformation grammars or transfer\ngrammars). In QA, the concern was with characterizing the question\npatterns encountered in a specific domain, and the relationship of\nthese question patterns to the forms in which answers might stored, for\ninstance in a relational database. By the mid-1960s a number of researchers emboldened by the increasing\npower and availability of general-purpose computers, and inspired by\nthe dream of human-level artificial intelligence, were designing\nsystems aimed at genuine language understanding and dialogue. The\ntechniques and theoretical underpinnings employed varied greatly. An\nexample of a program minimally dependent on linguistic or cognitive\ntheory was Joseph Weizenbaum's ELIZA program, intended to emulate (or\nperhaps caricature) a Rogerian psychiatrist. ELIZA relied on matching\nuser inputs to stored patterns (brief word sequences interspersed with\nnumbered slots, to be filled from the input), and returned one of a set\nof output templates associated with the matched input pattern,\ninstantiated with material from the input. While ELIZA and its modern\nchatbot descendants are often said to rely on mere trickery, it can be\nargued that human verbal behavior is to some degree reflexive in the\nmanner of ELIZA, i.e., we\nfunction in “preprogrammed” or formulaic\nmanner in certain situations, for example, in exchanging greetings, or\nin responding at a noisy party to comments whose contents, apart from\nan occasional word, eluded us. A very different perspective on linguistic processing was proffered in\nthe early years by researchers who took their cue from ideas about\nassociative processes in the brain. For example, M. Ross Quillian (1968)\nproposed a model of word sense disambiguation based on “spreading\nactivation” in a network of concepts (typically corresponding to senses\nof nouns) interconnected through relational links (typically\ncorresponding to senses of verbs or prepositions). Variants of this\n“semantic memory” model were pursued by researchers such as Rumelhart,\nLindsay and Norman (1972), and remain as an active research paradigm in\ncomputational models of language and cognition. Another psychologically\ninspired line of work was initiated in the 1960s and pursued for over\ntwo decades by Roger Schank and his associates, but in his case the\ngoal was full story understanding and inferential question answering. A\ncentral tenet of the work was that the representation of sentential\nmeaning as well as world knowledge centered around a few (e.g., 11)\naction primitives, and inference was driven by rules associated\nprimarily with these primitives; (a prominent exponent of a similar\nview was Yorick Wilks). Perhaps the most important aspect of Schank's\nwork was the recognition that language understanding and inference were\nheavily dependent on a large store of background knowledge, including\nknowledge of numerous “scripts” (prototypical ways in which familiar\nkinds of complex events, such as dining at a restaurant, unfold) and\nplans (prototypical ways in which people attempt to accomplish their\ngoals) (Schank & Abelson 1977). More purely AI-inspired approaches that also emerged in the 1960s\nwere exemplified in systems such as Sad Sam\n(Lindsay 1963), Sir (Raphael 1968)\nand Student (Bobrow 1968). These featured\ndevices such as pattern matching/transduction for analyzing and\ninterpreting restricted subsets of English, knowledge in the form of\nrelational hierarchies and attribute-value lists, and QA methods based\non graph search, formal deduction protocols and numerical algebra. An\ninfluential idea that emerged slightly later was that knowledge in AI\nsystems should be framed procedurally rather than\ndeclaratively—to know something is to be able to perform certain\nfunctions (Hewitt 1969). Two quite impressive systems that exemplified\nsuch a methodology were shrdlu (Winograd 1972)\nand\nLunar (Woods et al. 1972), which contained\nsophisticated proceduralized grammars  and syntax-to-semantics\nmapping rules, and were able to function fairly robustly in their\n“micro-domains” (simulated blocks on a table, and a lunar rock\ndatabase, respectively). In addition, shrdlu featured\nsignificant planning abilities, enabled by the\nmicroplanner goal-chaining language (a precursor of\nProlog). Difficulties that remained for all of these approaches were\nextending linguistic coverage and the reliability of parsing and\ninterpretation, and most of all, moving from microdomains, or coverage\nof a few paragraphs of text, to more varied, broader domains. Much of\nthe difficulty of scaling up was attributed to the “knowledge\nacquisition bottleneck”—the difficulty of coding or acquiring the\nmyriad facts and rules evidently required for more general\nunderstanding. Classic collections containing several articles on the\nearly work mentioned in the last two paragraphs are Marvin\nMinsky's Semantic Information Processing (1968) and Schank\nand Colby's Computer Models of Thought and Language (1973).\n \nSince the 1970s, there has been a gradual trend away from purely\nprocedural approaches to ones aimed at encoding the bulk of linguistic\nand world knowledge in more understandable, modular, re-usable forms,\nwith firmer theoretical foundations. This trend was enabled by the\nemergence of comprehensive syntactico-semantic frameworks such as\nGeneralized Phrase Structure Grammar (GPSG), Head-driven Phrase\nStructure Grammar (HPSG), Lexical-Functional Grammar (LFG),\nTree-Adjoining Grammar (TAG), and Combinatory Categorial Grammar (CCG),\nwhere in each case close theoretical attention was paid both to the\ncomputational tractability of parsing, and the mapping from syntax to\nsemantics. Among the most important developments in the latter area\nwere Richard Montague's profound insights into the logical (especially\nintensional) semantics of language, and Hans Kamp's and Irene Heim's\ndevelopment of Discourse Representation Theory (DRT), offering a\nsystematic, semantically formal account of anaphora in language.\n \nA major shift in nearly all aspects of natural language processing\nbegan in the late 1980s and was virtually complete by the end of 1995:\nthis was the shift to corpus-based, statistical approaches (signalled\nfor instance by the appearance of two special issues on the subject by\nthe quarterly Computational Linguistics in 1993). The new\nparadigm was enabled by the increasing availability and burgeoning\nvolume of machine-readable text and speech data, and was driven\nforward by the growing awareness of the importance of the\ndistributional properties of language, the development of powerful new\nstatistically based learning techniques, and the hope that these\ntechniques would overcome the scalability problems that had beset\ncomputational linguistics (and more broadly AI) since its\nbeginnings. The corpus-based approach has indeed been quite successful in producing\ncomprehensive, moderately accurate speech recognizers, part-of-speech\n(POS) taggers, parsers for learned probabilistic phrase-structure\ngrammars, and even MT and text-based QA systems and summarization\nsystems. However, semantic processing has been restricted to rather\nshallow aspects, such as extraction of specific data concerning\nspecific kinds of events from text (e.g.,\nlocation, date, perpetrators,\nvictims, etc., of terrorist bombings) or extraction of clusters of\nargument types, relational tuples, or paraphrase sets from text\ncorpora. Currently, the corpus-based, statistical approaches are still\ndominant, but there appears to be a growing movement towards\nintegration of formal logical approaches to language with corpus-based\nstatistical approaches in order to achieve deeper understanding and\nmore intelligent behavior in language comprehension and dialogue\nsystems. There are also efforts to combine connectionist and neural-net\napproaches with symbolic and logical ones. The following sections will\nelaborate on many of the topics touched on above. General references\nfor computational linguistics are Allen 1995,  Jurafsky and\nMartin 2009, and Clark et al. 2010. Language is structured at multiple levels, beginning in the case of\nspoken language with patterns in the acoustic signal that can be\nmapped to phones (the distinguishable successive sounds of\nwhich languages are built up). Groups of phones that are equivalent\nfor a given language (not affecting the words recognized by a hearer,\nif interchanged) are the phonemes of the language. The\nphonemes in turn are the constituents of morphemes (minimal\nmeaningful word segments), and these provide the constituents of\nwords. (In written language one speaks instead of characters,\ngraphemes, syllables, and words.) Words are grouped\ninto phrases, such as noun phrases, verb phrases, adjective\nphrases and prepositional phrases, which are the structural components\nof sentences, expressing complete thoughts. At still higher levels we\nhave various types of discourse structure, though this is generally\nlooser than lower-level structure. Techniques have been developed for language analysis at all of these\nstructural levels, though space limitations will not permit a serious\ndiscussion of methods used below the word level. It should be noted,\nhowever, that the techniques developed for speech recognition in the\n1980s and 1990s were very influential in turning NLP research towards\nthe new corpus-based, statistical approach referred to above.  One\nkey idea was that of hidden Markov\nmodels (HMMs), which model “noisy” sequences (e.g.,\nphone sequences, phoneme sequences, or word sequences) as if generated\nprobabilistically by “hidden” underlying states and their transitions.\nIndividually or in groups, successive hidden states model the more\nabstract, higher-level constituents to be extracted from observed noisy\nsequences, such as phonemes from phones, words from phonemes, or parts\nof speech from words. The generation probabilities and the state\ntransition probabilities are the parameters of such models, and\nimportantly these can be learned from training data. Subsequently the\nmodels can be efficiently applied to the analysis of new data, using\nfast dynamic programming algorithms such as the Viterbi algorithm.\nThese quite successful techniques were subsequently generalized to\nhigher-level structure, soon influencing all aspects on NLP. Before considering how grammatical structure can be represented,\nanalyzed and used, we should ask what basis we might have for\nconsidering a particular grammar “correct”, or a particular sentence\n“grammatical,” in the first place. Of course, these are primarily\nquestions for linguistics proper, but the answers we give certainly\nhave consequences for computational linguistics. Traditionally, formal grammars have been designed to capture linguists'\nintuitions about well-formedness as concisely as possible, in a way\nthat also allows generalizations about a particular language (e.g.,\nsubject-auxiliary inversion in English questions) and across languages\n(e.g., a consistent ordering\nof nominal subject, verb, and nominal\nobject for declarative, pragmatically neutral main clauses). Concerning\nlinguists' specific well-formedness judgments, it is worth noting that\nthese are largely in agreement not only with each other, but also with\njudgments of non-linguists—at least for “clearly grammatical” and\n“clearly ungrammatical” sentences (Pinker 2007). Also the discovery\nthat conventional phrase structure supports elegant compositional\ntheories of meaning lends credence to the traditional theoretical\nmethodology. However, traditional formal grammars have generally not covered any\none language comprehensively, and have drawn sharp boundaries between\nwell-formedness and ill-formedness, when in fact people's (including\nlinguists') grammaticality judgments for many sentences are uncertain\nor equivocal. Moreover, when we seek to process sentences “in the\nwild”, we would like to accommodate regional, genre-specific, and\nregister-dependent variations in language, dialects, and erroneous and\nsloppy language (e.g., misspellings, unpunctuated run-on sentences,\nhesitations and repairs in speech, faulty constituent orderings\nproduced by non-native speakers, and fossilized errors by native\nspeakers, such as “for you and I”—possibly a product of\nschoolteachers inveighing against “you and me” in subject position).\nConsequently linguists' idealized grammars need to be made\nvariation-tolerant in most practical applications. The way this need\nhas typically been met is by admitting a far greater number of phrase\nstructure rules than linguistic parsimony would sanction—say,\n10,000 or more rules instead of a few hundred. These rules are not\ndirectly supplied by linguists (computational or otherwise), but\nrather can be “read off” corpora of written or spoken language that\nhave been decorated by trained annotators (such as linguistics\ngraduate students) with their basic phrasal tree\nstructure. Unsupervised grammar acquisition (often starting with\nPOS-tagged training corpora) is another avenue \n(see section 9), but\nresults are apt to be less satisfactory. In conjunction with\nstatistical training and parsing techniques, this loosening of grammar\nleads to a rather different conception of what comprises a\ngrammatically flawed sentence: It is not necessarily one rejected by\nthe grammar, but one whose analysis requires some rarely used\nrules. As mentioned in section 1.2, the representations of grammars used\nin computational linguistics have varied from procedural ones to ones\ndeveloped in formal linguistics, and systematic, tractably parsable\nvariants developed by computationally oriented linguists. Winograd's\nshrdlu program, for example, contained code in\nhis programmar language expressing, \nTo parse a\nsentence, try parsing a noun phrase (NP); if this fails, return NIL,\notherwise try parsing a verb phrase (VP) next and if this fails, or\nsucceeds with words remaining, return NIL, otherwise return success. \nSimilarly Woods' grammar for lunar was based on a\ncertain kind of procedurally interpreted transition graph (an\naugmented transition network, or ATN), where the sentence subgraph\nmight contain an edge labeled NP (analyze an NP using the NP\nsubgraph) followed by an edge labeled VP (analogously\ninterpreted). In both cases, local feature values (e.g.,\nthe number and person of a NP and VP) are\nregistered, and checked for agreement as a condition for success. A\nclosely related formalism is that of definite clause grammars (e.g.,\nPereira & Warren 1982), which employ Prolog to assert “facts” such\nas that if the input word sequence contains an NP reaching from\nindex I1 to index I2 and a VP reaching from\nindex I2 to index I3, then the input contains a\nsentence reaching from index I1 to index I3. (Again,\nfeature agreement constraints can be incorporated into such assertions\nas well.) Given the goal of proving the presence of a sentence, the\ngoal-chaining mechanism of Prolog then provides a procedural\ninterpretation of these assertions. At present the most commonly employed declarative representations\nof grammatical structure are context-free grammars\n(CFGs) as defined by Noam Chomsky (1956, 1957), because of\ntheir simplicity and efficient parsability.  Chomsky had argued that\nonly deep linguistic representations are context-free, while surface\nform is generated by transformations (for example, in English\npassivization and in question formation) that result in a\nnon-context-free language. However, it was later shown that on the one\nhand, unrestricted Chomskian transformational grammars allowed for\ncomputationally intractable and even undecidable languages, and on the\nother, that the phenomena regarded by Chomsky as calling for a\ntransformational analysis could be handled within a context-free\nframework by use of suitable features in the specification of\nsyntactic categories. Notably, unbounded movement, such as\nthe apparent movement of the final verb object to the front of the\nsentence in “Which car did Jack urge you to buy?”, was shown to be\nanalyzable in terms of a gap (or slash) feature of\ntype /NP[wh] that is carried by each of the two embedded VPs,\nproviding a pathway for matching the category of the fronted object to\nthe category of the vacated object position. Within\nnon-transformational grammar frameworks, one therefore speaks of\nunbounded (or long-distance) dependencies\ninstead of unbounded movement. At the same time it should be noted\nthat at least some natural languages have been shown to be mildly\ncontext-sensitive (e.g., Dutch and Swiss German exhibit cross-serial\ndependencies where a series of nominals “NP1 NP2 NP3 …”\nneed to be  matched, in the same order, with a subsequent series\nof verbs, “V1 V2 V3 …”).  Grammatical frameworks that\nseem to allow for approximately the right degree of mild context\nsensitivity include Head Grammar, Tree-Adjoining Grammar (TAG),\nCombinatory Categorial Grammar (CCG), and Linear Indexed Grammar\n(LIG). Head grammars allow insertion of a complement between the head\nof a phrase (e.g., the initial verb of a VP, the final noun of a NP,\nor the VP of a sentence) and an already present complement; they were\na historical predecessor of Head-Driven Phrase Structure Grammar\n(HPSG), a type of unification grammar (see below) that has received\nmuch attention in computational linguistics. However, unrestricted\nHPSG can generate the recursively enumerable (in general only\nsemi-decidable) languages. A typical (somewhat simplified) sample fragment of a context-free\ngrammar is the following, where phrase types are annotated with\nfeature-value pairs: Here v, n, p, c are variables that can assume values such\nas ‘past’, ‘pres’, ‘base’,\n‘pastparticiple’, … (i.e., various verb forms),\n‘1’, ‘2’, ‘3’ (1st,\n2nd, and 3rd person), ‘sing’,\n‘plur’, and ‘subj’,\n‘obj’. The subcat feature indicates the\ncomplement requirements of the verb. The lexicon would supply entries\nsuch as allowing, for example, a phrase structure analysis of the sentence\n“Thetis loves a mortal” (where we have omitted the feature\nnames for simplicity, leaving only their values, and ignored the case\nfeature): As a variant of CFGs, dependency grammars (DGs)\nalso enjoy wide popularity. The difference from CFGs is that\nhierarchical grouping is achieved by directly subordinating words to\nwords (allowing for multiple dependents of\na head word), rather than phrases to phrases. For\nexample, in the sentence of figure 1 we would treat Thetis\nand mortal as dependents of loves, using dependency\nlinks labeled subj and obj respectively, and the\ndeterminer a would in turn be a dependent\nof mortal, via a dependency link mod\n(for modifier). Projective dependency grammars are\nones with no crossing dependencies (so that the descendants of a node\nform a continuous text segment), and these generate the same languages\nas CFGs. Significantly, mildly non-projective dependency grammars,\nallowing a head word to dominate two separated blocks, provide the\nsame generative capacity as the previously mentioned mildly\ncontext-sensitive frameworks that are needed for some languages\n(Kuhlmann 2013). As noted at the beginning of this section, traditional formal\ngrammars proved too limited in coverage and too rigid in their\ngrammaticality criteria to provide a basis for robust coverage of\nnatural languages as actually used, and this situation persisted until\nthe advent of probabilistic grammars derived from sizable\nphrase-bracketed corpora (notably the Penn Treebank). The simplest\nexample of this type of grammar is a probabilistic context-free\ngrammar or PCFG. In a PCFG, each phrase structure rule X\n→ Y1 … Yk is assigned a probability,\nviewed as the probability that a constituent of type X will be\nexpanded into a sequence of (immediate) constituents of\ntypes Y1, …, Yk. At the lowest level, the\nexpansion probabilities specify how frequently a given part of speech\n(such as Det, N, or V) will be realized as a particular word. Such a\ngrammar provides not only a structural but also a distributional model\nof language, predicting the frequency of occurrence of various phrase\nsequences and, at the lowest level, word sequences.  However, the simplest models of this type do not model the\nstatistics of actual language corpora very accurately, because the\nexpansion probabilities for a given phrase type (or part of speech) X\nignore the surrounding phrasal context and the more detailed\nproperties (such as head words) of the generated constituents. Yet\ncontext and detailed properties are very influential; for example,\nwhether the final prepositional phrase in “She detected a\nstar with {binoculars, planets}” modifies detected\nor planets is very dependent on word choice. Such modeling\ninaccuracies lead to parsing inaccuracies (see next subsection), and\ntherefore generative grammar models have been refined in various ways,\nfor example (in so-called lexicalized models) allowing for\nspecification of particular phrasal head words in rules, or\n(in tree substitution grammars) allowing expansion of\nnonterminals into subtrees of depth 2 or more. Nevertheless, it seems\nlikely that fully accurate distributional modeling of language would\nneed to take account of semantic content, discourse structure, and\nintentions in communication, not only of phrase\nstructure. Possibly construction grammars (e.g., Goldberg\n2003), which emphasize the coupling between the entrenched patterns of\nlanguage (including ordinary phrase structure, clichés, and\nidioms) and their meanings and discourse function, will provide a\nconceptual basis for building statistical  models of language\nthat are sufficiently accurate to enable more nearly human-like\nparsing accuracy.  \nNatural language analysis in the early days of AI tended to rely on\ntemplate matching, for example, matching templates such as\n(X has Y) or\n(how many Y are there\non X) to the input to be analyzed. This of course\ndepended on having a very restricted discourse and task domain. By the\nlate 1960s and early 70s, quite sophisticated recursive parsing\ntechniques were being employed. For example, Woods'\nlunar system used a top-down recursive parsing strategy\ninterpreting an ATN in the manner roughly indicated in \nsection 2.2 (though ATNs in principle allow other\nparsing styles). It also saved recognized constituents in a table,\nmuch like the class of parsers we are about to describe. Later parsers\nwere influenced by the efficient and conceptually elegant CFG parsers\ndescribed by Jay Earley (1970) and (separately) by John Cocke, Tadao\nKasami, and Daniel Younger (e.g., Younger 1967). The latter algorithm,\ntermed the CYK or CKY algorithm for the three separate authors, was\nparticularly simple, using a bottom-up dynamic programming approach to\nfirst identify and tabulate the possible types (nonterminal labels) of\nsentence segments of length 1 (i.e., words), then the possible types\nof sentence segments of length 2, and so on, always building on the\npreviously discovered segment types to recognize longer phrases. This\nprocess runs in cubic time in the length of the sentence, and a parse\ntree can be constructed from the tabulated constituents in quadratic\ntime. The CYK algorithm assumes a Chomsky Normal Form (CNF) grammar,\nallowing only productions of form Np →\nNq Nr, or Np\n→ w, i.e., generation of two nonterminals or a word from\nany given nonterminal.  This is only a superficial limitation, because\narbitrary CF grammars are easily converted to CNF. \nThe method most frequently employed nowadays in fully analyzing\nsentential structure is chart parsing.  This is a\nconceptually simple and efficient dynamic programming method closely\nrelated to the algorithms just mentioned; i.e., it begins by assigning\npossible analyses to the smallest constituents and then inferring\nlarger constituents based on these, until an instance of the top-level\ncategory (usually S) is found that spans the given text or text\nsegment. There are many variants, depending on whether only complete\nconstituents are posited or incomplete ones as well (to be\nprogressively extended), and whether we proceed left-to-right through\nthe word stream or in some other order (e.g., some seemingly\nbest-first order). A common variant is a left-corner chart\nparser, in which partial constituents are posited whenever their\n“left corner”—i.e., leftmost constituent on the\nright-hand side of a rule—is already in place. Newly completed\nconstituents are placed on an agenda, and items are\nsuccessively taken off the agenda and used if possible as left corners\nof new, higher-level constituents, and to extend partially completed\nconstituents. At the same time, completed constituents (or rather,\ncategories) are placed in a chart, which can be thought of as a\ntriangular table of width n and height n (the number\nof words processed), where the cell at indices (i, j),\nwith j > i, contains the\ncategories of all complete constituents so far verified reaching from\nposition i to position j in the input.  The chart is\nused both to avoid duplication of constituents already built, and\nultimately to reconstruct one or more global structural analyses. (If\nall possible chart entries are built, the final chart will allow\nreconstruction of all possible parses.) Chart-parsing methods carry\nover to PCFGs essentially without change, still running within a cubic\ntime bound in terms of sentence length. An extra task is maintaining\nprobabilities of completed chart entries (and perhaps bounds on\nprobabilities of incomplete entries, for pruning purposes). \nBecause of their greater expressiveness, TAGs and CCGs are harder to\nparse in the worst case (O(n6)) than CFGs\nand projective DGs (O(n3)), at least with\ncurrent algorithms (see Vijay-Shankar & Weir 1994 for parsing\nalgorithms for TAG, CCG, and  LIG based on bottom-up dynamic\nprogramming). However, it does not follow that TAG parsing or CCG\nparsing is impractical for real grammars and real language, and in\nfact parsers exist for both that are competitive with more common\nCFG-based parsers.\n  Finally we mention connectionist  models of parsing,\nwhich perform syntactic analysis using layered (artificial) neural\nnets (ANNs, NNs) (see Palmer-Brown et al. 2002; Mayberry and\nMiikkulainen 2008; and Bengio 2008 for surveys). There is typically a\nlayer of input units (nodes), one or more layers of hidden units, and\nan output layer, where each layer has (excitatory and inhibitory)\nconnections forward to the next layer, typically conveying evidence\nfor higher-level constituents to that layer. There may also be\nconnections within a hidden layer,  implementing cooperation or\ncompetition among alternatives. A linguistic entity such as a phoneme,\nword, or phrase of a particular type may be represented within a layer\neither by a pattern of activation of units in that layer\n(a distributed representation) or by a single activated unit\n(a localist representation).  One of the problems that connectionist models need to confront is that\ninputs are temporally sequenced, so that in order to combine\nconstituent parts, the network must retain information about recently\nprocessed parts. Two possible approaches are the use of simple\nrecurrent networks (SRNs)\nand, in localist networks, sustained\nactivation. SRNs use one-to-one feedback connections from the hidden\nlayer to special context units\naligned with the previous layer\n(normally the input layer or perhaps a secondary hidden layer), in\neffect storing their current outputs in those context units. Thus at\nthe next cycle, the hidden units can use their own previous outputs,\nalong with the new inputs from the input layer, to determine their next\noutputs. In localist models it is common to assume that once a unit\n(standing for a particular concept) becomes active, it stays active for\nsome length of time, so that multiple concepts corresponding to\nmultiple parts of the same sentence, and their properties, can be\nsimultaneously active. A problem that arises is how the properties of\nan entity that are active at a given point in time can be properly tied\nto that entity, and not to other activated entities. (This is the\nvariable binding problem, which has spawned a variety of\napproaches—see Browne and Sun 1999). One solution is to assume\nthat unit activation consists of pulses emitted at a globally fixed\nfrequency, and pulse trains that are in phase with one another\ncorrespond to the same entity (e.g., see Henderson 1994).  Much\ncurrent connectionist research borrows from symbolic processing\nperspectives, by assuming that parsing assigns linguistic phrase\nstructures to sentences, and treating the choice of a structure as\nsimultaneous satisfaction of symbolic linguistic constraints (or\nbiases). Also, more radical forms of hybridization and modularization\nare being explored, such as interfacing a NN parser to a symbolic\nstack, or using a neural net to learn the probabilities needed in a\nstatistical parser, or interconnecting the parser network with\nseparate prediction networks and learning networks.  For an overview\nof connectionist sentence processing and some hybrid methods (see\nCrocker 2010). \nIf natural language were structurally unambiguous with respect to some\ncomprehensive, effectively parsable grammar, our parsing technology\nwould presumably have attained human-like accuracy some time ago,\ninstead of levelling off at about 90% constituent recognition\naccuracy.  In fact, however, language is ambiguous at all structural\nlevels: at the level of speech sounds (“recognize speech”\nvs.  “wreck a nice beach”); morphology\n(“un-wrapped” vs. “unwrap-ped”); word category\n(round as an adjective, noun, verb or adverb); compound word\nstructure (wild goose chase); phrase category\n(nominal that-clause vs. relative clause in\n“the idea that he is entertaining”); and modifier\n(or complement) attachment (“He hit the man with the\nbaguette”). The parenthetical examples here have been chosen\nso that their ambiguity is readily noticeable, but ambiguities are far\nmore abundant than is intuitively apparent, and the number of\nalternative analyses of a moderately long sentence can easily run into\nthe thousands. Naturally, alternative structures lead to alternative meanings, as\nthe above examples show, and so structural disambiguation is\nessential. The problem is exacerbated by ambiguities in the meanings\nand discourse function even of syntactically unambiguous words and\nphrases, as discussed below (section 4). But\nhere we just mention some of the structural preference principles that\nhave been employed to achieve at least partial structural\ndisambiguation. First, some psycholinguistic principles that have been\nsuggested are Right Association (RA) (or Late\nClosure, LC), Minimal Attachment (MA), and Lexical\nPreference (LP). The following examples illustrate these\nprinciples: \nAnother preference noted in the literature is for parallel structure in\ncoordination, as illustrated by the following examples: Finally, the following example serves to illustrate the\nsignificance of frequency effects, though such effects are hard to\ndisentangle from semantic biases for any single sentence (improvements\nin parsing through the use of word and phrase frequencies provide more\ncompelling evidence):  \nLanguage serves to convey meaning. Therefore the analysis of\nsyntactic structure takes us only partway towards mechanizing that\ncentral function, and the merits of particular approaches to syntax\nhinge on their utility in supporting semantic analysis, and in\ngenerating language from the meanings to be communicated.\n This is not to say that syntactic analysis is of no value in\nitself—it can provide a useful support in applications such as\ngrammar checking and statistical MT. But for the more ambitious goal\nof inferring and expressing the meaning of language, an essential\nrequirement is a theory of semantic representation, and how it is\nrelated to surface form, and how it interacts with the representation\nand use of background knowledge. We will discuss logicist approaches,\ncognitive science approaches, and (more briefly) emerging statistical\napproaches to meaning representation. \nMost linguistic semanticists, cognitive scientists, and\nanthropologists would agree that in some sense, language is a mirror of\nmind. But views diverge concerning how literally or non-literally this\ntenet should be understood. The most literal understanding, which we\nwill term the logicist view,\nis the one that regards language\nitself as a logical meaning representation with a compositional,\nindexical semantics—at least when we have added brackets as\ndetermined by parse trees, and perhaps certain other augmentation\n(variables, lambda-operators, etc.)\nIn itself, such a view makes no commitments about mental\nrepresentations, but application of Occam's razor and the presumed\nco-evolution of thought and language then suggest that mentalese is\nitself language-like. The common objection that “human thinking is not\nlogical” carries no weight with logicists, because logical meaning\nrepresentations by no means preclude nondeductive modes of inference\n(induction, abduction, etc.);\nnor are logicists impressed by the objection that people quickly forget\nthe exact wording of verbally conveyed information, because both\ncanonicalization of inputs and systematic discarding of all but major\nentailments can account for such forgetting. Also assumption of a\nlanguage-like,\nlogical mentalese certainly does not preclude other modes of\nrepresentation and thought, such as imagistic ones, and synergistic\ninteraction with such modes (Paivio 1986; Johnston & Williams 2009). \nSince Richard Montague (see especially Montague 1970, 1973) deserves\nmuch of the credit for demonstrating that language can be logically\nconstrued, let us reconsider the sentence structure in figure 1 and\nthe corresponding grammar rules and vocabulary, but this time\nsuppressing features, and instead indicating how logical\ninterpretations expressed in (a variant of) Montague's type-theoretic\nintensional logic can be obtained compositionally. We slightly\n“twist” Montague's type system so that the possible-world\nargument always comes last, rather than first, in the denotation of a\nsymbol or expression.  For example, a two-place predicate will be of\ntype (e → (e → (s → t)))\n(successively applying to an entity, another entity, and finally a\npossible world to yield a truth value), rather than Montague's type\n(s → (e → (e → t))), where\nthe world argument is first. This dispenses with numerous applications\nof Montague's intension (∧) and extension\n(∨) operators, and also slightly simplifies truth\nconditions. For simplicity we are also ignoring contextual indices\nhere, and treating nouns and VPs as true or false of individuals,\nrather than individual concepts (as employed by Montague to account\nfor such sentences as “The temperature is 90 and rising”).\n \nHere primed constituents represent the intensional logic translations\nof the corresponding constituents. (Or we can think of them as\nmetalinguistic expressions standing for the set-theoretic denotations\nof the corresponding constituents.) Several points should be noted.\nFirst, each phrase structure rule is accompanied by a unique semantic\nrule (articulated as the rule-to-rule\nhypothesis by Emmon Bach (1976)),\nwhere the denotation of each phrase is fully determined by the\ndenotations of its immediate\nconstituents: the semantics is\ncompositional.  \nSecond, in the S′-rule, the subject is assumed to be a second-order\npredicate that is applied to the denotation of the VP (a monadic\npredicate) to yield a sentence intension, whereas we would ordinarily\nthink of the subject-predicate semantics as being the other way\naround, with the VP-denotation being applied to the subject. But\nMontague's contention was that his treatment was the proper one,\nbecause it allows all types of subjects—pronouns, names, and\nquantified NPs—to be handled uniformly. In other words, an NP\nalways denotes a second-order property, or (roughly speaking) a set of\nfirst-order properties (see also Lewis 1970). So for example,\nThetis denotes the set of all properties that Thetis (a\ncertain contextually determined individual with that name) has; (more\nexactly, in the present formulation Thetis denotes a function\nfrom properties to sentence intensions, where the intension obtained\nfor a particular property yields truth in worlds where the entity\nreferred to has that property); some woman denotes the union\nof all properties possessed by at least one woman; and\nevery woman denotes the set of properties shared by all\nwomen.  Accordingly, the S′-rule yields a sentence intension that is\ntrue at a given world just in case the second-order property denoted\nby the subject maps the property denoted by the VP to such a\ntruth-yielding intension. Third, in the VP′-rule, variables x and y\nare assumed to be of type e (they take basic individuals as\nvalues), and the denotation of a transitive verb should be thought of\nas a function that is applied first to the object, and then to the\nsubject (yielding a function from worlds to truth values—a sentence\nintension). The lambda-abstractions in the VP′-rule can be\nunderstood as ensuring that the object NP, which like any NP denotes a\nsecond-order property, is correctly applied to an ordinary property\n(that of being the love-object of a certain x), and the\nresult is a predicate with respect to the (still open) subject\nposition. The following is an interpreted sample vocabulary: Note the interpretation of the indefinite determiner\n(on line 2)\nas a generalized quantifier—in effect\na second-order predicate over two ordinary properties, where these\nproperties have intersecting truth domains. We could have used an\natomic symbol for this second-order predicate, but the above way of\nexpanding it shows the relation of the generalized quantifier to the\nordinary existential quantifier. Though it is a fairly self-evident\nmatter, we will indicate in section 4.1 how the sentence \n“Thetis loves a mortal” yields the following representation after some\nlambda-conversions: (The English sentence also has a generic or habitual reading,\n“Thetis loves mortals in general”, which we ignore here.) This\ninterpretation has rather a classical look to it, but only because of\nthe reduction from generalized to ordinary quantifiers that we have\nbuilt into the lexical semantics of the indefinite a in the\nabove rules, instead of using an atomic symbol for it. Montague was\nparticularly interested in dealing satisfactorily with intensional\nlocutions, such as \n“John seeks a unicorn.” This does not\nrequire the existence of a unicorn for its truth—John has a certain\nrelation to the unicorn-property, rather than to an existing\nunicorn. Montague therefore treated all predicate arguments as\nintensions; i.e., he rendered \n“John seeks a unicorn”\nas which can be reduced to a version where unicorn is extensionalized to\nunicorn*: But ultimately Montague's treatment of NPs, though it was in a sense\nthe centerpiece of his proposed conception of language-as-logic, was\nnot widely adopted in computational linguistics. This was in part\nbecause the latter community was not convinced that an omega-order\nlogic was needed for NL semantics, found the somewhat complex treatment\nof NPs in various argument positions and in particular, the treatment\nof scope ambiguities in terms of multiple syntactic analyses, \nunattractive, and was preoccupied with other semantic issues, such as\nadequately representing events and their relationships, and developing\nsystematic nominal and verb “ontologies” for broad-coverage NL\nanalysis. Nonetheless, the construal of language as logic left a strong\nimprint on computational semantics, generally steering the field\ntowards compositional approaches, and in some approaches such as CCG,\nproviding a basis for a syntax tightly coupled to a type-theoretic\nsemantics (Bach et al. 1987;\nCarpenter 1997). An alternative to Montague's syntax-based approach to quantifier scope\nambiguity is to regard NPs of form Det+N (or strictly, Det+N-bar) as\ninitially unscoped higher-order predicates in an underspecified logical\nform, to be subsequently “raised” so as to apply to a first-order\npredicate obtained by lambda-abstraction of the vacated term position.\nFor example, in the sentence \n“Everyone knows a poem”, with the object\nexistentially interpreted, we would have the underspecified LF \nknows〈a(poem)〉〈every(person)〉 \n (without reducing\ndeterminers to classical quantifiers) and we can now “raise”\n〈a(poem)〉 to yield \na(poem)(λy\nknows(y)〈every(person)〉,  \nand then “raise”\n〈every(person)〉 to yield either \na(poem)(λy\nevery(person)(λx knows(y)(x))), \n or \nevery(person)(λx\na(poem)(λy knows(y)(x))). \n Thus we obtain a reading according to which there is a poem that\neveryone knows, and another according to which everyone knows some\npoem (not necessarily the same one). (More on scope disambiguation\nwill follow in section 4). A systematic version\nof this approach, known as Cooper storage (see Barwise & Cooper\n1981) represents the meaning of phrases in two parts, namely a\nsequence of NP-interpretations (as higher-order predicates) and the\nlogical matrix from which the NP-interpretations were extracted. But one can also take a more conventional approach, where first of\nall, the use of “curried” (Schönfinkel-Church-Curry)\nfunctions in the semantics of predication is avoided in favor of\nrelational interpretations, using lexical semantic formulas such\nas loves′ =\nλyλx(loves(x, y)), and\nsecond, unscoped NP-interpretations are viewed as unscoped restricted\nquantifiers (Schubert & Pelletier 1982). Thus the unscoped LF\nabove would be knows(〈∃poem〉,\n〈∀person〉), and scoping of quantifiers, along with\ntheir restrictors, now involves “raising” quantifiers to\ntake scope over a sentential formula, with simultaneous introduction\nof variables. The two results corresponding to the two alternative\nscopings are then  (∃y: poem(y))(∀x:\nperson(x))knows(x, y),  \nand \n(∀x:\nperson(x))(∃y: poem(y))knows(x, y).  \nWhile this strategy departs\nfrom the strict compositionality of Montague Grammar, it achieves\nresults that are often satisfactory for the intended purposes and does\nso with minimal computational fuss. A related approach to logical form\nand scope ambiguity enjoying some current popularity is minimal\nrecursion semantics (MRS) (Copestake et al.\n2005), which goes even further in fragmenting the meaningful parts of\nan expression, with the goal of allowing incremental constraint-based\nassembly of these pieces into unambiguous sentential LFs. Another\ninteresting development is an approach based on\ncontinuations, a notion taken\nfrom programming language theory (where a\ncontinuation is a program execution state as determined by the steps\nstill to be executed after the current instruction). This also allows\nfor a uniform account of the meaning of quantifiers, and  provides\na handle on such phenomena as “misplaced modifiers”, as in \n“He had a quick cup of coffee” (Barker 2004). An important innovation in logical semantics was discourse\nrepresentation theory (DRT) (Kamp 1981; Heim 1982), aimed at a\nsystematic account of anaphora. In part, the goal was to provide a\nsemantic explanation for (in)accessibility of NPs as referents of\nanaphoric pronouns, e.g., in contrasting examples such as “John\ndoesn't drive a car; *he owns it,” vs.  “John drives a\ncar; he owns it”. More importantly, the goal was to account for\nthe puzzling semantics of sentences involving donkey anaphora, e.g.,\n\n“If John owns a donkey, he beats it.” Not only is the\nNP a donkey, the object of the if-clause, accessible as\nreferent of the anaphoric it, contrary to traditional\nsyntactic binding theory (based on the notion of C-command), but\nfurthermore we seem to obtain an interpretation of the type\n“John beats every donkey that he owns”, which cannot be\nobtained by “raising” the embedded indefinite a\ndonkey to take scope over the entire sentence. There is also a\nweaker reading of the type, “If John owns a donkey, he beats a\ndonkey that he owns”, and this reading also is not obtainable\nvia any scope analysis. Kamp and Heim proposed a dynamic process of\nsentence interpretation in which a discourse representation structure\n(DRS) is built up incrementally. A DRS consists of a set\nof discourse referents (variables) and a set of\nconditions, where these\nconditions may be simple predications or\nequations over discourse referents, or certain logical combinations of\nDRS's (not of conditions). The DRS for the sentence under consideration\ncan be written linearly as \nor diagrammed as Here x, y, u, v are discourse referents introduced\nby John, a donkey, he, and it, and the\nequations u=x, v=y represent the result of reference\nresolution for he and it. Discourse referents in the\nantecedent of a conditional are accessible in the consequent, and\ndiscourse referents in embedding DRSs are accessible in the embedded\nDRSs. Semantically, the most important idea is that discourse\nreferents are evaluated dynamically. We think of a variable assignment\nas a state, and this state changes as we evaluate a DRS\noutside-to-inside, left-to-right. For example (simplifying a bit), the\nconditional DRS in figure 4 is true (in a given model) if every\nassignment with domain {x,\ny} that makes the antecedent true can be extended to an\nassignment (new state) with domain\n{x, y, u, v} that makes the consequent\ntrue. On the face of it, DRT is noncompositional (though DRS construction\nrules are systematically associated with phrase structure rules); but\nit can be recast in compositional form, still of course with a dynamic\nsemantics. A closely related approach, dynamic predicate\nlogic (DPL) retains the classical quantificational\nsyntax, but in effect treats existential quantification as\nnondeterministic assignment, and provides an overtly compositional\nalternative to DRT (Groenendijk & Stokhof 1991). Perhaps\nsurprisingly, the impact of DRT on practical computational linguistics\nhas been quite limited, though it certainly has been and continues to\nbe actively employed in various projects. One reason may be that\ndonkey anaphora rarely occurs in the text corpora most intensively\ninvestigated by computational linguists so far (though it is arguably\npervasive  and extremely important in generic sentences and\ngeneric passages, including those found in lexicons or sources such as\nCommon Sense Open Mind—see sections\n 4.3 and 8.3).  Another reason\nis that reference resolution for non-donkey pronouns (and definite\nNPs) is readily handled by techniques such as Skolemization of\nexistentials, so that subsequently occurring anaphors can be\nidentified with the Skolem constants introduced earlier.  Indeed, it\nturns out that both explicit and implicit variants of Skolemization,\nincluding functional Skolemization, are possible even for donkey\nanaphora (e.g., in sentences such as “If every man has a gun, many\nwill use it”—see Schubert 2007). Finally, another reason for\nthe limited impact of DRT and other dynamic semantic theories may be\nprecisely that they are dynamic: The evaluation of a formula in\ngeneral requires its preceding and embedding context, and this\ninterferes with the kind of knowledge modularity (the ability to use\nany given knowledge item in a variety of different contexts) desirable\nfor inference purposes. Here it should be noted that straightforward\ntranslation procedures from DRT, DPL, and other dynamic theories to\nstatic logics exist (e.g., to FOL, for nonintensional versions of the\ndynamic approaches), but if such a conversion is desirable for\npractical purposes, then the question arises whether starting with a\ndynamic representation is at all advantageous. \nA long-standing issue in linguistic semantics has been the theoretical\nstatus of thematic roles in the  argument structure of verbs and\nother argument-taking elements of language (e.g., Dowty 1991). The\nsyntactically marked cases found in many languages correspond\nintuitively to such thematic roles as agent,\ntheme, patient, instrument,\nrecipient, goal, and so on, and in English, too, the sentence\nsubject\nand object typically correspond respectively to the agent and theme or\npatient of an action, and other roles may be added as an indirect\nobject or more often as prepositional phrase complements and adjuncts.\nTo give formal expression to these intuitions, many computational\nlinguists decompose verbal (and other) predicates derived from language\ninto a core predicate augmented with explicit binary relations\nrepresenting thematic roles. For example, the sentence might be represented (after referent determination) as where e is thought of as the kicking event. Such a\nrepresentation is called neo-Davidsonian, acknowledging\nDonald Davidson's advocacy of the view that verbs tacitly introduce\nexistentially quantified events (Davidson 1967a). The prefix\nneo- indicates that all arguments and adjuncts are\nrepresented in terms of thematic roles, which was not  part of\nDavidson's proposal but is developed, for example, in (Parsons\n1990). (Parsons attributes the idea of thematic roles to the 4th\ncentury BCE Sanskrit grammarian Pāṇini.) One\nadvantage of this style of representation is that it absolves the\nwriter of the interpretive rules from the vexing task of\ndistinguishing verb complements, to be incorporated into the argument\nstructure of the verb, from adjuncts, to be used to add modifying\ninformation. For example, it is unclear in (3.1) whether to the\nfence should be treated as supplying an argument of\nkick, or whether it merely modifies the action of John\nkicking the ball. Perhaps most linguists would judge the latter answer\nto be correct (because an object can be kicked without the intent of\npropelling it to a goal location), but intuitions are apt to be\nambivalent for at least one of a set of verbs such as dribble,\nkick, maneuver, move and transport. However, thematic roles also introduce new difficulties. As pointed out\nby Dowty (1991), thematic\nroles lack well-defined semantics. For\nexample, while (3.1) clearly involves an animate agent acting causally\nupon a physical object, and the PP evidently supplies a goal location,\nit is much less clear what the roles should be in (web-derived)\nsentences such as (3.2–3.4), and what semantic content they would\ncarry: As well, the uniform treatment of complements and adjuncts in terms\nof thematic relations does not absolve the computational linguist from\nthe task of identifying the subcategorized constituents of verb\nphrases (and similarly, NPs and APs), so as to guide syntactic and\nsemantic expectations in parsing and interpretation. And these\nsubcategorized constituents correspond closely to the complements of\nthe verb, as distinct from any adjuncts. Nevertheless, thematic role\nrepresentations are widely used, in part because they mesh well with\nframe-based knowledge representations for domain\nknowledge. These are representations that characterize a concept in\nterms of its type (relating this to supertypes and subtypes in an\ninheritance hierarchy), and a set of slots (also called\nattributes or roles) and corresponding values, with\ntype constraints on values. For example, in a purchasing domain, we\nmight have a purchase predicate, perhaps with supertype\nacquire, subtypes like\npurchase-in-installments,\npurchase-on-credit, or\npurchase-with-cash, and attributes with typed values\nsuch as (buyer (a person-or-group)), (seller (a\nperson-or-group)),  (item (a thing-or-service)), (price (a\nmonetary-amount)), and perhaps time, place, and other\nattributes. Thematic roles associated with relevant senses of verbs\nand nouns such as buy, sell, purchase, acquire, acquisition,\ntake-over, pick up, invest in, splurge on, etc., can easily be\nmapped to standard slots like those above. This  leads into the\nissue of canonicalization, which we briefly discuss below under a\nseparate heading. A more consequential issue in computational semantics has been the\nexpressivity of the semantic representation employed, with respect to\nphenomena such as event and temporal reference, nonstandard quantifiers\nsuch as most,  plurals,\nmodification, modality and other forms of\nintensionality, and reification. Full discussion of these phenomena\nwould be out of place here, but some commentary on each is warranted,\nsince the process of semantic interpretation and understanding (as well\nas generation) clearly depends on the expressive devices available in\nthe semantic representation. \nEvent and situation reference are essential in view of the fact that\nmany sentences seem to describe events or situations, and to qualify and\nrefer to them. For example, in the sentences the barking event is in effect predicated to have occurred last\nnight and to have lasted for several minutes, and the demonstrative\npronoun\nthis evidently refers directly\nto it; in addition the past tense places\nthe event at some point prior to the time of speech (and would do so\neven without the temporal adverbials). These temporal and causal\nrelations are readily handled within the Davidsonian (or\nneo-Davidsonian) framework mentioned above: However, examples (3.6) and (3.7) suggest that events can be\nintroduced by negated or quantified formulas, as was originally\nproposed by Reichenbach (1947): Barwise and Perry (1983) reconceptualized this idea in their\nSituation Semantics, though this lacks the tight coupling between\nsentences and events that is arguably needed to capture causal\nrelations expressed in language. Schubert (2000) proposes a solution\nto this problem in an extension of FOL incorporating an operator that\nconnects situations or events with sentences characterizing\nthem. Concerning nonstandard quantifiers such as most,\nwe have already sketched the generalized quantifier approach of\nMontague Grammar, and pointed out the alternative of using restricted\nquantifiers; an example might be (Most x:\ndog(x))friendly(x). Instead of\nviewing most as a second-order predicate, we can specify its\nsemantics by analogy with classical quantifiers: The sample formula is\ntrue (under a given interpretation) just in case a majority of\nindividuals satisfying\ndog(x) (when used as value of x) also\nsatisfy friendly(x). Quantifying determiners such\nas few, many, much, almost all, etc., can be treated\nsimilarly, though ultimately the problem of vagueness needs to be\naddressed as well (which of course extends beyond quantifiers to\npredicates and indeed all aspects of a formal semantic\nrepresentation).  Vague quantifiers, rather than setting rigid\nquantitative bounds, seem instead to convey probabilistic information,\nas if a somewhat unreliable measuring instrument had been applied in\nformulating the quantified claim, and the recipient of the information\nneeds to take this unreliability into account in updating\nbeliefs. Apart from their vagueness, the quantifiers under discussion\nare not first-order definable (e.g., Landman 1991), so that they\ncannot be completely axiomatized in FOL.  But this does not prevent\npractical  reasoning, either by direct use of such quantifiers in\nthe logical representations of sentences (an approach in the spirit\nof natural logic), or by reducing them to set-theoretic or\nmereological relations within an FOL framework. Plurals, as for instance in present a problem in that the argument of a predicate can be an\nentity comprised of multiple basic individuals (those we ordinarily\nquantify over, and ascribe properties to). Most approaches to this\nproblem employ a plural operator, say, plur, allowing us to\nmap a singular predicate P into a plural predicate\nplur(P), applicable to collective entities. These\ncollective entities are usually assumed to form a join semilattice\nwith atomic elements (singular entities) that are ordinary individuals\n(e.g., Scha 1981; Link 1983; Landman 1989, 2000). When an overlap\nrelation is assumed, and when all elements of the semilattice are\nassumed to have a supremum (completeness), the result is a complete\nBoolean algebra except for lack of a bottom element (because there is\nno null entity that is a part of all others). One theoretical issue is\nthe relationship of the semilattice of plural entities to the\nsemilattice of material parts of which entities are\nconstituted. Though there are differences in theoretical details\n(e.g., Link 1983; Bunt 1985), it is agreed that these semilattices\nshould be aligned in this sense: When we take the join\nof material parts of which several singular or plural entities are\nconstituted, we should obtain the material parts of the join of those\nsingular or plural entities. Note that while some verbal predicates,\nsuch as (intransitive) gather, are applicable only to\ncollections, others, such as ate a pizza, are variously\napplicable to individuals or collections. Consequently, a sentence\nsuch as allows for both a collective reading, where the children as a group\nate a single pizza, and a distributive reading, where each of the\nchildren ate a pizza (presumably a different one!)  One way of dealing\nwith such ambiguities in practice is to treat plural NPs as ambiguous\nbetween a collection-denoting reading and an “each member of the\ncollection” reading. For example the children in (3.9)\nwould be treated as ambiguous between the collection of\nchildren (which is the basic sense of the phrase) and each of\nthe children. This entails that a reading of type each of the people \nshould also be available in (3.8) — but we can assume that this is ruled out \nbecause (intransitive) gather requires a collective argument. \nIn a sentence such as we then obtain four readings, based on the two interpretations of\neach NP. No readings are ruled out, because both catching and being\ncaught can be individual or collective occurrences. Some theorists\nwould posit additional readings, but if these exist, they could be\nregarded as derivative from readings in which at least one of the\nterms is collectively interpreted. But what is uncontroversial is that\nplurals call for an enrichment in the semantic representation language\nto allow for collections as arguments. In an expression such\nas plur(child), both the plur operator,\nwhich transforms a predicate into another predicate, and the resulting\ncollective predicate, are of nonstandard types. Modification is a pervasive\nphenomenon in all languages, as illustrated\nin the following sentences: \nIn (3.11), very functions as a predicate modifier, in\nparticular a subsective modifier, since the set of things that\nare very(P) is a subset of the things that\nare P.  Do we need such modifiers in our logical forms? We\ncould avoid use of a modifier in this case by supposing\nthat smart has a tacit argument for the degree of smartness,\nwhere smart(x, d) means that x is smart to\ndegree d; adding that d > T for some\nthreshold T would signify that x is very smart.\nOther degree adjectives could be handled similarly. However, such a\nstrategy is unavailable for international celebrity in\n(3.12). International is again subsective (and not\nintersective—an international celebrity is not something that is\nboth international and a celebrity), and while one can imagine\ndefinitions of the particular combination, international\ncelebrity, in an ordinary FOL framework, requiring such\ndefinitions to be available for constructing initial logical forms\ncould create formidable barriers to broad-coverage\ninterpretation. (3.13) illustrates a third type of predicate\nmodification, namely VP-modification by an adverb. Note that the\nmodifier cannot plausibly be treated as an implicit\npredication utter(E) about a Davidsonian event\nargument of fail.  Taken together, the examples indicate the\ndesirability of allowing for monadic-predicate modifiers in a semantic\nrepresentation. Corroborative evidence is provided in the immediately\nfollowing discussion. \nIntensionality has already been mentioned in connection with\nMontague Grammar, and there can be no doubt that a semantic\nrepresentation for natural language needs to capture intensionality in\nsome way. The sentences all involve intensionality. The meaning (and thereby the truth\nvalue) of the attitudinal sentence (3.14) depends on the meaning\n(intension) of the subordinate clause, not just its truth value\n(extension). The meaning of (3.15) depends on the meaning of\nhappy, but does not require happy to be a property\nof John or anything else. The meaning of (3.16) does not depend on the\nactual existence of a starship, but does depend on the meaning of that\nphrase. And fake beard in (3.17) refers to something other\nthan an actual beard, though its meaning naturally depends on the\nmeaning of beard.  A Montagovian analysis certainly would\ndeal handily with such sentences. But again, we may ask how much of\nthe expressive richness of Montague's type theory is really essential\nfor computational linguistics. To begin with, sentences such as (3.14)\nare expressible in classical modal logics, without committing to\nhigher types. On the other hand (3.16) resists a classical modal\nanalysis, even more firmly than Montague's “John seeks a\nunicorn,” for which an approximate classical paraphrase is\npossible: “John tries (for him) to find a unicorn”. A\nmodest concession to Montague, sufficient to handle\n(3.15)–(3.17), is to admit intensional predicate modifiers into\nour representational vocabulary. We can then treat look as a\npredicate modifier, so that look(happy) is a new\npredicate derived from the meaning of happy. Similarly we can\ntreat design as a predicate modifier, if we are willing to\ntreat a starship as a predicative phrase, as we would in\n“The Enterprise is a starship”. And finally, fake\nis quite naturally viewed as a predicate modifier, though unlike most\nnominal modifiers, it is not intersective (#John wore something\nthat was a beard and was fake) or even subsective (#John wore\na particular kind of beard). Note that this form of\nintensionality does not commit us to a higher-order logic—we are\nnot quantifying over predicate extensions or intensions so far, only\nover individuals (aside from the need to allow for plural entities, as\nnoted). The rather compelling case for intensional predicate modifiers\nin our semantic vocabulary reinforces the case made above (on the\nbasis on extensional examples) for allowing predicate\nmodification. Reification, like the\nphenomena already enumerated, is also pervasive\nin natural languages. Examples are seen in the following sentences. (3.18)–(3.21) are all examples of predicate\nreification. Humankind in (3.18) may be regarded as the name\nof an abstract kind derived from the nominal predicate human,\ni.e., with lexical meaning K(human),\nwhere K maps predicate intensions to individuals. The status\nof abstract kinds as individuals is evidenced by the fact that the\npredicate “be on a path to self-destruction” applies as\nreadily to ordinary individuals as to kinds. The name-like character\nof the term is apparent from the fact that it cannot readily be\npremodified by an adjective. The subjects in (3.19) and (3.20) can be\nsimilarly analyzed in terms of kinds K(snow) and\nK(-ness(polite)). (Here -ness is a\npredicate modifier that transforms the predicate polite,\nwhich applies to ordinary (usually human) individuals, into a\npredicate over quantities of the abstract stuff, politeness.) But in\nthese cases the K operator does not originate in the lexicon,\nbut in a rule pair of type “NP →  N, NP′ =\nK(N′)”. This allows for modification of the nominal\npredicate before reification, in phrases such as fluffy snow\nor excessive politeness. The subject of (3.21) might be\nrendered logically as something like\nKa(-ly(reckless)(drive)), where\nKa reifies action-predicates, and -ly \ntransforms a monadic predicate intension into a subsective predicate\nmodifier. Finally (3.22) illustrates a type of sentential-meaning\nreification, again yielding a kind; but in this case it is a kind of\nsituation—the kind whose instances are characterized by John\nsulking. Here we can posit a reification operator Ke that\nmaps sentence intensions into kinds of situations. This type of\nsentential reification needs to be distinguished from\nthat-clause reification, such as appears to be involved in\n(3.14). We mentioned the possibility of a modal-logic analysis of\n(3.14), but a predicative analysis, where the predicate applies to a\nreified sentence intension (a proposition) is actually more plausible,\nsince it allows a uniform treatment of that-clauses in contexts like\n(3.14) and (3.23). The use of reification operators is a departure\nfrom a strict Montgovian approach, but is plausible if we seek to\nlimit the expressiveness of our semantic representation by taking\npredicates to be true or false of individuals, rather than of objects\nof arbitrarily high types, and likewise take quantification to be over\nindividuals in all cases, i.e., to be first-order. Some computational linguists and AI researchers wish to go much\nfurther in avoiding expressive devices outside those of standard\nfirst-order logic. One strategy that can be used to deal with\nintensionality within FOL is to functionalize all predicates, save one\nor two. For example, we can treat predications, such as that Romeo\nloves Juliet, as values of functions that “hold” at particular times:\nHolds(loves(Romeo, Juliet), t). Here\nloves is regarded as a function that yields a reified\nproperty, while Holds (or in some\nproposals, True), and perhaps equality, are the only\npredicates in the representation language. Then we can formalize\n(3.14), for example, without recourse to intensional semantics as (where t is some specific time). Humankind in\n(3.18) can perhaps be represented as the set of all humans as a\nfunction of time: (presupposing some axiomatization of naïve set theory); and, as\none more example, (4.22) might be rendered as \n(for some specific time t).  However, a difficulty with this\nstrategy is encountered for quantification within intensional\ncontexts, as in the sentence “John believes that every galaxy\nharbors some life-form.” While we can represent the\n(implausible) wide-scope reading “For every galaxy there is some\nlife-form such that John believes that the galaxy harbors that\nlife-form,” using the Holds strategy, we cannot readily\nrepresent the natural narrow-scope reading because FOL disallows\nvariable-binding operators within functional terms (but see McCarthy\n1990). An entirely different approach is to\nintroduce “eventuality” arguments into all predicates, and\nto regard a predication as providing a fact about the actual world\nonly if the eventuality corresponding to that predication has been\nasserted to “occur” (Hobbs 2003). The main practical\nimpetus behind such approaches is to be able to exploit existing FOL\ninference techniques and technology. However, there is at present no\nreason to believe that any inferences that are easy in FOL are\ndifficult in a meaning representation more nearly aligned with the\nstructure of natural language; on the contrary, recent work in\nimplementing natural logic (MacCartney & Manning 2009)\nsuggests that a large class of obvious inferences can be most readily\nimplemented in syntactically analyzed natural language\n(modulo some adjustments)—a framework closer to\nMontagovian semantics than an FOL-based approach. Another important issue has been canonicalization\n(or normalization):\nWhat transformations should be applied to initial\nlogical forms in\norder to minimize difficulties in making use of linguistically derived\ninformation? The uses that should be facilitated by the choice of\ncanonical representation include the interpretation of further texts in\nthe context of previously interpreted text (and general knowledge), as\nwell as inferential question answering and other inference tasks. We can distinguish two types of canonicalization: logical\nnormalization and conceptual canonicalization.  An\nexample of logical normalization in sentential logic and FOL is the\nconversion to clause form (Skolemized, quantifier-free conjunctive\nnormal form). The rationale is that reducing multiple logically\nequivalent formulas to a single form reduces the combinatorial\ncomplexity of inference. However, full normalization may not be\npossible in an intensional logic with a “fine-grained”\nsemantics, where for instance a belief that the Earth is round may\ndiffer semantically from the belief that the Earth is round and the\nMoon is either flat or not flat, despite the logical equivalence of\nthose beliefs. Conceptual canonicalization involves more radical changes:\nWe replace the surface predicates (and perhaps other elements of the\nrepresentational vocabulary) with canonical terms from a smaller\nrepertoire, and/or decompose them using thematic roles or frame slots.\nFor example, in a geographic domain, we might replace the relations\n(between countries) is next to, is adjacent to, borders on, is a\nneighbor of, shares a border with, etc., with a single canonical\nrelation, say borders-on. In the domain of physical,\ncommunicative, and mental events, we might go further and decompose\npredicates into configurations of primitive predicates. For example,\nwe might express “x walks” in the manner of Schank\nas \n∃e, e′(ptrans(e, x, x) ∧\nmove(e′, x, feet-of(x))\n∧ by-means-of(e′, e)), \nwhere ptrans(e, x, y) is a primitive\npredicate expressing that event e is a physical transport by\nagent x of object y, move\nexpresses bodily motion by an agent, and\nby-means-of expresses the instrumental-action\nrelation between the move\nevent and the ptrans event.\nAs discussed earlier, these multi-argument predicates might be further\ndecomposed, with ptrans(e, x, y) rewritten as\nptrans(e)\n∧ agent(e, x)\n∧ theme(e, y), and so on. As in\nthe case of logical normalization, conceptual canonicalization is\nintended to simplify inference, and to minimize the need for the\naxioms on which inference is based. A question raised by canonicalization, especially by the stronger\nversions involving reduction to primitives, is whether significant\nmeaning is lost in this process. For example, the concept of being\nneighboring countries, unlike mere adjacency, suggests the idea of\nside-by-side existence of the populations of the countries, in a way\nthat resembles the side-by-side existence of neighbors in a local\ncommunity. More starkly, reducing the notion of walking to transporting\noneself by moving one's feet fails to distinguish walking from running,\nhopping, skating, and perhaps even bicycling. Therefore it may be\npreferable to regard conceptual canonicalization as inference of\nimportant entailments, rather than as replacement of superficial\nlogical forms by equivalent ones in a more restricted vocabulary.\nAnother argument for the latter position is computational: If we\ndecompose complex actions, such as dining at a restaurant, into\nconstellations of primitive predications, we will need to match the\nmany primitive parts of such constellations even in answering simple\nquestions such as \n“Did John dine at a restaurant?”. We will comment\nfurther on primitives in the context of the following subsection. \nWhile many AI researchers have been interested in semantic\nrepresentation and inference as practical means for achieving\nlinguistic and inferential competence in machines, others have\napproached these issues from the perspective of modeling human\ncognition. Prior to the 1980s, computational modeling of NLP and\ncognition more broadly were pursued almost exclusively within a\nrepresentationalist paradigm, i.e.,\none that regarded all intelligent\nbehavior as reducible to symbol manipulation (Newell and Simon's\nphysical symbol systems hypothesis). In the 1980s, connectionist (or\nneural) models enjoyed a resurgence, and came to be seen by many as\nrivalling representationalist approaches. We briefly summarize these\ndevelopments under two subheadings below.  “A physical symbol system has the necessary and sufficient means\nfor general intelligent action.”\n–Allen Newell and Herbert Simon\n(1976: 116) \nSome of the cognitively motivated researchers working within a\nrepresentationalist paradigm have been particularly concerned with\ncognitive architecture, including the associative linkages\nbetween concepts, distinctions between types of memories and types of\nrepresentations (e.g., episodic vs. semantic memory,\nshort-term vs.  long-term memory, declarative vs. procedural\nknowledge), and the observable processing consequences of such\narchitectures, such as sense disambiguation, similarity judgments,\nand cognitive load as reflected in processing delays. Others have been\nmore concerned with uncovering the actual internal conceptual\nvocabulary and inference rules that seem to underlie language and\nthought. M. Ross Quillian's semantic memory model, and models\ndeveloped by Rumelhart, Norman and Lindsay (Rumelhart et al. 1972;\nNorman et al.  1975) and by Anderson and Bower (1973) are\nrepresentative of the former perspective, while Schank and his\ncollaborators (Schank and Colby 1973; Schank and Abelson 1977; Schank\nand Riesbeck 1981; Dyer 1983) are representative of the latter. A\ncommon thread in cognitively motivated theorizing about semantic\nrepresentation has been the use of graphical semantic memory\nmodels,  intended to capture direct relations as well as more\nindirect associations between concepts, as illustrated in Figure 3: This particular example is loosely based on Quillian\n(1968). Quillian suggested that one of the functions of semantic\nmemory, conceived in this graphical way, was to enable word sense\ndisambiguation through spreading activation. For example, processing\nof the sentence, \n“He watered the plants”, would involve\nactivation of the terms water and\nplant, and this activation would spread to concepts\nimmediately associated with (i.e., directly linked to) those terms,\nand in turn to the neighbors of those concepts, and so on. The\npreferred senses of the initially activated terms would be those that\nled to early “intersection” of activation signals originating from\ndifferent terms.  In particular, the activation signals propagating\nfrom sense 1 (the living-plant sense) of plant  would\nreach the concept for the stuff, water, in four steps (along\nthe pathways corresponding to the information that plants may get food\nfrom water), and the same concept would be reached in two steps from\nthe term water, used as a verb, whose semantic representation\nwould express the idea of supplying water to some target\nobject. Though the sense of plant  as a manufacturing\napparatus would probably lead eventually to the water concept\nas well, the corresponding activation path would be longer, and so the\nliving-plant sense of plant would “win”. Such conceptual representations have tended to differ from logical ones\nin several respects. One, as already discussed, has been the emphasis\nby Schank and various other researchers (e.g.,\nWilks 1978; Jackendoff\n1990) on “deep” (canonical) representations and primitives. An often\ncited psychological argument for primitives is the fact that people\nrather quickly forget the exact wording of what they read or are told,\nrecalling only the “gist”; it is this gist that primitive decomposition\nis intended to derive. However, this involves a questionable assumption\nthat subtle distinctions between, say, walking to the park,\nambling to the park, or\ntraipsing to the park are simply ignored\nin the interpretive process, and as noted earlier it neglects the\npossibility that seemingly insignificant semantic details are\npruned from memory after a short time, while major entailments are\nretained for a longer time. \nAnother common strain in much of the theorizing about conceptual\nrepresentation has been a certain diffidence concerning logical\nrepresentations and denotational semantics. The relevant semantics of\nlanguage is said to be the transduction from linguistic utterances to\ninternal representations, and the relevant semantics of the internal\nrepresentations is said to be the way they are deployed in\nunderstanding and thought. For both the external language and the\ninternal (mentalese) representation, it is said to be irrelevant\nwhether or not the semantic framework provides formal truth conditions\nfor them. The rejection of logical semantics has sometimes been\nsummarized in the dictum that one cannot compute with possible worlds. However, it seems that any perceived conflict between conceptual\nsemantics and logical semantics can be resolved by noting that these\ntwo brands of semantics are quite different enterprises with quite\ndifferent purposes. Certainly it is entirely appropriate for conceptual\nsemantics to focus on the mapping from language to symbolic structures\n(in the head, realized ultimately in terms of neural assemblies or\ncircuits of some sort), and on the functioning of these structures in\nunderstanding and thought. But logical semantics, as well, has a\nlegitimate role to play, both in considering how words (and larger\nlinguistic expressions) relate to the world and how the symbols and\nexpressions of the internal semantic representation relate to the\nworld. This role is metatheoretic\nin that the goal is not to\nposit cognitive entities that can be computationally manipulated, but\nrather to provide a framework for theorizing about the relationship\nbetween the symbols people use, externally in language and internally\nin their thinking, and the world in which they live. It is surely\nundeniable that utterances are at least sometimes intended to be\nunderstood as claims about  things, properties, and relationships\nin the world, and as such are at least sometimes true or false. It would be hard\nto understand how language and thought could have evolved as useful\nmeans for coping with the world, if they were incapable of capturing\ntruths about  it. \nMoreover, logical semantics shows how certain syntactic manipulations\nlead from truths to truths regardless of the specific meanings of the\nsymbols involved in these manipulations (and these notions can be\nextended to uncertain inference, though this remains only very\npartially understood). Thus, logical semantics provides a basis for\nassessing the soundness (or otherwise) of inference rules. While human\nreasoning as well as reasoning in practical AI systems often needs to\nresort to unsound methods (abduction, default reasoning, Bayesian\ninference, analogy, etc.), logical semantics nevertheless provides an\nessential perspective from which to classify and study the properties\nof such methods. A strong indication that cognitively motivated\nconceptual representations of language are reconcilable with logically\nmotivated ones is the fact that all proposed conceptual representations\nhave either borrowed deliberately from logic in the first place (in\ntheir use of predication, connectives, set-theoretic notions, and\nsometimes quantifiers) or can be transformed to logical representations\nwithout much difficulty, despite being cognitively motivated. As noted earlier, the 1980s saw the re-emergence of connectionist\ncomputational models within mainstream cognitive science theory (e.g.,\nFeldman and Ballard 1982; Rumelhart and McClelland 1986; Gluck and\nRumelhart 1990). We have already briefly characterized connectionist\nmodels in our discussion of connectionist parsing. But the\nconnectionist paradigm was viewed as applicable not only to specialized\nfunctions, but to a broad range of cognitive tasks including\nrecognizing objects in an image,  recognizing speech,\nunderstanding language, making inferences, and guiding physical\nbehavior. The emphasis was on learning, realized by adjusting the\nweights of the unit-to-unit connections in a layered neural network,\ntypically by a back-propagation\nprocess that distributes credit or\nblame for a successful or unsuccessful output to the units involved in\nproducing the output (Rumelhart and McClelland 1986). From one perspective, the renewal of interest in connectionism and\nneural modeling was a natural step in the endeavor to elaborate\nabstract notions of cognitive content and functioning to the point\nwhere they can make testable contact with brain theory and\nneuroscience. But it can also be seen as a paradigm shift, to the\nextent that the focus on subsymbolic processing began to be linked to a\ngrowing skepticism concerning higher-level symbolic processing as\nmodels of mind, of the sort associated with earlier semantic\nnetwork-based and rule-based architectures. For example, Ramsay et al.\n(1991)  argued that the demonstrated capacity of connectionist\nmodels to perform cognitively interesting tasks undermined the\nthen-prevailing view of the mind as a physical symbol system. But\nothers have continued to defend the essential role of symbolic\nprocessing. For example, Anderson (1983, 1993) contended that while\ntheories of symbolic thought need to be grounded in neurally plausible\nprocessing, and while subsymbolic processes are well-suited for\nexploiting the statistical structure of the environment, nevertheless\nunderstanding the interaction of these subsymbolic processes required a\ntheory of representation and behavior at the symbolic level. What would it mean for the semantic content of an utterance to be\nrepresented in a neural network, enabling, for example, inferential\nquestion-answering? The anti-representationalist (or “eliminativist”)\nview would be that no particular structures can be or need to be\nidentified as encoding semantic content. The input modifies the\nactivity of the network and the strengths of various connections in a\ndistributed way, such that the subsequent behavior of the network\neffectively implements inferential question-answering. However, this\nleaves entirely open how a network would learn this sort of behavior.\nThe most successful neural net experiments have been aimed at mapping\ninput patterns to class labels or to other very restricted sets of\noutputs, and they have required numerous labeled examples (e.g.,\nthousands of images labeled with the class of the objects depicted) to\nlearn their task. By contrast, humans excel at “one-shot”\nlearning, and can perform complex tasks based on such learning.  A less radical alternative to the eliminativist position, termed the\nsubsymbolic hypothesis, was\nproposed by Smolensky (1988), to the effect\nthat mental processing cannot be fully and accurately described in\nterms of symbol manipulation, requiring instead a description at the\nlevel of subsymbolic features, where these features are represented in\na distributed way in the network. Such a view does not preclude the\npossibility that\nassemblies of units in a connectionist system do in fact encode symbols\nand more complex entities built out of symbols, such as predications\nand rules. It merely denies that the behavior engendered by these\nassemblies can be adequately modelled as symbol manipulation.\nIn fact, much of the neural net research over the past two or three\ndecades has sought to understand how neural nets can encode symbolic\ninformation (e.g., see Smolensky et al.\n1992; Browne and Sun 2001).  Distributed schemes associate a set of units and their activation\nstates with particular symbols or values. For example, Feldman (2006)\nproposes that concepts are represented by the activity of a cluster of\nneurons; triples of such clusters representing a concept, a role, and a\nfiller (value) are linked together by triangle\nnodes to represent\nsimple attributes of objects. Language understanding is treated as a\nkind of simulation that maps language onto a more concrete domain of\nphysical action or experience, guided by background knowledge in the\nform of a temporal Bayesian network. Global schemes encode symbols in overlapping fashion over all units.\nOne possible global scheme is to view the activation states of the\nunits, with each unit generating a real value between −1 and 1, as\npropositions:  State p\nentails state q\n(equivalently, p is at\nleast as specific as q) if\nthe activation qi of each unit i\nin state q satisfies pi\n≤ qi ≤ 0, or\nqi = 0, or 0 ≤ qi ≤\npi depending on whether the\nactivation pi of that unit in state\np is negative, zero, or positive respectively. Propositional\nsymbols can then be interpreted in terms of such states, and truth\nfunctions in terms of simple max-min operations and sign inversions\nperformed on network states. (See Blutner, 2004; however, Blutner\nultimately focuses on a localist scheme in which units represent\natomic propositions and connections represent biconditionals.)\nHolographic neural network schemes (e.g., Manger et al. 1994;\nPlate 2003) can also be viewed as global; in the simplest cases these\nuse one “giant neuron” that multiplies an input vector whose\ncomponents are complex numbers by a complex-valued matrix; a component\nof the resultant complex-valued output vector, written in polar\ncoordinates as reiθ,\nsupplies a classification through the value of θ and a\nconfidence level through the value of r. A distinctive\ncharacteristic of such networks is their ability to classify or\nreconstruct patterns from partial or noisy inputs. The status of the subsymbolic hypothesis remains an issue for debate\nand further research. Certainly it is unclear how symbolic approaches\ncould match certain characteristics of neural network approaches, such\nas their ability to cope with novel instances and their graceful\ndegradation in the face of errors or omissions. On the other hand, some\nneural network architectures for storing knowledge and performing\ninference have been shown (or designed) to be closely related to “soft\nlogics” such as fuzzy logic (e.g.,\nKasabov 1996; Kecman 2001) or\n“weight-annotated Poole systems” (Blutner 2004), suggesting the\npossibility that neural network models of cognition may ultimately be\ncharacterizable as implementations of such soft logics. Researchers\nmore concerned with practical advances than biologically plausible\nmodeling have also explored the possibility of hybridizing the\nsymbolic and subsymbolic approaches, in order to gain the advantages of\nboth (e.g., Sun 2001). A quite\nformal example of this, drawing on ideas\nby Dov Gabbay, is d'Avila Garcez (2004). Finally, we should comment on the view expressed in some of the\ncognitive science literature that mental representations of language\nare primarily imagistic (e.g., Damasio 1994; Humphrey 1992). Certainly\nthere is ample evidence for the reality and significance of mental\nimagery (Johnson-Laird 1983; Kosslyn 1994). Also creative thought\noften seems to rely on visualization, as observed early in the 20th\ncentury by Poincaré (1913) and Hadamard (1945).  But as was\npreviously noted, symbolic and imagistic representations may well\ncoexist and interact synergistically. Moreover, cognitive scientists\nwho explore the human language faculty in detail, such as Steven\nPinker (1994, 2007) or any of the representationalist or connectionist\nresearchers cited above, all seem to reach the conclusion that the\ncontent derived from language (and the stuff of thought itself) is in\nlarge part symbolic—except in the case of the eliminativists\nwho deny representations altogether. It is not hard to see, however,\nhow raw intuition might lead to the meanings-as-images hypothesis. It\nappears that vivid consciousness is associated mainly with\nthe visual cortex, especially area V1, which is also crucially\ninvolved in mental imagery (e.g., Baars 1997: chapter 6). Consequently\nit is entirely possible that vast amounts of non-imagistic encoding\nand processing of language go unnoticed, while any evoked imagistic\nartifacts become part of our conscious experience. Further, the very\nact of introspecting on what sort of imagery, if any, is evoked by a\ngiven sentence may promote construction of imagery and awareness\nthereof. \nIn its broadest sense, statistical semantics is concerned with\nsemantic properties of words, phrases, sentences, and texts, engendered\nby their distributional characteristics in large text corpora. For\nexample, terms such as cheerful, exuberant, and depressed\nmay be\nconsidered semantically similar to the extent that they tend to occur\nflanked by the same (or in turn similar) nearby words. (For some\npurposes, such as information retrieval, identifying labels of\ndocuments may be used as occurrence contexts.) Through careful\ndistinctions among various occurrence contexts, it  may also be\npossible to factor similarity into more specific relations such as\nsynonymy, entailment, and antonymy. One basic difference between\n(standard) logical semantic relations and relations based on\ndistributional similarity is that the latter are a matter of degree.\nFurther, the underlying abstractions are very different, in that\nstatistical semantics does not relate strings to the world, but only to\ntheir contexts of occurrence (a notion similar to, but narrower than,\nWittgenstein's notion of meaning as use). However, statistical\nsemantics does admit elegant formalizations. Various concepts of\nsimilarity and other semantic relations can be captured in terms of\nvector algebra, by viewing the occurrence frequencies of an expression\nas values of the components of a vector, with the components\ncorresponding to the distinct contexts of occurrence. In this way, one\narrives at a notion of semantics based on metrics and operators in\nvector spaces, where vector operators can mimic Boolean operators in\nvarious ways (Gärdenfors 2000; Widdows 2004; Clarke 2012). But how does this bear on meaning representation of natural language\nsentences and texts? In essence, the representation  of sentences\nin statistical semantics consists of the sentences themselves. The idea\nthat sentences can be used directly, in conjunction with distributional\nknowledge, as objects enabling inference is a rather recent and\nsurprising one, though it was foreshadowed by many years of work on\nquestion answering based on large text corpora. The idea has gained\ntraction as a result of recent efforts to devise statistically based\nalgorithms for determining textual\nentailment, a program pushed forward\nby a series of Recognizing Textual Entailment (RTE) Challenges\ninitiated in 2005, organized by the PASCAL Network of Excellence, and\nmore recently by the National Institute of Standards and Technology\n(NIST). Recognizing textual entailment requires judgments as to whether\none given linguistic string entails a second one, in a sense of\nentailment that accords with human intuitions about what a person would\nnaturally infer (with reliance on knowledge about word meanings,\ngeneral knowledge such as that any person who works for a branch of a\ncompany also works for that company, and occasional well-known specific\nfacts). For example, \n“John is a fluent French speaker” textually\nentails \n“John speaks French”,\nwhile \n“The gastronomic capital of France is Lyon” does not entail that \n“The capital of France is Lyon”. Some\nexamples are intermediate; e.g.,\n\n“John was born in France” is\nconsidered to heighten the probability that John speaks French, without\nfully entailing it (Glickman and Dagan 2005). Initial results in the\nannual competitions were poor (not far above the random guessing mark),\nbut have steadily improved, particularly with the injection of some\nreasoning based on ontologies and on some general axioms about the\nmeanings of words, word classes,  relations, and phrasal patterns\n(e.g., de Salvo Braz et al. 2005). It is noteworthy that the conception of sentences as meaning\nrepresentations echoes Montague's contention that language is logic. Of\ncourse, Montague understood “sentences” as unambiguous syntactic trees.\nBut research in textual entailment seems to be moving towards a similar\nconception, as exemplified in the work of Dagan et al. (2008),\nwhere statistical entailment relations are based on syntactic trees,\nand these are generalized to templates that may replace subtrees by\ntyped variables. Also Clarke (2012) proposes a very general\nvector-algebraic framework for statistical semantics, where “contexts”\nfor sentences might include (multiple) parses and even (multiple)\nlogical forms for the sentences, and where statistical sentence\nmeanings can be built up compositionally from their proper parts. One\nway of construing degrees of entailment in this framework is in terms\nof the entailment probabilities relating each possible logical form of\nthe premise sentence to each possible logical form of the hypothesis in\nquestion. \nHaving surveyed three rather different brands of semantics, we are\nleft with the question of which of these brands serves best in\ncomputational linguistic practice. It should be clear from what has\nbeen said above that the choice of semantic “tool” will depend on the\ncomputational goals of the practitioner. If the goal, for example, is\nto create a dialogue-based problem-solving system for circuit fault\ndiagnosis, emergency response, medical contingencies, or vacation\nplanning, then an approach based on logical (or at least symbolic)\nrepresentations of the dialogue, underlying intentions, and relevant\nconstraints and knowledge is at present the only viable option. Here it\nis of less importance whether the symbolic representations are based on\nsome presumed logical semantics for language, or some theory of mental\nrepresentation—as long as they are representations that can be\nreasoned with. The most important limitations that disqualify\nsubsymbolic and statistical representations of meaning for such\npurposes are their very limited inferential reach and response\ncapabilities. They provide classifications or one-shot inferences\nrather than reasoning chains, and they do not generate plans,\njustifications, or extended linguistic responses. However, both neural\nnet techniques and statistical techniques can help to improve semantic\nprocessing in dialogue systems, for example by disambiguating word\nsenses,  or recognizing which of several standard plans is being\nproposed or followed, on the basis of observed utterances or actions. On the other hand, if the computational goal is to demonstrate\nhuman-like performance in a biologically plausible (or biologically\nvalid!) model of some form of language-related behavior, such as\nlearning to apply words correctly to perceived objects or\nrelationships, or learning to judge concept similarity, or to assess\nthe tone (underlying sentiment) of a discourse segment, then symbolic\nrepresentations need not play any role in the computational modeling.\n(However, to the extent that language is symbolic, and is a cognitive\nphenomenon, subsymbolic theories must ultimately explain how language\ncan come about.) In the case of statistical semantics, practical\napplications such as question-answering based on large textual\nresources, retrieval of documents relevant to a query, or machine\ntranslation are at present greatly superior to logical systems that\nattempt to fully understand both the query or text they are confronted\nwith and the knowledge they bring to bear on the task. But some of the\ntrends pointed out above in trying to link subsymbolic and statistical\nrepresentations with symbolic ones indicate that a gradual convergence\nof the various approaches to semantics is taking place.\n \nFor the next few paragraphs, we shall take semantic interpretation\nto refer to the process of deriving meaning representations from a word\nstream, taking for granted the operation of a prior or concurrent\nparsing phase. In other words, we are mapping syntactic trees to\nlogical forms (or whatever our meaning representation may be). Thus,\nunlike interpretation in the sense of assigning external denotations to\nsymbols, this is a form of “syntactic semantics” (Rapaport 1995). In the heyday of the proceduralist paradigm, semantic interpretation\nwas typically accomplished with sets of rules that matched patterns to\nparts of syntactic trees and added to or otherwise modified the\nsemantic representations of input sentences. The completed\nrepresentations might either express facts to be remembered, or might\nthemselves be executable commands, such as formal queries to a database\nor high-level instructions placing one block on another in a robot's\n(simulated or real) world. When it became clear in the early 1980s, however, that syntactic\ntrees could be mapped to semantic representations by using\ncompositional semantic rules associated with phrase structure rules in\none-to-one fashion, this approach became broadly favored over pure\nproceduralist ones. In our earlier discussion in section 3.1) of\nmeaning representations within logicist frameworks, we already\nforeshadowed the essentials of logical form computation. There we saw\nsample interpretive rules for a small number of phrase structure rules\nand vocabulary. The semantic rules, such as NP′\n= Det′(N′), clearly indicate how logical\nforms of lower-level constituents should be combined to yield those of\nhigher-level constituents. In the following figure, the sentence\n\n“Thetis loves a mortal” has been interpreted by applying the\nearlier set of lexical and interpretive rules to the nodes of the\nphrase structure tree in a bottom-up, left-to-right sweep:\n The interpretive rules are repeated at the tree nodes from \nsection 3.1, and the result of applying\nthe combinatory rules (with lambda-conversions where possible) are\nshown as well. As can be seen, the Montagovian treatment of NPs as\nsecond-order predicates leads to some complications, and these are\nexacerbated when we try to take account of quantifier scope\nambiguity. We mentioned Montague's use of multiple parses, the\nCooper-storage approach, and the unscoped-quantifier approach to this\nissue in section 3.1. In the\nunscoped-quantifier approach, with a relational interpretation of the\nverb, the respective interpretations of the leaf nodes (words) in\nFigure 4 would become Thetis,\nλyλx(loves(x, y),\nλP <〈∃P〉>),\nand mortal, and S′ at the root would\nbecome loves(Thetis,\n〈∃mortal〉), to be scoped uniquely to\n(∃x: mortal(x)\nloves(Thetis, x)). It is easy to see that multiple\nunscoped quantifiers will give rise to multiple permutations of\nquantifier order when the quantifiers are brought to the sentence\nlevel. Thus we will have multiple readings in sentences such as\n“Every man loves a certain woman”.\n \nAt this point we should pause to consider some interpretive methods\nthat do not conform with the above very common but not universally\nemployed syntax-driven approach. First, Schank and his collaborators\nemphasized the role of lexical knowledge, especially primitive actions\nused in verb decomposition, and knowledge about stereotyped patterns\nof behavior  in the interpretive process, nearly to the exclusion\nof syntax. For example, a sentence beginning \n“John went …” would lead to the generation of\na ptrans conceptualization (since go is lexically\ninterpreted in terms of ptrans), where John fills the\nagent role and where a phrase interpretable as a location is expected,\nas part of the configuration of roles that attach to\na ptrans act. If the sentence then continues as\n“… to the restaurant”, the expectation is\nconfirmed, and at this point instantiation of a restaurant script is\ntriggered, creating expectations about the likely sequence of actions\nby John and other agents in the restaurant (e.g., Schank and Abelson\n1977). These ideas had considerable appeal, and led to unprecedented\nsuccesses in machine understanding of some paragraph-length stories.\nAnother approach to interpretation that subordinates syntax to\nsemantics is one that employs domain-specific semantic grammars (Brown\nand Burton 1975). While these resemble context-free syntactic grammars\n(perhaps procedurally implemented in ATN-like manner), their\nconstituents are chosen to be meaningful in the chosen application\ndomain. For example, an electronics tutoring system might employ\ncategories such as measurement, hypothesis,\nor transistor instead of NP, and fault-specification\nor voltage-specification instead of VP.  The importance of\nthese approaches lay in their recognition of the fact that knowledge\npowerfully shapes our  ultimate interpretation of text and\ndialogue, enabling understanding even in the presence of noisy,\nflawed, and partial linguistic input. Nonetheless, most of the \nNL understanding community since the 1970s has treated syntactic\nparsing as an important aspect of the understanding process, in part\nbecause modularization of this complex process is thought to be\ncrucial for scalability, and in part because of the very plausible\nChomskian contention that human syntactic intuitions operate reliably\neven in the absence of clear meaning, as in his famous sentence\n“Colorless green ideas sleep furiously”. Statistical NLP has only recently begun to be concerned with\nderiving interpretations usable for inference and question answering\n(and as pointed out in the previous subsection, some of the literature\nin this area assumes that the NL text itself can and should be used\nas the basis for inference). However, there have been some noteworthy\nefforts to build statistical semantic parsers that learn to produce\nLFs after training on a corpus of LF-annotated sentences, or a corpus\nof questions and answers (or other exchanges) where the learning is\n“grounded” in a database or other supplementary models. We\nwill mention examples of this type of work, and comment on its\nprospects, in section 8. \nWe noted earlier that language is potentially ambiguous at all\nlevels of syntactic structure, and the same is true of semantic\ncontent, even for syntactically unambiguous words, phrases and\nsentences. For example, words like bank,\nrecover, and cool have\nmultiple meanings even as members of the same lexical category; nominal\ncompounds such as ice bucket, ice\nsculpture, olive oil, or baby\noil  leave unspecified the underlying relation between the\nnominals (such as constituency or purpose). At the sentential level,\neven for a determinate parse there may be quantifier scope ambiguities\n(“Every man admires a certain woman”—Rosa Parks vs. his mother); and habitual and generic\nsentences often involve temporal/atemporal ambiguities\n(“Racehorses are (often) skittish”), among others. Many techniques have been proposed for dealing with the various\nsorts of semantic ambiguities, ranging from psychologically\nmotivated principles, to knowledge-based methods,\nheuristics, and statistical approaches. Psychologically\nmotivated principles are exemplified by Quillian's spreading\nactivation model (described earlier) and the use of selectional\npreferences in word sense disambiguation. For example,\nin \n“The job took five hours,” took might be\ndisambiguated to the sense of taking up time because that sense of the\nverb prefers a temporal complement, and job might be\ndisambiguated to task (rather than, say, occupation)\nbecause of the direct associative link between the concept of a task\nand its time demands. Examples of knowledge-based disambiguation would\nbe the disambiguation of ice sculpture to a constitutive\nrelation based on the knowledge that sculptures may be carved or\nconstructed from solid materials, or the disambiguation of a man\nwith a hat to a wearing-relation based on the knowledge\nthat a hat is normally worn on the head. (The possible meanings may\nfirst be narrowed down using heuristics concerning the limited types\nof relations typically indicated by nominal compounding or by\nwith-modification.)  Heuristic principles used in scope\ndisambiguation include island constraints (quantifiers such\nas every and most cannot expand their scope beyond\ntheir local clause) and differing wide-scoping tendencies for\ndifferent quantifiers (e.g., each is apt to assume wider\nscope than some).  Statistical approaches typically extract\nvarious features in the vicinity of an ambiguous word or phrase that\nare thought to influence the choice to be made, and then make that\nchoice with a classifier that has been trained on an annotated text\ncorpus. The features used might be particular nearby words or their\nparts of speech or semantic categories, syntactic dependency\nrelations, morphological features, etc..  Such techniques have\nthe advantage of learnability and robustness, but ultimately will\nrequire supplementation with knowledge-based techniques. For example,\nthe correct scoping of quantifiers in contrasting sentence pairs such\nas seems to depend on world knowledge in\na way unlikely to be captured as a word-level statistical regularity. Habitual and generic sentences present particularly challenging\ndisambiguation problems, as they may involve temporal/atemporal\nambiguities (as noted), and in addition may require augmentation with\nquantifying adverbs and constraints on quantificational domains\nmissing from the surface form.  For example, without the quantifying adverb often is unambiguously\natemporal, ascribing enduring skittishness to purebred racehorses\nin general. (Thus in general appears to be the\nimplicit default adverbial.) But when the quantifying adverb is\npresent, the sentence admits both an atemporal reading according to\nwhich many purebred racehorses are characteristically skittish, as\nwell as a temporal reading to the effect that purebred racehorses\nin general are subject to frequent episodes of\nskittishness. If we replace purebred by at the starting\ngate,  then only the episodic reading of skittish\nremains available, while often may quantify over racehorses,\nimplying that many are habitually skittish at the starting gate, or it\nmay quantify over starting-gate situations, implying that racehorses\nin general are often skittish in such situations; furthermore, making\nformal sense of the phrase at the starting gate evidently\ndepends on knowledge about horse racing scenarios.  The interpretive challenges presented by such sentences are (or should\nbe) of great concern in computational linguistics, since much of\npeople's general knowledge about the world is most naturally expressed\nin the form of generic and habitual sentences. Systematic\nways of interpreting and disambiguating such sentences would\nimmediately provide a way of funneling large amounts of knowledge into\nformal knowledge bases from sources such as lexicons, encyclopedias,\nand crowd-sourced collections of generic claims such as those in Open\nMind Common Sense (e.g., Singh\net al. 2002; Lieberman et al. 2004; Havasi et\nal. 2007). Many theorists assume that the logical forms of such\nsentences should be tripartite structures with a quantifier that\nquantifies over objects or situations, a restrictor that limits the\nquantificational domain, and a nuclear scope (main clause) that makes\nan assertion about the elements of the domain (e.g., see Carlson\n2011; Cohen 2002; or Carlson & Pelletier\n1995).  The challenge lies in specifying a mapping from surface\nstructure to such a logical form. While many of the principles\nunderlying the ambiguities illustrated above are reasonably well\nunderstood, general interpretive algorithms are still lacking. It\nappears that such algorithms will involve stepwise elaboration of an\ninitially incomplete, ambiguous logical form, rather than a\nstraightforward syntax-semantics transduction, since the features on\nwhich the correct formalization depends transcend syntax: They include\nones such as Carlson's individual-level/stage-level distinction among\nverb phrases and his object-level/kind-level distinction among verb\narguments (Carlson 1977, 1982), as well as pragmatic features such as\nthe given/new distinction (influenced by phrasal accent), lexical\npresuppositions, linguistic context, and background knowledge. The dividing line between semantic interpretation (computing and\ndisambiguating logical forms) and discourse understanding—making\nsense of text—is a rather arbitrary one. However, heavily context-\nand knowledge-dependent aspects of the understanding process, such as\nresolving anaphora, interpreting context-dependent nominal compounds,\nfilling in “missing” material, determining implicit temporal and causal\nrelationships (among other “coherence relations”), interpreting loose\nor figurative language and certainly, integrating linguistically\nderived information with preexisting knowledge are generally counted as\naspects of discourse processing. Language has evolved to convey information as efficiently as\npossible, and as a result avoids lengthy identifying descriptions and\nother lengthy phrasings where shorter ones will do. One aspect of this\ntendency towards “shorthand” is seen in anaphora,\nthe phenomenon of coreference between an earlier, potentially more\ndescriptive NP and a later anaphoric pronoun or definite NP (with a\ndeterminer like the or\nthese). (The reverse sequencing, cataphora, is seen\noccasionally as well.)  Coreference is yet another source of ambiguity\nin language, scarcely noticeable by human language users (except in\nambivalent cases such as “When Flight 77 hit the Pentagon's\nwall, it disintegrated”), but problematic for machines. Determining the (co)referents of anaphors can be approached in a\nvariety of ways, as in the case of semantic disambiguation. Linguistic\nand psycholinguistic principles that have been proposed include\ngender and number agreement of coreferential terms,\nC-command principles (e.g., an anaphor must be a pronoun if\nits referent is a sibling of one of its ancestors in the parse tree),\n(non)reflexive constraints (e.g., the subject and\nobject cannot be coreferential in a simple clause such as\n\n“John blamed him for the accident”),\nrecency/salience (more recent/salient referents are preferred), and\ncentering (the most likely term to be pronominalized in an\nutterance is the “center of attention”). An early\nheuristic algorithm that employed several features of this type to\ninterpret anaphors was that of Hobbs (1979).  But selectional\npreferences are important as well. For example, in the sentence\n“He bumped against the sauce boat containing the\nraspberry syrup, spilling it,” the pronoun can be\ndetermined to be coreferential with the raspberry syrup\nrather than the sauce boat because\nspill prefers a liquid (or loose aggregate) as its\nobject. With the alternative continuation, “… knocking\nit over,” the choice of coreferent would be reversed,\nbecause knock over prefers something solid and upright as its\nobject. More subtle world knowledge may be involved as well, as in\nTerry Winograd's well-known example, “The city council\nrefused the women a parade permit because they\nfeared/advocated violence,” where\nthey may refer to the city council or the women, depending on\nthe choice of verb and the corresponding stereotypes that are\nevoked. Another complication concerns reference to collections of\nentities, related entities (such as parts), propositions, and events\nthat can become referents of pronouns (such as they, this,\nand that) or of definite NPs (such as this situation\nor the door (of the house)) without having appeared\nexplicitly as a noun phrase. Like other sorts of ambiguity,\ncoreference ambiguity has been tackled with statistical techniques.\nThese typically take into account factors like those mentioned, along\nwith additional features such as antecedent animacy and prior\nfrequency of occurrence, and use these as probabilistic evidence in\nmaking a choice of antecedent (e.g., Haghighi & Klein 2010).\nParameters of the model are learned from a corpus annotated with\ncoreference relations and the requisite syntactic analyses. Coming back briefly to nominal compounds of form N N, note that\nunlike conventional compounds such as ice bucket or ice\nsculpture—ones approachable using an enriched lexicon,\nheuristic rules, or statistical techniques—some compounds can\nacquire a variety of meanings as a function of context. For\nexample, rabbit guy could refer to entirely different things\nin a story about a fellow wearing a rabbit suit, or one about a rabbit\nbreeder, or one about large intelligent leporids from outer\nspace. Such examples reveal certain parallels between compound nominal\ninterpretation and anaphora resolution: At least in the\nmore difficult cases, N N interpretation depends on previously seen\nmaterial, and on having understood crucial aspects of that previous\nmaterial (in the current example, the concepts of wearing a rabbit\nsuit, being a breeder of rabbits, or being a rabbit-like creature). In\nother words N N interpretation, like anaphora resolution, is\nultimately knowledge-dependent, whether that knowledge comes from\nprior text, or from a preexisting store of background knowledge. A\nstrong version of this view is seen in the work of Fan et\nal. (2009), where it is assumed that in technical contexts, even\nmany seemingly conventional compounds require knowledge-based\nelaboration. For example, in a chemistry context, HCL\nsolution is assumed to require elaboration into something\nlike: solution whose base is a chemical whose basic structural\nconstituents are HCL molecules. Algorithms are provided (and\ntested empirically) that search for a relational path (subject to\ncertain general constraints) from the modified N to the modifying N,\nselecting such a relational path as the meaning of the N N\ncompound. As the authors note, this is essentially a\nspreading-activation algorithm, and they suggest more general\napplication of this method\n (see section 5.3 on integrated interpretive\nmethods). While anaphors and certain nominal compounds can be regarded as\nabbreviated encodings of semantic content, other forms of\n“shorthand” leave out semantically essential material\naltogether, requiring the reader or hearer to fill it in. One\npervasive phenomenon of this type is of course ellipsis, as\nillustrated earlier in sentences\n (2.5) and (2.6), or by the following\nexamples. In (5.1), so is a place-holder for the VP make up words (in an\ninverted sentence),  and (5.2) tacitly contains a final predicate\nsomething like under amount x of pressure, where that amount\nx needs to be related to the (larger) amount of pressure\nexperienced by Felix.  Interpreting ellipsis requires filling in of\nmissing material; this can often be found at the surface level as a\nsequence of consecutive words (as in the gapping and bare ellipsis\nexamples 2.5 and 2.6), but as seen in (5.1) and (5.2),  may\ninstead (or in addition) require adaptation of the imported material\nto fit semantically into the new context. Further complications arise\nwhen the imported material contains referring expressions, as in the\nfollowing variant of (5.2): Here the missing material  may refer either to Felix's boss\nor my boss (called the strict and sloppy\nreading respectively), a distinction that can be captured by regarding\nthe logical form of the antecedent VP as containing only one, or two,\noccurrences of the lambda-abstracted subject variable, i.e.,\nschematically,      \nλx[x is under more pressure from Felix's boss], versus \nλx[x is under more pressure from x's boss].\n The two readings can be thought of as resulting respectively from\nscoping his boss first, then\nfilling in the elided material, and the\nreverse ordering of these operations (Dalrymple et al. 1991; see also\nCrouch 1995; Gregory and Lappin 1997). Other challenging forms of\nellipsis are event ellipsis, as in (5.3) (where forgot stands for\nforgot to bring), entirely\nverbless sentences such as (5.4) and (5.5),\nand subjectless, verbless ones like (5.6) and (5.7): In applications these and some other forms of ellipsis are handled,\nwhere possible, by (a) making strong use of domain-dependent\nexpectations about the types of information and speech acts that are\nlikely to occur in the discourse, such as requests for flight\ninformation in an air travel adviser; and (b) interpreting utterances\nas providing augmentations or modifications of domain-specific\nknowledge representations built up so far. Corpus-based approaches to\nellipsis have so far focused mainly on identifying instances of VP\nellipsis in text, and finding the corresponding antecedent material, as\nproblems separate from that of computing correct logical forms (e.g.,\nsee Hardt 1997; Nielsen 2004). Another refractory missing-material phenomenon is that of implicit\narguments. For example, in the sentence the reader needs to conceptually expand its concentration\ninto its concentration in the air in the interior of the car,\nand hazard into\nhazard for occupants of the car.\nIn this example, lexical knowledge to\nthe effect that concentration (in the chemical sense)  refers to\nthe concentration of some substance in some medium could at least\nprovide the “slots” that need to be filled, and a similar comment\napplies in the case of hazard.\nHowever, not all of the fillers for\nthose slots are made explicitly available by the text—the carbon\nmonoxide referred to provides one of the fillers, but the air in the\ninterior of the car, and potential occupants of the car (and that they\nrather than, say, the upholstery would be at risk) are a matter of\ninference from world knowledge. Finally, another form of  shorthand that is common in certain\ncontexts is metonymy, where a\nterm saliently related to an intended\nreferent stands for that referent. For example, in an airport context, might stand for \n“Is this the departure lounge for flight 574?”\nSimilarly, in appropriate contexts cherry\ncan stand for cherry ice\ncream, and BMW can\nstand for BMW's stock market index: Like other types of underspecification, metonymy has been\napproached both from knowledge-based and corpus-based\nperspectives. Knowledge that can be brought to bear includes\nselectional preferences (e.g., companies in general do not literally\nrise),  lexical concept hierarchies (e.g., as provided by\nWordNet), generic knowledge about the types of metonymy relations\ncommonly encountered, such as part-for-whole, place-for-event,\nobject-for-user, producer-for-product, etc. (Lakoff and Johnson\n1980), rules for when to conjecture such relations (e.g., Weischedel\nand Sondheimer 1983), named-entity knowledge, and knowledge about\nrelated entities (e.g., a company may have a stock market index, and\nthis index may rise or fall) (e.g., Bouaud et al. 1996; Onyshkevych\n1998).  Corpus-based methods (e.g., see Markert and Nissim 2007) often\nemploy many of these knowledge resources, along with linguistic and\nstatistical features such as POS tags, dependency paths and\ncollocations in the vicinity of a potential metonym. As for other\nfacets of the interpretive process (including parsing), use of deep\ndomain knowledge for metonym processing can be quite effective in\nsufficiently narrow domains, while corpus-based, shallow methods scale\nbetter to broader domains, but are apt to reach a performance plateau\nfalling well short of human standards. Text and spoken language do not consist of isolated sentences, but\nof connected, interrelated utterances, forming a coherent\nwhole—typically, a temporally and causally structured narrative,\na systematic description or explanation, a sequence of instructions,\nor a structured argument for a conclusion (or in dialogue, as\ndiscussed later, question-answer exchanges, requests followed by\nacknowledgments, mixed-initiative planning, etc.). \nThis structure is already apparent at the level of pairs of consecutive\nclauses, such as \nIn (5.12), we understand that John's looking at the sky temporally\noverlaps the presence of the dark clouds in the sky (i.e., the\ndark-cloud situation at least contains the end of the looking event).\nAt a deeper level, we also understand that John perceived the\nsky to be dark with thunderclouds, and naturally assume that John took\nthe clouds to be harbingers of an impending storm, as we\nourselves would. In (5.13), the two clauses appear to\nreport successive events and furthermore, the first event is\nunderstood to have led causally to the second—John's decision\nwas driven by whatever he saw upon looking at the sky; and based on\nour knowledge of weather and the function of umbrellas, and the fact\nthat “everyone” possesses that knowledge, we further infer\nthat John perceived potential rainclouds, and intended to fend off any\nrain with his umbrella in an imminent excursion. The examples show that interpreting extended  multi-clausal\ndiscourses depends on both narrative conventions and world knowledge;\n(similarly for descriptive, instructional or argumentative text) . In\nparticular, an action sentence followed by a static observation, as in\n(5.12), typically suggests the kind of action-situation overlap we\nnoted, and successively reported actions or events, as in (5.13),\ntypically suggest temporal sequencing and perhaps a causal connection,\nespecially if one of the two clauses is not a volitional action. These\nsuggestive inferences presumably reflect the narrator's adherence to a\nGricean principle of orderliness, though such an observation is little\nhelp from a computational perspective. The concrete task is to\nformulate coherence principles for narrative and other forms of\ndiscourses and to elucidate, in a usable form, the particular\nsyntactico-semantic properties at various levels of granularity that\ncontribute to coherence. Thus various types of rhetorical or coherence relations (between\nclauses or larger discourse segments) have been proposed in the\nliterature, e.g., by Hobbs (1979), Grosz & Sidner (1986), and Mann\n& Thompson (1988). Proposed coherence relations are ones like\nelaboration, exemplification, parallelism, and contrast. We defer\nfurther discussion of rhetorical structure\nto section 6 (on language generation). “[I'm] behind the eight ball, ahead\nof the curve, riding the wave, dodging the bullet and pushing the\nenvelope. I'm on point, on task, on message and off drugs… I'm\nin the moment, on the edge, over the top and under the radar. A\nhigh-concept, low-profile, medium-range ballistic missionary.”\n–George Carlin\n(“Life is worth losing”, first broadcast on HBO, November\n5, 2005)\n We have already commented on processing metonymy, which is\nconventionally counted as a figure of speech—a word or phrase\nstanding for something other than its literal meaning. However, while\nmetonymy is essentially an abridging device, other figurative modes,\nsuch as metaphor, simile, idioms, irony, personification, or hyperbole\n(overstatement) convey meanings, especially connotative ones, not\neasily conveyed in other ways. We focus on metaphor here, as it is in\na sense a more general form of several other tropes. Moreover, it has\nreceived the most attention from computational linguists, because the\nargument can be made that metaphor pervades language, with no sharp\ndemarcation between literal and metaphorical usage (e.g., Wilks 1978;\nCarbonell 1980; Lakoff and Johnson 1980; Barnden 2006). For example,\nwhile “The temperature dropped” can be viewed as involving\na sense of\ndrop that is synonymous with decrease, it can also\nbe viewed as a conventional metaphor comparing the decreasing\ntemperature to a falling object. As a way of allowing for examples of\nthis type, Wilks offered a processing paradigm in which selectional\nconstraints (such as a physical-object constraint on the subject\nof drop) are treated as mere preferences rather than firm\nrequirements. However, processing metaphor requires more than relaxation of\npreferences; it is both context-dependent and profoundly\nknowledge-dependent. For example, \ncan be a literal description of a mundane act in a laundromat setting,\na literal description of a symbolic act by a boxer's handler, or a\nstock metaphor for conceding defeat in any difficult endeavor. But to\ngrasp the metaphorical meaning fully, including the connotation of a\npunishing, doomed struggle, requires a vivid conception of what a\nboxing match is like. In approaching metaphor computationally, some authors, e.g., Dedre\nGentner (see Falkenhainer et al.\n1989), have viewed it as depending on\nshared properties and relational structure (while allowing for\ndiscordant ones), directly attached to the concepts being compared. For\nexample, in comparing an atom to the solar system, we observe a\nrevolves-around relation between electrons and the nucleus on\nthe one hand and between planets and the sun on the other. But others\nhave pointed out that the implicit comparison may hinge on properties\nreached only indirectly. In this view, finding a metaphor for a\nconcept is a process of moving away from the original concept in a\nknowledge network in a series of steps, each of which transforms some\ncurrent characteristic into a related one. This is the process termed\n“slippage” by Hofstadter et al.  (1995). Others (e.g.,\nMartin 1990, drawing on Lakoff and Johnson 1980) emphasize preexisting\nknowledge of conventional ways of bridging metaphorically from one\nconcept to another, such as casting a nonliving thing as a living\nthing. In view of the dependence of metaphor on context and extensive\nknowledge, and the myriad difficulties still confronting all aspects\nof language understanding, it is not surprising that no general system\nfor processing metaphor in context exists, let alone for using\nmetaphor creatively. Still, Martin's MIDAS program was able to\ninterpret a variety of metaphors in the context of a language-based\nUnix adviser, relying on knowledge about the domain and about\nmetaphorical mapping, hand-coded in the KODIAK knowledge\nrepresentation language. Also, several other programs have\ndemonstrated a capacity to analyze or generate various examples of\nmetaphors, including the Structure Mapping Engine (SME) (Falkenhainer\net al.  1989), Met* (Fass 1991), ATT-Meta (Barnden 2001), KARMA\n(Narayanan 1997) and others. More recently, Veale and Hao (2008)\nundertook an empirical study of a slippage-based approach to metaphor,\nusing attributes collected from WordNet and the web. In a similar\nspirit, but taking SME as his inspiration, Turney (2008) implemented a\n“Latent Relation Mapping Engine” (LRME) to find the best\nmapping between the elements of two potentially comparable\ndescriptions (of equal size); the idea is to use web-based\nco-occurrence statistics to gauge not only the attribute similarity of\nany two given concepts (such as electron and planet)\nbut also the relational similarity of any two given pairs of concepts\n(such as electron:nucleus and planet:sun), using\nthese as metrics in optimizing the mapping. \nObviously, the many forms of syntactic, semantic, and pragmatic\nambiguity and underspecification enumerated in the preceding sections\ninteract with one another and with world knowledge. For example, word\nsense disambiguation, reference resolution, and metaphor interpretation\nare interdependent in the sentence Note first of all that it could refer syntactically to\nthe Nebraska Supreme Court or to the chair, but\nworld knowledge rules out the possibility of a neuter-gendered chair\nmanifesting a mental attitude.  Note as well that if it is\nreplaced by he, then the chair  is\nreinterpreted as a person and becomes the referent of the pronoun; at\nthe same time, threw out is then reconstrued as a metaphor\nmeaning removed from office (with an implication of\nruthlessness). Thus it seems essential to find a uniform framework for jointly\nresolving all forms of ambiguity and underspecification, at least to\nthe extent that their resolution impacts inference. Some frameworks\nthat have been proposed are weighted abduction, constraint\nsolving, and \n“loose-speak” interpretation. Weighted\nabduction (Hobbs et al. 1993) is based on the idea that the\ntask of the hearer or reader is to explain the word sequence\ncomprising a sentence by viewing the meaning of  that sentence as\na logical consequence of general and contextual knowledge along with\nsome assumptions, to be kept as “lightweight” as possible. The\nconstraint-solving approach views syntax, semantics, pragmatics,\ncontext, and world knowledge as supplying constraints on\ninterpretations that need to be satisfied simultaneously.  Often\nconstraints are treated as defeasible, or ranked, in which case the\ngoal is to minimize constraint violations, particularly of relatively\nstrong constraints.  (There is a connection here to\nOptimality Theory in cognitive language modeling.)\nLoose-speak interpretation (Fan et al. 2009, cited previously\nin connection with nominal compound interpretation) sets aside\nsyntactic ambiguity but tries to deal with pervasive semantic\nlooseness in the meanings of nominal compounds, metonymy, and other\nlinguistic devices. It does so by expanding semantic triples (from the\npreliminary logical form) of type 〈Class1,\nrelation, Class2〉, where the relation cannot\nproperly relate the specified classes, into longer chains containing\nthat relation and terminating at those classes. Finding such chains\ndepends on background knowledge about the relations that are possible\nin the domain of interest. The methods just mentioned have been applied in restricted tasks,\nbut have not solved the problem of comprehensive language\ninterpretation. They all face efficiency problems, and—since they\nall depend on a rich base of linguistic and world knowledge—the\nknowledge acquisition bottleneck. We comment on the efficiency issue\nhere, but leave the discussion of knowledge acquisition to \nsection 8. In view of the speed with which people disambiguate and comprehend\nlanguage, one may surmise that these processes more closely resemble\nfitting the observed texts or utterances to familiar patterns, than\nsolving complex inference or constraint satisfaction problems. For\nexample, in a sentence such as the pronoun is understood to refer to the glass, even though world\nknowledge would predict that the glass broke, whereas the cutting board\ndid not. (The idea that communication focuses on the unexpected is of\nno help here, because the referent remains unchanged if we change\ndidn't break to broke.) This would be expected if\nthe interpretive\nprocess simply found a match between the concept of a fragile object\nbreaking and the glass breaking (regardless of the exact logical\nstructure), and used that match in choosing a referent. The\nprocessing of the example from Winograd,\n in section 5.1, concerning\nrefusal of a parade permit to a group of women, may similarly depend in\npart on the familiarity of the idea that people who (seek to) parade\nmay advocate some cause. Note that in the women are still preferred as the referent of they, even though it\nis generally true that stalwarts of society, such as city councillors,\ndo not advocate violence. If disambiguation and (at least preliminary) interpretation  in\nlanguage understanding turn out to be processes guided more by learned\npatterns than by formalized knowledge, then methods similar to those\nused in feature-based statistical NLP may be applicable to their\neffective mechanization. Because language generation is a purposeful activity motivated by\ninternal goals, it is difficult to draw a boundary between\ngoal-directed thinking and the ensuing production of linguistic output.\nOften the process is divided into content\nplanning, microplanning,\nsurface realization, and physical\npresentation. While the last three of\nthese stages can be thought of as operating (in the order stated) on\nrelatively small chunks of information (e.g., resulting in one or two\nsentences or other utterance types), content planning  is often\nregarded as a continual process of goal-directed communicative\nplanning, which successively hands over small clusters of ideas to the\nremaining stages for actual generation when appropriate. We discuss the\nlatter transduction first, in order to highlight its relationship to\nunderstanding. The transduction of a small set of internally represented ideas into\nwritten or oral text is in an obvious sense the inverse of the\nunderstanding process, as we have outlined it in the preceding sections\n2–5. In other words, starting from a few internally represented ideas\nto be communicated, we need to proceed to an orderly linear arrangement\nof these ideas, to a surface-oriented logical form that is concise and\nnonrepetitive and appropriately indexical (e.g., in the use of I, you,\nhere, now, and referring expressions), and finally to an actual\nsurface\nrealization and physical presentation of that realization as spoken or\nwritten text. Most or all of the kinds of knowledge involved in\nunderstanding naturally come into play in generation as well—whether\nthe knowledge is about the structure of words and phrases, about the\nrelationship between structure and meaning representation, about\nconventional (or creative) ways of phrasing ideas, about discourse\nstructure and relations, or about the world. Despite this inverse relationship, language generation has\ntraditionally received less attention from computational linguists than\nlanguage understanding, because if the content to be verbalized is\navailable in an unambiguous, formal representation, standard output\ntemplates can often be used for generation, at least in sufficiently\nnarrow domains. Even for unrestricted domains, the transduction from an\nexplicit, unambiguous internal semantic representation to a word\nsequence is much less problematic than reconstruction of an explicit\nsemantic representation from an ambiguous word sequence. A similar\nasymmetry holds at the level of speech recognition and generation,\naccounting for the fact that serviceable speech generators (e.g.,\nreading machines for the blind) have been available much longer (since\nabout 1976) than serviceable speech recognizers (appearing around 1999). The microplanning process that leads from a few internal ideas to a\nsurface-oriented, indexical representation typically starts by\nidentifying “chunks” expected to be verbalized as\nparticular types of syntactic constituents, such as NPs, VPs or\nPPs. This is often followed by making choices of more surface-oriented\nconcepts (or directly, lexical heads) in terms of which the chunks\nwill be expressed. The nature of these processes depends very much on\nthe internal representation. For example, if the representation is\nframed in terms of very abstract primitives, thematic relations, and\nattribute-value descriptions of entities, then the chunks might be\nsets of thematic relations centered around an action, and sets of\nsalient attributes of entities to be referred to. If the internal\nrepresentation is instead more language-like, then chunks will be\nrelatively small, often single propositions, and the lexical choice\nprocess will resemble internal paraphrasing of the logical forms. If\nmore than one idea is being verbalized, ordering decisions will need\nto be made. For example, reporting that a bandit brandishing a gun\nentered a local liquor store might be reported in that order, or as\n“A bandit entered a local liquor store, brandishing a\ngun”. In dialogue, if a contribution involves both supplying and\nrequesting information, the request should come last. In other cases\ntransformations to more concise forms may be needed to bring the\nrepresented ideas stylistically closer to surface form. For example,\nfrom logical forms expressing that John ate a burrito containing\nchicken meat and Mary ate a burrito containing chicken\nmeat, a more compact surface-oriented LF might be generated to\nthe effect that John and Mary each had a chicken\nburrito. More subtle stylistic choices might be made as\nwell—for example, in casual discourse, eating might be\ndescribed as polishing off (assuming that the internal\nrepresentation allows for such surface-oriented distinctions).\nFurthermore, as already mentioned, a surface-oriented LF needs to\nintroduce contextually appropriate referring expressions such as\ndefinite descriptions and pronouns, in conformity with pragmatic\nconstraints on the usage of such expressions. The above outline is simplified in that it neglects certain\nsubtleties of discourse and context. A sentence or other utterance\ntype generally involves both new and old (given,\npresupposed) information, and furthermore, some of the concepts\ninvolved may be more strongly focused than others. For example, in the\nsentence “The item you ordered costs ninety dollars, not\nnine,” the existence and identity of the item, and the fact that\nit was ordered, are presumed to be part of the common ground in the\ncurrent context (old), and so is the addressee's belief that the cost\nof the item is $9; only the corrected cost of $90 is new\ninformation. The emphasis on ninety draws attention to the\npart of the presumed belief that is being corrected. Thus it is not\nonly conceptual content that needs to be available to the\nmicroplanning stage, but also indications as to what is new and what\nis old, and what aspects are to be stressed or focused. The planner\nneeds at least to apply knowledge about the phrasing of new and old\ninformation (e.g., the use of indefinite vs. definite NPs), about the\nlexical presuppositions and implicatures of the items used (e.g., that\nsucceeding presupposes trying, that regretting\nthat φ presupposes that φ, or that some\nimplicates not all), about the presuppositions of stress\npatterns, and about focusing devices (such as stress and\ntopicalization). The effect of applying these sorts of pragmatic\nknowledge will be to appropriately configure the surface-oriented LFs\nthat are handed off to the surface realizer, or, for pragmatic\nfeatures that cannot be incorporated into the LFs, to annotate the LFs\nwith these features (e.g., stress marking). The penultimate step is surface realization, using knowledge about the\nsyntax-semantics interface. In systems for restricted domains, this\nknowledge might consist of heuristic rules and templates (perhaps tree\nschemas) for verbalizing LFs. More broadly-aimed generators might make\nuse of invertible grammars,\nones formulated in rule-to-rule fashion and\nallowing transduction from logical forms to surface forms in much the\nsame way as the “forward” transduction from surface phrase structure to\nlogical form. Sophisticated generators also need to take account of\npragmatic annotations, such as stress, mentioned above. Finally, the\nlinguistically expressed content needs to be physically presented as\nspoken or written text, with due attention to articulation, stress, and\nintonation, or, for written text, punctuation, capitalization, choice\nof a or an as indefinite article, italics, line breaks, and so on. Returning now to content planning, this process may be expansive or\nvery limited in scope; for example, it may be aimed at providing\ndetails of a complex object, set of events, or argument, or it may seek\nto present no more than a single fact, greeting or acknowledgment. We\nleave discussion of the strongly interactive type of content planning\nneeded for dialogue to the following section, while we comment here on\nthe more expansive sorts of text generation. In this case the\norganization of the information to be presented is the central concern.\nFor example, the events of a narrative or the steps of a procedure\nwould generally be arranged chronologically; arguments for a conclusion\nmight be arranged so that any subarguments intended to buttress the\npresumptions of that step (immediately) precede it; descriptions of\nobjects might proceed from major characteristics and parts to details. An early method of producing well-organized, paragraph-length\ndescriptions and comparisons was pioneered in the TEXT system of\nMcKeown (1985), which used ATN-like organizational schemas to sequence\nsections of the description of a type of object, such as the object's\nmore general type, its major parts and distinctive attributes, and\nillustrative examples. Later work by Hovy (1988) and Moore and Paris\n(1988) tied content planning more closely to communicative goals by\nrelying on rhetorical structure theory (RST) (Mann and Thompson\n1987). RST posits over 20 possible coherence relations between spans\nof text (usually adjacent).  For example, a statement of some claim\nmay be followed by putative evidence for the claim, thus establishing\nan evidence relation between the claim (called the\nnucleus, because it is the main point) and the cited evidence\n(called a satellite because of its subsidiary role).  Another\nexample is the volitional result relation, where the nuclear\ntext span describes a situation or event of interest, and the\nsatellite describes a deliberate action that caused the situation or\nevent. Often these relations are signalled by discourse markers (cue\nwords and phrases), such as but, when, yet, or after\nall, and it is important in text generation to use these markers\nappropriately. For example, the use of when in the following\nsentence enhances coherence, by signalling a possible volitional result relation. Text spans\nlinked by rhetorical relations may consist of single or multiple\nsentences, potentially leading to a recursive (though not necessarily\nstrictly nested) structure. Rhetorical relations can serve\ncommunicative goals such as concept comprehension (e.g., via\nelaboration of a definition), belief (via presentation of evidence),\nor causal understanding (e.g., via a volitional result relation, as in\n(6.1)), and in this way tighten the connection between content\nplanning and communicative goals. “We\ncan ask just how it is that rhetoric somehow moves us …\nAristotle locates the essential nature of rhetorical undertakings in\nthe ends sought rather than in the purely formal properties.”\n\n–Daniel N. Robinson, Consciousness and Mental Life, (2007:\n171–2) \nDialogue is interactive goal-directed (purposive) behavior, and in that\nsense the most natural form of language. More than in narrative or\ndescriptive language, the flow of surface utterances and speaker\nalternation reflect the interplay of underlying speaker goals and\nintentions. By themselves, however, utterances in a dialogue are\nambiguous as to their purpose, and an understanding of the discourse\ncontext and the domain of discourse are required to formulate or\nunderstand them. For example, could be understood as a request for an answer such as\n\n“Thursday, June 24”, as a reminder of the importance of the\nday, or as a test of the addressee's mental alertness. The immediate goal of such an utterance is to change the mental state\n(especially beliefs, desires and intentions) of the hearer(s), and\nspeech act theory concerns the\nway in which particular types of speech\nacts effect such changes, directly or indirectly (Austin 1962; Grice\n1968; Searle 1969). To choose speech acts sensibly, each participant\nalso needs to take account of the mental state of the other(s); in\nparticular, each needs to recognize\nthe other's beliefs, desires and\nintentions. Discourse conventions in cooperative conversation are\nadapted to facilitate this process: The speakers employ locutions that\nreveal their intended effects, and their acknowledgments and\nturn-taking consolidate mutual understanding. In this way\nmixed-initiative dialogue and potentially, cooperative domain action\nare achieved. In the previous discussion of content\nplanning in language generation,\nwe said little about the formation of communicative intentions in this\nprocess. But in the context of purposive dialogue, it is\nessential to consider how a dialogue agent might arrive at the\nintention to convey certain ideas, such as episodic, instructional or\ndescriptive information, a request,  an acknowledgment and/or\nacceptance of a request, an answer to a question, an argument in\nsupport of a conclusion, etc. As in the case of generating extended descriptions, narrative,\narguments, etc., using RST, a\nnatural perspective here is one centered\naround goal-directed planning. In fact, the application of this\nperspective to dialogue historically preceded its application to\nextended discourses. In particular, Cohen and Perreault (1979) proposed\na reasoning, planning, and plan recognition framework that represents\nspeech acts in terms of their preconditions and effects. For example, a\nsimple INFORM speech act might have the following preconditions\n(formulated for understandability from a first-person speaker\nperspective): \nThe effect of implementing the INFORM as an utterance is then that the\nhearer knows whether X is true. An important feature of such\na framework is that it can account for indirect speech acts (Allen and\nPerreault 1980). For example, question (7.1), as an indirect request\nfor the date or day of the week, can be viewed as indicating the\nspeaker's awareness that the hearer can perform the requested\ninformation-conveying act only if the knowledge-precondition of that\nact is satisfied. Furthermore, since the hearer recognizes that\nquestioning a precondition of a potential act is one indirect way of\nrequesting the act, then (unless the context provides evidence to the\ncontrary) the hearer will make the inference that the speaker wants\nthe hearer to perform the information-conveying speech act. Note that\nthe reasoning and planning framework must allow for iterated\nmodalities such as “I believe that you want me to tell you what\ntoday's date is”, or “I believe (because of the request I\njust made) that you know that I want you to pass the salt shaker to\nme”. Importantly, there must also be allowance for mutual\nbeliefs and intentions, so that a common ground can be maintained as\npart of the context and collaboration can take place. A belief is\nmutual if each participant holds the belief, and the participants\nmutually believe that they mutually hold the belief. The mutual\nknowledge of the participants in a dialogue can be assumed to include\nthe overt contents of their utterances and common general knowledge,\nincluding knowledge of discourse conventions. Since the ultimate purpose of a dialogue may be to accomplish something\nin the world, not only in the minds of the participants, reasoning,\ngoal-directed planning, and action need to occur at the domain level as\nwell. The goals of speech acts are then not ends in themselves, but\nmeans to other ends in the domain, perhaps to be accomplished by\nphysical action (such as equipment repair). As a result, task-oriented\ndialogues are apt to be structured in a way that follows or “echoes”\nthe structure of the domain entities and the way they can be\npurposefully acted upon. Such considerations led to Grosz and Sidner's\ntheory of dialogue structure in task-oriented dialogues (Grosz and\nSidner 1986). Their theory centers around the idea of shifts of\nattention mediated by pushing and popping of “focus spaces” on a stack.\nFocus spaces hold in them structured representations of the domain\nactions under consideration. For example, setting a collaborative goal\nof attaching a part to some device would push a corresponding focus\nspace onto the stack. As dictated by knowledge about the physical task,\nthe participants might next verbally commit to the steps of using a\nscrewdriver and some screws to achieve the goal, and this part of the\ndialogue would be mediated by pushing corresponding subspaces onto the\nfocus stack. When a subtask is achieved, the corresponding focus space\nis popped from the stack. Implementation of reasoning and planning frameworks covering both the\niterated modalities needed for plan-based dialogue behavior and the\nrealities of a task domain has proved feasible for constrained\ndialogues in restricted domains (e.g.,\nSmith et al. 1995), but\nquickly\ncomes up against a complexity barrier when the coverage of language and\nthe scope of the domain of discourse are enlarged. Planning is in\ngeneral NP-hard, indeed PSPACE-complete even in propositional planning\nformalisms (Bylander 1994), and plan recognition can be exponential in\nthe number of goals to be recognized, even if all plans available for\nachieving the goals are known in advance (Geib 2004). In response to this difficulty, researchers striving to build usable\nsystems have experimented with a variety of strategies. One is to\npre-equip the dialogue system with a hierarchy of carefully engineered\nplans suitable for the type of dialogue to be handled (such as\ntutoring, repair, travel planning or schedule maintenance), and to\nchoose the logical vocabulary employed in NLU/NLG so that it meshes\nsmoothly with both the planning operators and with the surface\nrealization schemas aimed at the target domain. (As a noteworthy\nexample of this genre, see Moore & Paris 1993.) In this way\ndomain and\ntext planning and surface realization become relatively\nstraightforward, at least in comparison with systems that attempt to\nsynthesize plans from scratch, or to reason extensively about the\nworld, the interlocutor, the context, and the best way to express an\nidea at the surface level. But while such an\napproach is entirely defensible for an experimental system intended to\nillustrate the role of plans and intentions in a specialized domain, it\nleaves open the question of how large amounts of linguistic knowledge\nand world knowledge could be incorporated into a dialogue system, and\nused inferentially in\nplanning communicative (and other) actions. Another strategy for achieving more nearly practical performance has\nbeen to precode (and to some extent learn) more “reactive” (as opposed\nto deliberative) ways of participating in dialogue. Reactive techniques\ninclude (i) formulaic, schema-based responses (reminiscent of ELIZA)\nwhere such responses are likely to be appropriate; (ii) rule-based\nintention and plan recognition; e.g.,\nan automated travel agent\nconfronted with the elliptical input \n“Flights to Orlando” can usually\nassume that the user wishes to be provided with flight options from the\nuser's current city to Orlando, in a time frame that may have been\nestablished previously; (iii) statistical domain plan recognition based\non probabilistic modeling of the sequences of steps typically taken in\npursuit of the goals characteristic of the domain; and (iv) discourse\nstate modeling by classifying speech acts (or utterance acts) and\ndialogue states into relatively small numbers of types, and viewing\ntransitions between dialogue states as events determined by the current\nstate and current type of speech act. For example, in a state where the\ndialogue system has no immediate obligation, and the user asks a\nquestion, the system assumes the obligation of answering the question,\nand transitions to a state where it will try to discharge that\nobligation. However, systems that rely primarily on reactive techniques tend to\nlack deep understanding and behavioral flexibility. Essentially,\nknowledge-based inference and planning are replaced by rote behavior,\nconditioned by miscellaneous features of the current discourse state\nand observations in the world. Furthermore, deliberate reasoning and\nplan synthesis seem necessary for an agent that can acquire effective\ngoal-directed plans and behaviors autonomously. Although random trial\nand error (as in reinforcement learning), supervised learning, and\nlearning by imitation are other learning options, their potential is\nlimited. Random trial and error is apt to be impractical in the\nenormously large state space of linguistic and commonsense behavior;\nsupervised learning (of appropriate choices based on contextual\nfeatures) seems at best able to induce rote plan recognition and\ndiscourse state transitions (reactive behaviors of types (iii) and\n(iv) above); and imitation is possible only when relevant, readily\nobservable exemplary behaviors can be presented to the\nlearner—and by itself, it can lead only to rote, rather than\nreasoned, behavior. Integrating reactive methods with deliberate reasoning and planning may\nbe enabled in future by treating intentions and actions arrived at by\nreactive methods as tentative, to be verified and potentially modified\nby more deliberate reasoning if time permits. Excessive reasoning\nwith iterated modalities could also be avoided with stronger\nassumptions about the attainment of mutual belief. For example,\nwe might assume that both speaker and hearer spontaneously perform\nforward inferences about the world and about each other's mental states\nbased on discourse events and common knowledge, and that such forward\ninferences directly become mutual knowledge (on a “likemindedness”\nassumption), thus shortcutting many modally nested inferences. We have noted the dependence of language understanding and use on vast\namounts of shallow and deep knowledge, about the world, about lexical\nand phrasal meaning, and about discourse and dialogue structure and\nconventions. If machines are to become linguistically competent, we\nneed to impart this knowledge to them. Ideally, the initial, preprogrammed knowledge of a machine would be\nrestricted to the kinds of human knowledge thought to be innate (e.g.,\nobject persistence, motion continuity, basic models of animacy and\nmind, linguistic universals, means of classifying/taxonomizing the\nworld, of organizing events in time, of abstracting from experience,\nand other such knowledge and know-how). The rest would be learned in\nhuman-like fashion. Unfortunately, we do not have embodied agents with\nhuman-like sensory and motor equipment or human-like innate mental\ncapabilities; so apart from the simplest sort of verbal learning by\nrobots such as verbal labeling of objects or actions, or using spatial\nprepositions or two-word sentences (e.g.,\nFleischman and Roy 2005;\nMcClain and Levinson 2007; Cour et al.\n2008), most current work on\nknowledge acquisition uses either (1) hand-coding,\n(2) knowledge\nextraction from text corpora, or (3) crowdsourcing coupled with some\nmethod of converting collected, verbally expressed “facts” to a usable\nformat. We focus in this section on the acquisition of general\nbackground knowledge needed to support language understanding and\nproduction, leaving discussion of linguistic knowledge acquisition to\nsection 9. The best-known manually created body of commonsense knowledge is the\nCyc or ResearchCyc knowledge base (KB) (Lenat 1995). This contains an ontology of a\nfew hundred thousand concepts and several million facts and rules,\nbacked up by an inference engine. It has been applied to analysis,\ndecision support and other types of projects in business, education and\nmilitary domains. However, the Cyc ontology and KB contents have been\nmotivated primarily by knowledge engineering considerations (often for\nspecific projects) rather than by application to language\nunderstanding, and this is reflected in its heavy reliance on \nvery specific predicates expressed as concatenations of English words,\nand on higher-order operators. For example, the relation between\nkilling and dying is expressed using the predicates lastSubEvents,\nKillingByOrganism-Unique, and Dying,\nand relies on a higher-order\nrelation relationAllExists\nthat can be expanded into a quantified\nconditional statement. This remoteness from language makes it difficult\nto apply the Cyc KB to language understanding, especially if the goal\nis to extract relevant concepts and axioms from this KB and integrate\nthem with concepts and axioms formalized in a more linguistically\noriented representation (as opposed to adopting the CycL language, Cyc\nKB, axioms about English, and inference mechanisms wholesale)\n(e.g., Conesa et al. 2010). Other examples of hand-coded knowledge bases are the Component\nLibrary (CLib) (Barker et al. 2001), and a collection of commonsense\npsychological axioms by Hobbs and Gordon (2005). CLib provides a broad\nupper (i.e., high-level) ontology of several hundred concepts, and\naxioms about basic actions (conveying, entering,\nbreaking, etc.) and resultant change. However, the\nframe-based Kleo knowledge representation used in CLib\nis not close to language, and the coverage of the English lexicon is\nsparse. The psychological axioms of Hobbs and Gordon are naturally\nnarrow in focus (memories, beliefs, plans, and goals), and it remains\nto be seen whether they can be used effectively in conjunction with\nlanguage-derived logical forms (of the “flat” type favored by Hobbs)\nfor inference in discourse contexts. Knowledge adaptation from semi-formalized sources can, for example,\nconsist of extracting part-of-speech and subcategorization information\nas well as stock phrases and idioms from appropriate dictionaries. It\nmay also involve mapping hypernym hierarchies, meronyms (parts), or\nantonyms, as catalogued in sources like WordNet, into some form usable\nfor disambiguation and inference. The main limitations of manually\ncoded lexical knowledge are its grounding in linguistic intuitions\nwithout direct consideration of its role in language understanding,\nand its inevitable incompleteness, given the ever-expanding and\nshifting vocabulary, jargon, and styles of expression in all living\nlanguages.  Besides these sources of lexical knowledge, there are also sources of\nworld knowledge in semi-formalized form, such as tabulations and\ngazetteers of various sorts, and “info boxes” in online knowledge\nresources such as Wikipedia (e.g.,\nthe entries for notable personages\ncontain a box with summary attributes such as date of birth, date of\ndeath, residence, citizenship, ethnicity, fields of endeavor, awards,\nand others). But to the extent that such sources provide knowledge in a\nregimented, and thus easily harvested form, they do so only for named\nentities (such as people, organizations, places, and movies) and a few\nentity types (such as biological species and chemical compounds).\nMoreover, much of our knowledge about ordinary concepts, such as that\nof a tree or that of driving a car, is not easily captured in the form\nof attribute-value pairs, and is generally not available in that form. Knowledge extraction from unconstrained text has in recent years been\nreferred to as learning by reading.\nThe extraction method may be either\ndirect or indirect. A direct method takes sentential information from\nsome reliable source, such as word sense glosses in WordNet or\ndescriptive and narrative text in encyclopedias such as Wikipedia, and\nmaps this information into a (more) formal syntax for expressing\ngeneric knowledge. Indirect methods abstract (more or less) formal\ngeneric knowledge from the patterns of language found in miscellaneous\nreports, stories, essays, weblogs, etc. Reliably extracting knowledge by the direct method requires\nrelatively deep language understanding, and consequently is far from a\nmature technology. Ide and Véronis (1994) provide a survey of early\nwork on deriving knowledge from dictionary definitions, and the\ndifficulties faced by that enterprise. For the most part knowledge\nobtained in this way to date has been either low in quantity or in\nquality (from an inference perspective). More recent work that shows\npromise is that of Moldovan and Rus (2001), aimed at interpreting\nWordNet glosses for nominal artifact concepts, and that of Allen et\nal. (2013), aimed at forming logical theories of small clusters of\nrelated verbs (e.g., sleeping, waking up, etc.) by\ninterpreting their WordNet glosses. The most actively researched approach to knowledge extraction from\ntext in the last two decades has been the indirect one, beginning with\na paper by Marti Hearst demonstrating that hyponymy relations could be\nrather simply and effectively discovered by the use of lexicosyntactic\nextraction patterns (Hearst 1992). For example, extraction patterns\nthat look for noun phrases separated by “such as” or\n“and other” will match word sequences like “seabirds\nsuch as penguins and albatrosses” or “beans, nuts, and\nother legumes”, leading to hypotheses that\nseabird is a hypernym of penguin and\nalbatross, and that beans and nuts are\nhyponyms of legumes.  By looking for known hyponym-hypernym\npairs in close proximity, Hearst was able to expand the initial set of\nextraction patterns and hence the set of hypotheses. Many variants\nhave been developed since then, with improvements such as automation\nof the bootstrapping and pattern discovery methods, often with machine\nlearning techniques applied to selection, weighting and combination of\nlocal features in the immediate vicinity of the relata of\ninterest. Relations other than hyponymy that have been targeted, of\nrelevance to language understanding, include part-of relations, causal\nrelations, and telic relations  (such as that the use of\nmilk is to drink it).   While knowledge extraction using Hearst-like patterns is narrowly\naimed at certain predetermined types of knowledge, other approaches\nare aimed at open information extraction (OIE).  These seek to\ndiscover a broad range of relational knowledge, in some cases\nincluding entailments (in a rather loose sense) between different\nrelations. An early and quite successful system of this genre was Lin\nand Pantel's DIRT system (Discovery of Inference Rules from Text),\nwhich used collocational statistics to build up a database of\n“inference rules” (Lin and Pantel 2001). An example of a\nrule might be “X finds a solution to Y\n≈ X solves Y.” The statistical techniques\nused included clustering of nominals into similarity groups based on\ntheir tendency to occur in the same argument positions of the same\nverbs, and finding similar relational phrases (such as “finds a\nsolution to” and “solves”), based their tendency to\nconnect the same, or similar, pairs of nominals. Many of the rules\nwere later refined by addition of type constraints to the variables,\nobtained by abstracting from particular nominals via WordNet (Pantel\net al.  2007).   An approach to OIE designed for maximum speed is exemplified by the\nTextRunner system (Banko et al. 2007).  TextRunner is\nextraction pattern-based, but rather than employing patterns tuned to\na few selected relations, it uses a range of patterns obtained\nautomatically from a syntactically parsed training corpus, weighted\nvia Bayesian machine learning methods to extract miscellaneous\nrelations sentence-by-sentence from text. A rather different approach,\ntermed “open knowledge extraction” (OKE), derives logical forms from\nparsed sentences, and simplifies and abstracts these so that they will\ntend to reflect general properties of the world. This is exemplified\nby the Knext system (KNowledge EXtraction from Text)\n(e.g., Schubert and Tong 2003). For example, the sentence \n“I read a very informative book about China” allows KNEXT to abstract\n“factoids” to the effect that a person may occasionally read a book,\nand that books may occasionally be informative, and may occasionally\nbe about a country. (Note that the specific references to the speaker\nand China have been abstracted to classes.) Another interesting\ndevelopment has been the extraction of script-like sequences of\nrelations from large corpora by collocational methods (see Chambers\nand Jurafsky 2009). For example, the numerous newswire reports about\narrest and prosecution of criminals can be mined to abstract typical\nevent types involved, in chronological order, such as arrest,\narraignment, plea, trial, and so on. A difficulty in all of this work\nis that most of the knowledge obtained is too ambiguously and\nincompletely formulated to provide a basis for inference chaining (but\nsee for example Van Durme et al. 2009; Gordon and Schubert\n2010; Schoenmackers et al. 2010). \nThe crowdsourcing approach to the acquisition of general knowledge\nconsists of soliciting verbally expressed information, or annotations\nof such information, from large numbers of web users, sometimes using\neither small financial rewards or the challenge of participating in\nsimple games as inducements (Havasi et al. 2007; von Ahn\n2006). Crowdsourcing has proved quite reliable for simple\nannotation/classification tasks (e.g., Snow et al.\n2008; Hoffmann et al. 2009).  However, general knowledge\noffered by non-expert users is in general much less carefully\nformulated than, say, encyclopedia entries or word sense glosses in\nlexicons, and still requires natural language processing if formal\nstatements are to be abstracted. Nevertheless, the\nOpen Mind Common Sense\n project has produced a\nrelational network of informal commonsense knowledge (ConceptNet),\nbased on simple English statements from worldwide contributors, that\nproved useful for improving interpretation in speech recognition and\nother domains (Lieberman et al. 2004; Faaborg et al.\n2005).  The overall picture that emerges is that large-scale resources of\nknowledge for language, whether lexical or about the world, still\nremain too sparse and too imprecise to allow scaling up of\nnarrow-domain NLU and dialogue systems to broad-coverage\nunderstanding.  But such knowledge is expected to prove crucial\neventually in general language understanding, and so the quest for\nacquiring this general knowledge remains intensely active. “All\nthe thousands of times you've heard clause-final auxiliary\nverbs uncontracted strengthen the probability that they're not allowed\nto contract.”\n\n–Geoff Pullum (2011) We have already referred to miscellaneous statistical models and\ntechniques used in various computational tasks, such as\n(in section 2) HMMs in POS tagging,\nprobabilistic grammar modeling and parsing, statistical semantics,\nsemantic disambiguation (word senses, quantifier scope, etc.), plan\nrecognition, discourse modeling, and knowledge extraction from\ntext. Here we try to provide a brief, but slightly more systematic\ntaxonomy of the types of tasks addressed in statistical NLP, and some\nsense of the modeling techniques and algorithms that are most commonly\nused and have made statistical NLP so predominant in recent years,\nchallenging the traditional view of computational linguistics. This traditional view focuses on deriving meaning, and rests on the\nassumption that the syntactic, semantic, pragmatic, and world knowledge\nemployed in this derivation is “crisp” as opposed to probabilistic;\ni.e., the distributional properties of language are a mere byproduct of\nlinguistic communication, rather than an essential factor in language\nunderstanding, use, or even learning. Thus the emphasis, in this\nview, is on formulating nonprobabilistic syntactic, semantic,\npragmatic, and KR theories to be deployed in language understanding and\nuse. Of course, the problem of ambiguity has always been a focal issue\nin building parsers and language understanding systems, but the\nprevailing assumption was that ambiguity resolution could be\naccomplished by supplementing the interpretive routines with some\ncarefully formulated heuristics expressing syntactic and semantic\npreferences. However, experience has revealed that the ambiguities that afflict\nthe desired mappings are far too numerous, subtle, and interrelated to\nbe amenable to heuristic arbitration. Instead, linguistic phenomena\nneed to be treated as effectively stochastic, and the distributional\nproperties resulting from these stochastic processes need to be\nsystematically exploited to arrive at reasonably reliable hypotheses\nabout underlying structure. (The Geoff Pullum quote above is relevant\nto this point: The inadmissiblity of contracting the first occurrence\nof I am to I'm in “I'd rather be hated for who\nI am, than loved for who I am not” is not easily ascribed to any\ngrammatical principle, yet—based on positive evidence\nalone—becomes part of our knowledge of English usage.) Thus the\nemphasis has shifted, at least for the time being, to viewing NLP as a\nproblem of uncertain inference and learning in a stochastic\nsetting. This shift is significant from a philosophical perspective, not\njust a practical one: It suggests that traditional thinking about\nlanguage may have been too reliant on introspection. The limitation of\nintrospection is that very little of what goes on in our brains when\nwe comprehend or think about language is accessible to consciousness\n(see for example the discussion of “two-channel\nexperiments” in Baars 1997). We consciously register\nthe results of our understanding and thinking, apparently in\nsymbolic form, but not the understanding and\nthinking processes themselves; and these symbolic\nabstractions, to the extent that they lack quantitative or\nprobabilistic dimensions, can lead us to suppose that the underlying\nprocessing is nonquantitative as well. But the successes of\nstatistical NLP, as well as recent developments in cognitive science\n(e.g., Fine et al. 2013; Tenenbaum et al. 2011; Chater and\nOaksford 2008) suggest that language and thinking are not only\nsymbolic, but deeply quantitative and in particular probabilistic. For the first twenty years or so, the primary goals in statistical\nNLP  have been to assign labels, label sequences, syntax trees,\nor translations to linguistic inputs, using statistical language\nmodels trained on large corpora of observed language use. More fully,\nthe types of tasks addressed can be grouped roughly as follows (where\nthe appended keywords indicate typical applications): These\ngroups may seem to differ haphazardly, but as we will further discuss,\ncertain techniques and distinctions are common to many of them, notably We now try to provide some intuitive insight into the most\nimportant techniques and distinctions involved in the seven groups of\ntasks above.  For this purpose, we need not comment further on\nquantifier scoping (in the fourth group) or any of the items in the\nsixth and seventh groups, as these are for the most part covered\nelsewhere in this article. In all cases, the two major requirements\nare the development (aided by learning) of a probabilistic model\nrelating linguistic inputs to desired outputs, and the algorithmic use\nof the model in assigning labels or structures to previously unseen\ninputs.   Text and document classification: In classifying substantial\ndocuments, the features used might be normalized occurrence\nfrequencies of particular words (or word classes) and\npunctuation. Especially for shorter texts, various discrete features\nmay be included as well, such as 0, 1-valued functions indicating the\npresence or absence of certain key words or structural features. In\nthis way documents are represented as numerical vectors, with\nvalues  in a high-dimensional space, with separate classes\npresumably forming somewhat separate clusters in that space. A variety\nof classical pattern recognition techniques are applicable to the\nproblem of learning to assign new documents (as vectors) to the\nappropriate class (e.g., Sebestyen 1962; Duda and Hart 1973). Perhaps\nthe simplest approach (most easily applied when features are binary)\nis a naïve Bayesian one, which assumes that each class generates\nfeature values that are independent of one another. The generative\nfrequencies are estimated from the training data, and class membership\nprobabilities for an unknown document (vector) are computed via Bayes'\nrule (which can be done using successive updates of the prior class\nprobabilities).  Choosing the class with the highest resultant\nposterior probability then provides a decision criterion. A common\ngenerative model for real-valued features, allowing for feature\ninteractions, views the known members of any given class as a sample\nof a multivariate normal (Gaussian) random variable. Learning in this\ncase consists of estimating the mean and covariance matrix of each\nclass (an example of maximum likelihood estimation). A traditional discriminative approach, not premised on any\ngenerative model, involves the computation of hyperplanes that\npartition the clusters of known class instances from one another\n(optimizing certain metrics involving in-class and between-class\nvariance); new instances are assigned to the class into whose\npartition they fall. Perceptrons provide a related technique,\ninsofar as they decide class membership on the basis of a linear\ncombination of feature values; their particular advantage is that they\ncan learn incrementally (by adjusting feature weights) as more and\nmore training data become available. Another durable discriminative\napproach—not dependent on linear separability of classes—is the\nk nearest neighbors (kNN) method, which assigns an unknown\ntext or document to the class that is most prevalent among its k\n(e.g., 1–5) nearest neighbors in vector space. While all the\npreviously mentioned methods depended on parameter estimation (e.g.,\ngenerative probabilities, Gaussian parameters, or coefficients of\nseparating planes), kNN uses no such parameters—it is\nnonparametric; however, finding a suitable measure of\nproximity or similarity can be challenging, and errors due to\nhaphazard local data point configurations in feature space are hard to\navoid. Another nonparametric discriminative method worth mentioning is\nthe use of decision trees, which can be learned using\ninformation-theoretic techniques; they enable choice of a class by\nfollowing a root-to-leaf path, with branches chosen via tests on\nfeatures of a given input vector. A potentially useful property is\nthat learned decision trees can provide insight into what the most\nimportant features are (such insight can also be provided by\ndimensionality reduction methods). However, decision trees\ntend to converge to nonglobal optima (global optimization is NP-hard),\nand by splitting data, tend to block modeling of feature interactions;\nthis defect can be alleviated to some extent through the use of\ndecision forests.  Having mentioned some of the traditional classification methods, we\nnow sketch two techniques that have become particularly prominent in\nstatistical NLP since the 1990s. The first, with mathematical roots\ndating to the 1950s, is maximum entropy (MaxEnt), also called\n(multinomial) logistic regression (e.g., Ratnaparkhi\n1997). Features in this case are any desired 0, 1-valued (binary)\nfunctions of both a given linguistic input and a possible class. (For\ncontinuous features, supervised or unsupervised\ndiscretization methods may be applied, such as entropy-based\npartitioning into some number of intervals.) Training data provide\noccurrence frequencies for these features, and a distribution is\nderived for the conditional probability of a class, given a\nlinguistic input. (As such, it is a discriminative method.) As\nits name implies, this conditional probability function is a\nmaximum-entropy distribution, constrained to conform with the binary\nfeature frequencies observed in the training data. Its form (apart\nfrom a constant multiplier) is an exponential whose exponent is a\nlinear combination of the binary feature values for a given input and\ngiven class. Thus it is a log-linear model (a distribution\nwhose logarithm is linear in the features)—a type of model now\nprevalent in many statistical NLP tasks.  Note that since its\nlogarithm is a linear combination of binary feature values for any\ngiven input and any given class, choosing the maximum-probability\nclass for a given input amounts to linear decision-making, much as in\nsome of the classical methods; however, MaxEnt generally provides\nbetter classification performance, and the classification\nprobabilities it supplies can be useful in further computations (e.g.,\nexpected utilities). Another method important in the emergence and successes of\nstatistical NLP is the support vector machine (SVM) method\n(Boser et al. 1992; Cortes and Vapnik 1995). The great\nadvantage of this method is that it can in principle distinguish\narbitrarily configured classes, by implicitly projecting the original\nvectors into a higher- (or infinite-) dimensional space, where the\nclasses are linearly separable.  The projection is mediated by a\nkernel function—a similarity metric on pairs of vectors,\nsuch as a polynomial in the dot product of the two vectors. Roughly\nspeaking, the components of the higher-dimensional vector correspond\nto terms of the kernel function, if it were expanded out as a sum of\nproducts of the features of the original, unexpanded pair of\nvectors. But no actual expansion is performed, and moreover the\nclassification criterion obtained from a given training corpus only\nrequires calculation of the kernel function for the given feature\nvector (representing the document to be classified) paired with\ncertain special “support vectors”, and comparison of a linear\ncombination of the resulting values to a threshold. The support\nvectors belong to the training corpus, and define two parallel\nhyperplanes that separate the classes in question as much as possible\n(in the expanded space). (Hence this is a “max-margin” discriminative\nmethod.) SVMs generally provide excellent accuracy, in part because\nthey allow for nonlinear feature interaction (in the original space),\nand in part because the max-margin method focuses on class separation,\nrather than conditional probability modeling of the classes. On the\nother hand, MaxEnt classifiers are more quickly trainable than SVMs,\nand often provide satisfactory accuracy.  General references covering\nthe classification methods we have sketched are (Duda et al.\n2001; Bishop 2006).  Classification of selected words or phrases in sentential or\nbroader contexts: As noted earlier, examples include WSD, named\nentity recognition, and sentence boundary detection. The only point of\ndistinction from text/document classification is that it is not a\nchunk of text as a whole, but rather a word or phrase in the context\nof such a chunk that is to be classified. Therefore features are\nchosen to reflect both the features of the target word or phrase (such\nas morphology) and the way it relates to its context, in terms of,\ne.g., surrounding words or word categories, (likely) local syntactic\ndependency relations, and features with broader scope such as word\nfrequencies or document class. Apart from this difference in how\nfeatures are chosen, the same (supervised) learning and classification\nmethods discussed above can be applied.  However, sufficiently\nlarge training corpora may be hard to construct. For example, in\nstatistical WSD (e.g., Yarowsky 1992; Chen et al.\n2009),  since thousands of words have multiple senses in sources\nsuch as WordNet, it is difficult to  construct  a\nsense-annotated training corpus that contains sufficiently many\noccurrences of all of these senses to permit statistical\nlearning. Thus annotations are typically restricted to  the\nsenses of a few polysemous words, and statistical WSD has been shown\nto be feasible for the selected words, but broad-coverage WSD tools\nremain elusive. \nSequence labeling: There is a somewhat arbitrary line between\nthe preceding task and sequence labeling. For example, it is quite\npossible to treat POS tagging as a task of classifying words in a text\nin relation to their context.  However, such an approach fails to\nexploit the fact that the classifications of adjacent words are\ninterdependent. For example, in the sentence (from the web) \n“I don't fish like most people”, the occurrence of don't\nshould favor classification of fish as a verb, which in turn\nshould favor classification of like as a preposition. (At\nleast such preferences make sense for declarative sentences; replacing\n‘I’ by ‘why’ would change matters—see\nbelow.) Such cascaded influences are not easily captured through\nsuccessive independent classifications, and they motivate generative\nsequence models such as HMMs. For POS tagging, a labeled training\ncorpus can supply estimates of the probability of any POS for the next\nword, given the POS of the current word. If the corpus is large\nenough, it can also supply estimates of word “emission” probabilities\nfor a large proportion of words generally seen in text, i.e.,\ntheir probability of occurring, given the POS label. (Smoothing\ntechniques are used to fill in non-zero probabilities for unknown\nwords, given a POS.) We previously mentioned the Viterbi\nalgorithm as an efficient dynamic programming algorithm for\napplying an HMM (trained as just mentioned) to the task of assigning a\nmaximum-probability POS tag sequence to the words of a text.  Two\nrelated algorithms, the forward and backward\nalgorithms, can be used to derive probabilities of the\npossible labels at each word position i, which may be more\nuseful than the “best” label sequence for subsequent higher-level\nprocessing. The forward algorithm in effect (via dynamic\nprogramming) sums the probabilities of all label sequences up to\nposition i that end with a specified label X at word\nposition i and that generate the input up to (and including)\nthat word. The backward algorithm sums the probabilities of\nall label sequences that begin with label X at position\ni, and generate the input from position\ni+1 to the end.  The product of the forward\nand backward probabilities, normalized so that the probabilities of\nthe alternative labels at position i sum to 1, give the\nprobability of X at i, conditioned on the entire\ninput.  All learning methods referred to so far have been supervised\nlearning methods—a corpus of correctly labeled texts was\nassumed to be available for inferring model parameters. But methods\nhave been developed for unsupervised (or\nsemi-supervised) learning as well. An important unsupervised\nmethod of discovering HMM models for sequence labeling is the\nforward-backward (or Baum-Welch) algorithm. A simple\nversion of this algorithm in the case of POS tagging relies on a\nlexicon containing the possible tags for each word (which are easily\nobtained from a standard lexicon). Some initial, more or less\narbitrarily chosen values of the HMM transition and emission\nprobabilities are then iteratively refined based on a training corpus.\nA caricature of the iterative process would be this: We use the\ncurrent guesses of the HMM parameters to tag the training corpus; then\nwe re-estimate those parameters just as if the corpus were\nhand-tagged. We repeat these two steps till convergence. The actual\nmethod used is more subtle in the way it uses the current HMM\nparameters. (It is a special case of EM—Expectation\nMaximization.) Rather than re-estimating the parameters based on\noccurrence frequencies in the current “best” tag sequence, it uses the\nexpected number of occurrences of particular pairs of\nsuccessive states (labels), dividing this by the expected\nnumber of occurrences of the first member of the pair. These expected\nvalues are determined by the conditional probability distribution over\ntag sequences, given the training corpus and the current HMM\nparameters, and can be obtained using the forward and backward\nprobabilities as described above (and thus, conditioned on the entire\ncorpus). Revised emission probabilities for any X →\nw can be computed as the sum of probabilities of\nX-labels at all positions where word w occurs in the\ncorpus, divided by the sum of probabilities of X-labels at\nall positions, again using (products of) forward and backward\nprobabilities. Unfortunately EM is not guaranteed to find a globally optimal\nmodel.  Thus good results can be achieved only by starting with a\n“reasonable” initial HMM, for example assigning very low\nprobabilities to certain transitions (such as determiner →\ndeterminer, determiner → verb, adjective → verb).\nSemi-supervised learning might start with a relatively small\nlabeled training corpus, and use the corresponding HMM parameter\nestimates as a starting point for unsupervised learning from further,\nunlabeled texts.   A weakness of HMMs themselves is that the Markov assumption\n(independence of non-neighbors, given the neighbors) is violated by\nlonger-range dependencies in text. For example, in the context of a\nrelative clause (signaled by a noun preceding that clause), a\ntransitive verb may well lack an NP complement ( “I collected\nthe money he threw down on the table.”), and as a result,\nwords following the verb may be tagged incorrectly (down as a\nnoun).  A discriminative approach that overcomes this difficulty is\nthe use of conditional random fields (CRFs). Like HMMs (which\nthey subsume), these allow for local interdependence of hidden states,\nbut employ features that depend not only on adjacent pairs of these\nstates, but also on any desired properties of the entire input.\nMathematically, the method is very similar to MaxEnt (as discussed\nabove). The feature coefficients can be learned from training data\neither by gradient ascent or by an incremental dynamic programming\nmethod related to the Baum-Welch algorithm, called improved\niterative scaling (IIS) (Della Pietra et al. 1997; Lafferty et\nal. 2001).  CRFs have been successful in many applications other than\nPOS tagging, such as sentence and word boundary detection (e.g., for\nChinese), WSD, extracting tables from text, named entity recognition,\nand—outside of NLP—in gene prediction and computer\nvision. Structure assignment to sentences: The use of probabilistic\ncontext-free grammars (PCFGs) was briefly discussed in \nsection 2. Supervised learning of PCFGs can be\nimplemented much like supervised learning of HMMs for POS tagging. The\nrequired conditional probabilities of phrase expansion are easily\nestimated if a large corpus annotated with phrase bracketings (a\ntreebank) is available (though estimates of POS → word\nexpansion probabilities are best supplemented with additional data).\nOnce learned, a PCFG can be used to assign probabilistically weighted\nphrase structures to sentences using the chart parsing method\nmentioned in\nsection 2—again a dynamic programming method.  Also, unsupervised learning of PCFGs is possible using the EM\napproach. This is important, since it amounts to grammar\ndiscovery.  The only assumption we start with, theoretically,\nis that there is some maximum number of nonterminal symbols, and each\ncan be expanded into any two nonterminals or into any word (Chomsky\nnormal form). Also we associate some more or less arbitrary initial\nexpansion probabilities with these rules. The probabilities are\niteratively revised using expected values of the frequency of\noccurrence of the possible expansions, based on the current PCFG\nmodel, conditioned on the corpus. The analogue of the forward-backward\nalgorithm for computing these expectations is the\ninside-outside algorithm. Inside probabilities specify the\nprobability that a certain proper segment of a given sentence will be\nderived from a specified nonterminal symbol. Outside probabilities\nspecify the probability that all but a certain segment of the\ngiven sentence will be derived from the start (sentence) symbol, where\nthat “missing” segment remains to be generated from a specified\nnonterminal symbol. The inside and outside probabilities play roles\nanalogous to the backward and forward probabilities in HMM learning\nrespectively. Conceptually, they require summations over exponentially\nmany possible parse trees for a given sentence, but in fact inside\nprobabilities can be computed efficiently by the CYK algorithm\n(section 2), and outside probabilities can also be computed\nefficiently, using a top-down recursive “divide and conquer” algorithm\nthat makes use of previously computed inside probabilities. Modest successes have been achieved in learning grammars in this\nway. The complexity is high (cubic-time in the size of the training\ncorpus as well as in the number of nonterminals), and as noted, EM\ndoes not in general find globally optimal models.  Thus it is\nimportant to place some constraints on the initial grammar, e.g.,\nallowing nonterminals to generate either pairs of nonterminals or\nwords, but not both, and also severely limiting the number of allowed\nnonterminals. A method of preferring small rule sets over large ones,\nwithout setting a fixed upper bound, is the use of a Dirichlet\nprocess that supplies a probability distribution over the\nprobabilities of an unbounded number of rules. (This method is\nnonparametric, in the sense that it does not commit to any fixed\nnumber of building blocks or parameters in the modeling.) Whatever\nmethod of bounding the rules is used, the initial PCFG must be\ncarefully chosen if a reasonably good, meaningful rule set is to be\nlearned. One method is to start with a linguistically motivated\ngrammar and to use “symbol splitting” (also called\n“state splitting”) to generate variants of nonterminals\nthat differ in their expansion rules and\nprobabilities. Recent spectral algorithms offer a relatively\nefficient, and globally optimal, alternative to EM (Cohen et\nal. 2013), and they can be combined with symbol splitting.   Like HMMs, PCFGs are generative models, and like them suffer from\ninsufficient sensitivity of local choices to the larger context. CRFs\ncan provide greater context-sensitivity (as in POS tagging and other\ntypes of sequence labeling); though they are not directly suited to\nstructure assignment to text, they can be used to learn shallow\nparsers, which assign phrase types only to nonrecursive phrases (core\nNPs, PPs, VPs, etc.) (Sha and Pereira 2003). \nIn the current grammar-learning context, we should also mention\nconnectionist models once more. Such models have shown some capacity\nfor learning to parse from a set of training examples, but achieving\nfull-scale parsing in this way remains a challenge. Also a\ncontroversial issue is the capacity of nonsymbolic NNs to\nexhibit systematicity in unsupervised learning, i.e.,\ndemonstrating a capacity to generalize from unannotated examples. This\nrequires, for example, the ability to accept or generate sentences\nwherein verb arguments appear in positions different from those seen\nin the training set. According to Brakel and Frank (2009),\nsystematicity can be achieved with simple recurrent networks\n(SRNs). However, computational demonstrations have generally been\nrestricted to very simple, English-like artificial languages, at least\nwhen inputs were unannotated word streams. \nA structure assignment task that can be viewed as a step towards\nsemantic interpretation is semantic role labeling\n(Palmer et al. 2010). The goal is to assign thematic roles\nsuch as agent, theme, recipient, etc. to core phrases or\nphrasal heads in relation to verbs (and perhaps other\ncomplement-taking words). While this can be approached as a sequence\nlabeling problem, experimental evidence shows that computing parse\ntrees and using resulting structural features for role assignment (or\njointly computing parse trees and roles) improves precision. A\nfrequently used training corpus for such work is PropBank, a version\nof the Penn Treebank annotated with “neutral” roles arg0, arg1,\narg2, etc. Sentence transduction: The most intensively studied type of\nstatistical sentence transduction to date has been statistical\nMT (SMT) (e.g., Koehn 2010; May 2012). Its successes beginning in\nthe late 1980s and early 90s came as something of a surprise to the\nNLP community, which had been rather pessimistic about MT prospects\never since the report by Bar-Hillel (1960) and the ALPAC report\n(Pierce et al. 1966), negatively assessing the results of a\nmajor post-WW2 funding push in MT by the US government. MT came to be\nviewed as a large-scale engineering enterprise that would not have\nbroad impacts until it could be adequately integrated with semantics\nand knowledge-based inference. The statistical approach emerged in the\nwake of successful application of “noisy channel” models to speech\nrecognition in the late 1970s and during the 80s, and was propelled\nforward by new developments in machine learning and the increasing\navailability of large machine-readable linguistic corpora, including\nparallel texts in multiple languages.  The earliest, and simplest type of translation method\nwas word-based. This was grounded in the following sort of\nmodel of how a foreign-language sentence f  (say, in\nFrench) is generated from an English sentence e (which we\nwish to recover, if the target language is English): First, e\nis generated according to some simple model of English, for instance\none based on bigram frequencies. Individual words of e are\nthen assumed to generate individual words of f with some\nprobability, allowing for arbitrary word-order scrambling (or biased\nin some way). In learning such a model, the possible correspondences\nand word-translation probabilities can be estimated from parallel\nEnglish-French corpora, whose sentences and words have\nbeen aligned by hand or by statistical techniques. Such a\nmodel can then be used for “decoding” a given French\nsentence f into an English sentence e by Bayesian\ninference—we derive e as the English sentence with highest\nposterior probability, given its French “encoding” as f. This\nis accomplished with dynamic programming algorithms, and might use an\nintermediate stage where the n best choices of e are\ncomputed (for some predetermined n), and subsequently\nre-ranked discriminatively using features of e and f\nignored by the generative model. However, the prevailing SMT systems (such as Google\nTranslate or Yahoo! Babel Fish) are\nphrase-based rather than word-based. Here\n“phrase” refers to single words or groups of words that\ntend to occur adjacent to each other. The idea is that phrases are\nmapped to phrases, for example, the English word pair red\nwine to French phrases vin rouge, du vin rouge, or\nle vin rouge. Also, instead of assuming arbitrary word order\nscrambling, reordering models are used, according to which a\ngiven phrase may tend to be swapped with the left or right neighboring\nphrase or displaced from the neighbors, in the translation\nprocess. Furthermore, instead of relying directly on a Bayesian\nmodel, as in the word-based approach, phrase-based approaches\ntypically use a log-linear model, allowing for incorporation of\nfeatures reflecting not only the language model (such as trigram\nfrequencies), the phrase translation model (such as phrase translation\nfrequencies), and the reordering model, but also miscellaneous\nfeatures such as the number of words created, the number of phrase\ntranslations used, and the number of phrase reorderings (with larger\npenalties for larger displacements). While phrase-based SMT models have been quite successful, they are\nnonetheless prone to production of syntactically disfluent or\nsemantically odd translations, and much recent research has sought to\nexploit linguistic structure and patterns of meaning to improve\ntranslation quality. Two major approaches to syntactic transfer\nare hierarchical phrase-based translation\nand tree-to-string (TTS) transduction\nmodels.  Hierarchical phrase-based approaches use synchronous\ngrammar rules, which simultaneously expand partial derivations of\ncorresponding sentences in two languages. These are automatically\ninduced from an aligned corpus, and the lowest hierarchical layer\ncorresponds to phrase-to-phrase translation rules like those in\nordinary phrase-based translation. While quite successful, this\napproach provides little assurance that “phrases” in the\nresulting synchronous grammars are semantically coherent units, in the\nlinguistic sense. TTS models obtain better coherency through use of\nparsers trained on phrase-bracketed text corpora (treebanks). The\nencoding of English sentences into French (in keeping with our\npreviously assumed language pair) is conceptualized as beginning with\na parsed English sentence, which is then transformed by (learned)\nrules that progressively expand the original or partially transformed\npattern of phrases and words until all the leaves are French\nwords. Apart from MT, another important type of sentence transduction\nis semantic parsing, in the sense of mapping sentences in\nsome domain to logical forms usable for question answering. (Note that\nsemantic role labeling, discussed above, can also be viewed as a step\ntowards semantic parsing.) Several studies in this relatively recent\narea have employed supervised learning, based on training corpora\nannotated with LFs (e.g., Mooney 2007; Zettlemoyer & Collins 2007)\nor perhaps syntactic trees along with LFs (e.g., Ge and Mooney\n2009). Typical domains have been QA about geography (where LFs are\ndatabase queries), about Robocup soccer, or about travel\nreservations. Even unsupervised learning has been shown to be possible\nin restricted domains, such as QA based on medical abstracts (Poon and\nDomingos 2009) or the travel reservation domain (Poon 2013). Ideas\nused in this work include forming synonym clusters of nominal terms\nand verbal relations much as in Lin and Pantel's DIRT system, with\ncreation of logical names (reflecting their word origins) for these\nconcepts and relations; and learning (via Markov logic, a\ngeneralization of Markov networks) to annotate the nodes of\ndependency parse trees with database entities, types, and relations on\nthe basis of a travel reservation dialogue corpus (where the data\nneeded for the travel agent's answers are known to lie in the\ndatabase). Whether such methods can be generalized to less restricted\ndomains and forms of language remains to be seen. The recent creation\nof a general corpus annotated with an “abstract meaning\nrepresentation”, AMR, is likely to foster progress in that direction\n(Banarescu et al. 2013). The topics we have touched on in this section are technically\ncomplex, so that our discussion has necessarily been shallow. General\nreferences for statistical language processing are Manning and\nSchütze 1999 and Jurafsky and Martin 2009. Also the\nstatistical NLP community has developed remarkably comprehensive\ntoolkits for researchers, such as MALLET (MAchine Learning for\nLanguagE Toolkit), which includes brief explanations of many of the\ntechniques.   What are the prospects for achieving human-like language learning\nin machines?  There is a growing recognition that statistical learning\nwill have to be linked to perceptual and conceptual modeling of the\nworld.  Recent work in the area of grounded language learning\nis moving in that direction. For example, Kim and Mooney (2012)\ndescribe methods of using sentences paired with graph-based\ndescriptions of actions and contexts to hypothesize PCFG rules for\nparsing NL instructions into action representations, while learning\nrule probabilities with the inside-outside algorithm. However, they\nassumed a very restricted domain, and the question remains how far the\nmodeling of perception, concept formation, and of semantic and\nepisodic memory needs to be taken to support unrestricted language\nlearning.  As in the case of world knowledge acquisition by\nmachines (see the preceding section), the modeling capabilities may\nneed to achieve equivalence with those of a newborn, allowing for\nencoding percepts and ideas in symbolic and imagistic languages of\nthought, for taxonomizing entity types,  recognizing animacy and\nintentionality, organizing and abstracting spatial relations and\ncausal chains of events, and more. Providing such capabilities is\nlikely to require, along with advances in our understanding of\ncognitive architecture, resolution of the very issues concerning the\nrepresentation and use of linguistic, semantic, and world knowledge\nthat have been the traditional focus in computational\nlinguistics.  \nAs indicated at the outset, applications of computational\nlinguistics techniques range from those minimally dependent on\nlinguistic structure and meaning, such as document retrieval and\nclustering, to those that attain some level of competence in\ncomprehending and using language, such as dialogue agents that provide\nhelp and information in limited domains like personal scheduling,\nflight booking, or help desks, and intelligent tutoring systems. In the\nfollowing we enumerate some of these applications. In several cases\n(especially machine translation) we have already provided considerable\ndetail, but the intent here is to provide a bird's eye view of\nthe state of the art, rather than technical elucidations. With the advent of ubiquitous computing, it has become increasingly\ndifficult to provide a systematic categorization of NLP applications:\nKeyword-based retrieval of documents (or snippets) and database access\nare integrated into some dialogue agents and many voice-based services;\nanimated dialogue agents interact with users both in tutoring systems\nand games; chatbot techniques are incorporated into various useful or\nentertaining agents as a backends; and language-enabled robots, though\ndistinctive in combining vision and action with language, are gradually\nbeing equipped with web access, QA abilities, tutorial functions, and\nno doubt eventually with collaborative problem solving abilities. Thus\nthe application categories in the subsections that follow, rather than\nbeing mutually exclusive, are ever more interwined in practice.\n One of the oldest MT systems is SYSTRAN,\nwhich was developed as a rule-based system beginning in the 1960s, and\nhas been extensively used by US and European government agencies, and\nalso in Yahoo! Babel Fish and (until 2007) in Google Translate. \nIn 2010, it was hybridized with statistical MT techniques. As mentioned, Google\nTranslate currently uses phrase-based MT, with English serving as an\ninterlingua for the majority of language pairs. Microsoft's Bing\nTranslator employs dependency structure analysis together with\nstatistical MT. Other very comprehensive translation systems include\nAsia Online and WorldLingo. Many systems for small language groups\nexist as well, for instance for translating between Punjabi and Hindi\n(the Direct MT system), or between a few European languages (e.g.,\nOpenLogos, IdiomaX, and GramTrans). Translations remain error-prone, but their quality is usually\nsufficient for readers to grasp the general drift of the source\ncontents. No more than that may be required in many cases, such as\ninternational web browsing (an application scarcely anticipated in\ndecades of MT research). Also, MT applications on hand-held devices,\ndesigned to aid international travellers, can be sufficiently accurate\nfor limited purposes such as asking directions or emergency help,\ninteracting with transportation personnel, or making purchases or\nreservations,  When high-quality translations are required,\nautomatic methods can be used as an aid to human translators, but\nsubtle issues may still absorb a large portion of a translator's time. \nInformation retrieval has long been a central theme of information\nscience, covering retrieval of both structured data such as are found\nin relational databases as well as unstructured text documents (e.g.,\nSalton 1989). Retrieval criteria for the two types of data are not\nunrelated, since both structured and unstructured data often require\ncontent-directed retrieval. For example, while users of an\nemployee database may wish at times to retrieve employee records by\nthe unique name or ID of employees, at other times they may wish to\nretrieve all employees in a certain employment category, perhaps with\nfurther restrictions such as falling into a certain salary bracket.\nThis is accomplished with the use of “inverted files” that\nessentially index entities under their attributes and values rather\nthan their identifiers. In the same way, text documents might be\nretrieved via some unique label, or they might instead be\nretrieved in accord with their relevance to a certain query\nor topic header. The simplest notion of relevance is that the\ndocuments should contain the terms (words or short phrases) of the\nquery.  However, terms that are distinctive for a document should\nbe given more weight. Therefore a standard measure of relevance, given\na particular query term, is the tf–idf  (term\nfrequency–inverse document frequency) for the term, which\nincreases (e.g.,\nlogarithmically) with the frequency of occurrences of\nthe term in the document but is discounted to the extent that it occurs\nfrequently in the set of documents as a whole. Summing the tf-idf's of\nthe query terms yields a simple measure of document relevance. Shortcomings of this method are first, that it underrates term\nco-occurrences if each term occurs commonly in the document collection\n(for instance, for the query “rods and cones of the eye”,\nco-occurrences of rods, cones, and eye may\nwell characterize relevant documents, even though all three terms\noccur quite commonly in non-physiological contexts), and second, that\nrelevant documents might have few occurrences of the query terms,\nwhile containing many semantically related terms. Some of the vector\nmethods mentioned in connection with document clustering can be used\nto alleviate these shortcomings. We may reduce the dimensionality of\nthe term-based vector space using LSA, obtaining a much smaller\n“concept space” in which many terms that tend to co-occur\nin documents will have been merged into the same dimensions\n(concept). Thus sharing of concepts, rather than sharing of specific\nterms, becomes the basis for measuring relevance. Document clustering is useful when large numbers of documents need\nto be organized for easy access to topically related items, for\ninstance in collections of patent descriptions, medical histories or\nabstracts, legal precedents, or captioned images, often in hierarchical\nfashion. Clustering is also useful in exploratory data analysis (e.g.,\nin exploring\ntoken occurrences in an unknown language), and indirectly supports\nvarious NLP applications because of its utility in improving language\nmodels, for instance in providing word clusters to be used for backing\noff from specific words in cases of data sparsity. Clustering is widely used in other areas, such as biological and\nmedical research and epidemiology, market research and grouping and\nrecommendation of shopping items, educational research, social network\nanalysis, geological analysis, and many others. Document retrieval and clustering often serve as preliminary steps\nin information extraction (IE) or text mining, two overlapping\nareas concerned with extracting useful knowledge from documents, such\nas the main features of named entities (category, roles in relation to\nother entities, location, dates, etc.) or of particular types of\nevents, or inferring rule-like correlations between relational terms\n(e.g., that purchasing of one\ntype of product correlates with\npurchasing another). We will not attempt to survey IE/text mining applications\ncomprehensively, but the next two subsections, on summarization and\nsentiment analysis, are subareas of particular interest here because of\ntheir emphasis on the semantic content of texts.\n Extracting knowledge or producing summaries from unstructured text are ever more\nimportant applications, in view of the deluge of documents issuing\nforth from news media, organizations of every sort, and individuals.\nThis unceasing stream of information makes it difficult to gain an\noverview of the items relevant to some particular purpose, such as\nbasic data about individuals, organizations and consumer products, or\nthe particulars of accidents, earthquakes, crimes, company take-overs,\nproduct maintenance and repair activities, medical research results,\nand so on. One commonly used method in both knowledge extraction and certain\ntypes of “rote” summarization relies on the use of\nextraction patterns; these are designed to match the kinds of\nconventional linguistic patterns typically used by authors to express\nthe information of interest.  For example, text corpora or\nnewswire might be mined for information about companies, by keying in\non known company names and terms such as “Corp.”, \n“.com”, “headquartered at”, and “annual\nrevenue of”, as well as parts of speech and dependency\nrelations, and matching regular-expression patterns against local text\nsegments containing key phrases or positioned close to them. As\nanother example, summarization of earthquake reports might extract\nexpected information such as the epicenter of the quake, its magnitude\non the Richter scale, the time and duration of the event, affected\npopulation centers, extent of death tolls, injuries, and property\ndamage, consequences such as fires and tsunamis, etc. Extraction\npatterns can usually be thought of as targeting particular attributes\nin predetermined attribute-value frames (e.g., a frame for company\ninformation or a frame for facts about an earthquake), and the\nfilled-in frames may themselves be regarded as summaries, or may be\nused to generate natural-language summaries. Early systems of this\ntype were FRUMP (DeJong 1982) and JASPER (Andersen et al. 1992). Among\nthe hundreds of more modern extraction systems, a particularly\nsuccessful one in competitions has been SRI's\n“Fastus” (Hobbs et al. 1997). Note that whether a pattern-based system is viewed as a knowledge\nextraction system or summarization system depends on the text it is\napplied to. If all the information of interest is bundled together in a\nsingle, extended text segment (as in the case of earthquake reports),\nthen the knowledge extracted can be viewed as a summary of the segment.\nIf instead the information is selectively extracted from miscellaneous\nsentences scattered through large text collections, with most of the\nmaterial being ignored as irrelevant to the purposes of extraction,\nthen we would view the activity of the system as information extraction\nrather than summarization. When a document to be summarized cannot be assumed to fall into\nsome predictable category, with the content structured and expressed\nin a stereotyped way, summarization is usually performed by selecting\nand combining “central sentences” from the document. A\nsentence is central to the extent that many other sentences in the\ndocument are similar to it, in terms of shared word content or some\nmore sophisticated similarity measure such as one based on the tf-idf\nmetric for terms, or a cosine metric in a dimensionality-reduced\nvector space (thus it is as if we were treating individual sentences\nas documents, and finding a few sentences whose\n“relevance” to the remaining sentences is\nmaximal). However, simply returning a sequence of central sentences\nwill not in general yield an adequate summary. For example, such\nsentences may contain unresolved pronouns or other referring\nexpressions, whose referents may need to be sought in non-central\nsentences. Also, central “sentences” may actually be\nclauses embedded in lengthier sentences that contain unimportant\nsupplementary information. Heuristic techniques need to be applied to\nidentify and excise the extra material, and extracted clauses need to\nbe fluently and coherently combined. In other cases, complex\ndescriptions should be more simply and abstractly paraphrased. For\nexample, an appropriate condensation of a sentence such as “The\ntornado carried off the roof of a local farmhouse, and reduced its\nwalls and contents to rubble” might be “The tornado\ndestroyed a local farmhouse.” But while some of these issues are\npartially addressed in current systems, human-like summarization will\nrequire much deeper understanding than is currently\nattainable. Another difficulty in this area (even more so than in\nmachine translation) is the evaluation of summaries. Even human\njudgments differ greatly, depending, for instance, on the sensitivity\nof the evaluator to grammatical flaws, versus inadequacies in\ncontent. \nSentiment analysis refers to the detection of positive or negative\nattitudes (or more specific attitudes such as belief or contempt) on\nthe part of authors of articles or blogs towards commercial products,\nfilms, organizations, persons, ideologies, etc. This has become a very\nactive area of applied computational linguistics, because of its\npotential importance for product marketing and ranking, social network\nanalysis, political and intelligence analysis, classification of\npersonality types or disorders based on writing samples, and other\nareas. The techniques used are typically based on sentiment lexicons\nthat classify the affective polarity of vocabulary items, and on\nsupervised machine learning applied to texts from which word and\nphrasal features have been extracted and that have been hand-labeled\nas expressing positive or negative attitudes towards some\ntheme. Instead of manual labeling, existing data can sometimes be used\nto provide a priori classification information. For example,\naverage numerical ratings of consumer products or movies produced by\nbloggers may be used to learn to classify unrated materials belonging\nto the same or similar genres. If fact, affective lexical categories\nand contrast relations may be learnable from such data; for example,\nfrequent occurrences of phrases such as great movie\nor pretty good movie or terrible movie in blogs\nconcerning movies with high, medium, and low average ratings may well\nsuggest that great, pretty good, and\nterrible belong to a contrast spectrum ranging from a very\npositive to a very negative polarity. Such terminological knowledge\ncan in turn boost the coverage of generic sentiment lexicons. However,\nsentiment analysis based on lexical and phrasal features has obvious\nlimitations, such as obliviousness to sarcasm and irony ( “This\nis the most subtle and sensitive movie since The Texas Chainsaw\nMassacre”), quotation of opinions contrasting with the author's\n(“According to the ads, Siri is the greatest app since iTunes,\nbut in fact …”), and lack of understanding of entailments\n(“You'll be much better off buying a pair of woolen undies for\nthe winter than purchasing this item”). Thus researchers are\nattempting to integrate knowledge-based and semantic analysis with\nsuperficial word- and phrase-based sentiment analysis.\n \nCurrent chatbots are the descendants of Weizenbaum's ELIZA\n(see section 1.2), and are typically used (often with an animated\n“talking head” character) for entertainment, or to engage\nthe interest of visitors to the websites of certain\n“dotcoms”. They may be equipped with large hand-crafted\nscripts (keyword-indexed input-response schemas) that enable them to\nanswer simple inquiries about the company and their products, with some\nability to respond to miscellaneous topics and to exchange greetings\nand pleasantries. A less benign application is the use of chatbots\nposing as visitors to social network sites, or interactive game sites,\nwith the aim of soliciting private information from unwitting human\nparticipants, or recommending websites or products to them. As a\nresult, many social networking sites have joined other bot-targeted\nsites in using CAPTCHAS to foil bot entry.\n \nCompanionable dialogue agents (also called relational agents) have\nso far relied rather heavily on chatbot techniques, i.e., authored\ninput patterns and corresponding outputs. But the goal is to transcend\nthese techniques, creating agents (often with talking heads or other\nanimated characters) with personality traits and capable of showing\nemotion and empathy; they should have semantic and episodic memory,\nlearning about the user over the long term and providing services to\nthe user. Those services might include, besides companionship and\nsupport: advice in some areas of life, health and fitness, schedule\nmaintenance, reminders, question answering, tutoring (e.g., in\nlanguages), game playing, and internet services. Yorick Wilks has\nsuggested that ideally such characters would resemble “Victorian\ncompanions”, with such characteristics as politeness, discretion,\nmodesty, cheerfulness, and well-informedness (Wilks 2010). However, such goals are far from being achieved, as speech\nrecognition, language understanding, reasoning and learning are not\nnearly far enough advanced. As a noteworthy example of the state of\nthe art, we might mention the HWYD (“How Was Your Day”)\nsystem of Pulman et al.  (2010), which won a best demonstration prize\nat an autonomous agents conference. The natural language processing in\nthis system is relatively sophisticated. Shallow syntactic and\nsemantic processing is used to find instantiations of some 30\n“event templates”, such as ones for “argument at\nwork between X and Y,” or “meeting\nwith X about Y”. The interpretation process\nincludes reference and ellipsis resolution, relying on an information\nstate representation maintained by the dialogue manager. Goals\ngenerated by the dialogue manager lead to responses via planning,\nwhich involves instantiation and sequencing of response paradigms. The\nauthors report the system's ability to maintain consistent dialogues\nextending over 20 minutes. Systems of a rather different sort, aimed at clinically\nwell-founded health counseling, have been under development as\nwell. For example, the systems described in (Bickmore et\nal. 2011) rely on an extensive, carefully engineered\nformalization of clinically proven counseling strategies and\nknowledge, expressed within a description logic (OWL) and a\ngoal-directed task description language. Such systems have proved to\nperform in a way comparable to human counselors. However, though\ndialogues are plan-driven, they ultimately consist of scripted system\nutterances paired with multiple-choice lists of responses offered to\nthe client. Thus companionable systems remain very constrained in the dialogue\nthemes they can handle, their understanding of language, and \ntheir ability to bring extensive general knowledge to a conversation,\nlet alone to use such knowledge inferentially. \nText-based adventure (quest) games, such as Dungeons and Dragons, Hunt\nthe Wumpus (in its original version), and Advent began to be developed\nin the early and middle 1970s, and typically featured textual\ndescriptions of the setting and challenges confronting the player, and\nallowed for simple command-line input from the player to select\navailable actions (such as “open box”, “take\nsword” or “read note”). While the descriptions of\nthe settings (often accompanied by pictures) could be quite elaborate,\nmuch as in adventure fiction, the input options available to the\nplayer were, and have largely remained, restricted to simple\nutterances of the sort that can be anticipated or collected in\npre-release testing by the game programmers, and for which responses\ncan be manually prepared. Certainly more flexible use of NL (\n“fend off the gremlin with the sword!”, “If I give\nyou the gold, will you open the gate for me?”) would enliven the\ninteraction between player and the game world and the characters in\nit. In the 1980s and 90s text-based games declined in favor of games\nbased primarily on graphics and animation, though an online\ninteractive fiction community grew over the years that drove the\nevolution of effective interactive fiction development software. A\nhighly touted program (in the year 2000) was Emily Short's\n‘Galatea’, which enabled dialogue with an animated\nsculpture. However, this is still an elaborately scripted program,\nallowing only for inputs that can be heuristically mapped to one of\nvarious preprogrammed responses. Many games in this genre also make\nuse of chatbot-like input-output response patterns in order to gain a\nmeasure of robustness for unanticipated user inputs.\n \nThe most popular PC video games in the 1990s and beyond were Robyn and\nRand Miller's Myst, a first-person adventure game, and Maxis Software's\nThe Sims, a life-simulation game. Myst, though relying on messages in\nbooks and journals, was largely nonverbal, and The Sims' chief\ndeveloper, Will Wright, finessed the problem of natural language\ndialogue by having the inhabitants of SimCity babble in Simlish, a\nnonsense language incorporating elements of Ukrainian, French and\nTagalog. Commercial adventure games and visual novels continue to rely on\nscripted dialogue trees—essentially branching alternative\ndirections in which the dialogue can be expected to turn, with\nELIZA-like technology supporting the alternatives. More sophisticated\napproaches to interaction between users and virtual characters are\nunder development in various research laboratories, for example at the\nCenter for Human Modeling and Simulation at the University of\nPennsylvania, and the USC-affiliated Institute for Creative\nTechnologies. While the dialogues in these scenarios are still based\non carefully designed scripts, the interpretation of the user's spoken\nutterances exploits an array of well-founded techniques in speech\nrecognition, dialogue management, and reasoning. Ongoing research can\nbe tracked at venues such as IVA (Intelligent Virtual Agents), AIIDE\n(AI and Interactive Digital Entertainment), and AAMAS (Autonomous\nAgents and Multiagent Systems). \nThe topic of NL user interfaces subsumes a considerable variety of\nNL applications, ranging from text-based systems minimally dependent on\nunderstanding to systems with significant comprehension and inference\ncapabilities in text- or speech-based interactions. The following\nsubsections briefly survey a range of traditional and current\napplications areas.\n Text-based QA is practical to the extent that the types of\nquestions being asked can be expected to have ready-made answers\ntucked away somewhere in the text corpora being accessed by the QA\nsystem. This has become much more feasible in this age of burgeoning\ninternet content than a few decades ago, though questions still need\nto be straightforward, factual ones (e.g., \n“Who killed President Lincoln?”) rather than ones requiring inference (e.g., \n“In what century did Catherine the Great live?”, let alone\n\n“Approximately how many 8-foot 2-by-4s do I need to build a\n4-foot high, 15-foot long picket fence?”). Text-based QA begins with question classification (e.g., yes-no\nquestions, who-questions, what-questions, when-questions, etc.),\nfollowed by information retrieval for the identified type of question,\nfollowed by narrowing of the search to paragraphs and finally sentences\nthat may contain the answer to the question. The successive narrowing\ntypically employs word and other feature matching, and ultimately\ndependency and role matching, and perhaps limited textual inference to\nverify answer candidates. Textual inference may, for instance, use\nWordNet hypernym knowledge to try to establish that a given candidate\nanswer sentence supports the truth of the declarative version of the\nquestion. Since the chosen sentence(s) may contain irrelevant material\nand anaphors, it remains to extract the relevant material (which may\nalso include supporting context) and generate a well-formed,\nappropriate answer. Many early text-based QA systems up to 1976 are\ndiscussed in Bourne & Hahn 2003. Later surveys (e.g., Maybury\n2004) have tended to include the full spectrum of QA methods, but TREC\nconference proceedings \n(https://trec.nist.gov/) \nfeature numerous papers\non implemented systems for text-based QA. In open-domain QA, many questions are concerned with properties of\nnamed entities, such as birth date, birth place, occupation, and other\npersonal attributes of well-known present and historical individuals,\nlocations, ownership, and products of various companies, facts about\nconsumer products, geographical facts, and so on. For answering such\nquestions, it makes sense to pre-assemble the relevant factoids into a\nlarge knowledge base, using knowledge acquisition methods like those in\nsection 8. Examples of systems containing an abundance of factoids\nabout named entities are several developed at the University of\nWashington, storing factoids as text fragments, and various\nsystems that map harvested factoids into RDF\n(Resource Description Framework) triples\n (see references in Other Internet Resources). Some\nof these systems obtain their knowledge not only from open information\nextraction and targeted relation extraction, but also from such sources\nas Wikipedia “infoboxes” and (controlled) crowdsourcing. Here we are\nalso stretching the notion of question answering, since several of the\nmentioned systems require the use of key words or query patterns for\nretrieval of factoids. From a general user perspective, it is unclear how much added\nbenefit can be derived from such constructed KBs, given the remarkable\nability of Google and other search engines to provide rapid answers\neven to such questions as “Which European countries are\nlandlocked?” (typed without quotes—with quotes, Google\nfinds the top answer using True Knowledge), or “How many Supreme\nCourt justices did Kennedy appoint?” Nonetheless, both Google\nand Microsoft have recently launched vast “knowledge\ngraphs” featuring thousands of relations among hundreds of\nmillions of entities. The purpose is to provide direct answers (rather\nthen merely retrieved web page snippets) to query terms and natural\nlanguage questions, and to make inferences about the likely intent of\nusers, such as purchasing some type of item or service.\n \nNatural-language front ends for databases have long been considered an\nattractive application of NLP technology, beginning with such systems\nas LUNAR (Woods et al.  1972) and REL (Thompson et al.  1969; Thompson\n& Thompson 1975). The attractiveness lies in the fact that\nretrieval and manipulation of information from a relational (or other\nuniformly structured) database can be assumed to be handled by an\nexisting db query language and process. This feature sharply limits\nthe kinds of natural language questions to be expected from a user,\nsuch as questions aimed at retrieving objects or tuples of objects\nsatisfying given relational constraints, or providing summary or\nextremal properties (longest rivers, lowest costs, and the like) about\nthem. It also greatly simplifies the interpretive process and\nquestion-answering, since the target logical forms—formal db\nqueries—have a known, precise syntax and are executed\nautomatically by the db management system, leaving only the work of\ndisplaying the computed results in some appropriate linguistic,\ntabular or graphical form. Numerous systems have been built since then, aimed at applications\nsuch as navy data on ships and their deployment\n(Ladder: Hendrix et al.  1978),\nland-use planning (Damerau 1981), geographic QA\n(Chat-80: Pereira & Warren 1982),\nretrieval of company records and product records for insurance\ncompanies, oil companies, manufacturers, retailers, banks,\netc. (Intellect: Harris 1984), compilation\nof statistical data concerning customers, services,\nassets, etc., of a company (Cercone et al. 1993), and\nmany more (e.g., see Androutsopoulos & Ritchie 2000). However, the\ncommercial impact of such systems has remained scant, because they\nhave generally lacked the reliability and some of the functionalities\nof traditional db access. We have noted certain limited inferential capabilities in text-based\nQA systems and NL front ends for databases, such as the ability to\nconfirm entailment relations between candidate answers and questions,\nusing simple sorts of semantic relations among the terms involved, and\nthe ability to sort or categorize data sets from databases and compute\naverages or even create statistical charts. However, such limited, specialized inference methods fall far short\nof the kind of general reasoning based on symbolic knowledge that has\nlong been the goal in AI question answering. One of the earliest\nefforts to create a truly inferential QA system was the ENGLAW project\nof L. Stephen Coles (Coles 1972). ENGLAW was intended as a prototype\nof a kind of system that might be used by scientists and engineers to\nobtain information about physical laws. It featured a KB of axioms (in\nfirst-order logic) for 128 important physical laws, manually coded\nwith the aid of a reference text. Questions (such as “In the\nPeltier Effect, does the heat developed depend on the direction of the\nelectric current?”) were rendered into logic via a\ntransformational grammar parser, and productions (aided by various\nLisp functions) that map phrase patterns to logical expressions. The\nsystem was not developed to the point of practical usefulness, but its\nintegration of reasoning and NLP technologies and its methods of\nselectively retrieving axioms for inferential QA were noteworthy\ncontributions. An example of a later larger-scale system aimed at practical goals\nwas BBN's JANUS system (Ayuso et al.\n1990). This was intended for naval\nbattle management applications, and could answer questions about the\nlocations, readiness, speed and other attributes of ships, allowing for\nchange with the passage of time. It mapped English queries to a very\nexpressive initial representation language with an “intension” operator\nto relate formulas to times and possible worlds, and this was in turn\nmapped into the NIKL description logic, which proved adequate for the\nmajority of inferences needed for the targeted kinds of QA. Jumping forward in time, we take note of the web-based\nWolfram|Alpha (or WolframAlpha) answer engine, developed by Wolfram\nResearch and consisting of 15 million lines of Mathematica code\ngrounded in curated data bases, models, and algorithms for thousands\nof different domains. (Mathematica is a mathematically oriented\nhigh-level programming language developed by the British scientist\nStephen Wolfram.) The system is tilted primarily towards quantitative\nquestions (e.g., “What is the GDP of France?”, or\n“What is the surface area of the Moon?”) and often\nprovides charts and graphics along with more direct answers. The\ninterpretation of English queries into functions applied to various\nknown objects is accomplished with the pattern matching and symbol\nmanipulation capabilities of Mathematica.  However, the comprehension\nof English is not particularly robust at the time of writing. For\nexample “How old was Lincoln when he died?”, “At\nwhat age did Lincoln die?” and other variants were not\nunderstood, though in many cases of misunderstanding, Wolfram|Alpha\ndisplays enough retrieved information to allow inference of an\nanswer. A related shortcoming is that Wolfram|Alpha's quantitative\nskills are not supplemented with significant qualitative reasoning\nskills. For example, “Was Socrates a man?” (again, at the\ntime of writing) prompts display of summary information about\nSocrates, including an image, but no direct answer to the\nquestion. Still, Wolfram|Alpha's quantitative abilities are not only\ninteresting in stand-alone mode, but also useful as augmentations of\nsearch engines (such as Microsoft Bing) and of voice-based personal\nassistants such as Apple's Siri (see below). Another QA system enjoying wide recognition because of its\ntelevised victory in the Jeopardy! quiz show is IBM's\n“Watson” (Ferrucci 2012; Ferrucci et al. 2010; Baker\n2011). Like Wolfram|Alpha, this is in a sense a brute force program,\nconsisting of about a million lines of code in Java, C++, Prolog and\nother languages, created by a core team of 20 researchers and software\nengineers over the course of three years. The program runs 3000\nprocesses in parallel on ninety IBM Power 750 servers, and has access\nto 200 million pages of content from sources such as Wordnet,\nWikipedia (and its structured derivatives YAGO and DBpedia), thesauri,\nnewswire articles, and literary texts, amounting to several terabytes\nof human knowledge. (This translates into roughly 1010\nclausal chunks—a number likely to be around 2 orders of\nmagnitude greater than the number of basic facts over which any one\nhuman being disposes.) Rather than relying on any single method of linguistic or semantic\nanalysis, or method of judging relevance of retrieved passages and\ntextual “nuggets” therein, Watson applies multiple methods to the\nquestions and candidate answers, including methods of question\nclassification, focal entity detection, parsing, chunking, lexical\nanalysis, logical form computation, referent determination, relation\ndetection, temporal analysis, and special methods for question-answer\npairs involving puns, anagrams, and other twists common in Jeopardy!.\nDifferent question analyses are used separately to retrieve relevant\ndocuments, and to derive, analyze and score potential answers from\npassages and sentences in those documents. In general, numerous\ncandidate answers to a question are produced, and their analyses\nprovide hundreds of features whose weights for obtaining ranked answers\nwith corresponding confidence levels are learned by ML methods applied\nto a corpus of past Jeopardy! questions and answers (or officially,\nanswers and questions, according to the peculiar conceit of the\nJeopardy! protocol). Watson's wagers are based on the confidence levels\nof its potential answers and a complex regression model. How well does Watson fit under our heading of inferential,\nknowledge-based QA? Does it actually understand the questions\nand the answers it produces? Despite its impressive performance\nagainst Jeopardy! champions, Watson reasons, and understands English\nin only very restricted senses. The program exploits the fact that the\ntarget of a Jeopardy! question is usually a named entity, such\nas Jimmy Carter, Islamabad, or Black Hole of\nCalcutta, though other types of phrases are occasionally\ntargeted. Watson is likely to find multiple sentences that mention a\nparticular entity of the desired type, and whose syntactic and\nsemantic features are close to the features of the question, thereby\nmaking the named entity a plausible answer without real understanding\nof the question. For example, a “recent history” question\nasking for the president under whom the US gave full recognition to\nCommunist China (Ferrucci 2012) might well zero in on such sentences\nas Although\nhe was the president who restored full diplomatic\nrelations with China in 1978, Jimmy Carter has never visited that country\n…\n(New York Times, June 27, 1981)\n \nor\n \nExchanges\nbetween the two countries' nuclear scientists had begun\nsoon after President Jimmy Carter officially recognized China in 1978.\n(New York Times, Feb. 2, 2001) \nWhile the links between such sentences and the correct answer are\nindirect (e.g., dependent on resolving he and who to\nJimmy Carter, and associating restored diplomatic\nrelations with recognized, and Communist China\nwith China), correct analysis of those links is not a\nrequirement for success—it is sufficient for the cluster of\nsentences favoring the answer Jimmy Carter (in virtue of their word\nand phrasal content and numerous other features) to provide a larger\nnet weight to that answer than any competing clusters. This type of\nstatistical evidence combination based on stored texts seems unlikely\nto provide a path to the kind of understanding that even first-graders\nbetray in answering simple commonsense questions, such as “How\ndo people keep from getting wet when it rains?”, or “If\nyou eat a cookie, what happens to the cookie?” At the same time,\nvast data banks utilized in the manner of Watson can make up for\ninferential weakness in various applications, and IBM is actively\nredeveloping Watson as a resource for physicians, one that should be\nable to provide diagnostic and treatment possibilities that even\nspecialists may not have at their fingertips. In sum, however, the\ngoal of open-domain QA based on genuine understanding and\nknowledge-based reasoning remains largely unrealized.\n \nVoice-based services, especially on mobile devices, are a rapidly\nexpanding applications area. Services range from organizers (for\ngrocery lists, meeting schedules, reminders, contact lists, etc.), to\nin-car “infotainment” (routing, traffic conditions, hazard warnings,\niTunes selection, finding nearby restaurants and other venues, etc.),\nto enabling use of other miscellaneous apps such as email dictation,\ndialing contacts, financial transactions, reservations and placement of\norders, Wikipedia access, help-desk services, health advising, and\ngeneral question answering. Some of these services (such as\ndialing and iTunes selection) fall into the category of hands-free\ncontrols, and such controls are becoming increasingly important in\ntransport (including driverless or pilotless vehicles), logistics\n(deployment of resources), and manufacturing. Also chatbot\ntechnology and companionable dialogue agents (as discussed in \nsection 10.5) \nare serving as general backends to more specific voice-based\nservices. The key technology in these services is of course speech\nrecognition, whose accuracy and adaptability has been gradually\nincreasing.  The least expensive, narrowly targeted systems\n(e.g., simple organizers) exploit strong expectations about user\ninputs to recognize, interpret and respond to those inputs; as such\nthey resemble menu-driven systems. More versatile systems, such as car\ntalkers that can handle routing, musical requests, searches for\nvenues, etc., rely on more advanced dialogue management\ncapabilities. These allow for topic switches and potentially for the\nattentional state of the user (e.g., delaying answering a driver's\nquestion if the driver needs to attend to a turn). The greatest\ncurrent “buzz” surrounds advanced voice-based assistants, notably\niPhone's Siri (followed by Android's Iris, True Knowledge's Evi,\nGoogle Now, and others). While previous voice control and dictation\nsystems, like Android's Vlingo, featured many of the same\nfunctionalities, Siri adds personality and improved dialogue handling\nand service integration—users feel that they are interacting\nwith a lively synthetic character rather than an app. Besides Nuance\nSR technology, Siri incorporates complex techniques that were to some\nextent pushed forward by the Calo (Cognitive Assistant\nthat Learns and Organizes) project carried out by SRI International\nand multiple universities from 2003–2008 (Ambite et al. 2006;\nCALO [see Other Internet Resources]). \nThese techniques include aspects of NLU, ML, goal-directed and\nuncertain inference, ontologies, planning, and service delegation. But\nwhile delegation to web services, including Wolfram|Alpha QA, or\nchatbot technology provides considerable robustness, and there is\nsignificant reasoning about schedules, purchasing and other targeted\nservices, general understanding is still very shallow, as users soon\ndiscover. Anecdotal examples of serious misunderstandings are\n“Call me an ambulance” eliciting the response “From\nnow on I will call you ‘an ambulance’”. However, the\nstrong interest and demand in the user community generated by these\nearly (somewhat) intelligent, quite versatile assistants is likely to\nintensify and accelerate research towards ever more life-like virtual\nagents, with ever more understanding and common sense. \nWe discuss collaborative problem solving systems (also referred to as\n“mixed-initiative” or “task-oriented” dialogue\nsystems) and tutorial dialogue systems (i.e., tutorial systems in\nwhich dialogue plays a pivotal role) under a common heading because\nboth depend on rather deep representations or models of the domains\nthey are aimed at as well as the mental state of the users they\ninteract with. \nHowever, we should immediately note that collaborative problem solving\nsystems typically deal with much less predictable domain situations\nand user inputs than tutorial systems, and accordingly the former\nplace much greater emphasis on flexible dialogue handling than the\nlatter. For example, collaborators in emergency evacuation (Ferguson\nand Allen 1998, 2007) need to deal with a dynamically changing domain,\nat the same time handling the many dialogue states that may occur,\ndepending on the participants' shared and private beliefs, goals,\nplans and intentions at any given point. By contrast, in a domain such\nas physics tutoring (e.g., Jordan et al. 2006; Litman and\nSilliman 2004), the learner can be guided through a network of\nlearning goals with authored instructions, and corresponding to those\ngoals, finite-state dialogue models can be designed that classify\nstudent inputs at each point in a dialogue and generate a prepared\nresponse likely to be appropriate for that input. \nIt is therefore not surprising that tutorial dialogue systems are\ncloser to commercial practicality, with demonstrated learning benefits\nrelative to conventional instruction in various evaluations, than\ncollaborative problem solving systems for realistic applications.\nTutorial dialogue systems have been built for numerous domains and\npotential clienteles, ranging from K-12 subjects to computer literacy\nand novice programming, qualitative and quantitative physics, circuit\nanalysis, operation of machinery, cardiovascular physiology, fire\ndamage control on ships, negotiation skills, and more (e.g., see Boyer\net al. 2009; Pon-Barry et al. 2006). Among the most successful\ntutorial systems are reading tutors (e.g., Mostow and Beck 2007; Cole\net al. 2007), since the materials presented to the learner (in a\n“scaffolded” manner) are relatively straightforward to\ndesign in this case, and the responses of the learner, especially when\nthey consist primarily of reading presented text aloud, are relatively\neasy to evaluate. For the more ambitious goal of fostering reading\ncomprehension, the central problem is to design dialogues so as to\nmake the learner's contributions predictable, while also making the\ninteraction educationally effective (e.g., Aist and Mostow 2009). Some tutoring systems, especially ones aimed at children, use\nanimated characters to heighten the learner's sense of\nengagement. Such enhancements are in fact essential for systems\naimed at learners with disabilities like deafness (where mouth and\ntongue movements of the virtual agent observed by the learner can help\nwith articulation), autism, or aphasia (Massaro et al. 2012; Cole et\nal. 2007). As well, if tutoring is aimed specifically at\ntraining\ninterpersonal skills, implementation of life-like characters (virtual\nhumans) becomes an indispensable part of system development (e.g., Core\net al. 2006; Campbell et al. 2011). Modeling the user's state of mind in tutoring systems is primarily a\nmatter of determining which of the targeted concepts and skills have,\nor have not yet, been acquired by the user, and diagnosing\nmisunderstandings that are likely to have occurred, given the session\ntranscript so far. Some recent experimental systems can also adapt\ntheir strategies to the user's apparent mood, such as frustration or\nboredom, as might be revealed by the user's inputs, tone of voice, or\neven facial expressions or gestures analyzed via computer vision. \nOther prototype systems can be viewed as striving towards more general\nmental modeling, by incorporating ideas and techniques from\ntask-oriented dialogue systems concerning dialogue states, dialogue\nacts, and deeper language understanding (e.g., Callaway et al. 2007). In task-oriented dialogue systems, as already noted, dialogue\nmodeling is much more challenging, since such systems are expected not\nonly to contribute to solving the domain problem at hand, but to\nunderstand the user's utterances, beliefs, and intentions, and to hold\ntheir own in a human-like, mixed-initiative dialogue. This requires\ndomain models, general incremental collaborative planning methods,\ndialogue management that models rational communicative interaction, and\nthorough language understanding (especially intention recognition) in\nthe chosen domain. Prototype systems have been successfully built\nfor domains such as route planning, air travel planning, driver and\npedestrian guidance, control and operation of external devices,\nemergency evacuation, and medication advising (e.g., Allen et al. 2006;\nRich and Sidner 1998; Bühler and Minker 2011; Ferguson and Allen\n1998, 2007), and these hold very significant practical promise.\nHowever, systems that can deal with a variety of reasonably complex\nproblems, especially ones requiring broad commonsense knowledge about\nhuman cognition and behavior, still seem out of reach at this time. \nAs noted at the beginning of \nsection 10, robots are beginning to be\nequipped with web services, question answering abilities, chatbot\ntechniques (for fall-back and entertainment), tutoring functions, and\nso on. The transfer of such technologies to robots has been slow,\nprimarily because of the very difficult challenges involved in just\nequipping a robot with the hardware and software needed for basic\nvisual perception, speech recognition, exploratory and goal-directed\nnavigation (in the case of mobile robots), and object manipulation.\nHowever, the keen public interest in intelligent robots and their\nenormous economic potential (for household help, eldercare, medicine,\neducation, entertainment, agriculture, industry, search and rescue,\nmilitary missions, space exploration, and so on) will surely continue\nto energize the drive towards greater robotic intelligence and\nlinguistic competence. A good sense of the state of the art and difficulties in human-robot\ndialogue can be gained from (Scheutz et\nal. 2011). Some of the dialogue\nexamples presented there, concerning boxes and blocks, are reminiscent\nof Winograd's shrdlu, but they also exhibit the\nchallenges involved in\nreal interaction, such as the changing scenery as the robot moves,\nspeech recognition errors, disfluent and complex multi-clause\nutterances, perspective-dependent utterances (\n“Is the red box to the left of the blue box?”), and deixis \n(“Go down there”). In\naddition, all of this must be integrated with physical action planned\nso as to fulfill the instructions as understood by the robot. While the\nability of recent robots to handle these difficulties to some degree is\nencouraging, many open problems remain, such as the problems of speech\nrecognition in the presence of noise, better, broader linguistic\ncoverage, parsing, and dialogue handling, adaptation to novel problems,\nmental modeling of the interlocutor and other humans in the\nenvironment, and greater general knowledge about the world and the\nability to use it for inference and planning (both at the domain level\nand the dialogue level). While task-oriented robot dialogues involve all these challenges,\nwe should note that some potentially useful interactions with\n“talking” robots require little in the way of linguistic\nskills. For example, the Rubi robot described in\n(Movellan et al. 2009), displayed objects on its\nscreen-equipped “chest” to toddlers, asking them to touch\nand name the objects. This resulted in improved word learning by the\ntoddlers, despite the simplicity of the interaction. Another example\nof a very successful talking robot with no real linguistic skills was\nthe “museum tour guide” Rhino\n(Burgard et al. 1999). Unlike Rubi it was able\nto navigate among unpredictably moving humans, and kept its audience\nengaged with its prerecorded messages and with a display of its\ncurrent goals on a screen. In the same way, numerous humanoid robots\n(for example, Honda's Asimo) under past and present development across\nthe world still understand very little language and rely mostly on\nscripted output. No doubt their utility and appeal will continue to\ngrow, thanks to technologies like those mentioned above—games,\ncompanionable agent systems, voice-based apps, tutors, and so on; and\nthese developments will also fuel progress on the deeper aspects of\nperception, motion, manipulation, and meaningful dialogue.\n","contact.mail":"schubert@cs.rochester.edu","contact.domain":"cs.rochester.edu"}]
