[{"date.published":"2008-01-16","url":"https://plato.stanford.edu/entries/innateness-language/","author1":"Fiona Cowie","entry":"innateness-language","body.text":"\n\n\n\nThe philosophical debate over innate ideas and their role in the \nacquisition of knowledge has a venerable history. It is thus \nsurprising that very little attention was paid until early last \ncentury to the questions of how linguistic knowledge is \nacquired and what role, if any, innate ideas might play in that \nprocess.\n\n\n\nTo be sure, many theorists have recognized the crucial part played by\nlanguage in our lives, and have speculated about the (syntactic \nand/or semantic) properties of language that enable it to play that \nrole. However, few had much to say about the properties of \nus in virtue of which we can learn and use a natural language. \nTo the extent that philosophers before the 20th century dealt with \nlanguage acquisition at all, they tended to see it as a product of \nour general ability to reason — an ability that makes us \nspecial, and that sets us apart from other animals, but that is not \ntailored for language learning in particular.\n\n\n\nIn Part 5 of the Discourse on the Method, for instance, \nDescartes identifies the ability to use language as one of two \nfeatures distinguishing people from “machines” or \n“beasts” and speculates that even the stupidest people \ncan learn a language (when not even the smartest beast can do so) \nbecause human beings have a “rational soul” and beasts \n“have no intelligence at all.” (Descartes 1984: 140-1.) \nLike other great philosopher-psychologists of the past, Descartes \nseems to have regarded our acquisition of concepts and knowledge \n(‘ideas’) as the main psychological mystery, taking \nlanguage acquisition to be a relatively trivial matter in comparison;\nas he puts it, albeit ironically, “it patently requires very \nlittle reason to be able to speak.” (1984: 140.)\n\n\n\nAll this changed in the early twentieth century, when linguists, \npsychologists, and philosophers began to look more closely at the \nphenomena of language learning and mastery. With advances in syntax \nand semantics came the realization that knowing a language was not \nmerely a matter of associating words with concepts. It also crucially\ninvolves knowledge of how to put words together, for it's \ntypically sentences that we use to express our thoughts, not words in\nisolation.\n\n\n\nIf that's the case, though, language mastery can be no simple matter.\nModern linguistic theories have shown that human languages are vastly\ncomplex objects. The syntactic rules governing sentence formation and\nthe semantic rules governing the assignment of meanings to sentences \nand phrases are immensely complicated, yet language users apparently \napply them hundreds or thousands of times a day, quite effortlessly \nand unconsciously. But if knowing a language is a matter of knowing \nall these obscure rules, then acquiring a language emerges as the \nmonumental task of learning them all. Thus arose the \nquestion that has driven much of modern linguistic theory: How could \nmere children learn the myriad intricate rules that govern linguistic\nexpression and comprehension in their language — and learn them\nsolely from exposure to the language spoken around them?\n\n\n\nClearly, there is something very special about the brains of human \nbeings that enables them to master a natural language — a feat \nusually more or less completed by age 8 or so. §2.1 of this \narticle introduces the idea, most closely associated with the work of\nthe MIT linguist Noam Chomsky, that what is special about human \nbrains is that they contain a specialized ‘language \norgan,’ an innate mental ‘module’ or \n‘faculty,’ that is dedicated to the task of mastering a \nlanguage.\n\n\n\nOn Chomsky's view, the language faculty contains innate knowledge of \nvarious linguistic rules, constraints and principles; this innate \nknowledge constitutes the ‘initial state’ of the language\nfaculty. In interaction with one's experiences of language during \nchildhood — that is, with one's exposure to what Chomsky calls \nthe ‘primary linguistic data’ or \n‘pld’ (see §2.1) — it gives rise to a\nnew body of linguistic knowledge, namely, knowledge of a specific \nlanguage (like Chinese or English). This ‘attained’ or \n‘final’ state of the language faculty constitutes one's \n‘linguistic competence’ and includes knowledge of the \ngrammar of one's language. This knowledge, according to Chomsky, is \nessential to our ability to speak and understand a language \n(although, of course, it is not sufficient for this ability: much \nadditional knowledge is brought to bear in ‘linguistic \nperformance,’ that is, actual language \n use).[1]\n\n\n\n§§2.2-2.5 discuss the main arguments used by Chomsky and \nothers to support this ‘nativist’ view that what makes \nlanguage acquisition possible is the fact that much of our linguistic\nknowledge is unlearned; it is innate or inborn, part of the initial \nstate of the language \n faculty.[2]\n Section 3 presents a number of other avenues \nof research that have been argued to bear on the innateness of \nlanguage, and shows how recent empirical research about language \nlearning and the brain may challenge the nativist position. Because \nmuch of this material is very new, and because my conclusions (many \nof which are tentative) are highly controversial, more references to \nthe empirical literature than are normal in an encyclopedia article \nare included. The reader is encouraged to follow up on the research \ncited and assess the plausibility of linguistic nativism for him or \nherself: whether language is innate or not is, after all, an \nempirical issue.\n\n\n\nThe behaviorist psychologist B.F. Skinner was the first theorist to \npropose a fully fledged theory of language acquisition in his book, \nVerbal Behavior (Skinner 1957). His theory of learning was \nclosely related to his theory of linguistic behavior itself. He \nargued that human linguistic behavior (that is, our own utterances \nand our responses to the utterances of others) is determined by two \nfactors: (i) the current features of the environment impinging on the\nspeaker, and (ii) the speaker's history of reinforcement (i.e., the \ngiving or withholding of rewards and/or punishments in response to \nprevious linguistic behaviors). Eschewing talk of the mental as \nunscientific, Skinner argued that ‘knowing’ a language is\nreally just a matter of having a certain set of behavioral \ndispositions: dispositions to say (and do) appropriate things in \nresponse to the world and the utterances of others. Thus, knowing \nEnglish is, in small part, a matter of being disposed to utter \n“Please close the door!” when one is cold as a result of \na draught from an open door, and of being disposed (other things \nbeing equal) to utter “OK” and go shut a door in response\nto someone else's utterance of that formula. \n\nGiven his view that knowing a language is just a matter of having a \ncertain set of behavioral dispositions, Skinner believed that \nlearning a language just amounts to acquiring that set of \ndispositions. He argued that this occurs through a process that he \ncalled operant conditioning. (‘Operants’ are \nbehaviors that have no discernible law-like relation to particular \nenvironmental conditions or ‘eliciting stimuli.’ They are\nto be contrasted with ‘respondents,’ which are reliable \nor reflex responses to particular stimuli. Thus, blinking when \nsomeone pokes at your eye is a respondent; episodes of infant \nbabbling are operants.) Skinner held that most human verbal behaviors\nare operants: they start off unconnected with any particular stimuli.\nHowever, they can acquire connections to stimuli (or other behaviors)\nas a result of conditioning. In conditioning, the behavior in \nquestion is made more (or in some paradigms less) likely to occur in \nresponse to a given environmental cue by the imposition of an \nappropriate ‘schedule of reinforcement’: rewards or \npunishments are given or withheld as the subject's response to the \ncue varies over time. \n\nAccording to Skinner, language is learned when children's verbal \noperants are brought under the ‘control’ of environmental\nconditions as a result of training by their caregivers. They are \nrewarded (by, e.g., parental approval) or punished (by, say, a \nfailure of comprehension) for their various linguistic productions \nand as a result, their dispositions to verbal behavior gradually \nconverge on those of the wider language community. Likewise, Skinner \nheld, ‘understanding’ the utterances of others is a \nmatter of being trained to perform appropriate behaviors in response \nto them: one understands ‘Shut the door!’ to the extent \nthat one responds appropriately to that utterance. \n\nIn his famous review of Skinner's book, Chomsky (1959) effectively \ndemolishes Skinner's theories of both language mastery and language \nlearning. First, Chomsky argued, mastery of a language is not merely \na matter of having one's verbal behaviors ‘controlled’ by\nvarious elements of the environment, including others' utterances. \nFor language use is (i) stimulus independent and (ii) historically \nunbound. Language use is stimulus independent: virtually any\nwords can be spoken in response to any environmental stimulus, \ndepending on one's state of mind. Language use is also \nhistorically unbound: what we say is not determined by our \nhistory of reinforcement, as is clear from the fact that we can and \ndo say things that we have not been trained to say. \n\nThe same points apply to comprehension. We can understand sentences \nwe have never heard before, even when they are spoken in odd or \nunexpected situations. And how we react to the utterances of others \nis again dependent largely on our state of mind at the time, rather \nthan any past history of training. There are linguistic conventions \nin abundance, to be sure, but as Chomsky rightly pointed out, human \n‘verbal behavior’ is quite disanalogous to a pigeon's \ndisk-pecking or a rat's maze-running.. Mastery of language is not a \nmatter of having a bunch of mere behavioral dispositions. Instead, it\ninvolves a wealth of pragmatic, semantic and syntactic knowledge. \nWhat we say in a given circumstance, and how we respond to what \nothers say, is the result of a complex interaction between our \nhistory, our beliefs about our current situation, our desires, \nand our knowledge of how our language works. Skinner's first\nbig mistake, then, was in failing to recognize that language mastery \ninvolves knowledge (or, as Chomsky later called it \n‘cognizance’) of linguistic rules and conventions. \n\nHis second big mistake was related to this one: he failed to \nrecognize that acquiring mastery of a language is not a matter of \nbeing trained what to say. It's simply false, says Chomsky, that \n“a careful arrangement of contingencies of reinforcement by the\nverbal community is a necessary condition of language \nlearning.” (1959:39) First, children learning language do not \nappear to be being ‘conditioned’ at all! Explicit \ntraining (such as a dog receives when learning to bark on command) is\nsimply not a feature of language acquisition. It's only comparatively\nrarely that parents correct (or explicitly reward) their children's \nlinguistic sorties; children learn much of what they know about \nlanguage from watching TV or passively listening to adults; immigrant\nchildren learn a second language to native speaker fluency in the \nschool playground; and even very young children are capable of \nlinguistic innovation, saying things undreamt of by their parents. As\nChomsky concludes: “It is simply not true that children can \nlearn language only through ‘meticulous care’ on the part\nof adults who shape their verbal repertoire through careful \ndifferential reinforcement.” (1959:42) \n\nSecondly, Chomsky argued — and here we see his first invocation\nof the famous ‘poverty of the stimulus’ argument, to be \ndiscussed in more detail in §2.2 below — it is unclear \nthat conditioning could even in principle give rise to a set\nof dispositions rich enough to generate the full range of a person's \nlinguistic behavior. In order, for example, to acquire the \nappropriate set of dispositions concerning the word car, one\nwould have to be trained on vast numbers of sentences containing that\nword: one would have to hear car in object position and \ncar in subject position; car modified by adjectives\nand car unmodified; car embedded in opaque contexts\n(e.g. in propositional attitude ascriptions) and car used \ntransparently; and so on. But the ‘primary linguistic \ndata,’ usually referred to as the ‘pld’ \nand comprising the set of sentences to which a child is exposed \nduring language learning (plus any analysis performed by the child on\nthose sentences; see below), simply cannot be assumed to contain \nenough of these ‘minimally differing sentences’ to fully \ndetermine a person's dispositions with respect to that word. Instead,\nChomsky argued, what determines one's dispositions to use \ncar is one's knowledge of that word's syntactic and semantic\nproperties (e.g., car is a noun referring to cars), together\nwith one's knowledge of how elements with those properties function \nin the language as a whole. So even if language mastery were (in \npart) a matter of having dispositions concerning car, the \nmechanism of conditioning would be unable to give rise to them. The \ntraining set to which children have access is simply too limited: it \ndoesn't contain enough of the right sorts of exemplars. \n\nIn sum: Skinner was mistaken on all counts. Language mastery is not \nmerely a matter of having a set of bare behavioral dispositions. \nInstead, it involves intricate and detailed knowledge of the \nproperties of one's language. And language learning is not a matter \nof being trained what to say. Instead, children learn language just \nfrom hearing it spoken around them, and they learn it effortlessly, \nrapidly, and without much in the way of overt instruction. \n\nThese insights were to drive linguistic theorizing for the next fifty\nyears, and it's worth emphasizing just how radical and exciting they \nwere at the time. First, the idea that explaining language use \ninvolves attributing knowledge to speakers flouted the \nprevailing behaviorist view that talking about mental states was \nunscientific because mental states are unobservable. It also raised \nseveral pressing empirical question that linguists are still \ndebating. For example, what is the content of speakers' \nknowledge of \n language?[3]\n What sorts of facts about language are represented in speakers' \nheads? And how does this knowledge actually function in the \npsychological processes of language production and comprehension: \nwhat are the mechanisms of language use? \n\nSecondly, the idea that children learn language essentially on their \nown was a radical challenge to the prevailing behaviorist idea that \nall learning involves reinforcement. In addition, it made clear our \nneed for a more ‘cognitive’ or ‘mentalistic’ \nconception of how language learning occurs, and vividly raised the \nquestion — our focus in this article — of what might be \nthe preconditions for that process. As we will see in the \nnext section, Chomsky was ready with a theory addressing each of \nthese points. \n\nAt the same time as the behaviorist program in psychology was waning \nunder pressure from Chomsky and others, linguists were abandoning \nwhat is known as ‘American Structuralism’ in the theory \nof syntax. Like the behaviorists, the structuralists (e.g., Harris, \n1951) refused to postulate irreducibly theoretical entities; they \ninsisted that syntactic categories (such as ‘noun phrase’\n(‘NP’) or ‘verb phrase’ (‘VP’), \netc.) be reducible to properties of actual utterances (collected in \n‘corpora’ — lists of things people have said). In \nhis landmark book, Syntactic Structures (1957), however, \nChomsky argued that because corpora can contain only finitely many \nsentences, no attempt at reduction can succeed. Linguists need \ntheoretical constructs that capture regularities going beyond the set\nof actual utterances, and that allow them to predict the properties \nof novel utterances. But if the category NP, for instance, is to \ninclude noun phrases that haven't been uttered yet, the meaning of \nnoun phrase can't be exhausted by what's in the corpus: the \nstructuralists' positivistic strictures on theoretical kinds are \nmisguided. \n\nIn addition, the structuralists had attempted to capture the \nsyntactic properties of languages in terms of simple rewrite rules \nknown as ‘phrase structure rules.’ Phrase structure rules\ndescribe the internal syntactic structures of sentence types; \ninterpreted as rewrite rules, they can be used to generate or \nconstruct sentences. Thus, the rule S → NP VP, for instance, \nsays that a sentence symbol S can be rewritten as the symbol NP \nfollowed by the symbol VP, and tells you that a sentence consists of \na noun phrase followed by a verb phrase. (This information can be \nrepresented via a tree-diagram, as in Fig. 1a, or by a phrasemarker \n(or labeled bracketing), as in Fig. 1b.) \n\nFigure 1. Phrasemarkers representing a sentence as consisting of a \nnoun phrase and a verb phrase via (a) a tree diagram or (b) a labeled\nbracketing. \n\nOther rules, (such as NP → Det N, VP → V NP, Det → \na, the, …, etc., V→ hit, kiss…, \netc.; N → boy, girl,…, etc.) are subsequently \napplied, and (with still further rules not discussed here) allow for \nthe generation of sentences such as The boy kissed the girl, The \ngirl hits the boy, and so on.  \n\nChomsky argued (on technical grounds; see Chomsky 1957, ch.1) that \ngrammars must be enriched with a second type of rule, known as \n‘transformations.’ Unlike phrase structure rules, \ntransformations operate on whole sentences (or more strictly, their \nphrasemarkers); they allow for the generation of new sentences \n(/phrasemarkers) out of old ones. The Passive transformation\ndescribed in Chomsky 1957:112, for instance, specifies how to turn an\nactive sentence (/phrasemarker) into a passive one. Simplifying \nsomewhat, you take an active phrasemarker of the form NP — \nAux — V — NP, like Kate is biting \nMark, and rearrange its elements x1 — \nx2 — x3 — \nx4 as follows: x4 — \nx2 + be + en — \nx3 + by — x1 \nto get Mark bite (+ is + en) by \nKate. The parenthetical + en and + is invoke \nfurther operations on the verb bite that transform it into \nis being bitten, and ultimately Kate is biting Mark\nis ‘transformed’ into Mark is being bitten by \nKate. \n\nOnly a grammar containing both phrase structure and transformation \nrules, Chomsky argued, could generate a natural language — \n‘generate’ in the sense that by stepwise application of \nthe rules, one could in principle build up from scratch all and only \nthe sentences that the language contains. Hence, Chomsky urged the \ndevelopment of generative grammars of this type. \n\nSyntactic theory has now gone well beyond this early vision — \nboth phrase structure and transformation rules were abandoned in \nsuccessive linguistic revolutions wrought by Chomsky and his students\nand colleagues (see Newmeyer 1986, 1997 for a history of generative \nlinguistics). \n\nBut what has not changed — and what is important for our \npurposes — is that in every version of the grammar of (say) \nEnglish, the rules governing the syntactic structure of sentences and\nphrases are stated in terms of syntactic categories that are highly \nabstracted from the properties of utterances that are accessible to \nexperience. As an example of this, consider the notion of a \ntrace. Traces are symbols that appear in phrasemarkers and \nmark the path of an element as it is moved from one position to \nanother at various stages of a sentence's derivation, as in (1), \nwhere ti markes the NP Jacob's position at an \nearlier stage in the derivation. \n\nBut while traces are vital to the statement of many syntactic rules \nand regularities, they are ‘empty categories’ — \nthey are not audible in the sentence as spoken. (See Chomsky 1981 \nand Lasnik and Uriagereka 1986 for more on traces and other empty \ncategories.) Traces (and other similarly abstract properties of \nlanguages) thus raise a question for the theory of language \nacquisition. For if, as Chomsky maintains, mastery of language \ninvolves knowledge of rules stated in terms of sentences' syntactic \nproperties, and if those properties are not so to speak \n‘present’ in the data, but are rather highly abstract and\n‘unobservable,’ then it becomes hard to see how children \ncould possibly acquire knowledge of the rules concerning them. As a \nconsequence, children's feat in learning a language appears \nmiraculous: how could a child learn the myriad rules governing \nlinguistic expression given only her exposure to the sentences spoken\naround \n her?[4] \n\nIn response to this question, most 20th century theorists followed \nChomsky in holding that language acquisition could not occur unless \nmuch of the knowledge eventually attained were innate or inborn. The \ngap between what speaker-hearers know about language (its grammar, \namong other things) and the data they have access to during learning \n(the pld) is just too broad to be bridged by any process of \nlearning alone. It follows that since children patently do learn \nlanguage, they are not linguistic ‘blank slates.’ \nInstead, Chomsky and his followers maintained, human children are \nborn knowing the ‘Universal Grammar’ or ‘UG,’\na theory describing the most fundamental properties of all natural \nlanguages (e.g., the facts that elements leave traces behind when \nthey move, and that their movements are constrained in various ways).\nLearning a particular language thus becomes the comparatively simple \nmatter of elaborating upon this antecedently possessed knowledge, and\nhence appears a much more tractable task for young children to \nattempt. \n\nOver the years, two conceptions of the innate contribution to \nlanguage learning and its elaboration during the learning process \nhave been proposed. In earlier writings (e.g., Chomsky 1965), Chomsky\nsaw learning a language as basically a matter of formulating and \ntesting hypotheses about its grammar — unconsciously, of \ncourse. He argued that in order to acquire the correct grammar, the \nchild must innately know a “a linguistic theory that specifies \nthe form of the grammar of a possible human language” (1965:25)\n— she must know UG in other words. He saw this knowledge as \nbeing embodied in a suite of innate linguistic abilities, concepts, \nand constraints on the kinds of grammatical rules learners can \npropose for testing. On this view (1965:30-31), the inborn UG \nincludes (i) a way of analyzing and representing the incoming \nlinguistic data; (ii) a set of linguistic concepts with which to \nstate grammatical hypotheses; (iii) a way of telling how the data \nbear on those hypotheses (an ‘evaluation metric’); and \n(iv) a very restrictive set of constraints on the hypotheses that are\navailable for consideration. (i) through (iv) constitute the \n‘initial state’ of the language faculty, and the child \narrives at the final state (knowledge of her language) by performing \nwhat is basically a kind of scientific inquiry into its nature. \n\nBy the 1980's, a less intellectualized conception of how language is \nacquired began to supplant the hypothesis-testing model. Whereas the \nearly model saw the child as a ‘little scientist,’ \nactively (if unconsciously) figuring out the rules of grammar, the \nnew ‘parameter-setting’ model conceived language \nacquisition as a kind of growth or maturation; language acquisition \nis something that happens to you, not something you do. The innate UG\nwas no longer viewed as a set of tools for inference; rather, it was \nconceived as a highly articulated set of representations\nof actual grammatical principles. Of course, since not everyone ends \nup speaking the same language, these innate representations must \nallow for some variation. This is achieved in this model via\nthe notion of a ‘parameter’: some of the innately \nrepresented grammatical principles contain variables that may take \none of a certain (highly restricted) range of values. These different\n‘parameter settings’ are determined by the child's \nlinguistic experience, and result in the acquisition of different \nlanguages. Thus, Chomsky (1988:61-62) compared the learner to a \nswitchbox: just as a switchbox's circuitry is all in place but for \nsome switches that need to be flicked to one position or another, the\nlearner's knowledge of language is basically all in place, but for \nsome linguistic ‘switches’ that are set by linguistic \nexperience. \n\nTo illustrate how parameter setting works, consider a simplified \nexample (discussed in more detail in Chomsky 1990:644-45). All \nlanguages require that sentences have subjects, but whereas some \nlanguages (like English) require that the subject be overt in the \nutterance, other languages (like Spanish) allow you to leave the \nsubject out of the sentence when it is written or spoken. Thus, a \nSpanish speaker who wanted to say that he speaks Spanish could say \nHablo español (leaving out the first personal pronoun\nyo) without violating the rules of Spanish, whereas an \nEnglish speaker wanting to express that thought could not say \n*Speak Spanish without violating the rules of English: to \nspeak grammatically, he must say I speak Spanish. The \nparameter-setting model accommodates this sort of difference by \nproposing that there is a ‘Null Subject Parameter,’ which\nis set differently in English and Spanish speakers: Spanish speakers \nset it to ‘Subject Optional,’ whereas in English \nspeakers, it is set to ‘Subject Obligatory.’ How? One \nproposal is that the parameter is set by default to ‘Subject \nObligatory’ and that hearing a subjectless sentence causes it \nto be set to ‘Subject Optional.’ Since children learning \nSpanish frequently hear subjectless sentences, whereas those learning\nEnglish do not, the parameter setting is switched in the Spanish \nlearner, but remains set at the default for the English learner. \n(Roeper and Williams 1987 is the locus classicus for \nparameter-setting models; Ayoun 2003 is more up-to-date; Pinker, \n1997: ch.3 provides a helpful, non-technical overview.) \n\nThese two approaches to language acquisition clearly differ \nsignificantly in their conception of the nature of the learning \nprocess and the learner's role in it, but we are not concerned to \nevaluate their respective merits here. Rather, the important point \nfor our purposes is that they both attribute substantial amounts of \ninnate information about language to the language learner. \nIn what follows, we will look in more detail at the various arguments\nthat have been used to support this ‘nativist’ theory of \nlanguage acquisition. We will focus on the following question: \n\nTerminological Note: As Chomsky acknowledges (e.g., 1986:28-29), \n‘Universal Grammar’ is used with a systematic ambiguity \nin his writings. Sometimes, the term refers to the inborn knowledge \nof language that learners are hypothesized to possess — the \ncontent of the ‘initial state’ of the language faculty \n— whatever that knowledge (/content) turns out to be. \nOther times, ‘Universal Grammar’ is used to refer to \ncertain specific proposals as to the content of our innate \nlinguistic knowledge, such as the Government-Binding theorist's claim\nthat we have inborn knowledge of such things as the Principle of \nStructure Dependence, Binding theory, Theta theory, the Empty \nCategory Principle, etc. \n\nThis ambiguity is important when one is evaluating Chomskyan claims \nthat we have innate knowledge of UG. For on the first reading of \n‘Universal Grammar’ distinguished above, that claim will \nbe true so long as any form of nativism turns out to be true\nof language learners (i.e., so long as they possess any \ninborn knowledge about language). On the second reading, however, it \nis possible that learners have innate knowledge of language \nwithout that knowledge's being knowledge of UG (as currently\ndescribed by linguists): learners might know things about language, \nyet not know Binding Theory, or the Principle of Structure \nDependence, etc. \n\nIn this entry, ‘Universal Grammar’ will always be used in\nthe second of these senses, to refer to a specific theory as to the \ncontent of learners' innate knowledge of language. Where the issue \nconcerns merely their having some or other innate knowledge \nabout language (and is neutral on the question of whether any \nparticular theory about that knowledge is true), I will talk of \n‘innate linguistic information.’ Clearly, an argument to \nthe effect that speakers have inborn knowledge of UG entails the \nclaim that they have innate linguistic information at their disposal.\nThe reverse, however, is not the case: there might be reason to think\nthat a speaker knows something about language innately, without its \nconstituting reason to think that what they know is Universal Grammar\nas described by Chomksyan linguists; Chomksy might be right that we \nhave innate knowledge about language, but wrong about what the \ncontent of that knowledge is. These issues will be \nclarified, as necessary, below. \n\nAs we saw in §1.1, one of the conclusions Chomsky drew from his \n(1959) critique of the Skinnerian program was that language cannot be\nlearned by mere association of ideas (such as occurs in \nconditioning). Since language mastery involves knowledge of grammar, \nand since grammatical rules are defined over properties of utterances\nthat are not accessible to experience, language learning must be more\nlike theory-building in science. Children appear to be ‘little \nlinguists,’ making highly theoretical hypotheses about the \ngrammar of their language and testing them against the data provided \nby what others say (and do): \n\nHowever, argued Chomsky, just as conditioning was too weak a learning\nstrategy to account for children's ability to acquire language, so \ntoo is the kind of inductive inference or hypothesis-testing that \ngoes on in science. Successful scientific theory-building requires \nhuge amounts of data, both to suggest plausible-seeming \nhypotheses and to weed out any false ones. But the data children have\naccess to during their years of language learning (the ‘primary\nlinguistic data’ or ‘pld’) are highly \nimpoverished, in two important ways: \n\nThe first type of inadequacy is, of course, endemic to any kind of \nempirical inquiry: it is simply the problem of the underdetermination\nof theories by their evidence. Cowie has argued elsewhere that \nunderdetermination per se cannot be taken to be evidence for \nnativism: if it were, we would have to be nativists about \neverything that people learn (Cowie 1994; 1999). What of the\nsecond kind of impoverishment? If the evidence about language \navailable to children does not enable them to reject false \nhypotheses, and if they nonetheless hit on the correct grammar, then \nlanguage learning could not be a kind of scientific inquiry, which \ndepends in part on being able to find evidence to weed out incorrect \ntheories. And indeed, this is what Chomsky argues: since the \npld are not sufficiently rich or varied to enable a learner \nto arrive at the correct hypothesis about the grammar of the language\nshe is learning, language could not be learned from the pld. \n\nFor consider: The fact (i) that the pld are finite whereas \nnatural languages are infinite shows that children must be \ngeneralizing beyond the data when they are learning their language's \ngrammar: they must be proposing rules that cover as-yet unheard \nutterances. This, however, opens up room for error. In order to \nrecover from particular sorts of error, children would need access to\nparticular kinds of data. If those data don't exist, as (ii) asserts,\nthen children would not be able to correct their mistakes. Thus, \nsince children do eventually converge on the correct grammar for \ntheir language, they mustn't be making those sorts of errors in the \nfirst place: something must be stopping them from making \ngeneralizations that they cannot correct on the basis of the \npld. \n\nChomsky (e.g., 1965: 30-31) expresses this last point in terms of the\nneed for constraints — on grammatical concepts, on the\nhypothesis space, on the interpretation of data — and proposes \nthat it is innate knowledge of UG that supplies the needed \nlimitations. On this view, children learning language are not \nopen-minded or naïve theory generators — they are not \n‘little scientists.’ Instead, the human language-learning\nmechanism (the ‘language acquisition device’ or \n‘LAD’) embodies built-in knowledge about human languages,\nknowledge that prevents learners from entertaining most possible \ngrammatical theories. As Chomsky puts it: \n\nChomsky rarely states the argument from the poverty of the stimulus \nin its general form, as Cowie has done here. Instead, he typically \npresents it via an example. One of these concerns learning \nhow to form ‘polar interrogatives,’ i.e., questions \ndemanding yes or no by way of answer, via \na mechanism known as ‘auxiliary \n fronting.’[5]\n Suppose that a child heard pairs of sentences like the following: \n\nShe wants to figure out the rule you use to turn declaratives like \n(1a) and (2a) into interrogatives like (1b) and (2b). Here are two \npossibilities: \n\nBoth hypotheses are adequate to account for the data the learner has \nso far encountered. To any unbiased scientist, though, H1 would \nsurely appear preferable to H2, for it is simpler — it\nis shorter, for one thing, and does not refer to theoretical \nproperties, like being a NP, being instead formulated in terms of \n‘observable’ properties like word order. Nonetheless, H1 \nis false, as is evident when you look at examples like (3): \n\nH1 generates the ungrammatical question (3b), whereas H2 generates \nthe correct version, \n (3c).[6]\n Now, you and I and every other English speaker know (in some sense \n— see §3.2.1a) that H1 is false and H2 is correct. \nThat we know this is evident, Chomsky argues, from the fact \nthat we all know that (3b) is not the right way to say (3c). The \nquestion is how we could have learnt this. \n\nSuppose, for example, that based on her experience of (1) and (2), a \nchild were to adopt H1. How would she discover her error? There would\nseem to be two ways to do this. First, she could use H1in her own \nspeech, utter a sentence like (3b), and be corrected by her parents \nor caregivers; second, she could hear a sentence like (3c) uttered by\na competent speaker, and realize that that sentence is not generated \nby her hypothesis, H1. But typically parents don't correct their \nchildren's ill formed utterances (see §2.2.1(c) for more on this), and\nworse, according to Chomsky, sentences like (3c) — sentences \nthat are not generated by the incorrect rule H1 and hence would \nfalsify it— do not occur often enough in the pld to \nguarantee that every native English speaker will be able to get it \nright. \n\nSo in answer to the question: how do we learn that H2 is\nbetter than H1, Chomsky argued that we don't learn\nthis at all! A better explanation of how we all know that\nH2 is right and H1 is wrong is that we\nwere born knowing this fact. Or, more accurately, we were\nborn knowing a certain principle of UG (the ‘Principle of\nStructure Dependence’), which tells us that rules like\nH1 are not worth pursuing, their ostensible\n‘simplicity’ notwithstanding, and that we should always\nprefer rules, like H2, which are stated in terms of\nsentences' structural properties. In sum, we know that\nH2 is a better rule than H1, but we didn't learn\nthis from our experience of the language. Rather, this fact is a\nconsequence of our inborn knowledge of UG. \n\nChomskyans contest that there are many other cases in which \nspeaker-hearers know grammatical rules, the critical evidence in \nfavor of which is missing from the pld. Kimball 1973:73-5, \nfor instance, argues that complex auxiliary sequences like might \nhave been are “vanishingly rare” in the \npld, hence that children acquire competence with these \nconstructions (in the sense of knowing the order in which to put the \nmodal, perfect and progressive elements) without relevant experience.\n(Pullum and Scholz 2002, discuss two other well known examples.) \nNativists thus conclude that numerous other principles of UG are \ninnately known as well. Together, these UG principles place strong \nconstraints on learners' linguistic theorizing, preventing them from \nmaking errors for which there are no falsifying data. \n\nSo endemic is the impoverishment of the pld, according to \nChomskyans, that it began to seem as if the entire learning paradigm \nwere inapplicable to language. As more and more and stricter and \nstricter innate constraints needed to be imposed on the learner's \nhypothesis space to account for their learning rules in the absence \nof relevant data, notions like hypothesis generation and testing \nseemed to have less and less purchase. This situation fuelled the \nrecent shift away from hypothesis testing models of language \nacquisition and towards parameter setting models discussed in \n§2.1 above. \n\nMany, probably most theorists in modern linguistics and cognitive \nscience have accepted Chomsky's poverty of the stimulus argument for \nthe innateness of UG. As a result, a commitment to linguistic \nnativism has underpinned most research into language acquisition over\nthe last 40-odd years. Nonetheless, it is important to understand \nwhat criticisms have been leveled against the argument, which I \nschematize as follows for convenience: \n\nThe General Form of the Argument from the Poverty of the Stimulus \n\nSo \n\nSo \n\nIn the 1970's, philosophers contested Chomsky's use of the word \n‘know’ to describe speakers' relations to grammar, \narguing that unlike standard cases of propositional knowledge, most \nspeakers are utterly unaware of grammatical rules (e.g., \n“Anaphors are bound, and pronominals and R-expressions are free\nin their binding domains”) and many probably wouldn't \nunderstand them even if told what they are (Stich 1971). In \nresponse, Chomsky (e.g., 1980:92) began to use a technical term, \n‘cognize,’ to describe the speaker-grammar relation, \navoiding the philosophically loaded term, ‘knowledge.’ \n\nHowever, while it is certainly legitimate to propose a special \nrelationship between speakers and grammars, unanswered questions \nremain about the precise nature of cognizance. Is it a \nrepresentational relation, like belief? If not, what does \n‘learning a grammar’ amount to? If so, are speakers' \nrepresentations of grammar ‘explicit’ or \n‘implicit’ or ‘tacit’ — and what, \nexactly, do any of these terms mean? (See the papers collected in \nMacDonald 1995, for discussion of this last issue; see Devitt 2006 \nfor arguments that there is no good reason to suppose that speakers \nuse any representations of grammatical rules in their production and \ncomprehension of language.) Relatedly, how does a speaker's \ncognizance of grammar (her ‘competence,’ in Chomskyan \nparlance) function in her linguistic ‘performance’ \n— i.e., in the actual production or comprehension of an \nutterance? \n\nThese issues bear on the argument from the poverty of the stimulus \nbecause that argument may appear more or less impressive depending on\nthe answers one gives to them. If, for instance, one held that \ngrammars are belief-like entities, explicitly represented in our \nheads in some internal code (cf. Stich 1978), then the question of \nhow those beliefs are acquired and justified is indeed a pressing one\n— as, for different reasons, is the question of how they \nfunction in performance (see Harman 1967, 1969). However, if one \nwere to deny that grammar is represented at all in the heads of \nspeakers, like Devitt 2006 and Soames 1984, then the issue of how \nlanguage is learned and what role ‘evidence’ etc. might \nplay in that process takes on a very different cast. Or if, to take a\nthird possibility, one were to reject generative syntax altogether \nand adopt a different conception of what the content of speakers' \ngrammatical knowledge is — along the lines of Tomasello \n(2003), say — then that again affects how one views the learning \nprocess. In other words, one's ideas about what is learned \naffect one's conception of what is needed to learn it. Less \n‘demanding’ conceptions of the outputs of language \nacquisition require less demanding conceptions of its input (whether \nexperiential or inborn); this last approach to the problem of \nlanguage learning is discussed further in §2.2.1 below. \n\nIn the example of polar interrogatives, discussed above, we saw how \nchildren apparently require explicit falsifying evidence in order to \nrule out the plausible-seeming but false hypothesis, H1. Premiss 2 of\nthe argument generalizes this claim: there are many instances in \nwhich learners need specific kinds of falsifying data to correct \ntheir mistakes (data that the argument goes on to assert are \nunavailable). These claims about the data learners would need in \norder to learn grammar are underpinned by certain assumptions about \nthe learning algorithm they employ. For example, the idea that false \nhypotheses are rejected only when they are explicitly falsified in \nthe data suggests that learners are incapable of taking any kind of \nprobabilistic or holistic approach to confirmation and \ndisconfirmation. Likewise, the idea that learners unequipped with \ninborn knowledge of UG are very likely indeed to entertain false \nhypotheses suggests that their method of generating hypotheses is \ninsensitive to background information or past experience. (e.g., \ninformation about what sorts of generalizations have worked in other \ncontexts \n\nThe non-nativist language learner as envisaged by Chomsky in the \noriginal version of the poverty of the stimulus argument, in other \nwords, is limited to a kind of Popperian methodology — one that\ninvolves the enumeration of all possible grammatical hypotheses, each\nof which is tested against the data, and each of is rejected just in \ncase it is explicitly falsified. As much work in philosophy of \nscience over the last half century has indicated, though, nothing \nmuch of anything can be learned by this method: the world quite \ngenerally fails to supply falsifying evidence. Instead, hypothesis \ngeneration must be inductively based, and (dis)confirmation is a \nholistic matter. \n\nThus arise two problems for the Chomskyan argument. First, it is not \nall that surprising to discover that if language learners employed a \nmethod of conjecture and refutation, then language could not be \nlearned from the data. In other words, the poverty of the stimulus \nargument doesn't tell us much we didn't know already. Secondly, and \nas a result, the argument is quite weak: it makes the negative point \nthat language acquisition does not occur via a Popperian learning \nstrategy, but it favors no specific alternative to this acquisition \ntheory. In particular, the argument gives no more support to a \nnativist (UG-based) theory than to one that proposed (say) that \nlearners formulate grammatical hypotheses based on their extraction \nof statistical information about the pld and that they may \nreject them for reasons other than outright falsification — \nbecause they lack explicit confirmation, or because they do not \ncohere with other parts of the grammar, for instance. \n\nIn reply, some Chomskyans (e.g., Matthews 2001) challenge \nnon-nativists to produce these alternative theories and submit them \nto empirical test. It's pointless, they claim, for nativists to try \nto argue against theories that are mere gleams in the empiricist's \neye, particularly when Chomsky's approach has been so fruitful and \nthus may be supported by a powerful inference to the best \nexplanation. Others have argued explicitly against particular \nnon-nativist theories — Marcus 1998, 2001, for instance, \ndiscusses the shortcomings of connectionist accounts of language \nacquisition. \n\nA recent book by Michael Tomasello (Tomasello 2003) addresses the \nnativist's demand for an alternative theory directly. Tomasello \nargues that language learners acquire knowledge of syntax by using \ninductive, analogical and statistical learning methods, and by \nexamining a broader range of data for the purposes of confirmation \nand disconfirmation. He argues that children formulate abstract \nsyntactic generalizations rather late in the learning process (around\nthe age of 4 or 5) and that their earliest utterances are governed by\nmuch less general rules of thumb, or ‘constructions.’ \nMore abstract constructions, framed in increasingly adult-like and \n‘syntactic’ terms, are progressively formulated through \nthe application of pattern-recognition skills (‘analogy’)\nand a kind of statistical analysis of both incoming data and \npreviously acquired constructions, which Tomasello calls \n‘functional distributional \n analysis.’[7] \n\nTomasello's theory differs from a Chomskyan approach in three \nimportant respects. First, and taking up a point mentioned in the \nprevious section, it employs a different conception of linguistic \ncompetence, the end state of the learning process. Rather than \nthinking of competent speakers as representing the rules of grammar \nin the maximally abstract, simple and elegant format devised by \ngenerative linguists, Tomasello conceives of them as employing rules \nat a variety of different levels of abstraction, and, importantly, as\nemploying rules that are not formulated in purely syntactic terms. He\nadopts a different type of grammar, called \n‘cognitive-functional grammar’ or ‘usage-based \ngrammar,’ in which rules are stated partly in terms of \nsyntactic categories, but also in semantic terms, that is, in terms \nof their patterns of use and communicative function. A second respect\nin which Tomasello's approach differs from that of most theorists in \nthe Chomskyan tradition, is in employing a much richer conception of \nthe ‘primary linguistic data,’ or pld. For \ngenerative linguists, the pld comprises a set of sentences, \nperhaps subject to some preliminary syntactic analysis, and the child\nlearning grammar is thought of as embodying a function which maps \nthat set of sentences onto the generative grammar for her language. \nOn Tomasello's conception, the pld includes not just a set \nof sentences, but also facts about how sentences are used by speakers\nto fulfill their communicative intentions. On his view, semantic and \ncontextual information is also used by children for the purposes of \nacquiring grammatical knowledge. \n\nTomasello argues that by adopting a more ‘user-friendly’ \nconception of natural language grammars and by radically expanding \none's conception of the language-relevant information available to \nchildren learning language, the ‘gap’ exploited by the \nargument from the poverty of the stimulus — that is, the gap \nbetween what we know about language and the data we learn it from \n— in large part disappears. This gives rise to a third \nimportant respect in which Tomasello's theory differs from that of \nthe linguistic nativist. On his view, children learn language without\nthe aid of any inborn linguistic information: what children bring to \nthe language learning task — their innate endowment — is \nnot language-specific. Instead, it consists of ‘mind \nreading,’ together with perceptual and cognitive skills that \nare employed in other domains as well as language learning. These \nskills include: (i) the ability to share attention with others; (ii) \nthe ability to discern others' intentions (including their \ncommunicative intentions); (iii) the perceptual ability to segment \nthe speech stream into identifiable units at different levels of \nabstraction; and (iv) general reasoning skills, such as the ability \nto recognize patterns of various sorts in the world, the ability to \nmake analogies between patterns that are similar in certain respects,\nand the ability to perform certain sorts of statistical analysis of \nthese patterns. Thus, Tomasello's theory contrasts strongly with the \nnativist approach. \n\nAlthough assessing Tomasello's theory of language acquisition is \nbeyond the scope of this entry, this much can be said: the \noft-repeated charge that empiricists have failed to provide \ncomprehensive, testable alternatives to Chomskyanism is no longer \nsustainable, and if the what and how of language acquisition are \nalong the lines that Tomasello describes, then the motivation for \nlinguistic nativism largely disappears. \n\nA third problem with the poverty of the stimulus argument is that \nthere has been little systematic attempt to provide empirical \nevidence supporting its assertions about what the pld \ncontain. This is an old complaint (cf. Sampson 1989) which has \nrecently been renewed with some vigor by Pullum and Scholz 2002, \nScholz and Pullum 2002, and Sampson 2002. Pullum and Scholz provide\nevidence that, contrary to what Chomsky asserts in his discussion of \npolar interrogatives, children can expect to encounter plenty of data\nthat would alert them to the falsity of H1. Sampson  2002, mines the\n‘British National Corpus/demographic,’ a 100 million word\ncorpus of everyday British speech (available online at \nhttp://info.ox.ac.uk/bnc/), for evidence that contrary to Kimball's \ncontention that complex auxiliaries are ‘vanishingly \nrare,’ they in fact occur quite frequently (somewhere from once\nevery 10,000 words to once every 70,000 words, or once every couple \nof days to once a week). \n\nChomskyans respond in two main ways to findings like this. First, \nthey argue, it is not enough to show that some children can \nbe expected to hear sentences like Is the girl in the jumping \ncastle Kayley's daughter? All children learn the \ncorrect rule, so the claim must be that all children are \nguaranteed to hear sentences of this form — and this claim is \nstill implausible, data like those just discussed \n notwithstanding.[8]\n In order to take this question further, it would be necessary to \ndetermine when in fact children master the relevant structures, and \nvanishingly little work has been done on this topic. Sampson \n2002:82ff. found no well-formed auxiliary fronted questions (like \nIs the girl who is in the jumping castle Kayley's daughter?)\nin his sample of the British National Corpus. He notes that in \naddition to supporting Chomsky's claims about the poverty of the \npld, such data simultaneously problematize his claims about \nchildren's knowledge of the auxiliary-fronting rule itself. Sampson \nfound that speakers invariably made errors when apparently attempting\nto produce complex auxiliary-fronted questions, and often emended \ntheir utterance to a tag form instead (e.g., The girl who's in \nthe jumping castle is Kayley's daughter, isn't she?). \nHespeculates that the construction is not idiomatic even in adult \nlanguage, and that speakers learn to form and decode such questions \nmuch later in life, after encountering them in written English. If \nthat were the case, then the lack of complex auxiliary fronted \nquestions in the pld would be both unsurprising and \nunproblematic: young children don't hear the sentences, but nor do \nthey learn the rule. To my knowledge, children's competence with the \nauxiliary fronting rule has not been addressed \n empirically.[9] \n\nSecondly, Chomskyans may produce other versions of the poverty of the\nstimulus argument. For instance, Crain 1991 constructs a poverty of \nthe stimulus argument concerning children's acquisition of knowledge \nof certain constraints on movement. However, while Crain's argument \ncarefully documents children's conformity to the relevant grammatical\nrules, its nativist conclusion still relies on unsubstantiated \nintuitions as to the non-occurrence of relevant forms or evidence in \nthe pld. It is thus inconclusive. (Cf. Crain 1991; Crain's \nexperiments and their implications are discussed in Cowie \n1999 ; Cf. also Crain and Pietrowski  2001, 2002). \n\nThe argument from (1), (2), and (3) to (4) appears valid. However, as\nis implicit in my discussion of premiss (2), an equivocation between \ndifferent senses of ‘learning’ threatens. What (1)-(3) \nshow, if true, is that grammar G can't be learned from the \npld by a learner using a ‘Popperian’ learning\nstrategy, that is, a strategy of ‘bold conjecture’\nand refutation. What (4) concludes, however, is that G is unlearnable,\nperiod, from the pld — a move that several authors,\nparticularly connectionists, have objected to. (See especially\nElman et al. 1996 and Elman 1998 for criticisms of Chomskyan\nnativism along these lines; see Marcus 1998 and 2001 for\nresponses.) \n\nChomskyans typically take this point, conceding that the argument \nfrom the poverty of the stimulus is not apodeictic. Nonetheless, they\nclaim, it's a very good argument, and the burden of proof belongs \nwith their critics. After all, nativists have shown the falsity of \nthe only non-nativist acquisition theories that are well-enough \nworked out to be empirically testable, namely, Skinnerian behaviorism\nand Popperian conjecture and refutation. In addition, they have \nproposed an alternative theory, Chomskyan nativism, which is more \nthan adequate to account for the phenomena. In empirical science, \nthis is all that they can reasonably be required to do. The fact that\nthere might be other possible acquisition algorithms which \nmight account for children's ability to learn language is \nneither here nor there; nativists are not required to argue against \nmere possibilities. \n\nIn response, some non-nativists have argued that UG-based theories \nare not in fact good theories of language acquisition. Tomasello \n(2003: 182ff.), for instance, identifies two major areas of \ndifficulty for UG-based theories, such as the \nprinciples-and-parameters approach. First, there is the \n‘linking’ problem, deriving from the fact of linguistic \ndiversity: almost no UG-based accounts explain how children link the \nhighly abstract categories of UG to their instantiations in the \nparticular language they happen to be \n learning.[10]\n His example is the category ‘Head,’ In order to set the \n‘Head parameter,’ a child needs to be able to identify \nwhich words in the stream of noise she is hearing are in fact clausal\nheads. But heads “do not come with identifying tags on them in \nparticular languages; they share no perceptual features in common \nacross languages, and so their means of identification cannot be \nspecified in [UG]” (Tomasello 2003:183). Second, there is the \nproblem of developmental change, also emphasized by Sokolov and Snow,\n1991. It is difficult to see how UG-based approaches can account for \nthe fact that children's linguistic performance seems to emerge \npiecemeal over time, rather than emerging in adult-like form all at \nonce, as the parameter-setting model suggests it \n should.[11]\n In response, generativists have appealed to such notions as \n‘maturational factors’ or ‘performance \nfactors.’ But, Tomasello argues, such measures are ad hoc in \nthe absence of a detailed specification of what these maturational or\nperformance factors are, and how they give rise to children's actual \nperformance. \n\nAt the very least, such objections serve to equalize the burden of \nproof: non-nativists certainly have work to do, but so too do \nnativists. Merely positing an innate UG and a \n‘triggering’ mechanism by which it ‘grows’ \ninto full-fledged language is insufficient. Nativists need to show \nhow their theory can account for the known course of language \nacquisition. Merely pointing out that there is a possibility\nthat such theories are true, and that they would, if true, explain \nhow language learning occurs in the face of an allegedly impoverished\nstimulus, is only part of the job. \n\nBecause they are defending the view that all of UG is inborn, \nChomskyans must be credited with holding that the primary data are \nimpoverished quite generally. That is, if the innateness of UG \ntout court is to be supported by poverty of the stimulus \nconsiderations, the idea must be that the cases that nativists \ndiscuss in detail (polar interrogatives, complex auxiliaries, etc.) \nare but the tip of the unlearnable iceberg. Nativists quite \nreasonably do not attempt to defend this claim by endless enumeration\nof cases. Rather, they turn to another kind of argument to support \nthe ‘global impoverishment’ position. This argument is \nsometimes called the ‘Logical Problem of Language \nAcquisition’; here, we will call it ‘The Unlearning \nProblem.’ It will be discussed in section 3. \n\nSuppose that the primary linguistic data were impoverished in all the\nways that nativists claim and suppose, too, that children know a \nbunch of things for which there is no evidence available — \nsuppose, as Hornstein and Lightfoot (1981:9) put it, that \n“[p]eople attain knowledge of the structure of their language \nfor which no evidence is available in the data to which they\nare exposed as children.” What follows from this is that there \nmust be constraints on the learning mechanism: children do not \nenumerate all possible grammatical hypotheses and test them against \nthe data. Some possible hypotheses must be ruled out a \npriori. But, critics allege, what does not follow from \nthis is any particular view about the nature of the requisite \nconstraints. (Cowie 1999: ch.8.) A fortiori, what does not \nfollow from this is the view that Universal Grammar (construed as a \ntheory about the structural properties common to all natural \nlanguages, per Terminological Note 2 above) is inborn. \n\nFor all the poverty of the stimulus argument shows, the constraints \nin question might indeed be language-specific and innate, but with \ncontents quite different from those proposed in current theories of \nUG. Or, the constraints might be innate, but not language-specific. \nFor instance, as Tomasello 2003 argues, children's early linguistic \ntheorizing appears to be constrained by their inborn abilities to \nshare attention with others and to discern others' communicative \nintentions. On his view, a child's early linguistic hypotheses are \nbased on the assumption that the person talking to him is attempting \nto convey information about the thing(s) that they are both currently\nattending to. (Another example of an innate but non-language specific\nconstraint on language learning derives from the structure of the \nmammalian auditory system; ‘categorical perception,’ and \nis relation to the acquisition of phonological knowledge is discussed\nbelow, §3.3.4.). Another alternative is that the constraints \nmight be learned, that is, derived from past experiences. An example \nagain comes from Tomasello (2003). He argues that \nentrenchment, or the frequency with which a linguistic \nelement has been used with a certain communicative function, is an \nimportant constraint on the development of children's later syntactic\nknowledge. For instance, it has been shown experimentally that the \nmore often a child hears an element used for a particular \ncommunicative purpose, the less likely she is to extend that element \nto new contexts. (See Tomasello 2003:179). \n\nIn short, there are many ways to constrain learners' hypotheses about\nhow their language works. Since the poverty of the stimulus argument \nmerely indicates the need for constraints, it does not speak to the \nquestion of what sorts of constraints those might be. \n\nIn response to this kind of point, Chomskyans point out that the \ninnateness of UG is an empirical hypothesis supported by a perfectly \nrespectable inference to the best explanation. Of course there is a \nlogical space between the conclusion that something \nconstrains the acquisition mechanism and the Chomskyan view that \nthese constraints are inborn representations of Binding Theory, Theta\ntheory, the ECP, the principle of Greed or Shortest Path and so on. \nBut the mere fact that the argument from the poverty of the stimulus \ndoesn't prove that UG is innately known is hardly reason to \ncomplain. This is science, after all, and demonstrative proofs are \nneither possible nor required. What the argument from the poverty of \nthe stimulus provides is good reason to think that there are strong \nconstraints on the learning mechanism. UG is at hand to supply a \ntheory of those constraints. Moreover, that theory has been highly \nproductive of research in numerous areas (linguistics, \npsycholinguistics, developmental psychology, second language \nresearch, speech pathology etc. etc.) over the last 50 years. These \nsuccesses far outstrip anything that non-nativist learning theorists \nhave able to achieve even in their wildest dreams, and support a \npowerful inference to the best explanation in the Chomskyan's favor. \n\nAs seen above (§2.2.1(d)), however, the strength of the \nChomskyan's ability to explain the phenomena of language acquisition \nhas been questioned, and with it, implicitly, the strength of her \ninference to the best explanation. In addition, there is a general \ndebate within the philosophy of science as to the soundness of \ninferences to the best explanation: does an explanation's being the \nbest available give any additional reason (over and above its ability\nto account for the phenomena within its domain) to suppose it true? \n[Link to Encyclopedia Article ‘Abduction’ by Peter \nAchinstein for more on this topic.] \n\nIn the linguistic case, what sometimes seems to underpin people's \npositions on such issues is differing intuitions as to who has the \nburden of proof in this debate. Empiricists or non-nativists contend \nthat Chomskyans have not presented enough data (or considered enough \nalternative hypotheses) to establish their case. Chomskyans reply \nthat they have done more than enough, and that the onus is on their \ncritics either to produce data disconfirming their view or to produce\na testable alternative to it. \n\nThat such burden-shifting is endemic to discussions of linguistic \nnativism (the exchange in Ritter 2002 is illustrative) suggests to \nme that neither side in this debate has as yet fulfilled its \nobligations. Empiricists about language acquisition have ably \nidentified a number of points of weakness in the Chomskyan case, but \nhave only just begun to take on the demanding task of developing \ndevelop non-nativist learning theories, whether for language or \nanything much else. Nativists have rested content with hypotheses \nabout language acquisition and innate knowledge that are based on \nplausible-seeming but largely unsubstantiated claims about what the \npld contain, and about what children do and do not know and \nsay. \n\nIt is unclear how to settle such arguments. While some may disagree\n(especially some Chomskyans), it seems that much work still needs to\nbe done to understand how children learn language — and not just\nin the sense of working out the details of which parameters get set\nwhen, but in the sense of reconceiving both what linguistic competence\nconsists in, and how it is acquired. In psychology, a new,\nnon-nativist paradigm for thinking about language and learning has\nbegun to emerge over the last 10 or so years, thanks to the work of\nresearchers like Elizabeth Bates, Jeffrey Elman, Patricia Kuhl,\nMichael Tomasello and others. The reader is referred to Elman et\nal. 1996, Tomasello 2003 and §3 below for an entrée\ninto this way of thinking. \n\nFor now, considerations of space demand a return to our topic, viz., \nlinguistic nativism, rather than further discussion of alternatives \nto it. \n\nWe saw in the previous section that in order to support the view that\nall of UG is innately known, nativists about language need to hold \nnot just that the data for language learning is impoverished in a few\nisolated instances, but that it's impoverished across the board. That\nis, in order to support the view that the innate contribution to \nlanguage acquisition is something as rich and detailed as knowledge \nof Universal Grammar, nativists must hold that the inputs to language\nacquisition are defective in many and widespread cases. (After all, \nif the inputs were degenerate only in a few isolated instances, such \nas those discussed above, the learning problem could be solved simply\nby positing innate knowledge of a few relevant linguistic hints, \nrather than all of UG.) \n\nPullum and Scholz (2002:13) helpfully survey a number of ways in \nwhich nativists have made this point, including: \n\nIn this section, I will set aside features (i) and (ii) as being \ncharacteristic of any empirical domain: the data are \nalways finite, and they always underdetermine one's\ntheory. No doubt it's an important problem for epistemologists and \nphilosophers of science to explain how general theories can \nnonetheless be confirmed and believed. No doubt, too, it's an \nimportant problem for psychologists to explain the mechanisms by \nwhich individuals acquire general knowledge about the world on the \nbasis of their experience. But underdetermination and the finiteness \nof the data are everyone's problem: if these features of the language\nlearning situation per se supported nativism, then we should accept \nthat all learning, in every domain, requires inborn domain-specific \nknowledge. But while it's not impossible that everything we know that\ngoes beyond the data is a result of our having domain-specific innate\nknowledge, this view is so implausible as to warrant no further \ndiscussion here. \n\nI also set aside features (iii) and (iv). For one thing, it is \nunclear exactly how degenerate the pld are; according to one\nearly estimate, an impressive 99.7% of utterances of mothers to their\nchildren are grammatically impeccable (Newport, Gleitman and \nGleitman 1977). And even if the data are messier than this figure \nsuggests, it is not unreasonable to suppose that the vast weight of \ngrammatically well-formed utterances would easily swamp any residual \nnoise. As to the idiosyncrasy of different children's data sets, this\nis not so much a matter of stimulus poverty as stimulus difference. \nAs such, idiosyncrasy becomes a problem for a non-nativist only on \nthe assumption that different children's states of linguistic \nknowledge differ from one another less than one would expect given \nthe differences in their experiences. As far as I know, no serious \ncase for this last claim has ever been \n made.[12] \n\nIn this section, we will focus on features (v) and (vi) of the \npld. For it is consideration of the positivity of the data \nset, and the lack of feedback available to children, that has given \nrise to what I am calling the ‘Unlearning Problem,’ \notherwise known (somewhat misleadingly) as the ‘Logical Problem\nof Language Acquisition.’ (For statements of the argument, see,\ne.g., Baker 1979; Lasnik; 1989:89-90; Pinker 1989.) Figure 2.  Five possible relations between the language generated\nby hypothesis (H) and the target grammar (L) \n\nTake a child learning the grammar of her language, L. \nFigure 2 represents the 5 possible relations that \nmight obtain between the language generated by her current \nhypothesis, H, and that generated by the target grammar, \nL. (v) represents the end point of the learning process: the\nlearner has figured out the correct grammar for her language. A \nlearner in situation (i), (ii) or (iii) is in good shape, for she can\neasily use the pld as a basis for correcting her hypothesis \nas follows: whenever she encounters a sentence in the data (i.e., a \nsentence of L) that is not generated by H, she has \nto ‘expand’ her hypothesis so that it generates that \nsentence. In this way, H will keep moving, as desired, towards \nL. However, suppose that the learner finds herself in \nsituation (iv), where her hypothesis generates all of the target \nlanguage, L, and more besides. (Children frequently find \nthemselves in this position, for example, they invariably go through \na phase in which they overgeneralize regular past tense verb endings \nto irregular verbs; their grammars generate the incorrect *I \nbreaked it as well as the correct I broke it.) There, \nshe is in deep trouble, for she cannot use the pld to \ndiscover her error. Every sentence of L, after all, is \nalready a sentence of H. In order to ‘shrink’ \nher hypothesis — to ‘unlearn’ the rules that \ngenerate *I breaked it — she needs to know which \nsentences of H are not sentences of L — she \nneeds to figure out that *I breaked it is not a sentence of \nEnglish. But — and this is the problem — this kind of \nevidence, often called ‘negative evidence,’ is held to be\nunavailable to language learners. \n\nFor as we have seen, the pld is mostly just a sample of \nsentences, of positive instances of the target language. It contains \nlittle, if any, information about strings of words that are not \nsentences. For instance, children aren't given lists of ungrammatical\nstrings. Nor are they typically corrected when they make mistakes. \nAnd nor can they simply assume that strings that haven't made their \nway into the sample are ungrammatical: there are infinitely many \nsentences that are absent from the data for the simple reason that \nno-one's had occasion to say them yet. \n\nIn sum: a child who is in situation (iv) — a child whose \ngrammar ‘overgenerates’ — would need negative \nevidence in order to recover from her error. Negative evidence, \nhowever, does not appear to exist. Since children do manage to learn \nlanguages, they must never get themselves into situation (iv): they \nmust never need to ‘unlearn’ any grammatical rules. There\nare two ways they could do this. One would be never to generalize \nbeyond the data at all. But clearly, children do generalize, else \nthey'd never succeed in learning a language. The other would be if \nthere were something that ensured that when they generalize \nbeyond the data, they don't overgeneralize, something, that \nis, that ensures that children don't make errors that they could only\ncorrect on the basis of negative evidence. According to the \nlinguistic nativist, this something is innate knowledge of UG. \n\nFirst, let's make a distinction between: \n\nand \n\nSecond, let's abandon the idea, which reappears in many presentations\nof the Argument from the Unlearning Problem; that learners' \nhypotheses must be explicitly falsified in the data in order to be \nrejected. Let's suppose instead that learners proceed more like \nactual scientists do — provisionally abandoning theories due to\nlack of confirmation, making theoretical inferences to link data with\ntheories, employing statistical information, and making defeasible, \nprobabilistic (rather than decisive, all-or-nothing) judgments as to \nthe truth or falsity of their \n theories.[13] \n\nIntuitively, viewing the learner as employing more stochastic and \nprobabilistic inductive techniques enables one to see how the \nunlearning problem might have been overblown. What the argument \nclaims, rightly, is that negative data near enough do not \nexist in the pld. However, what learners need in order to \nrecover from overgeneralizations, is not negative data per \nse, but negative evidence, and arguably, the \npld do contain significant amounts of that. For example: \n\nNon-occurrence of structural types as negative evidence: \nSuppose that a child's grammar predicted that a certain string is \npart of the target language. Suppose further that that string never \nappears in the data, even when the context seems appropriate. \nProponents of the unlearning problem say that non-occurrence cannot \nconstitute negative evidence — maybe Dad simply always chooses \nto say The girl who is in the jumping castle is Kayley's \ndaughter, isn't she? rather than the auxiliary-fronted version, \nIs the girl who is in the jumping castle Kayley's daughter? \nIf so, it would be a mistake for the child to conclude on the basis \nof this information that the latter string is ungrammatical. \n\nBut suppose that the child is predicting not strings of words, \nsimpliciter, but rather strings of words under a certain\nsyntactic description (or, perhaps more plausibly, \nquasi-syntactic description — the categories employed need not \nbe the same as those employed in adult \n grammars).[14]\n This would enable her to make much better use of non-occurrence as \nnegative evidence. For non-occurring strings will divide into two \nbroad kinds: those whose structures have been encountered \nbefore in the data, and those whose structures have not been\nheard before. In the former case, the child has positive evidence \nthat strings of that kind are grammatical, evidence that would enable\nher to suppose that the non-occurrence of that particular string was \njust an accident. (E.g., she could reason that since she's heard \nIs that mess that is on the floor in there yours? many \ntimes, and since that string has the same basic structure as Is \nthat girl that's in the jumping castle Kayley's daughter?, the \nlatter string is probably OK even though Dad chose not to say it.) \n\nIn the case in which the relevant form has never been \nencountered before in the data, however, the child is better off: the\nfact that she has never heard any utterance with the structure of \n*Is that girl who in the jumping castle is Kayley's daughter\nor *Is that mess that that on the floor in there is yours? \nis evidence that strings of that type are not sentences. Again, the \nevidence is not decisive, and the child should be prepared to revise \nher grammar should strings of that kind start appearing. Nonetheless,\nthe non-occurrence of a string, suitably interpreted in the light of \nother linguistic information, can constitute negative evidence and \nprovide learners with reason to reject overgeneral grammars. \n\nPositive Evidence as Negative Evidence. Relatedly, learners \ncan also exploit positive evidence as to which strings occur in the \npld as a source of negative evidence — again in a \ntentative and revisable \n way.[15]\n Suppose that the child's grammar generated two strings as appropriate\nin a given kind of context, but that only one sort of string was ever\nproduced by those around her. The fact that only strings of the first\nkind occur is in this case negative evidence — defeasible, to \nbe sure, but negative evidence nonetheless. \n\nIn fact, the use of positive evidence to disconfirm hypotheses is \nendemic to science. For instance, Millikan used positive evidence to \ndisconfirm the theory that electrical charge is a quantity that \nvaries continuously. In his famous ‘Oil Drop’ experiment,\nhe found that the amount of charge possessed by a charged oil drop \nwas always a whole-number multiple of —(1.6 x 10-19)C. The \nfinding that all observed charges were ‘quantized’ in \nthis manner disconfirmed the competing ‘continuous \ncharges’ hypothesis in the same way that positive evidence can \ndisconfirm grammatical \n hypotheses.[16] \n\nHowever, more recent findings have uncovered evidence indicating that\nfailures of understanding occur with some regularity, and that there \nis a wealth of feedback about correct usage in the language-learning \nenvironment. For example: \n\nChomsky has recognized the existence of such ‘indirect’ \nnegative data in the pld. However, he concluded that they \nwere too few and ambiguous to be of aid to the language learner. The \nsorts of findings reported above seem to show that negative evidence \nis pervasive in the pld. But can children learn from these \nsorts of statistical regularities? \n\nStandard \n formulations[17]\n of the ‘Unlearning Problem’, assume that they cannot: the\nview seems to be that learning can only take place under idealized \nconditions where the world supplies unambiguous evidence pro or con \nthe language learner's grammatical theories. Given such a conception \nof the learner, none of the examples of feedback just discussed will \nseem relevant to the problem. For only a learner employing fairly \nsophisticated data-analysis techniques and a confirmation measure \nthat is sensitive to small changes in probabilities would be able to \nexploit the sorts of regularities in the linguistic environment that \nwe have just discussed. However, there is increasing evidence that \nchildren are in fact remarkably sensitive to subtle feedback, in both\nlinguistic and non-linguistic domains. For instance: \n\nIn addition, it is becoming increasingly clear that babies, children,\nadults and many other mammals are highly sensitive not just to \nfeedback, but to other non-obvious statistical regularities in their \nexperience. For example: \n\nTaken together, these kinds of results raise the possibility that \nsome of the foundational learning mechanisms involved in language \nacquisition are not language specific. If it turns out that babies \nemploy the sorts of distributional analysis studied by Saffran, \nRedington and Chater, Pena, and Mintz not only in learning artificial\nlanguages, but also in learning natural languages, then that is \nevidence against linguistic nativism. For this type of learning is \nemployed by humans and other animals in other contexts as well: \nwhatever is involved in language learning — be it innate or not\n— is not language-specific. \n\nThe previous objections to the Unlearning Problem Argument made the \npoints, first, that negative evidence does exist in the pld \n(in the form of regularities both in others' language use and in how \nothers react to children's own productions), and second, that \nchildren (and other animals) seem very good at exploiting this kind \nof information for the purposes of learning about their world. This \nwould seem to be rather a good thing, given that there is reason to \nthink that learners must be able to learn in domains where \nexplicit negative data do not exist, and in the absence of \nspecialized innate knowledge of those domains. For the unlearning \nproblem is a problem for learning from experience quite generally. \nThat is, there are many domains in which learners lack explicit \nevidence as to what things are not: trees are not cars, \nIrish stews are not curries, birds are not fish and MacDonald's is \nnot a branch of the CIA. No-one ever told you any of these things, \nbut it's crazy to think that you now know them because you possess \nanalogs to the ‘Language Acquisition Device’ for each of \nthese domains. Clearly, in at least some areas, people are able to \nlearn an awful lot on the basis of largely positive data, and while \nthis of course does nothing to show that language is one of those \nareas, it does indicate that the Unlearning problem argument by \nitself is no argument for linguistic nativism at all, let alone for \nthe Chomskyan (UG-based) version of that position. \n\nIn this section, I will mention some other avenues of research that \nhave been argued to have a bearing on the innateness of language. My \ngoal is not to give an exhaustive survey of these matters, but rather\nto provide the interested reader with a way into the relevant \nliteratures. Still, I will try to give enough details so as to make a\ncase that current empirical findings, together with the flaws \nidentified in §§1 and 2 in the positive arguments for \nlinguistic nativism, tend to militate against that position. \n\nChomsky and others (e.g., Chomsky 1988:46-7; Pinker 1994:237-8) \nhave pointed to the existence of ‘linguistic universals’ \nas supporting the idea that language is the product of a distinct \nfaculty of mind. Universals are features thought to be common to all \nnatural languages, such as the existence of constraints on the \nmovement of elements during a derivation or, less controversially, \nthe existence of a syntactic distinction between nouns and verbs. But\nnot only is the existence of true universals a contested matter (see \ne.g., Maratsos 1989:111), it is unclear what the correct explanation\nof them — assuming they exist — is would be. \n\nOne explanation is certainly the Chomskyan one that they are \nconsequences of speakers' innate knowledge of UG. Another is that \nthey derive from other, non-linguistically-specific features of \ncognition, such as memory or processing constraints (e.g., Berwick \nand Weinberg 1983 trace certain constraints on movement to \nlimitations on parsing imposed by the structure of human memory). Yet\nanother is that they derive from universal demands of the \ncommunication situation (e.g., Sapir 1921, argued that the \ndistinction between nouns and verbs arises from the fact that \nlanguage is used to communicate propositions, hence needs a way to \nbring an object subject to mind and a way to say something about it).\nFinally, as Putnam 1971 speculated, universals might be relics of an\nancestral Ur-language from which all other languages evolved. This \nlast hypothesis has generally been rejected as lacking in empirical \nsupport. However, recent findings in genetics and historical \nlinguistics are converging to suggest that all human populations \nevolved from a small group migrating from Africa in the fairly recent\npast, and that all human languages have probably evolved from the \nlanguage spoken by that group. (Cavalli-Sforza 1997.) \n\nThe Ur-language hypothesis is not, of course, inconsistent with \nlinguistic nativism. However, if true, it does weaken any \nargument from the existence of universals to the innateness \nof linguistic knowledge. For if languages have a common ancestor, \nthen it is possible to explain universals — even ones that seem\nstrange from a functional point of view — as being the result \nof our ancestors' having adopted a certain solution to a linguistic \ncoordination problem. Like driving on the right side of the road, a \nsolution once established may become entrenched, because the benefits\nof everyone's conforming to the same rule outweigh the costs of \nchanging to a different rule, and this may be so even if the new rule\nwere in some sense more ‘reasonable.’ Thus, even \narbitrary or odd features of language can be explained historically, \nwithout positing either compelling functional considerations or \ninborn linguistic \n constraints.[18] \n\nIf, by contrast, language emerged independently in a number of areas,\nthe existence of universals would be a strong argument for nativism, \nFor in that case, it would be implausible to maintain that each \nancestral group ‘just happened’ to select the same \nsolutions to the various coordination problems they encountered. More\nplausible would be the supposition that the different groups' choice \nof the rule was driven by something internal to speakers, \nsuch as, perhaps, an innate representation of UG. In short: if \nlanguages have a common ancestor, then common descent from originally\narbitrary linguistic conventions is a possible explanation of \nlinguistic universals, including the ‘odd’ or \n‘arbitrary’ ones that don't seem to have any real \nfunctional significance. If they don't, then such universals \nseemingly could only be explained in terms of features internal to \nspeakers. \n\nFigure 3. Broca's area and Wernicke's area \n\nBeginning with the work of Broca and Wernicke in the 19th century, a \npopular view has been that language is localized to certain areas of \nthe brain (see Fig. 3), almost always the left \n hemisphere,[19]\n and that it is subject to characteristic patterns of breakdown, \ncalled ‘aphasias.’ (See Saffran 2000 for a survey of the\nvarious aphasias.) For example, Broca's area is strongly implicated \nin speech production, and damage to this area can result in a \ncharacteristic inability (‘Broca's aphasia’ or \n‘agrammatism’) to produce fluent speech, especially \ncomplex grammatical structures and grammatical morphemes. The fact \nthat syntax can apparently be selectively interfered with by lesions \nto Broca's area has been taken by some to indicate that grammatical \nknowledge is localized to that area, and this in turn has been taken \nto show support the view that there is a special biological inborn \nbasis for that knowledge. (Lenneberg 1964, 1967 is the original \nproponent of this argument, which is echoed in more recent \ndiscussions, such as Pinker 1994:297-314.) \n\nIt is unclear, however, why this inference should seem compelling.\nFirst, as Elman et al. 1996 argue, neural localization of\nfunction can occur as a result of virtually any developmental\ntrajectory: the localization of some function bears not at all on its\ninnateness. \n\nSecondly, it is now known that neural localization for language is \nvery much a relative, rather than an all-or-nothing matter (Dronkers \net al. 2000, Dick et al. 2001, Martin 2003). Not\nonly is language processing widely distributed over the brain (see\nFig. 4), but traditionally language-specific areas of cortex are\nimplicated in a variety of non-linguistic tasks as well. Broca's area,\nfor instance, ‘lights up’ on MEG scans\n(magnetoencephalography, a method for measuring changes in the\nmagnetic properties of the brain due to electrical activity) when\nsubjects hear a discordant musical sequence in much the same way as it\ndoes when they hear an ungrammatical utterance. (Maess et\nal. 2001; a special issue of Nature Neuroscience, 6(7),\nJuly 2003, explores the implications of this finding.) \n\nFinally, recent studies of cortical plasticity have shown that even\nthe most plausible candidates for innate specification — such as\nthe use of visual cortex for vision or the use of auditory cortex for\nhearing — exhibit high degrees of experience-dependent\nplasticity. For example, in congenitally blind subjects, the areas of\nthe brain normally used for seeing are taken over for the processing\nof Braille (Sadato et al. 1996; Hamilton and Pascual-Leone,\n1998) and even in those with late-onset blindness, significant\n‘rewiring’ of visual cortex for other perceptual tasks is\napparent (Kujala et al. 1997). Likewise, in the congenitally deaf,\nauditory cortex is used for the processing of sign language (Nishimura\net al. 1999, von Melchner, Pallas and Sur 2000). (See Shimojo and\nShams 2001, for a review.) \n\nFigure 4. Pet scan showing brain regions involved in various language\ntasks. From Posner and Raichle (1997, 15). Used by permission of M. \nRaichle. \n\nAs Marcus (2004:40-45) points out in response to Elman et\nal. 1996, the ability of the brain to ‘rewire’ itself\nunder exceptional circumstances is consistent with its having been\n‘prewired,’ or set up, differently by the genes. However,\nthese sorts of data indicate that complex functions, such as are\ninvolved in processing sign language, can be carried out in areas of\nbrain that are ‘prewired’ (if they are) to do something\nquite different.  This suggests that these abilities require little in\nthe way of task-specific pre-wiring, and are learned largely on the\nbasis of experience (together with whatever sort of 'prewiring' is\nsupplied for the cortex as a whole). That is, if sign language\nprocessing tasks can be carried out by areas of cortex that are\npresumably innately predisposed (if they are) to do auditory\nprocessing, then the former competence must be being learned in the\nabsence of inborn constraints or knowledge that are specific to that\ntask. Of course, these are pathological cases, and it is unclear\nwhether the subjects in these experiments had any special training in\norder that their brains were ‘rewired’ in these\nways. Nonetheless, examples like these provide an existence proof of\nthe brain's ability to acquire complex processing capacities —\nindeed, processing capacities relevant to language — in the\ncomplete absence of inborn, domain-specific information. As such, they\nraise the possibility that other aspects of language processing are\nsimilarly acquired in the absence of task-specific constraints. \n\nIn sum, the neuroscientific evidence currently available provides no \nsupport for linguistic nativism. The suggestion that localization of \nfunction is indicative of a substantial degree of innate \nprespecification is no longer tenable: localization can arise in many\ndifferent ways. In addition, linguistic functions do not seem to be \nparticularly localized: language use and understanding are complex \ntasks, involving many different brain areas — areas that are in\nat least some cases implicated also in other tasks. It is hard to see\nhow to reconcile these facts with the Chomskyan postulation of a \nmonolithic ‘language organ,’ the development or \n‘growth’ of which is controlled largely by the genes. \nFinally, the fact that complex functions can be learned and carried \nout by areas of brain that are innately ‘prewired’ (if at\nall) to do quite different sorts of processing indicates that such \ncompetences can be and are acquired without any inborn, task-specific\nguidance. This is not, of course, to say that language is one of the \ncompetences that are acquired in this way. For all the current \nevidence shows, many areas of cortex in which language develops may \nindeed be ‘prewired’ for that task: linguistic nativism \nis still consistent with what is now known. It is, however, to \nsuggest that although there may be other reasons to be a linguistic \nnativist, general considerations to do with brain organization or \ndevelopment as currently understood give no especial support to that \nposition. \n\nLenneberg (1964, 1967) also argued that although language acquisition\nis remarkably robust, in the sense that all normal (and many \nabnormal) children do it, it can occur unproblematically only during \na ‘critical period’ — roughly, up to late childhood\nor early puberty. On analogy with other supposedly innately specified\nprocesses like imprinting or visual development, Lenneberg used the \nexistence of a critical period as further evidence that language \npossesses a proprietary basis in biology. \n\nIn support of the critical period hypothesis about language, \nLenneberg cited the facts (i) that retarded (e.g., Downs syndrome) \nchildren's language development stops around puberty; (ii) that \nwhereas very young children are able to (re)learn language after \naphasias produced by massive left-hemisphere trauma (including \nhemispherectomy), aphasias in older children and adults are typically\nnot reversible; and (iii) that so-called ‘wild children,’\nviz., those who grow up with no or little exposure to human language,\nexhibit severely compromised language skills. (Lenneberg, \n1957:142-55; see Curtiss 1977 for the (in)famous case of Genie, a \nmodern-day ‘wild child’ from suburban Los Angeles, who \nwas unable to acquire any but the most rudimentary grammatical \ncompetence after a miserable and wordless childhood spent locked \nalone in a room, tied to her potty chair or bed.) \n\nAs further support for the critical period hypothesis, others have \nadded the observation that although children are able to learn a \nsecond language rapidly and to native speaker fluency, adult learners\nof second languages typically are not: the capacity to learn a second\nlanguage tapers off after puberty, no matter how much exposure to the\nlanguage one has. (Newport 1990). Thus, it was speculated, the \ninnate knowledge base for language learning (e.g., knowledge of UG) \nbecomes unavailable for normal acquisition at puberty, and adult \nlearners must rely on less efficient learning methods. (Johnson and \nNewport 1989.) \n\nAs a preliminary to discussing these arguments (many of which are \npresented in more detailed in Stromswold 2000) it is worth \ndistinguishing two notions that often get conflated under the name \n‘critical period’: \n\nCritical Period: a time during development which is \nliterally critical; the relevant competence either cannot \ndevelop or will be permanently lost unless certain inputs are \nreceived during that period. \n\nSensitive Period: a time during development in which a \ncompetence is acquired ‘normally,’ or \n‘easily,’ or ‘naturally.’ The competence can \nbe acquired outside the sensitive period, but perhaps less easily and\nnaturally, and or perhaps with less ultimate success. \n\nThe classic example of a critical period is due to the Nobel \nprize-winning work of Hubel and Wiesel. By suturing shut one of a \nkitten's eyes at various stages of development and for various \nperiods of time, Hubel and Wiesel (1970) showed that certain cortical\nand thalamic areas supporting binocular vision (specifically, ocular \ndominance \n columns[20]\n and cells in the lateral geniculate body) will not develop normally \nunless kittens receive patterned visual stimulation during the 4th to\n12th weeks of life. They found that while the damage was sometimes \nreversible to some extent, depending on the exact duration and timing\nof the occlusion, occlusion for the entire first three months of life\nproduced irreversible blindness in the deprived \n eye.[21] \n\nLanguage, however, is not like this. As we will see, there is little \nevidence for a critical period for language acquisition, \nalthough there is considerable evidence that there is a sensitive\nperiod during which language is acquired more easily. The \nimplications of this for claims about the innateness of language will\nbe addressed in §3.3.4. \n\nLenneberg cited the superior ability of children to (re)learn \nlanguage after left brain injury in support of the critical period \nhypothesis. But while there clearly is a difference between the \nabilities of young children, on the one hand, and older children and \nadults, on the other, to recover from left brain insults, the contrast \nin recovery course and outcome is not as stark as is often supposed. \n\nFirst, older children — even those who have not succeeded in \nlearning language previously — can substantially recover from \nleft hemisphere trauma occurring well after the supposed closure of \nthe ‘sensitive’ or ‘critical’ period; in \neffect, they learn language from scratch as adolescents. \nVargha-Khadem et al. 1997, for instance, report the case of Alex, \nwho failed to speak at all during childhood and whose receptive \nlanguage was at age 3-4 level at age 9. After his left cortex was \nremoved at age 9, Alex suddenly began to learn language with gusto, \nand by age 15, his skills were those of an 8-10 year old. \n\nSecondly, most adults suffering infarcts in the left hemisphere \nlanguage areas do in fact recover at least some degree of language \ncompetence and many recover substantially normal competence, \nespecially with treatment (Holland et al. 1996). This is thought to \nbe due both to the regeneration of damaged speech areas and to \ncompensatory development in other areas, particularly in the right \nhemisphere (Karbe et al. 1998). Similar processes seem to be at work\nin young children with left hemisphere damage. Muller et al. 1999, \nfor instance, document significant relearning of language, together \nwith increased right-hemisphere involvement in language tasks, after \nleft-hemisphere lesions in both children (<10) years) and adults \n(>20 years). \n\nFinally, not even very young children are guaranteed to recover \nlanguage after serious insults, whether to the left or right \nhemisphere. As Bates and Roe (2001) argue in their survey of the \nchildhood aphasia literature, outcomes differ wildly from case to \ncase, and the reported studies exhibit numerous methodological \nconfounds (e.g., inability to localize the lesion or to know its \ncause, different measures of linguistic competence, different time \nframes for testing, statistical irregularities, and failure to \ncontrol for other factors known to affect language such as seizure \nhistory) that cast doubt on the degree of empirical support possessed\nby Lenneberg's claim in this instance. \n\nIt has long been recognized that interpretation of the ‘wild\nchild’ literature — helpfully surveyed in Skuse 1993\n— is confounded by the fortunate rarity of these ‘natural\nexperiments,’ the generally poor reporting of them, and the\nother environmental factors (abuse, malnutrition, neglect, etc.) that\noften go along with extreme linguistic deprivation.  However, in work\npioneered by Goldin Meadow and colleagues (e.g., Goldin Meadow and\nMylander 1983, 1990), a new population of individuals, who are\nlinguistically but not otherwise deprived, has begun to be\nstudied. Deaf but otherwise normal children of hearing parents who are\nneither educated in sign language nor sent to special schools for the\ndeaf do not acquire language, although they usually develop their own\nrudimentary signing systems, called ‘homesign,’ to use\nwith their families. Studies of what happens to such children after\nthey are exposed to natural languages (signed or verbal) at various\nages promise to offer new insights into the critical and sensitive\nperiod hypotheses. \n\nAt this time, however, there are still very few case reports in the \nliterature, and the data so far obtained in these studies are \nequivocal with respect to the sensitive and critical period \nhypotheses. Some adolescents do seem to be able to acquire language \ndespite early linguistic deprivation, and others do not. It is \nunclear what the explanation of these different outcomes is, but one \nimportant factor appears to be whether the new language is a signed \nlanguage (e.g., ASL) or a spoken language. Perhaps because their \nchildhood perceptual deficits prevented normal auditory and \narticulatory development, deaf children whose hearing is restored \nlater in life do not seem to be able to acquire much in the way of \nspoken language. (Grimshaw et al. 1998.) \n\nThe issue of second language acquisition (“SLA”) has been\nargued to bear on the innateness of language by supporting a critical\n(or sensitive) period hypothesis. For instance, Johnson and Newport \n(1989) found that among immigrants arriving in the U.S. before \npuberty, English performance as adults was better the earlier in life\nthey arrived, but that there were no effects of arrival age on \nlanguage performance for those arriving after puberty. The fact that \nthe amount of exposure to the second language mattered for speakers \nif it occurred before puberty but not after, was \ntaken to confirm the critical period hypothesis. \n\nHowever, these results have failed to be replicated (Birdsong and\nMolis 2001) and while it still has its supporters, the ‘critical\nperiod’ hypothesis regarding second language acquisition is\nincreasingly being criticized (Hakuta, Bialystok and Wiley 2003;\nNikolov and Djugunovich 2006). Newer studies have argued, for\ninstance, that the degree of proficiency in a second language\ncorrelates better with, such factors as the learner's level of\neducational attainment in that language, her length of residence in\nthe new country,) and the grammatical similarities between the first\nand second languages, and/or length of residence in the new\ncountry. (Flege, Yeni-Komshian and Liu 1999; Bialystok,\n1997)[22] \n\nThe fact that many adults and older children can learn both first and\nsecond languages to a high degree of proficiency makes clear that \nunlike the kitten visual system studied by Hubel and Wiesel, the \nlanguage acquisition system in humans is not subject to a critical \nperiod in the strict sense. This finding is consistent with the \nemerging view that the cortex remains highly plastic throughout life,\nand that contrary to received wisdom, even old dogs can be quite good\nat learning new tricks. (See Buonomano and Merzenich 1998; Cowen and\nGavazzi 1998; Quartz and Sejnowski 1997; and Stiles 2000.) It is \nalso consistent with the idea, which seems more plausible \nthan the critical period hypothesis, that there is a \nsensitive period for language acquisition — a time, \nfrom roughly birth age 1 to age 6 or 7, in which language is acquired\nmost easily and naturally, and when a native-like outcome is \nvirtually guaranteed. (Cf. Mayberry and Eichen 1991.) The \nimplications of this conclusion for linguistic nativism are examined \nin the next section. \n\nWhat does the existence of a sensitive period for language mastery\ntell us about the innateness of language? In this section, we will\nlook at a case, namely phonological learning, in which the existence\nof a sensitive period has received much press, and in which the\ninference from sensitivity to the existence of language-specific\ninnate information has been made explicitly (see Eimas 1975). One can\nargue that even in this case, the inference to linguistic nativism is\nweak. \n\nMuch rarer than mastery of second language morphology and syntax is \nattainment of a native-like accent, something that first language \nlearners acquire automatically in \n childhood.[23]\n A child's ability to perceive language-specific sounds begins in \nutero, as demonstrated, for instance, by newborns' preference for the\nsounds of their mother's voice and their parents' language, and by \ntheir ability to discriminate prose passages that they have heard \nduring the final trimester from novel passages. In the first few \nmonths of life, babies reliably discriminate many different natural \nlanguage phonemes, whether or not they occur in what is soon to \nbecome their language. By ages 6 months to 1 year, however, this \nsensitivity to unheard phonemes largely disappears, and by age 1, \nchildren tend to make only the phonological distinctions made in the \nlanguage(s) they hear around them. For example, Japanese children \nlose the ability to discriminate English /r/ and /l/ (Kuhl et al., \n1997b). As adults, people continue to be unable to perceive some \nphonetic contrasts not marked by their language, and many fail to \nlearn how to produce even those second language sounds which they can\n\n distinguish.[24]\n For instance, many English speakers of French have great difficulty \nin producing the French /y/ (as in tu) and \nback-of-the-throat /r/. \n\nThus, in the case of phonological learning, there does seem to be an \ninborn predisposition to segment vocal sounds into language-relevant \nunits, or \n phonemes.[25]\n However, there is also evidence that learning plays a role in shaping\nphonological knowledge — and not just by ‘pruning \naway’ unwanted ‘phonological representations,’ as \nEimas (1975) hypothesized, but also by shaping the precise boundaries\nof adult phonemic categories. For example, caregivers reliably speak \na special ‘language’ (“Motherese” or \n“Parentese”) to young babies, raising pitch, shortening \nsentences, emphasizing stressed morphemes and word boundaries and \n— most relevant here — exaggerating the acoustical \ndifferences between certain crucial vowels (in English, /i/, /a/ and \n/u/) . This ‘stretching’ of the distance between vowels \n(demonstrated in Finnish and Russian as well as English by Kuhl et \nal. 1997a) facilitates the infant's representation of clearly \ndistinguishable vowel prototypes. Kuhl 2000 argues that these \nprototypes subsequently function as ‘magnets’ around \nwhich subsequent linguistic experiences are organized, and form the \nset points of the language-specific phonological ‘map’ \nthat emerges by the end of the first year. \n\nIf this is indeed how phonological learning works, it is clear that \nwhile experience clearly plays a role, the inborn contribution to \nthat process is quite substantial. For discriminating phonemes \n— however those discriminations might be shaped by subsequent \nexperience — is no simple matter. It involves what is called \n‘categorical perception, that is, the segmenting of a signal \nthat varies continuously along a number of physical dimensions (e.g.,\nvoice onset time and formant frequency) into discrete categories, so \nthat signals within the category are counted as the same, even though\nacoustically, they may differ from one another more than do two \nsignals in different categories (see Fig. 5). (Harnad 1987 is a \nuseful collection of work on categorical perception to the \nmid-1980s.) \n\nBut is this inborn contribution to phonological learning language\nspecific, that is, does it support the conclusion that (this \naspect of) language is innate? And to this question, the answer \nappears to be ‘No.’ First, the ‘chunking’ of \ncontinuously varying stimuli into discrete categories is a feature \nnot just of speech perception, but of human perception generally. For\ninstance, it has been demonstrated in the perception of \nnon-linguistic sounds, like musical pitch, key and melody, and \nmeaningless chirps and bleats (Pastore and Layer 1990). It has also \nbeen demonstrated in the processing of visual stimuli like faces \n(Beale and Keil 1995), facial expressions (Etcoff and Magee 1992; \nKotsoni, de Haan and Johnson 2001); facial gender (Campanella, \nChrysochoos and Bruyer 2001); and familiar physical objects (Newell \nand Bulthoff 2002). Secondly, it is known that other animals too \nperceive categorically. For instance, crickets segment consepecific \nsongs in terms of frequency (Wyttenbach, May and Hoy 1996), swamp \nsparrows ‘chunk’ notes of differing durations (Nelson and\nMarler 1989), and rhesus monkeys can recognize melodies when \ntransposed by one or two octaves, but not by 1.5 or 2.5 octaves, \nindicating a grasp of musical key (Wright et al. 2000). Finally, \nother species respond categorically to human speech! Chinchillas \n(Kuhl and Miller 1975) and cotton-top tamarins (Ramus et al. 2000) \nmake similar phonological distinctions to those made by human \ninfants. \n\nTogether, as Kuhl 1994, 2000 argues, these findings cast doubt on \nthe language-specificity of the inborn perceptual and categorization \ncapacities that form the basis of human phonological learning. For \ngiven the fact that human (and animal) perception quite generally is \ncategorical, it is arguable that languages have evolved so as to \nexploit the perceptual distinctions that humans are able to make, \nrather than humans' having evolved the abilities to make just the \ndistinctions that are made in human languages, as a view like Eimas' \nwould suggest.  \n\nFigure 5. Note that the pair of sounds circled in blue differ in F2 \nstarting frequency less than those circled in red, yet the former are\nboth reliably counted as instances of the sounds /b/ whereas the \nlatter are reliably classified as different sounds, /d/ and /g/. This\npattern, together with the abrupt switch from one classification to \nanother (e.g. /b/ to /g/), is characteristic of categorical \nperception. \n\nThe same may be true in non-phonological domains too. The notion that\nat least some of the capacities responsible for syntactic learning \nare non-language specific is suggested by analogous results about the\nnon-species specificity of recursive rule learning and generalization\n— an ability that Chomsky has recently suggested forms the core\nof the human language faculty. (Hauser, Chomsky and Fitch 2002; see \nbelow, 3.4 for further discussion.) Other species, notably cotton top\ntamarins, seem capable of learning simple recursive rules (Hauser, \nWeiss, and Marcus 2002). In addition, Hauser and McDermott 2003 \nargue that musical and syntactic processing involve similar \ncompetences, which are again seen in other species. Together, these \nfindings suggest that there are aspects of the human ‘language \nfaculty’ that are neither task-specific nor species-specific. \nInstead, language learning and linguistic processing make use of \nabilities that predate language phylogenetically, and that are used \nin humans and in animals for other sorts of tasks. (See e.g., Hauser,\nWeiss, and Marcus 2002 for an account of recent work on rule \nlearning by cotton top tamarins; see Hauser and McDermott 2003 for \nthe suggestion that aspects of musical and syntactic processing \ninvolve similar competences, which are again seen in other species.) \nRather than viewing the human mind as being innately specialized for \nlanguage language learning, it seems at least as reasonable to think \nof languages as being specialized so as to be learnable and usable by\nthe human mind; of this, more in §3.4 below. \n\nThis brings us to the question of language evolution: if knowledge of\nlanguage (say, of the principles of UG) really is inborn in the human\nlanguage faculty, how did such inborn knowledge evolve? For many \nyears, Chomsky himself refused to speculate about this matter, \nstating that “[e]volutionary theory…has little to say, \nas of now, about questions of this nature” (1988:167). Other \ntheorists have not been so reticent, and a large literature has grown\nup in which the selective advantages of having a language are \nadumbrated. It's good for communicating with, for instance, when \ntrying to figure out what conspecifics are up to (Pinker and Bloom, \n1990; Dunbar 1996). It's a mechanism of group cohesion, analogous to\nprimate grooming (Dunbar 1996). It's a non-genetic mechanism of \nphenotypical plasticity, allowing organisms to adapt to their \nenvironment in non-evolutionary time (Brandon and Hornstein 1986; \nSterelny 2003). It's a mechanism by which we can bend others to our \nwill (Dawkins and Krebs 1979; Catania 1990), or make social \ncontracts (Skyrms 1996). Language makes us smarter, perhaps by being\ninternalized and functioning as a ‘language of thought’ \n(Bickerton 1995, 2000). And so on. \n\nThe ability to speak and understand a language no doubt provided and \ncontinues to provide us with many of these benefits. Consequently \n(and assuming that the costs were not too great — as patently \nthey weren't), one can be sure that whatever it is about human beings\nthat enables them to learn and use language would have been subjected\nto strong positive selection pressure once it began to emerge in our \nspecies. \n\nBut none of this speaks directly to the issue of linguistic nativism.\nThe fact that Mother Nature would have favored individuals or groups \npossessing linguistic abilities tells us nothing about the \nmeans she chose to get the linguistic phenotype built. That \nis, it tells us nothing about the sorts of psychological mechanisms \nthat were recruited to enable human beings to learn, and subsequently\nuse, a natural language. \n\nNativism is, of course, one possibility. Natural selection might have\nbuilt a specialized language faculty, containing inborn knowledge \nabout language (e.g., knowledge of UG), which subsequently was \nselected for because it helped human children to acquire linguistic \ncompetence, and having linguistic competence enhanced our ancestors' \nfitness. A problem with this hypothesis, however, is that it is \nunclear how a language faculty containing innate representations of \nUG might have arisen in the human mind. One view is that the language\nfaculty was built up piecemeal by natural selection. This approach \nunderlies Pinker and Bloom's (1990) and Jackendoff's (1999) proposals\nas to the adaptive functions of various grammatical features and \ndevices. Other nativists, however, reject the adaptationist \nframework. For instance, Berwick 1998, has argued that efforts to \nexplain the piecemeal development of knowledge of linguistic \nuniversals in our species may be unnecessary in light of the new, \nMinimalist conception of syntax (see Chomsky 1995). On this view, \nall parametric constraints and rules of syntax are consequences of a \nfundamental syntactic process called Merge: once Merge was in place, \nBerwick argues, the rest of UG automatically followed. Chomsky, \ntaking another tack, has suggested that language is a \n‘spandrel,’ a byproduct of other non-linguistically \ndirected selective processes, such as “the increase in brain \nsize and complexity” (1982:23). And finally Bickerton 1998, on \nyet another tack, posits a massive saltative episode in which large \nchunks of syntax emerged all at once, although this posit is \nimplicitly withdrawn in Calvin and Bickerton 2000. \n\nThe literature on language evolution is too large to survey in this \narticle (but see Botha 2003 for an excellent overview and critique).\nSuffice it to note that as yet, no consensus has emerged as to how\n innate knowledge of UG might have evolved from \nwhatever preadaptations existed in our ancestors. Of course, this is \nnot in itself a problem for linguistic nativists: formulating and \ntesting hypotheses about human cognitive evolution is a massively \ndifficult enterprise, due largely to the difficulty of finding \nevidence bearing on one's hypothesis. (See Lewontin 1998 and Sterelny\n2003:95-116.) \n\nIt's worth noting, however, that linguistic nativism is just one \npossibility for how Nature got language up and running. Just as it \nmay be that a language faculty embodying knowledge of UG was somehow \nencoded in the human genome, it's also possible that that our ability\nto learn a language is based on a congeries of pre-existing \ncompetences, none of which is (or was initially — see below) \nspecialized for language learning. Tomasello's theory of language \nacquisition, discussed above (§2.2.1.b), invites this \nalternative evolutionary perspective. On his view, the fundamental \nskills with which linguistic competence is acquired are skills that \noriginally served, and still continue to serve, quite different, \nnon-linguistic functions. For example, he argues that children's \nearly word and phrase learning rest in part on their ability to share\nattention with others, to discern others' communicative intentions, \nand to imitate aspects of their behavior. There is reason to think \nthat these abilities evolved independently of language, at least \ninitially: imitation learning enabled the fast and high-fidelity \ntransfer of learned skills between generations (see Tomasello 1999, \n2000) and the ability to form beliefs about the mental states of \nothers (‘mind-reading’ or ‘theory of mind’) \nenabled highly intelligent animals, such as our hominid ancestors, to\nnegotiate a complex social environment made up of similarly \nintelligent conspecifics. (See, e.g., Sterelny 2003.) On this sort \nof view, the ability to learn language piggy-backed on other \ncapacities, which originally evolved for other reasons and which \ncontinue to serve other functions in addition to their linguistic \nones. \n\nYou might wonder, however, whether this latter kind of account really\ndiffers substantively from that of a nativist. Assuming that she does\nnot reject adaptationism altogether, the nativist will presumably be \ncommitted to the idea that the innate language organ, or faculty \nembodying knowledge of UG, was derived from pre-existing structures \nthat were either functionless or had non-linguistic functions. These \nstructures subsequently acquired linguistic functions through being \nselected for that reason: they became adaptations for language. But \nso too would the various capacities postulated by Tomasello. As soon \nas they started being used for language learning, that's to say, they\nwould have been selected for that function (in addition to any other \nfunctions they might serve, and always assuming that linguistic \nabilities were on balance beneficial). Hence they too will over time \nbecome adaptations for language. On both Tomasello's and the \nnativist's view, in other words, the inborn structures responsible \nfor language acquisition will have acquired the biological function \nof enabling language acquisition: they will be specialized for that \npurpose. Is Tomasello, then, a nativist? \n\nNo. First, even though the psychological abilities and mechanisms \nthat Tomasello posits have been selected for linguistic functions, \nthese abilities and mechanisms have continued to be used (and, \nplausibly, selected) for non-linguistic purposes, such as face \nrecognition, theory of mind, non-linguistic perception, etc. So, \nwhereas a central tenet of linguistic nativism is its insistence that\nthe structures responsible for language learning are \ntask-specific, Tomasello sees those structures as being much\nmore general-purpose. In addition, and this is a second reason not to\ncount Tomasello as a nativist, the inborn structures he posits are \nnot plausibly interpreted as containing any kind of language specific\ninformation or representations. Yet a commitment to the role of \ninborn, language-specific information (such as knowledge of UG) is \nanother hallmark of linguistic nativism. \n\nSeveral theorists (e.g., Clark 1996, Tomasello 1999, and Sterelny, \n2003) have stressed that in addition to working on human linguistic \nabilities directly, via changes to the parts of the genome coding for\nthose abilities, natural selection can also bring about such changes \nindirectly, by making sure that our minds are embedded in certain \nkinds of environments. All sorts of animals create environments for \nthemselves: this is called ‘niche construction.’ (The \nterm is due to Odling-Smee, Laland and Feldman 1996.) Many animals \nalso (or thereby) create environments for their offspring as well. \nAnd as Odling-Smee et al. 1996, Avital and Jablonka 2000, and \nSterelny 2003 stress, animals' dispositions to modify the \nenvironments of both themselves and their offspring in certain ways \nare just as much potential objects of selection as are other of their\ntraits. \n\nTo see this, suppose that an organism O has a genetically encoded \ndisposition N to build a special kind of nest; suppose further that \nbeing raised in this kind of nest causes O-type offspring to have \ncharacteristic C; and suppose finally, that Os with C enjoy greater \nreproductive success than those without. Then, assuming that there is\nvariation in N in the population, natural selection can operate so as\nto increase the proportion of Os with N — and hence also those \nwith characteristic C — in the population. Down the track, Os \nwill have C not by virtue of acquiring a special, genetically-encoded\ndisposition-for-C. Rather, they will have C because their parents \nhave the genetically-encoded disposition N, and Os whose parents have\nN ‘automatically’ develop C. \n\nThis toy example illustrates a further route by which language might \nhave evolved in human beings. In addition to creating inborn \nlanguage-learning mechanisms in individuals, natural selection may \nalso have created dispositions to construct particular kinds of \nlinguistic learning environments in their parents. \nFor example, as Clark (1996) and Sterelny (2003) both speculate, \nMother Nature might have worked on our dispositions to use \n‘Motherese’ to our children, and/or on our tendency to \ntalk about things that are current objects of the child's perceptual \nattention, in order to create learning environments conducive to the \nacquisition of language. \n\nIn principle, the existence of this sort of ‘niche\nconstruction’ can be accepted by all parties to the nativism\ncontroversy. That is, both Tomasello and Chomsky could agree that\ndispositions to construct ‘linguistic niches’ —\nenvironments in which languages are easy for human offspring to learn\n— may have been selected for in our species. Nevertheless, the\nnotion of niche construction militates against the nativist,\nparticularly when one takes into account the related notion of\n‘cumulative downstream niche construction.’ \n\nCases of what Sterelny (2003: 149ff) calls ‘cumulative \ndownstream niche construction’ occur when a generation of \nanimals modifies an environment that has already been modified by \nearlier generations. A mountain thornbill's nest is an instance of \ndownstream niche construction (since its offspring are affected by \nthe thornbill's efforts). However, the construction is not \ncumulative, since the nest is built anew each year. By contrast, a \nrabbit warren extended and elaborated over several generations is an \ninstance of cumulative construction: successive generations of \noffspring inherit an ever-more-complex niche and their other \nbehaviors are tuned accordingly in ever-more-complex ways. Tomasello,\n1999 and Sterelny 2003 stress that niche construction, including \ndownstream niche construction, is not limited to the physical world: \nanimals make changes to their social and epistemic worlds as well. \nFor instance, chimpanzees live in groups (= construction of a social \nniche) and dogs mark their territory (= a change in their epistemic \nniche, relieving them of the necessity of remembering where the \nboundaries of their territory are). Humans, says Sterelny, echoing a \ntheme of Tomasello 1999, are niche constructors “with a \nvengeance” (2003:149) and many of the changes they make to \ntheir physical, social and epistemic environments accumulate over \nmany generations (think of a city, a democracy, modern science, a \nnatural language). Such cumulative modifications allow for what \nTomasello calls a “ratchet effect”: a “cycle in \nwhich an improvement is made, becomes standard for the group, and \nthen becomes a basis for further innovation.” (Sterelny 2003: \n150-1) \n\nThe idea of cumulative niche construction has obvious application to \nthe case of language. If parents shape the linguistic environment of \ntheir offspring, and if we all shape the linguistic environments of \nour conspecifics (merely by talking to them!) then the possibility of\na ‘linguistic ratchet effect’ is clearly open. Small \nchanges made to the language of the group by one generation — \nchanges which perhaps make it easier to learn, or easier to \nunderstand or produce — will be transmitted to later \ngenerations, who may in turn make further changes geared to \nincreasing language learnability and ease of use. This scenario \nraises the possibility, already mentioned at the end of the last \nsection, that language may have evolved so as to be learnable and \nusable by us, in addition to the converse scenario (stressed in much \nwork on the evolution of language) that we had to change in \nmany and complex ways in order to learn and use a language. Thus, we \nmight speculate, languages' phonetic systems evolved so as to be \ncongenial to our animal ears; their expressive resources (in \nparticular, their vocabularies) evolved so as to fit our \ncommunicative needs; and perhaps, as Clark 1997 has suggested and as \nTomasello 2003 implicitly takes for granted, natural language syntax\nevolved so as to suit our pre-existing cognitive and processing \ncapacities. To be sure, the languages we have coded in our heads look\ncomplex and weird to linguists and psychologists and philosophers who\nare trying to put together theories about them. But, if languages and\nhuman minds have evolved in tandem, as surely they have, then \nlanguages may not look weird at all from the point of view of the \nbrains that implement and use them. \n\nAll of these processes have likely played a role in the evolution of\nour capacities to learn and use a natural language.  Pre-existing\npsychological, perceptual and motor capacities would have been\nrecruited for the task of language learning and use. These capacities\nwould have been honed and specialized further by natural selection for\nthe performance of linguistic tasks. The functions of some of them,\nperhaps, would have become so specialized for language-related tasks\nthat they cease to perform any non-linguistic functions at all —\nand to this extent, perhaps, linguistic nativism would be\nvindicated. At the same time, however, language itself would have been\nevolving so as the better to suit our cognitive and perceptual\ncapacities, and our communicative needs.  Given the fact that many\ndifferent perceptual, motor and cognitive systems are implicated in\nlanguage use and learning, and given the co-evolution of our minds and\nour languages, the truth about language evolution, when it emerges, is\nunlikely to be a simple. For this reason, it is unlikely to\nvindicate the nativist's notion that a specialized and monolithic\n‘language organ’ or ‘faculty’ is at the root\nof our linguistic capacities. \n\nBefore leaving the question of language evolution, it is necessary to\nmention a recent paper by Hauser, Chomsky and Fitch 2002 on this \ntopic. First, they distinguish (2002:1571) what they call the \n‘faculty of language in the narrow sense,’ or \n‘FLN,’ from the ‘faculty of language in the broad \nsense,’ or ‘FLB.’ The FLN is the “abstract \nlinguistic computational system alone…which generates internal\nrepresentations and maps them into the sensory-motor interface by the\nphonological system, and into the conceptual-intentional interface by\nthe (formal) semantic system.” (Ibid.) The FLB \nincludes the FLN plus all the other systems (motor systems, \nconceptual systems, perceptual systems, and learning skills) which \ncontribute to language acquisition and use. \n\nNext, Hauser et al. speculate that the only thing that's really \nspecial about the human FLB is the FLN. That is, with the exception \nonly of the FLN, FLB comprises systems that are shared with (or only \nslight modifications of) systems in other animals. Consequently, \nthere is no mystery (or no more mystery than usual) about how these \nlanguage-related abilities evolved. FLN, on the other hand, is \ndistinctive to humans and what is special about it is its power of \nrecursion, that is, its ability to categorize linguistic \nobjects into hierarchically organized classes, and (on the behavioral\nside) for the generation of infinitely many sentences out of finitely\nmany words. According to Hauser et al., the only real evolutionary \nmystery about language is how this capacity for recursion evolved \n— and this question, argue Hauser et al, is eminently \naddressable by normal biological methods (e.g., comparative studies \nto determine possible precursor mechanisms, etc.). \n\nHowever, there are two difficulties with this scenario. First, there \nis evidence that the power of recursion posited by Hauser et al. as \nbeing distinctive of the human FLN is in fact not distinctive to \nhumans, because it is not species specific. (See Esser, et al. 1997\nand McGonigle, Chalmers and Dickinson, 2003.) Second, \nrecursiveness is not language specific either, but is a feature of \nother domains of human cognition and endeavor as well. Our conceptual\nspace, for instance, appears to be hierarchically ordered (poodles \nare a kind of dog, which are a kind of quadruped, which are a kind of\nanimal, etc.). Similarly, the planning and execution of \nnon-linguistic actions seems often to involve the sequencing and \ncombining of smaller behavioral units into larger wholes. Recursion \nmight well be an important part of the human language faculty, but \nit's apparently not specific either to us or to that faculty. Or, to \nput the point more bluntly: if it's Chomsky's view that recursiveness\nis the pivotal feature of the language faculty, and if recursiveness \nis a feature of human cognition and action more generally, then it's \nnot clear that Chomsky remains a linguistic \n nativist.[26] \n\nIt has been argued (by, e.g., Bickerton  1981, and Pinker, \n1994:32-9) that the process by which a pidgin turns into a creole \nprovides direct evidence of the operation of an innate language \nfaculty. Pidgins are rudimentary communication systems that are \ndeveloped when people speaking different languages come together \n(often in a commercial setting or when one people has conquered and \nis exploiting another) and need to communicate about practical \nmatters. Creoles arise when pidgins are elaborated both syntactically\nand semantically, and take on the characteristics of bona \nfide natural languages. \n\nBickerton and, following him, Pinker, argue that creolization occurs \nwhen children take a pidgin as the input to their first language \nlearning, and urge that the added complexity of the creole reflects \nthe operation of the child's inborn language faculty. Moreover, they \nargue, since creole languages all tend to be elaborated in the same \nways, and since they all respect the constraints of UG, the \nphenomenon of creolization also supports the idea that the inborn \ncontribution to language acquisition is not just some general drive \nfor an effective system of communication, but rather knowledge of \nlinguistic universals. \n\nThere are two problems with this ‘language bioprogram \nhypothesis,’ as it is known in the creolization literature. The\nfirst concerns the claim (e.g., Bickerton 1981:43-70) that even \ncreoles that developed in quite different areas of the world, and in \ncomplete isolation from one another, bear “uncanny \nresemblances” (Pinker 1994:25) to each other, not just in \nrespecting the constraints of UG, but — even more surprisingly \n— in using fundamentally the same means to elaborate their root\npidgins (e.g., in using the same syntactic devices to mark tense, \naspect and modality). The stronger claim made by Bickerton — \nthat Creoles use the same devices for the same grammatical purposes \n— is simply not true. For example, as Myhill (1991) argues, \nJamaican Creole, Louisiana Creole, Mauritian Creole and Guyanese \nCreole mark tense, aspect and modality in ways that are quite \ndifferent from those that Bickerton (1981) proposed as universal. \n(See, however, Mufwene 1999 for a case that confirms Bickerton's \npredictions.) The weaker claim — that creoles respect the \nconstraints imposed by UG — has not, so far as I know, been \ncontested. So we will assume, in what follows, that creoles, like \nother NLs, respect UG. The important question for our purposes is: \nhow does this come about? \n\nThe bioprogram hypothesis claims that creolization occurs as a result\nof the action of the language faculty: children who learn language \nfrom degraded (e.g., pidgin) inputs are compelled by their innate \nknowledge of grammar to produce a fully-fledged natural language (the\ncreole) as output. As an example of how children add UG-constrained \nstructure to languages learned from degraded inputs, Pinker cites the\ncase of Simon, a deaf child studied by Newport and her colleagues, \nwho learned American Sign Language (ASL) from parents who themselves \nwere not exposed to ASL until their late teens. Although they used \nASL as their primary language, Simon's parents were “in many \nways…like pidgin speakers,” says Pinker (1994:38). For \ninstance, they used inflectional markers in an inconsistent way and \noften failed to respect the structure-dependence of the rules \ngoverning topicalization in that \n language.[27]\n But “astoundingly,” says Pinker, “though Simon saw \nno ASL but his parents' defective version, his own signing was far \nbetter ASL then theirs…Simon must somehow have shut out his \nparents' ungrammatical ‘noise.’ He must have latched on \nto the inflections that his parents used inconsistently, and \ninterpreted them as mandatory.” (1994:39) Pinker views this as \na case of “creolization by a single living child” \n(ibid.) and explains Simon's conformity to ASL grammar in \nterms of the operation of his innate language faculty during the \nacquisition period. \n\nIn a recent overview of the Simon data from the last 10 or so years, \nhowever, Newport 2001, stresses a number of facts that Pinker's \npresentation obscures or downplays. First, Simon's performance was \nnot that of a native signer, although he did develop “his own \nversion of ASL whose structure was more like that of other natural \nlanguages [than that of his parents' ASL]” (Newport 2001:168).\nFor instance, Simon's morphology stabilized at a level that was \n“not as complex as native ASL” and he didn't acquire \nstandard classifier morphemes if they were not used by his parents \n(ibid.). Secondly, Simon's success in learning a given rule \nseemed to vary with how well or badly his parents signed. For \ninstance, Simon's parents used the correct inflectional morphology \n60-75% of the time for a large class of verbs of motion, and in this \ncase, Simon's own use of such morphology was 90% correct. However, \nsome members of the class of classifier morphemes were correctly used\nby the parents only 40% of the time, and in this case, although \nSimon's performance was better than his parents', it was not at \nnative signer level. \n\nNewport argues that Simon appears to be ‘cleaning up’ his\nparents' language, that is, “bas[ing] his learning heavily on \nhis input, but reorganiz[ing] this input to form a cleaner, more \nrule-governed system than the one to which he was exposed.” \n(2001:168) She agrees that this result could be due to constraints \nimposed by an innate language faculty, but argues that it is also \nconsistent with the existence of some more generalized propensity in \nchildren to generate systematic rules from noisy inputs, rightly \npointing out that the latter hypothesis cannot be ruled out in \nadvance of empirical test. (In this context, she notes (p.170) some \npreliminary studies suggesting that inferring systematic rules from \nmessy data may indeed be a more general feature of learning in young \nchildren (though, interestingly, not in adults), for they can be seen\nto exhibit this tendency in non-linguistic pattern-learning contexts \ntoo.) Newport concludes that “the contrasts between Simon and \nhis parents are in certain ways less extreme, and more \nreorganizational, than might be suggested by the Language Bioprogram \nHypothesis…[H]e does not appear to be creating an entirely new\nlanguage from his own innate specifications; rather, he appears to be\nfollowing the predominant tendencies of his input, but he sharpens \nthem, extends them, and forces them to be internally \nconsistent.” (2001:173). \n\nIf Newport et al. are right, the case of Simon does not seem to give \nmuch support to the nativist hypothesis. Moreover, the argument from \ncreolization suffers a number of additional flaws. First, the \nBickerton-Pinker view, which assigns a dominant role to child \nlanguage learners in the creation of creoles, is but one of three \ncompeting hypotheses currently being explored in the creolization \nliterature. According to the ‘superstratist’ hypothesis, \ncreolization occurs not when children acquire language from pidgins, \nbut when successive waves of adult speakers try to learn the language\nof the dominant culture as a second language. (Chaudenson 1992, for \ninstance, defends this view about the origins of French creoles.) On \nthis view, the additional devices seen in creoles are corruptions of \ndevices seen in the dominant language. According to the \n‘substratist’ hypothesis, creoles are again created by \nsecond language learners, rather than children, only the source of \nadded structure is the first language of the learner. (Lumsden 1999 \nargues that numerous traces of a variety of African languages in \nHaitian creole support this hypothesis.) One need not take a stand on\nwhich of these views is correct in order to see that these competing \nexplanations of creolization undermine Bickerton and Pinker's \n‘bioprogram’ hypothesis. If creoles arise out of the \nattempts of adult learners to learn (and subsequently pass on to \ntheir children) another, non-native language, then what one might \ncall ‘contamination of the stimulus,’ rather than the \ninfluence of an inborn UG in the learner, is what accounts for the \nUG-respecting ways in which creoles are elaborated. \n\nHowever, there is a case of creolization in which these other \nhypotheses apparently fail to gain purchase, as Pinker (1994:37ff.) \nemphasizes. This is the case of the development of Idioma de Signos \nNicaragüense (ISN, Nicaraguan Sign Language), a brand-new \nnatural sign language which first emerged around 30 years ago in \nschools for the deaf in and around Managua. These schools were first \nset up in the 1970s, and ISN evolved from the hodge-podge of homesign\nsystems used by students who entered the schools at that time. ISN is\nan interesting test case of the bioprogram hypothesis for two \nreasons. First, homesign systems are idiosyncratic and possess little\nsyntactic structure: the natural-languagelike syntax of ISN could \ntherefore not derive from substrate influence. And Spanish, the only \npotential candidate for superstrate influence was allegedly \ninaccessible to signers because of its auditory modality. Pinker \nclaims that ISN provides another example of creolization and the \nworkings of the innate language faculty: it is \n“created…in one leap when the younger children were \nexposed to the pidgin singing of the older children.” \n(1994:36-7) \n\nIn their discussion of the development of ISN, however, Kegl, Senghas\nand Coppola (1999) show that things are not quite this \nstraightforward. ISN did not develop ‘in one leap,’ from \nthe very rudimentary homesigns or ‘Mimicas’ spoken by \nindividual deaf students. Instead, its evolution was more gradual and\nwas preceded by the creation of what Kegl et al. call “Lenguage\nde Signos Nicaragüense” (LSN), a kind of “pidgin or \njargon” (181) that “developed from the point when these \nhomesigners came together in the schools and began to share their \nhomesigns with each other, quickly leading to more and more shared \nsigns and grammatical devices” (180). In addition, the signers \nhad access to Spanish language dictionaries, and their language was \nalso influenced by the signing of Spanish-speaking, non-deaf teachers\nat the schools — signing which likely incorporated such \ngrammatical devices of the teachers' language as were transferable to\na non-vocal medium. (K. Stromswold, private communication.) \n\nWhile Kegl et al. endorse the language bioprogram hypothesis that ISN\nemerged ‘in one leap,’ in the minds of children exposed \nto degraded Mimica or LSN inputs, their data are equally consistent \nwith the idea that ISN developed more gradually by means of \nsuccessive elaborations and innovations among a community of \nhighly-motivated (because language-starved) young users. Indeed, as \nKegl et al. themselves describe the history (p.187), this is \nprecisely what happened. First, a group of signers, each with his or \nher own idiosyncratic form of Mimicas, entered the schools. Members \nof this group gained in expressive power as their individual Mimicas \nwere enriched by borrowings from others' homesign systems. Then, a \nnew cohort of Mimicas signers entered the schools. Their sign systems\nbenefitted both from exposure to the Mimicas of their peers and from \nexposure to the richer system developed by the earlier group. Through\na process of successive elaborations in this manner, LSN developed \nand then, by a similar series of steps, ISN developed. At present, \nall three sign systems are still being used in Nicaragua, presumably \nreflecting the different ages at which people are exposed to language\nand the kinds of inputs (ISN or LSN vs. signed and written Spanish or\nlipreading in regular schools) they receive. In addition, ISN and to \na lesser extent LSN are still constantly changing — as one \nwould expect if ISN were a community-wide work in progress, not the \nfinished product of an individual child's mind, \n\nDissociations of language disorders acquired in adulthood (e.g., \nBroca's and Wernicke's aphasia) may tell us something about how \nlanguage is organized in the mature brain, but cannot tell us much \nabout how language is acquired or the role of innate knowledge in \nthat process — a fact that nativists about language generally \nacknowledge. By contrast, language dissociations arising during \nchildhood are sometimes held to bear strongly on the question of \nwhether language is innate. Pinker (1994:297-314) articulates this \nlatter line of thought, arguing that there is a double dissociation \nbetween ‘general intelligence’ and language in two \ndevelopmental disorders called Williams Syndrome (WS) and Specific \nLanguage Impairment (SLI). People with WS have IQs well below the \nnormal range (50-60), yet are able to speak fluently and engagingly \nabout many topics. Those with SLI, by contrast, have normal \n(≈90) non-verbal intelligence but speak effortfully and slowly,\nfrequently making errors in their production and comprehension of \nsentences and words. Pinker argues that there is a double \ndissociation here, and that it supports the view that there is a \nspecial ‘language acquisition device’ that is separable \nfrom any general learning abilities children might possess. In \naddition, following Gopnik 1990a,b, and Gopnik and Crago 1991, he \nurges that the fact that the dissociation appears to concern aspects \nof syntax in particular indicates that the language faculty in \nquestion is the grammar faculty. Finally, and again following Gopnik,\nhe argues that since SLI appears to run in families and, in at least \none case, displays a Mendelian inheritance pattern, what we have here\nis evidence not just of a ‘grammar faculty,’ but of a \n‘grammar gene.’ \n\nWS is a rare genetic disorder with a complex phenotype. Physically, \nWS individuals display dismorphic facial features, abnormal growth \npatterns, gastrointestinal problems, early puberty, neurological \nabnormalities (including hypotonia, hyperreflexia, hyperacuisis and \ncerebellar dysfunction), defective vision and eye development, bad \nteeth, connective tissue abnormalities, and heart problems. \nPsychologically, in addition to their low non-verbal IQ and \ncomparatively spared language abilities, they display relatively good\naudiovisual memory but very impaired visual-spatial abilities, \nleading to difficulties in daily life (e.g., getting dressed). They \nhave outgoing personalities and are highly sociable to the point of \noverfriendliness, but also display numerous behavioral and emotional \nproblems (especially hyperactivity and difficulty concentrating in \nchildhood, and anxiety in later life). (Morris and Mervis 2000; \nMervis et al. 2000.) \n\nAs to their language, there is currently a debate in the literature\nwith regard to its normalcy. According to one point of view, the\nlinguistic competence of WS individuals is remarkably normal,\nespecially in comparison with that of similarly retarded individuals,\nsuch as those with Downs syndrome (Pinker 1994, 1997; Clahsen and\nAlmazan 1998; Bello et al. 2004; Bellugi et\nal. 1998). While rather unusual in their choice of words (e.g.,\nproducing chihuahua, ibis and condor in addition to\nmore usual animal words in a word fluency test) and despite\nan excessive use of clichés and social stock-phrases, their\nability to use language, especially in conversational contexts, is\nmore or less intact. For example, they may appear relatively normal in\nsocial interactions, and their processing of conditional questions and\nability to repeat sentences with complex syntax is closer to that of\nnormal controls than to matched Downs syndrome controls\n(Bellugi et al. 2000: 13, 15). \n\nAccording to another school of thought, however, the language \nabilities of WS subjects might be more normal than those of Downs \nsyndrome individuals, and might look remarkable in contrast to their \nown marked disabilities in other areas, but nonetheless display a \nnumber of abnormal characteristics across a variety of measures when \ninvestigated further. WS language shows “massively \ndelayed” early acquisition, especially of vocabulary (Bellugi \net al. 2000:11) and grammatical morphemes (Caprirci et\nal. 1996); overregularization of regular plural and past tense\nendings as well defective competence with regard to irregular nouns\nand verbs (Clahsen and Almazan 2001); “inordinate difficulty\nwith morphosyntax” (Morris and Mervis 2000: 467; see also\nVolterra et al. 1996; Karmiloff-Smith et al. 1997;\nLevy and Hermon 2003); and impaired mastery of relative clause\nconstructions (Grant et al., 2002), embedded sentences, and\n(in French) grammatical gender assignment (Karmiloff-Smith et\nal. 1997). Indeed, Bellugi et al., 2000, found that WS\nchildren's performance on a sentence-repetition task was\nindistinguishable from that of matched controls diagnosed with\nSpecific Language Impariment, or SLI (see below, §3.6.2).\nFindings such as these lead experts such as Annette Karmilloff-Smith\nto urge “dethroning the myth” of WS' “intact”\nsyntactic abilities (Karmiloff-Smith et al. 2003) and move\nUrsula Bellugi — formerly a proponent of the ‘spared\nlanguage’ viewpoint — to caution that “because their\nlanguage abilities are often at a level that is higher than their\noverall cognitive abilities, individuals with WMS might be perceived\nto be more capable than they really are.” (Bellugi et\nal. 1999.) \n\nIn contrast to its cognitive profile, which is, as we have seen, a \nsubject of debate, the genetic basis of WS is known. It results from \na ≈1.5 Mb deletion encompassing the elastin gene ELN at \nchromosome 7q11.23; most cases appear to be due to new mutations. ELN\nis crucial in synthesizing elastin, a protein which holds cells \ntogether in the elastic fibers found in connective tissues throughout\nthe body and in especially high concentrations in cartilege, \nligaments and arterial walls. Failure to synthesize this protein \ndisrupts development in numerous ways, from the first trimester \nonwards, and gives rise through processes that are not well \nunderstood to the raft of symptoms associated with the syndrome. \n(Morris and Mervis 2000; Mervis et al. 2000.) \n\nIn contrast with Williams syndrome, in which one sees comparatively \nspared language in the face of mild to moderate mental retardation \nand numerous physical defects, specific language impairment \n(‘SLI’) is diagnosed when (i) non-verbal intelligence as \nmeasured by standard IQ tests is normal; (ii) verbal IQ is well below\nnormal; and (iii) obvious causes of language impairment (e.g., \ndeafness, frank neurological damage) can be ruled out. As one might \nexpect given these diagnostic criteria, a diagnosis of SLI embraces a\nhighly heterogeneous collection of language-related deficits, not all\nof which co-occur in every case of language impairment. (Bishop, \n1994; Bishop et al,. 2000.) These include: \n\nAs a consequence of this heterogeneity, SLI, researchers have \nintroduced a number of subtypes of the disorder, including such \nthings as ‘Verbal auditory agnosia,’ \n‘Lexical-syntactic deficit syndrome’ and \n‘Phonological programming syndrome’ (Bishop 1994). Also \nas a consequence, and in part because studies do not always \ndistinguish between different subtypes, the etiology of SLI in \ngeneral is not well understood (O'Brien et al. 2003), although \nrecent research suggests at least two distinct genetic loci are \ninvolved in at least some subtypes of the disorder (Bishop 2006). \nSome posit an underlying defect in the ‘grammar module.’ \nFor instance, Rice and Wexler (1996) attribute SLI individuals' \nmorphological deficits to a missing UG principle, namely, the \nprinciple of inflection, and Van der Laly and Stollwerk 1997, \nattribute some SLI children's difficulty with anaphora to their \nfailure to acquire Binding Theory. Others see non-linguistic defects,\nsuch as auditory, memory or processing deficits as the root problem. \nFor instance, Tallal 1980, 1985 argue that many SLI cases result \nfrom deficits in the processing of rapid auditory stimuli, giving \nrise to a failure to learn to distinguish phonemes correctly, which \nin turn leads to a failure to acquire other aspects of grammar. \nOthers, such as and Norbury, Bishop and Briscoe 2002 argue that such \nchildren's limited processing capacities are the culprit. \n\nWhile the varied symptomatology of SLI suggests that no unified \ntheory of its etiology might be forthcoming, the cause of the \ndisorder is comparatively well understood in the case of one subtype,\ninvolving a severe disruption of morphosyntax (i.e., the rules \ngoverning the formation of words from smaller semantic units, or \nmorphemes). This subtype, seen in about half the members of a large, \nthree-generation English family, the KE's, and in another, unrelated \nindividual, has been traced to a specific genetic mutation, the \nfunction of which is actively under investigation. \n\nThe KE family has received much press since the early 1990s, when \nGopnik 1990a,b and Gopnik and Crago 1991 (see also Gopnik 1997) \nproposed that their morphosyntactic deficits were caused by a \nmutation in a single dominant gene normally responsible for the \nencoding of grammatical features, such as function words and the \ninflections used to mark number, tense, aspect, etc. According to \nGopnik, the affected KE's are ‘feature blind’ as a \nconsequence of this mutation. And according to Pinker (1994), their \npedigree (Fig. 6) and specifically morphosyntactic deficits \nconstitutes “suggestive evidence for grammar genes … \ngenes whose effects seem most specific to the development of the \ncircuits underlying parts of grammar” (Pinker 1994:325). \n\nFigure 6. The KE family pedigree \n (Image used by permission of Simon E. Fisher)\n  \n\nOther intensive studies of the KE family, by Vargha-Khadem and\ncolleagues (e.g., Vargha-Khadem et al 1995, 1998;\nWatkins et al., 2002) have vigorously disputed the hypothesis\nthat the root cause of the KE's language disorder is a syntactic\ndeficit. Instead, they argue, the KE phenotype is much broader than\nGopnik's account suggests, and their ‘feature blindness’\nis merely one among the many effects of an underlying articulatory\nproblem. As characterized by Vargha-Khadem's team, the affected KE's\nspeech is effortful, “sometimes agrammatical and often\nunintelligible” (Watkins et al. 2002:453), and shows\nimpairments not just in morphosyntax (e.g., regular plural and past\ntense endings) but also in the formation of irregular past tenses\n(where correct usage is lexically determined, rather than\nrule-governed) and in sentence-level syntax, particularly word order.\nComprehension, too, is impaired at the level of syntax as well as\nwords, and as is their reading of both words and non-words. These\nresults indicate that the KE's problems go beyond morphosyntax, and\nthe fact that affected KE's have significantly lower non-verbal IQs\n(by 18-19 points; Vargha-Khadem et al. 1995) than unaffected\nfamily members indicates that their deficits may be further reaching\n still.[28]\n Finally, affected KE's have trouble sequencing and executing\nnon-language-related face, mouth and tongue movements and show\nabnormal activation not just of speech but also of motor areas on fMRI\nscans (Liegeois, et al. 2003); this deficiency in\n‘orofacial praxis’ supports Vargha-Khadem's hypothesis\nthat the root problem for the KE's is articulatory. \n\nAs Gopnik noted, the pattern of inheritance in the KE family suggests\nthat a single, dominant gene is responsible for the disorder. (See\nfig. 6) In the early 1990's, Fisher and colleagues began working to\nisolate the gene. First, it was localized to a region on chromosome\n7q31 containing about 100 genes (Fisher et al. 1997, 1998;\nO'Brien et al. 2003). Later, it was identified (Lai et\nal. 2001; Fisher et al. 2003) as the gene FOXP2, which encodes a\nregulatory protein or ‘transcription factor’ (i.e., a\nprotein that helps to regulate the rate of transcription of other\ngenes in the genome — in the case of FOXP2, the protein acts to\ninhibit transcription of the downstream gene(s)). In affected family\nmembers, a single base-pair substitution in the gene coding for this\nregulatory protein leads to the insertion of the amino acid arginine\n(instead of the normal histamine) in an area of the protein (viz., the\n‘forkhead binding domain’) that is critical for its\nability to modulate the transcription of the downstream DNA. As a\nconsequence, FOXP2 cannot perform its normal regulatory role in\naffected KE family members. \n\nThe failure of FOXP2 to perform its normal role in turn leads to\nabnormal brain development in affected KE individuals. Studies of\nother animals and humans (e.g., Lai et al. 2003;\nTakahashi et al., 2003; Ferland et al. 2003;\nTeramitsu et al. 2004) show that FOXP2 is normally highly\nexpressed in both development and adulthood in two distinct brain\ncircuits, One is a corticostriatal circuit, in which inputs from the\nprefrontal and premotor cortex are modulated by the basal ganglia and\nthe thalamus, and then sent back to prefrontal and premotor cotical\nareas; the other is an olivocerebellar circuit, in which sensory input\nis sent via the spinal cord for processing in the medulla, cerebellum\nand thalamus before being handed on to prefrontal cortex. (See\nfig. 7.) The basal ganglia are known to be involved in the sequencing\nand reward-based learning of motor behaviors (Graybiel 1995, 1998),\nand the cerebellar circuit, while less well understood, is thought to\nbe a proprioceptive circuit involved in motor regulation and\ncoordination (Lieberman 2002). FOXP2 is expressed in homologues of\nthese areas in other species (e.g., canaries, zebra finches, rats) and\nin all species studied, these areas are involved in motor sequencing\nand coordination (Sharff and White 2004). \n \n \n\nFigure 7. Two circuits in which FOXP2 is expressed. (Based on figures\nby Diana Weedman Molavi, The Washington University School of Medicine\n\n Neuroscience Tutorial).\n  \n\nSo, what appears to be the case is that affected KE family members' \nlanguage difficulties result from a mutation in the FOXP2 gene, which\nresults in abnormal development of the striatal, cerebellar and \ncortical areas necessary for the sequencing and coordination of \nspeech-related movements of the mouth, tongue and possibly larynx; \nMRI scans of affected family members showing reduced gray matter \ndensity in these areas support this hypothesis, as do fMRI scans \nshowing abnormal striatal and cortical activation during receptive \nand active language processing (Belton et al. 2003; Liegeois, et \nal. 2003.) \n\nVargha-Khadem speculates (cf. Watkins et al. 2002:463) that those of\nthe KE's deficits that do not appear to be motor related (e.g., their\ncomprehension and reading problems, their difficulties with word \norder and syntax) are a result of impaired learning that itself \nresults from their motor deficits. For instance, impaired \narticulation could lead to impoverished phonological representations,\nwhich would then impair the acquisition of morphological and \nmorphosyntactic knowledge, which would then constitute a poor basis \nfor further syntactic learning. Impaired representation at all of \nthese levels would then express itself in receptive language and \nreading, as well as in the realm of spoken language. Another possible\nexplanation of the KE's non-articulatory deficits, which is not \nnecessarily in competition with the previous one, derives from the \nfact that the basal ganglia are also known to be implicated in \nworking memory (Bosnan 2004) and reward-based learning (e.g., \nclassical conditioning) that is mediated by dopaminergic circuits \nthat interact with basilar structures (Lieberman 2002). If \nreward-based learning and working memory are impaired in the KE's, \nthen this could explain not only their higher-level syntactic \ndeficits, but also their overall lower IQ (Lieberman 2002). \n\nNeither of these explanations of the KE's seems especially congenial \nto the linguistic nativist. For both tacitly assume that language \nlearning, including syntactic learning, is not (or not entirely) \nsubserved by special-purpose mechanisms. Rather, it is mediated by \nmore general motor circuitry (according to the Vargha-Khadem \nhypothesis) or reward-based learning and working memory abilities \n(according to Lieberman) that are also involved in other learning \ntasks. \n\nOn the other hand, however, there is evidence that FOXP2 is\nparticularly implicated in vocal learning and expression. First, it is\nhighly expressed in songbirds that modify their innate vocal\nrepertoires: in canaries it is expressed seasonally, when adult birds\nmodify their songs (Teramitsu et al. 2004) and in zebra\nfinches, it is expressed more at the time when young birds learn their\nsongs (Haessler et al. 2004). In addition, there is evidence\nthat the variant of the FOXP2 gene that is present in humans has\nundergone strong positive selection in the hominid line (Enard et\nal 2002; Zhang et al. 2002). The protein produced by\nhuman FOXP2 differs in just three out of its 715 constituent amino\nacids from that of the mouse, and a recent analysis (Zhang et\nal. 2003) indicates that two of these differences are unique to\nthe hominid lineage. According to Enard et al 2002, the fact\nthat these two differences are fixed in the human genome, whereas no\nfixed substitutions occurred in the lineage of our closest relatives,\nthe chimpanzees, suggests that those changes were strongly selected\nfor in our lineage; Enard et al.  put the date of fixation of\nthese changes in the human population at around 200,000 years\nago. This date accords well with at least some estimates of the\nemergence of modern human language, suggesting that the vocal\ncapacities underwritten by FOXP2 — and impaired in those lacking\nthe gene — are after all critical to language competence. \n\nAt this point, two questions arise. First, is there a double \ndissociation between language capacities and general cognitive \ncapacities to be found in a comparison of Williams syndrome and SLI? \nSecond, what does our current knowledge of the role of FOXP2 in \nlanguage development tell us about linguistic nativism? \n\nAs to the first question, there seems to be no double dissociation. \nFirst of all, WS individuals' language, while startling in contrast \nwith their level of mental retardation, is not normal; indeed, as we \nhave seen, it is indistinguishable on some tests from that of \nlanguage impaired individuals. In addition, as Thomas and \nKarmiloff-Smith 2002, caution, it is not at all clear that one can \nassume, in the case of a pervasive developmental disorder like \nWilliams syndrome, that apparently ‘intact’ competences \nare a result of normal development of the underlying neurological and\npsychological structures. That is, given the known capacity of the \nbrain to compensate for deficits in one area by cobbling together a \nsolution in another, one cannot assume that there is a \n‘language module’ in WS patients which develops more or \nless normally despite other cognitive systems' being massively \ndisrupted. Thomas and Karmiloff-Smith argue that the numerous \ndiscrepancies between WS language development and that of normal \nchildren suggests that this ‘residual normality’ \nassumption is misguided in this case, thus undermining the claim that\nwhat is spared in WS is ‘the language (or grammar) \nmodule.’ \n\nMoving to the other side of the dissociation, since it is hard to say\nexactly what about language is disrupted in cases of SLI, it is\ndifficult to determine whether this disruption is specific to\nlanguage, let alone grammar. While researchers like Van der Lely and\nChristian 2000, and van der Lely and Ullman 2001 argue that there is\na purely grammatical form of the deficit, which does support the\nhypothesis of a grammar module, this is controversial, as we have seen\nabove. Certainly consideration of the KE's does not support such a\nhypothesis. Their root deficit appears to concern orofacial praxis,\nrather than language specifically; and in addition, their\n‘general intelligence,’ as measured by tests of non-verbal\nintelligence, while “normal,” nonetheless appears to have\nbeen affected by their neurological and/or linguistic abnormalities\n— witness their scores 18-19 points lower than those of their\nrelatives. It is, in other words, unclear that there is any\ndissociation of language and general intelligence in this case at\nall. One can conclude that as things stand now, SLI seems to be so\nheterogeneous a disorder as to defy neat characterization, and that\nconsideration of this disorder does not support the view that there is\na language or grammar module that functions independently of other\ncognitive processes. \n\nThe second question asked above was: what can be learned about the \ninnate basis of language from a consideration of the KE's and FOXP2. \nIn a recent article, Marcus and Fisher (2003) argue that the kinds of\nresults discussed above offer valuable insights into the ways that \nlanguage is implemented in the brain and controlled (to the extent \nthat it is) by the genes. However, they refrain — rightly in my\nview — from drawing morals to the effect that FOXP2 is a \n“gene for language” or even “for \narticulation.” The effects of FOXP2 are wider than this (it is \nexpressed in the developing heart and lungs, in addition to the brain\n-- REF) and the functions of the neural circuits in which it is \nactive are as yet too poorly understood to do more than gesture at \nthe ways in which FOXP2 is involved in constructing the human \nlinguistic phenotype. \n\nAll the topics covered in §3 deserve books of their own. My aim\nhere has been to sketch the ways in which modern understanding of the\nmind reveals the inadequacy and implausibility of the claim that\nhumans have innate representations of UG that are responsible for\ntheir acquisition of language. There are likely many, many processes\nimplicated in the attainment of linguistic competence, that many of\nthem are likely specialized by natural selection for linguistic tasks,\nbut that many of them also retain their other, and older,\nfunctions. The linguistic nativist's theory views our acquisition of\ngrammatical competence as a simple matter — one that\ncan be described at one level of explanation, and in terms of a single\nkind of process. This is very unlikely to be the case. Multiple\nsystems and multiple processes are at work in the acquisition of\nlinguistic knowledge, and our understanding of language acquisition,\nwhen it comes, is likely to involve theories of many kinds and at many\ndifferent levels, and to resemble the theory of the Chomskyan nativist\nin few or no\n respects.[29]"}]
