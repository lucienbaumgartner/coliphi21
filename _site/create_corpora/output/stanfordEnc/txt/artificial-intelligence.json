[{"date.published":"2018-07-12","url":"https://plato.stanford.edu/entries/artificial-intelligence/","author1":"Selmer Bringsjord","author2":"Naveen Sundar Govindarajulu","author1.info":"http://www.rpi.edu/~brings","entry":"artificial-intelligence","body.text":"\n\n\nArtificial intelligence (AI) is the field devoted to building\nartificial animals (or at least artificial creatures that – in\nsuitable contexts – appear to be animals) and, for\nmany, artificial persons (or at least artificial creatures that\n– in suitable contexts – appear to be\n persons).[1]\n Such goals immediately ensure that AI is a discipline of considerable\ninterest to many philosophers, and this has been confirmed (e.g.) by\nthe energetic attempt, on the part of numerous philosophers, to show\nthat these goals are in fact un/attainable. On the constructive side,\nmany of the core formalisms and techniques used in AI come out of, and\nare indeed still much used and refined in, philosophy: first-order\nlogic and its extensions; intensional logics suitable for the modeling\nof doxastic attitudes and deontic reasoning; inductive logic,\nprobability theory, and probabilistic reasoning; practical reasoning\nand planning, and so on. In light of this, some philosophers conduct\nAI research and development as philosophy. \n\n\nIn the present entry, the history of AI is briefly recounted, proposed\ndefinitions of the field are discussed, and an overview of the field\nis provided. In addition, both philosophical AI (AI pursued as and out\nof philosophy) and philosophy of AI are discussed, via\nexamples of both. The entry ends with some de rigueur\nspeculative commentary regarding the future of AI. \n\nThe field of artificial intelligence (AI) officially started in 1956,\nlaunched by a small but now-famous\n DARPA-sponsored\n summer conference at Dartmouth College, in Hanover, New Hampshire.\n(The 50-year celebration of this conference,\n AI@50,\n was held in July 2006 at Dartmouth, with five of the original\nparticipants making it\n back.[2]\n What happened at this historic conference figures in the final\nsection of this entry.) Ten thinkers attended, including John McCarthy\n(who was working at Dartmouth in 1956), Claude Shannon, Marvin Minsky,\nArthur Samuel, Trenchard Moore (apparently the lone note-taker at the\noriginal conference), Ray Solomonoff, Oliver Selfridge, Allen Newell,\nand Herbert Simon. From where we stand now, into the start of the new\nmillennium, the Dartmouth conference is memorable for many reasons,\nincluding this pair: one, the term ‘artificial\nintelligence’ was coined there (and has long been firmly\nentrenched, despite being disliked by some of the attendees, e.g.,\nMoore); two, Newell and Simon revealed a program – Logic\nTheorist (LT) – agreed by the attendees (and, indeed, by nearly\nall those who learned of and about it soon after the conference) to be\na remarkable achievement. LT was capable of proving elementary\ntheorems in the propositional\n calculus.[3][4] \nThough the term ‘artificial intelligence’ made\nits advent at the 1956 conference, certainly the field of AI,\noperationally defined (defined, i.e., as a field constituted by\npractitioners who think and act in certain ways), was in operation\nbefore 1956. For example, in a famous Mind paper of 1950,\nAlan Turing argues that the question “Can a machine\nthink?” (and here Turing is talking about standard computing\nmachines: machines capable of computing functions from the natural\nnumbers (or pairs, triples, … thereof) to the natural numbers\nthat a Turing machine or equivalent can handle) should be replaced\nwith the question “Can a machine be linguistically\nindistinguishable from a human?.” Specifically, he proposes a\ntest, the\n “Turing Test”\n (TT) as it’s now known. In the TT, a woman and a computer are\nsequestered in sealed rooms, and a human judge, in the dark as to\nwhich of the two rooms contains which contestant, asks questions by\nemail (actually, by teletype, to use the original term) of the\ntwo. If, on the strength of returned answers, the judge can do no\nbetter than 50/50 when delivering a verdict as to which room houses\nwhich player, we say that the computer in question has passed\nthe TT. Passing in this sense operationalizes linguistic\nindistinguishability. Later, we shall discuss the role that TT has\nplayed, and indeed continues to play, in attempts to define AI. At the\nmoment, though, the point is that in his paper, Turing explicitly lays\ndown the call for building machines that would provide an existence\nproof of an affirmative answer to his question. The call even includes\na suggestion for how such construction should proceed. (He suggests\nthat “child machines” be built, and that these machines\ncould then gradually grow up on their own to learn to communicate in\nnatural language at the level of adult humans. This suggestion has\narguably been followed by Rodney Brooks and the philosopher Daniel\nDennett (1994) in the Cog Project. In addition, the Spielberg/Kubrick\nmovie A.I. is at least in part a cinematic exploration of\nTuring’s\n suggestion.[5])\n The TT continues to be at the heart of AI and discussions of its\nfoundations, as confirmed by the appearance of (Moor 2003). In fact,\nthe TT continues to be used to define the field, as in\nNilsson’s (1998) position, expressed in his textbook for the\nfield, that AI simply is the field devoted to building an artifact\nable to negotiate this test. Energy supplied by the dream of\nengineering a computer that can pass TT, or by controversy surrounding\nclaims that it has already been passed, is if anything\nstronger than ever, and the reader has only to do an internet search\nvia the string  \nturing test passed  \nto find up-to-the-minute attempts at reaching this dream, and attempts\n(sometimes made by philosophers) to debunk claims that some such\nattempt has succeeded. \nReturning to the issue of the historical record, even if one bolsters\nthe claim that AI started at the 1956 conference by adding the proviso\nthat ‘artificial intelligence’ refers to a nuts-and-bolts\nengineering pursuit (in which case Turing’s\nphilosophical discussion, despite calls for a child machine,\nwouldn’t exactly count as AI per se), one must confront the fact\nthat Turing, and indeed many predecessors, did attempt to build\nintelligent artifacts. In Turing’s case, such building was\nsurprisingly well-understood before the advent of programmable\ncomputers: Turing wrote a program for playing chess before there were\ncomputers to run such programs on, by slavishly following the code\nhimself. He did this well before 1950, and long before Newell (1973)\ngave thought in print to the possibility of a sustained, serious\nattempt at building a good chess-playing\n computer.[6] \nFrom the perspective of philosophy, which views the systematic\ninvestigation of mechanical intelligence as meaningful and productive\nseparate from the specific logicist formalisms (e.g., first-order\nlogic) and problems (e.g., the Entscheidungsproblem) that gave\nbirth to computer science, neither the 1956 conference, nor\nTuring’s Mind paper, come close to marking the start of\nAI. This is easy enough to see. For example, Descartes proposed TT\n(not the TT by name, of course) long before Turing was\n born.[7]\n Here’s the relevant passage:  \nAt the moment, Descartes is certainly carrying the\n day.[8]\n Turing predicted that his test would be passed by 2000, but the\nfireworks across the globe at the start of the new millennium have\nlong since died down, and the most articulate of computers still\ncan’t meaningfully debate a sharp toddler. Moreover, while in\ncertain focussed areas machines out-perform minds (IBM’s famous\nDeep Blue prevailed in chess over Gary Kasparov, e.g.; and more\nrecently, AI systems have prevailed in other games, e.g.\nJeopardy! and Go, about which more will momentarily be said),\nminds have a (Cartesian) capacity for cultivating their expertise in\nvirtually any sphere. (If it were announced to Deep Blue, or\nany current successor, that chess was no longer to be the game of\nchoice, but rather a heretofore unplayed variant of chess, the machine\nwould be trounced by human children of average intelligence having no\nchess expertise.) AI simply hasn’t managed to create\ngeneral intelligence; it hasn’t even managed to produce\nan artifact indicating that eventually it will create such a\nthing.  \nBut what about IBM Watson’s famous nail-biting victory in the\nJeopardy! game-show\n contest?[9]\n That certainly seems to be a machine triumph over humans on their\n“home field,” since Jeopardy! delivers a\nhuman-level linguistic challenge ranging across many domains. Indeed,\namong many AI cognoscenti, Watson’s success is considered to be\nmuch more impressive than Deep Blue’s, for numerous reasons. One\nreason is that while chess is generally considered to be\nwell-understood from the formal-computational perspective (after all,\nit’s well-known that there exists a perfect strategy for playing\nchess), in open-domain question-answering (QA), as in any\nsignificant natural-language processing task, there is no consensus as\nto what problem, formally speaking, one is trying to solve. Briefly,\nquestion-answering (QA) is what the reader would think it is: one asks\na question of a machine, and gets an answer, where the answer has to\nbe produced via some “significant” computational process.\n(See Strzalkowski & Harabagiu (2006) for an overview of what QA,\nhistorically, has been as a field.) A bit more precisely, there is no\nagreement as to what underlying function, formally speaking,\nquestion-answering capability computes. This lack of agreement stems\nquite naturally from the fact that there is of course no consensus as\nto what natural languages are, formally\n speaking.[10]\n Despite this murkiness, and in the face of an almost universal belief\nthat open-domain question-answering would remain unsolved for a decade\nor more, Watson decisively beat the two top human Jeopardy!\nchampions on the planet. During the contest, Watson had to answer\nquestions that required not only command of simple factoids\n(Question1), but also of some amount of rudimentary\nreasoning (in the form of temporal reasoning) and commonsense\n(Question2): \nQuestion1: The only two consecutive U.S. presidents\nwith the same first name.  \nQuestion2: In May 1898, Portugal celebrated the\n400th anniversary of this explorer’s arrival in India.  \nWhile Watson is demonstrably better than humans in\nJeopardy!-style quizzing (a new human Jeopardy! master\ncould arrive on the scene, but as for chess, AI now assumes that a\nsecond round of IBM-level investment would vanquish the new human\nopponent), this approach does not work for the kind of NLP challenge\nthat Descartes described; that is, Watson can’t converse on the\nfly. After all, some questions don’t hinge on sophisticated\ninformation retrieval and machine learning over pre-existing data, but\nrather on intricate reasoning right on the spot. Such questions may\nfor instance involve anaphora resolution, which require even deeper\ndegrees of commonsensical understanding of time, space, history, folk\npsychology, and so on. Levesque (2013) has catalogued some alarmingly\nsimple questions which fall in this category. (Marcus, 2013, gives an\naccount of Levesque’s challenges that is accessible to a wider\naudience.) The other class of question-answering tasks on which Watson\nfails can be characterized as dynamic question-answering. These\nare questions for which answers may not be recorded in textual form\nanywhere at the time of questioning, or for which answers are\ndependent on factors that change with time. Two questions that fall in\nthis category are given below (Govindarajulu et al. 2013): \nQuestion3: If I have 4 foos and 5 bars, and if foos\nare not the same as bars, how many foos will I have if I get 3 bazes\nwhich just happen to be foos? \nQuestion4: What was IBM’s Sharpe ratio in the\nlast 60 days of trading?  \nClosely following Watson’s victory, in March 2016,\n Google DeepMind’s AlphaGo\n defeated one of Go’s top-ranked players, Lee Seedol, in four\nout of five matches. This was considered a landmark achievement within\nAI, as it was widely believed in the AI community that computer\nvictory in Go was at least a few decades away, partly due to the\nenormous number of valid sequences of moves in Go compared to that in\n Chess.[11]\n While this is a remarkable achievement, it should be noted that,\ndespite breathless coverage in the popular\n press,[12]\n AlphaGo, while indisputably a great Go player, is just that. For\nexample, neither AlphaGo nor Watson can understand the rules of Go\nwritten in plain-and-simple English and produce a computer program\nthat can play the game. It’s interesting that there is one\nendeavor in AI that tackles a narrow version of this very problem: In\ngeneral game playing, a machine is given a description of a\nbrand new game just before it has to play the game (Genesereth et al.\n2005). However, the description in question is expressed in a formal\nlanguage, and the machine has to manage to play the game from this\ndescription. Note that this is still far from understanding even a\nsimple description of a game in English well enough to play it.  \nBut what if we consider the history of AI not from the perspective of\nphilosophy, but rather from the perspective of the field with which,\ntoday, it is most closely connected? The reference here is to computer\nscience. From this perspective, does AI run back to well before\nTuring? Interestingly enough, the results are the same: we find that\nAI runs deep into the past, and has always had philosophy in its\nveins. This is true for the simple reason that computer science grew\nout of logic and probability\n theory,[13]\n which in turn grew out of (and is still intertwined with) philosophy.\nComputer science, today, is shot through and through with logic; the\ntwo fields cannot be separated. This phenomenon has become an object\nof study unto itself (Halpern et al. 2001). The situation is no\ndifferent when we are talking not about traditional logic, but rather\nabout probabilistic formalisms, also a significant component of\nmodern-day AI: These formalisms also grew out of philosophy, as nicely\nchronicled, in part, by Glymour (1992). For example, in the one mind\nof Pascal was born a method of rigorously calculating probabilities,\nconditional probability (which plays a particularly large role in AI,\ncurrently), and such fertile philosophico-probabilistic arguments as\n Pascal’s wager,\n according to which it is irrational not to become a Christian.  \nThat modern-day AI has its roots in philosophy, and in fact that these\nhistorical roots are temporally deeper than even Descartes’\ndistant day, can be seen by looking to the clever, revealing cover of\nthe second edition (the third edition is the current one) of the\ncomprehensive textbook\n Artificial Intelligence: A Modern Approach\n (known in the AI community as simply AIMA2e for Russell &\nNorvig, 2002). \nCover of AIMA2e (Russell & Norvig 2002) \nWhat you see there is an eclectic collection of memorabilia that might\nbe on and around the desk of some imaginary AI researcher. For\nexample, if you look carefully, you will specifically see: a picture\nof Turing, a view of Big Ben through a window (perhaps R&N are\naware of the fact that Turing famously held at one point that a\nphysical machine with the power of a universal Turing machine is\nphysically impossible: he quipped that it would have to be the size of\nBig Ben), a planning algorithm described in Aristotle’s De\nMotu Animalium,\n Frege’s fascinating notation for first-order logic,\n a glimpse of Lewis Carroll’s (1958) pictorial representation of\nsyllogistic reasoning, Ramon Lull’s concept-generating wheel\nfrom his 13th-century Ars Magna, and a number of\nother pregnant items (including, in a clever, recursive, and\nbordering-on-self-congratulatory touch, a copy of AIMA itself).\nThough there is insufficient space here to make all the historical\nconnections, we can safely infer from the appearance of these items\n(and here we of course refer to the ancient ones: Aristotle conceived\nof planning as information-processing over two-and-a-half millennia\nback; and in addition, as Glymour (1992) notes, Artistotle can also be\ncredited with devising the first knowledge-bases and ontologies, two\ntypes of representation schemes that have long been central to AI)\nthat AI is indeed very, very old. Even those who insist that AI is at\nleast in part an artifact-building enterprise must concede that, in\nlight of these objects, AI is ancient, for it isn’t just\ntheorizing from the perspective that intelligence is at bottom\ncomputational that runs back into the remote past of human history:\nLull’s wheel, for example, marks an attempt to capture\nintelligence not only in computation, but in a physical artifact that\nembodies that\n computation.[14] \nAIMA has now reached its the third edition, and those interested in\nthe history of AI, and for that matter the history of philosophy of\nmind, will not be disappointed by examination of the cover of the\nthird installment (the cover of the second edition is almost exactly\nlike the first edition). (All the elements of the cover, separately\nlisted and annotated, can be found\n online.)\n One significant addition to the cover of the third edition is a\ndrawing of Thomas Bayes; his appearance reflects the recent rise in\nthe popularity of probabilistic techniques in AI, which we discuss\nlater. \nOne final point about the history of AI seems worth making.  \nIt is generally assumed that the birth of modern-day AI in the 1950s\ncame in large part because of and through the advent of the modern\nhigh-speed digital computer. This assumption accords with\ncommon-sense. After all, AI (and, for that matter, to some degree its\ncousin, cognitive science, particularly computational cognitive\nmodeling, the sub-field of cognitive science devoted to producing\ncomputational simulations of human cognition) is aimed at implementing\nintelligence in a computer, and it stands to reason that such a goal\nwould be inseparably linked with the advent of such devices. However,\nthis is only part of the story: the part that reaches back but to\nTuring and others (e.g., von Neuman) responsible for the first\nelectronic computers. The other part is that, as already mentioned, AI\nhas a particularly strong tie, historically speaking, to reasoning\n(logic-based and, in the need to deal with uncertainty,\ninductive/probabilistic reasoning). In this story, nicely told by\nGlymour (1992), a search for an answer to the question “What is\na proof?” eventually led to an answer based on Frege’s\nversion of first-order logic (FOL): a (finitary) mathematical proof\nconsists in a series of step-by-step inferences from one formula of\nfirst-order logic to the next. The obvious extension of this answer\n(and it isn’t a complete answer, given that lots of classical\nmathematics, despite conventional wisdom, clearly can’t be\nexpressed in FOL; even the Peano Axioms, to be expressed as a finite\nset of formulae, require SOL) is to say that not only\nmathematical thinking, but thinking, period, can be expressed in FOL.\n(This extension was entertained by many logicians long before the\nstart of information-processing psychology and cognitive science\n– a fact some cognitive psychologists and cognitive scientists\noften seem to forget.) Today, logic-based AI is only part of\nAI, but the point is that this part still lives (with help from logics\nmuch more powerful, but much more complicated, than FOL), and it can\nbe traced all the way back to Aristotle’s theory of the\n syllogism.[15]\n In the case of uncertain reasoning, the question isn’t\n“What is a proof?”, but rather questions such as\n“What is it rational to believe, in light of certain\nobservations and probabilities?” This is a question posed and\ntackled long before the arrival of digital computers.  \nSo far we have been proceeding as if we have a firm and precise grasp\nof the nature of AI. But what exactly is AI? Philosophers\narguably know better than anyone that precisely defining a particular\ndiscipline to the satisfaction of all relevant parties (including\nthose working in the discipline itself) can be acutely challenging.\nPhilosophers of science certainly have proposed credible accounts of\nwhat constitutes at least the general shape and texture of a given\nfield of science and/or engineering, but what exactly is the\nagreed-upon definition of physics? What about biology? What, for that\nmatter, is philosophy, exactly? These are remarkably difficult, maybe\neven eternally unanswerable, questions, especially if the target is a\nconsensus definition. Perhaps the most prudent course we can\nmanage here under obvious space constraints is to present in\nencapsulated form some proposed definitions of AI. We do\ninclude a glimpse of recent attempts to define AI in detailed,\nrigorous fashion (and we suspect that such attempts will be of\ninterest to philosophers of science, and those interested in this\nsub-area of philosophy).  \nRussell and Norvig (1995, 2002, 2009), in their aforementioned\nAIMA text, provide a set of possible answers to the “What\nis AI?” question that has considerable currency in the field\nitself. These answers all assume that AI should be defined in terms of\nits goals: a candidate definition thus has the form “AI is the\nfield that aims at building …” The answers all fall under\na quartet of types placed along two dimensions. One dimension is\nwhether the goal is to match human performance, or, instead, ideal\nrationality. The other dimension is whether the goal is to build\nsystems that reason/think, or rather systems that act. The situation\nis summed up in this table: \nFour Possible Goals for AI According to AIMA  \nPlease note that this quartet of possibilities does reflect (at least\na significant portion of) the relevant literature. For example,\nphilosopher John Haugeland (1985) falls into the Human/Reasoning\nquadrant when he says that AI is “The exciting new effort to\nmake computers think … machines with minds, in the\nfull and literal sense.” (By far, this is the quadrant that most\npopular narratives affirm and explore. The recent\n Westworld\n TV series is a powerful case in point.) Luger and Stubblefield (1993)\nseem to fall into the Ideal/Act quadrant when they write: “The\nbranch of computer science that is concerned with the automation of\nintelligent behavior.” The Human/Act position is occupied most\nprominently by Turing, whose test is passed only by those systems able\nto act sufficiently like a human. The “thinking\nrationally” position is defended (e.g.) by Winston (1992). While\nit might not be entirely uncontroversial to assert that the four bins\ngiven here are exhaustive, such an assertion appears to be quite\nplausible, even when the literature up to the present moment is\ncanvassed.  \nIt’s important to know that the contrast between the focus on\nsystems that think/reason versus systems that act, while found, as we\nhave seen, at the heart of the AIMA texts, and at the heart of\nAI itself, should not be interpreted as implying that AI researchers\nview their work as falling all and only within one of these two\ncompartments. Researchers who focus more or less exclusively on\nknowledge representation and reasoning, are also quite prepared to\nacknowledge that they are working on (what they take to be) a central\ncomponent or capability within any one of a family of larger systems\nspanning the reason/act distinction. The clearest case may come from\nthe work on planning – an AI area traditionally making central\nuse of representation and reasoning. For good or ill, much of this\nresearch is done in abstraction (in vitro, as opposed to in vivo), but\nthe researchers involved certainly intend or at least hope that the\nresults of their work can be embedded into systems that actually do\nthings, such as, for example, execute the plans.  \nWhat about Russell and Norvig themselves? What is their answer to the\nWhat is AI? question? They are firmly in the the “acting\nrationally” camp. In fact, it’s safe to say both that they\nare the chief proponents of this answer, and that they have been\nremarkably successful evangelists. Their extremely influential\nAIMA series can be viewed as a book-length defense and\nspecification of the Ideal/Act category. We will look a bit later at\nhow Russell and Norvig lay out all of AI in terms of intelligent\nagents, which are systems that act in accordance with various\nideal standards for rationality. But first let’s look a bit\ncloser at the view of intelligence underlying the AIMA text. We\ncan do so by turning to Russell (1997). Here Russell recasts the\n“What is AI?” question as the question “What is\nintelligence?” (presumably under the assumption that we have a\ngood grasp of what an artifact is), and then he identifies\nintelligence with rationality. More specifically, Russell sees\nAI as the field devoted to building intelligent agents, which\nare functions taking as input tuples of percepts from the external\nenvironment, and producing behavior (actions) on the basis of these\npercepts. Russell’s overall picture is this one: \nThe Basic Picture Underlying Russell’s Account of\nIntelligence/Rationality  \nLet’s unpack this diagram a bit, and take a look, first, at the\naccount of perfect rationality that can be derived from it. The\nbehavior of the agent in the environment \\(E\\) (from a class \\(\\bE\\)\nof environments) produces a sequence of states or snapshots of that\nenvironment. A performance measure \\(U\\) evaluates this sequence;\nnotice the box labeled “Performance Measure” in the above\nfigure. We let \\(V(f,\\bE,U)\\) denote the expected utility\naccording to \\(U\\) of the agent function \\(f\\) operating on\n \\(\\bE\\).[16]\n Now we identify a perfectly rational agent with the agent function:\n \nAccording to the above equation, a perfectly rational agent can be\ntaken to be the function \\(f_{opt}\\) which produces the maximum\nexpected utility in the environment under consideration. Of course, as\nRussell points out, it’s usually not possible to actually build\nperfectly rational agents. For example, though it’s easy enough\nto specify an algorithm for playing invincible chess, it’s not\nfeasible to implement this algorithm. What traditionally happens in AI\nis that programs that are – to use Russell’s apt\nterminology – calculatively rational are constructed\ninstead: these are programs that, if executed infinitely fast,\nwould result in perfectly rational behavior. In the case of chess,\nthis would mean that we strive to write a program that runs an\nalgorithm capable, in principle, of finding a flawless move, but we\nadd features that truncate the search for this move in order to play\nwithin intervals of digestible duration.  \nRussell himself champions a new brand of intelligence/rationality for\nAI; he calls this brand bounded optimality. To understand\nRussell’s view, first we follow him in introducing a\ndistinction: We say that agents have two components: a program, and a\nmachine upon which the program runs. We write \\(Agent(P, M)\\) to\ndenote the agent function implemented by program \\(P\\) running on\nmachine \\(M\\). Now, let \\(\\mathcal{P}(M)\\) denote the set of all\nprograms \\(P\\) that can run on machine \\(M\\). The bounded\noptimal program \\(P_{\\opt,M}\\) then is: \nYou can understand this equation in terms of any of the mathematical\nidealizations for standard computation. For example, machines can be\nidentified with Turing machines minus instructions (i.e., TMs are here\nviewed architecturally only: as having tapes divided into squares upon\nwhich symbols can be written, read/write heads capable of moving up\nand down the tape to write and erase, and control units which are in\none of a finite number of states at any time), and programs can be\nidentified with instructions in the Turing-machine model (telling the\nmachine to write and erase symbols, depending upon what state the\nmachine is in). So, if you are told that you must\n“program” within the constraints of a 22-state Turing\nmachine, you could search for the “best” program given\nthose constraints. In other words, you could strive to find the\noptimal program within the bounds of the 22-state architecture.\nRussell’s (1997) view is thus that AI is the field devoted to\ncreating optimal programs for intelligent agents, under time and space\nconstraints on the machines implementing these\n programs.[17] \nThe reader must have noticed that in the equation for \\(P_{\\opt,M}\\)\nwe have not elaborated on \\(\\bE\\) and \\(U\\) and how equation\n\\eqref{eq1} might be used to construct an agent if the class of\nenvironments \\(\\bE\\) is quite general, or if the true environment\n\\(E\\) is simply unknown. Depending on the task for which one is\nconstructing an artificial agent, \\(E\\) and \\(U\\) would vary. The\nmathematical form of the environment \\(E\\) and the utility function\n\\(U\\) would vary wildly from, say, chess to Jeopardy!. Of\ncourse, if we were to design a globally intelligent agent, and not\njust a chess-playing agent, we could get away with having just one\npair of \\(E\\) and \\(U\\). What would \\(E\\) look like if we were\nbuilding a generally intelligent agent and not just an agent that is\ngood at a single task? \\(E\\) would be a model of not just a single\ngame or a task, but the entire physical-social-virtual universe\nconsisting of many games, tasks, situations, problems, etc. This\nproject is (at least currently) hopelessly difficult as, obviously, we\nare nowhere near to having such a comprehensive theory-of-everything\nmodel. For further discussion of a theoretical architecture put\nforward for this problem, see the\n Supplement on the AIXI architecture.\n  \nIt should be mentioned that there is a different, much more\nstraightforward answer to the “What is AI?” question. This\nanswer, which goes back to the days of the original Dartmouth\nconference, was expressed by, among others, Newell (1973), one of the\ngrandfathers of modern-day AI (recall that he attended the 1956\nconference); it is:  \nThe above definition can be seen as fully specifying a concrete\nversion of Russell and Norvig’s four possible goals. Though few\nare aware of this now, this answer was taken quite seriously for a\nwhile, and in fact underlied one of the most famous programs in the\nhistory of AI: the ANALOGY program of Evans (1968), which solved\ngeometric analogy problems of a type seen in many intelligence tests.\nAn attempt to rigorously define this forgotten form of AI (as what\nthey dub Psychometric AI), and to resurrect it from the days of\nNewell and Evans, is provided by Bringsjord and Schimanski (2003) [see\nalso e.g. (Bringsjord 2011)]. A sizable private investment has been\nmade in the ongoing attempt, now known as\n Project Aristo,\n to build a “digital Aristotle”, in the form of a machine\nable to excel on standardized tests such at the AP exams tackled by US\nhigh school students (Friedland et al. 2004). (Vibrant work in this\ndirection continues today at the\n Allen Institute for Artificial Intelligence.)[18]\n In addition, researchers at Northwestern have forged a connection\nbetween AI and tests of mechanical ability (Klenk et al. 2005).  \nIn the end, as is the case with any discipline, to really know\nprecisely what that discipline is requires you to, at least to some\ndegree, dive in and do, or at least dive in and read. Two decades ago\nsuch a dive was quite manageable. Today, because the content that has\ncome to constitute AI has mushroomed, the dive (or at least the swim\nafter it) is a bit more demanding. \nThere are a number of ways of “carving up” AI. By far the\nmost prudent and productive way to summarize the field is to turn yet\nagain to the AIMA text given its comprehensive overview of the\nfield.  \nAs Russell and Norvig (2009) tell us in the Preface of AIMA:\n \nThe basic picture is thus summed up in this figure: \nImpressionistic Overview of an Intelligent Agent \nThe content of AIMA derives, essentially, from fleshing out\nthis picture; that is, the above figure corresponds to the different\nways of representing the overall function that intelligent agents\nimplement. And there is a progression from the least powerful agents\nup to the more powerful ones. The following figure gives a high-level\nview of a simple kind of agent discussed early in the book. (Though\nsimple, this sort of agent corresponds to the architecture of\nrepresentation-free agents designed and implemented by Rodney Brooks,\n1991.) \nA Simple Reflex Agent \nAs the book progresses, agents get increasingly sophisticated, and the\nimplementation of the function they represent thus draws from more and\nmore of what AI can currently muster. The following figure gives an\noverview of an agent that is a bit smarter than the simple reflex\nagent. This smarter agent has the ability to internally model the\noutside world, and is therefore not simply at the mercy of what can at\nthe moment be directly sensed. \nA More Sophisticated Reflex Agent \nThere are seven parts to AIMA. As the reader passes through\nthese parts, she is introduced to agents that take on the powers\ndiscussed in each part. Part I is an introduction to the agent-based\nview. Part II is concerned with giving an intelligent agent the\ncapacity to think ahead a few steps in clearly defined environments.\nExamples here include agents able to successfully play games of\nperfect information, such as chess. Part III deals with agents that\nhave declarative knowledge and can reason in ways that will be quite\nfamiliar to most philosophers and logicians (e.g., knowledge-based\nagents deduce what actions should be taken to secure their goals).\nPart IV of the book outfits agents with the power to handle\nuncertainty by reasoning in probabilistic\n fashion.[19]\n In Part V, agents are given a capacity to learn. The following figure\nshows the overall structure of a learning agent. \nA Learning Agent \nThe final set of powers agents are given allow them to communicate.\nThese powers are covered in Part VI.  \nPhilosophers who patiently travel the entire progression of\nincreasingly smart agents will no doubt ask, when reaching the end of\nPart VII, if anything is missing. Are we given enough, in general, to\nbuild an artificial person, or is there enough only to build a mere\nanimal? This question is implicit in the following from Charniak and\nMcDermott (1985):  \nTo their credit, Russell & Norvig, in AIMA’s Chapter\n27, “AI: Present and Future,” consider this question, at\nleast to some\n degree.[]\n They do so by considering some challenges to AI that have hitherto\nnot been met. One of these challenges is described by R&N as\nfollows:  \nWhile there has seen some advances in addressing this challenge (in\nthe form of deep learning or representation learning),\nthis specific challenge is actually merely a foothill before a range\nof dizzyingly high mountains that AI must eventually somehow manage to\nclimb. One of those mountains, put simply, is\n reading.[21]\n Despite the fact that, as noted, Part V of AIMA is devoted to\nmachine learning, AI, as it stands, offers next to nothing in the way\nof a mechanization of learning by reading. Yet when you think about\nit, reading is probably the dominant way you learn at this stage in\nyour life. Consider what you’re doing at this very moment.\nIt’s a good bet that you are reading this sentence because,\nearlier, you set yourself the goal of learning about the field of AI.\n Yet\n the formal models of learning provided in AIMA’s Part IV\n(which are all and only the models at play in AI) cannot be applied to\nlearning by\n reading.[22]\n These models all start with a function-based view of learning.\nAccording to this view, to learn is almost invariably to produce an\nunderlying function \\(\\ff\\) on the basis of a restricted set of\npairs \nFor example, consider receiving inputs consisting of 1, 2, 3, 4, and\n5, and corresponding range values of 1, 4, 9, 16, and 25; the goal is\nto “learn” the underlying mapping from natural numbers to\nnatural numbers. In this case, assume that the underlying function is\n\\(n^2\\), and that you do “learn” it. While this narrow\nmodel of learning can be productively applied to a number of\nprocesses, the process of reading isn’t one of them. Learning by\nreading cannot (at least for the foreseeable future) be modeled as\ndivining a function that produces argument-value pairs. Instead, your\nreading about AI can pay dividends only if your knowledge has\nincreased in the right way, and if that knowledge leaves you\npoised to be able to produce behavior taken to confirm sufficient\nmastery of the subject area in question. This behavior can range from\ncorrectly answering and justifying test questions regarding AI, to\nproducing a robust, compelling presentation or paper that signals your\nachievement.  \nTwo points deserve to be made about machine reading. First, it may not\nbe clear to all readers that reading is an ability that is central to\nintelligence. The centrality derives from the fact that intelligence\nrequires vast knowledge. We have no other means of getting systematic\nknowledge into a system than to get it in from text, whether text on\nthe web, text in libraries, newspapers, and so on. You might even say\nthat the big problem with AI has been that machines really don’t\nknow much compared to humans. That can only be because of the fact\nthat humans read (or hear: illiterate people can listen to text being\nuttered and learn that way). Either machines gain knowledge by humans\nmanually encoding and inserting knowledge, or by reading and\nlistening. These are brute facts. (We leave aside supernatural\ntechniques, of course. Oddly enough, Turing didn’t: he seemed to\nthink ESP should be discussed in connection with the powers of minds\nand machines. See Turing,\n 1950.)[23] \nNow for the second point. Humans able to read have invariably also\nlearned a language, and learning languages has been modeled in\nconformity to the function-based approach adumbrated just above\n(Osherson et al. 1986). However, this doesn’t entail that an\nartificial agent able to read, at least to a significant degree, must\nhave really and truly learned a natural language. AI is first and\nforemost concerned with engineering computational artifacts that\nmeasure up to some test (where, yes, sometimes that test is from the\nhuman sphere), not with whether these artifacts process information in\nways that match those present in the human case. It may or may not be\nnecessary, when engineering a machine that can read, to imbue that\nmachine with human-level linguistic competence. The issue is\nempirical, and as time unfolds, and the engineering is pursued, we\nshall no doubt see the issue settled.  \nTwo additional high mountains facing AI are subjective consciousness\nand creativity, yet it would seem that these great challenges are ones\nthe field apparently hasn’t even come to grips with. Mental\nphenomena of paramount importance to many philosophers of mind and\nneuroscience are simply missing from AIMA. For example,\nconsciousness is only mentioned in passing in AIMA, but\nsubjective consciousness is the most important thing in our lives\n– indeed we only desire to go on living because we wish to go on\nenjoying subjective states of certain types. Moreover, if human minds\nare the product of evolution, then presumably phenomenal consciousness\nhas great survival value, and would be of tremendous help to a robot\nintended to have at least the behavioral repertoire of the first\ncreatures with brains that match our own (hunter-gatherers; see Pinker\n1997). Of course, subjective consciousness is largely missing from the\nsister fields of cognitive psychology and computational cognitive\nmodeling as well. We discuss some of these challenges in the\n Philosophy of Artificial Intelligence\n section below. For a list of similar challenges to cognitive science,\nsee the relevant\n section of the entry on cognitive science.[24] \nTo some readers, it might seem in the very least tendentious to point\nto subjective consciousness as a major challenge to AI that it has yet\nto address. These readers might be of the view that pointing to this\nproblem is to look at AI through a distinctively philosophical prism,\nand indeed a controversial philosophical standpoint.  \nBut as its literature makes clear, AI measures itself by looking to\nanimals and humans and picking out in them remarkable mental powers,\nand by then seeing if these powers can be mechanized. Arguably the\npower most important to humans (the capacity to experience) is nowhere\nto be found on the target list of most AI researchers. There may be a\ngood reason for this (no formalism is at hand, perhaps), but there is\nno denying the state of affairs in question obtains, and that, in\nlight of how AI measures itself, that it’s worrisome.  \nAs to creativity, it’s quite remarkable that the power we most\npraise in human minds is nowhere to be found in AIMA. Just as\nin (Charniak & McDermott 1985) one cannot find\n‘neural’ in the index, ‘creativity’\ncan’t be found in the index of AIMA. This is particularly\nodd because many AI researchers have in fact worked on creativity\n(especially those coming out of philosophy; e.g., Boden 1994,\nBringsjord & Ferrucci 2000).  \nAlthough the focus has been on AIMA, any of its counterparts\ncould have been used. As an example, consider Artificial\nIntelligence: A New Synthesis, by Nils Nilsson. As in the case of\nAIMA, everything here revolves around a gradual progression\nfrom the simplest of agents (in Nilsson’s case, reactive\nagents), to ones having more and more of those powers that\ndistinguish persons. Energetic readers can verify that there is a\nstriking parallel between the main sections of Nilsson’s book\nand AIMA. In addition, Nilsson, like Russell and Norvig,\nignores phenomenal consciousness, reading, and creativity. None of the\nthree are even mentioned. Likewise, a recent comprehensive AI textbook\nby Luger (2008) follows the same pattern.  \nA final point to wrap up this section. It seems quite plausible to\nhold that there is a certain inevitability to the structure of an AI\ntextbook, and the apparent reason is perhaps rather interesting. In\npersonal conversation, Jim Hendler, a well-known AI researcher who is\none of the main innovators behind Semantic Web (Berners-Lee, Hendler,\nLassila 2001), an under-development “AI-ready” version of\nthe World Wide Web, has said that this inevitability can be rather\neasily displayed when teaching Introduction to AI; here’s how.\nBegin by asking students what they think AI is. Invariably, many\nstudents will volunteer that AI is the field devoted to building\nartificial creatures that are intelligent. Next, ask for examples of\nintelligent creatures. Students always respond by giving examples\nacross a continuum: simple multi-cellular organisms, insects, rodents,\nlower mammals, higher mammals (culminating in the great apes), and\nfinally human persons. When students are asked to describe the\ndifferences between the creatures they have cited, they end up\nessentially describing the progression from simple agents to ones\nhaving our (e.g.) communicative powers. This progression gives the\nskeleton of every comprehensive AI textbook. Why does this happen? The\nanswer seems clear: it happens because we can’t resist\nconceiving of AI in terms of the powers of extant creatures with which\nwe are familiar. At least at present, persons, and the creatures who\nenjoy only bits and pieces of personhood, are – to repeat\n– the measure of\n AI.[25] \nReasoning based on classical deductive logic is monotonic; that is, if\n\\(\\Phi\\vdash\\phi\\), then for all \\(\\psi\\), \\(\\Phi\\cup\n\\{\\psi\\}\\vdash\\phi\\). Commonsense reasoning is not monotonic. While\nyou may currently believe on the basis of reasoning that your house is\nstill standing, if while at work you see on your computer screen that\na vast tornado is moving through the location of your house, you will\ndrop this belief. The addition of new information causes previous\ninferences to fail. In the simpler example that has become an AI\nstaple, if I tell you that Tweety is a bird, you will infer that\nTweety can fly, but if I then inform you that Tweety is a penguin, the\ninference evaporates, as well it should. Nonmonotonic (or defeasible)\nlogic includes formalisms designed to capture the mechanisms\nunderlying these kinds of examples. See the separate entry on\n logic and artificial intelligence,\n which is focused on nonmonotonic reasoning, and reasoning about time\nand change. It also provides a history of the early days of\nlogic-based AI, making clear the contributions of those who founded\nthe tradition (e.g., John McCarthy and Pat Hayes; see their seminal\n1969 paper).  \nThe formalisms and techniques of logic-based AI have reached a level\nof impressive maturity – so much so that in various academic and\ncorporate laboratories, implementations of these formalisms and\ntechniques can be used to engineer robust, real-world software. It is\nstrongly recommend that readers who have an interest to learn where AI\nstands in these areas consult (Mueller 2006), which provides, in one\nvolume, integrated coverage of nonmonotonic reasoning (in the form,\nspecifically, of circumscription), and reasoning about time and change\nin the situation and event calculi. (The former calculus is also\nintroduced by Thomason. In the second, timepoints are included, among\nother things.) The other nice thing about (Mueller 2006) is that the\nlogic used is multi-sorted first-order logic (MSL), which has\nunificatory power that will be known to and appreciated by many\ntechnical philosophers and logicians (Manzano 1996).  \nWe now turn to three further topics of importance in AI. They are: \nThis trio is covered in order, beginning with the first. \nDetailed accounts of logicist AI that fall under the agent-based\nscheme can be found in (Nilsson 1991, Bringsjord & Ferrucci\n 1998).[26].\n The core idea is that an intelligent agent receives percepts from the\nexternal world in the form of formulae in some logical system (e.g.,\nfirst-order logic), and infers, on the basis of these percepts and its\nknowledge base, what actions should be performed to secure the\nagent’s goals. (This is of course a barbaric simplification.\nInformation from the external world is encoded in formulae,\nand transducers to accomplish this feat may be components of the\nagent.)  \nTo clarify things a bit, we consider, briefly, the logicist view in\nconnection with arbitrary logical systems\n \\(\\mathcal{L}_{X}\\).[27]\n We obtain a particular logical system by setting \\(X\\) in the\nappropriate way. Some examples: If \\(X=I\\), then we have a system at\nthe level of FOL [following the standard notation from model theory;\nsee e.g. (Ebbinghaus et al. 1984)]. \\(\\mathcal{L}_{II}\\) is\nsecond-order logic, and \\(\\mathcal{L}_{\\omega_I\\omega}\\) is a\n“small system” of infinitary logic (countably infinite\nconjunctions and disjunctions are permitted). These logical systems\nare all extensional, but there are intensional ones as\nwell. For example, we can have logical systems corresponding to those\nseen in standard propositional modal logic (Chellas 1980). One\npossibility, familiar to many philosophers, would be propositional\nKT45, or\n \\(\\mathcal{L}_{KT45}\\).[28]\n In each case, the system in question includes a relevant alphabet\nfrom which well-formed formulae are constructed by way of a formal\ngrammar, a reasoning (or proof) theory, a formal semantics, and at\nleast some meta-theoretical results (soundness, completeness, etc.).\nTaking off from standard notation, we can thus say that a set of\nformulas in some particular logical system \\(\\mathcal{L}_X\\),\n\\(\\Phi_{\\mathcal{L}_X}\\), can be used, in conjunction with some\nreasoning theory, to infer some particular formula\n\\(\\phi_{\\mathcal{L}_X}\\). (The reasoning may be deductive, inductive,\nabductive, and so on. Logicist AI isn’t in the least restricted\nto any particular mode of reasoning.) To say that such a situation\nholds, we write \n\n    \\[\n    \\Phi_{\\mathcal{L}_X} \\vdash_{\\mathcal{L}_X} \\phi_{\\mathcal{L}_X}\n    \\]\n\n  \nWhen the logical system referred to is clear from context, or when we\ndon’t care about which logical system is involved, we can simply\nwrite \n\n    \\[\n    \\Phi \\vdash \\phi\n    \\]\n\n  \nEach logical system, in its formal semantics, will include objects\ndesigned to represent ways the world pointed to by formulae in this\nsystem can be. Let these ways be denoted by \\(W^i_{{\\mathcal{L}_X}}\\).\nWhen we aren’t concerned with which logical system is involved,\nwe can simply write \\(W^i\\). To say that such a way models a formula\n\\(\\phi\\) we write \n\n    \\[\n    W_i \\models \\phi\n    \\]\n\n  \nWe extend this to a set of formulas in the natural way:\n\\(W^i\\models\\Phi\\) means that all the elements of \\(\\Phi\\) are true on\n\\(W^i\\). Now, using the simple machinery we’ve established, we\ncan describe, in broad strokes, the life of an intelligent agent that\nconforms to the logicist point of view. This life conforms to the\nbasic cycle that undergirds intelligent agents in the AIMA\nsense.  \nTo begin, we assume that the human designer, after studying the world,\nuses the language of a particular logical system to give to our agent\nan initial set of beliefs \\(\\Delta_0\\) about what this world is like.\nIn doing so, the designer works with a formal model of this world,\n\\(W\\), and ensures that \\(W\\models\\Delta_0\\). Following tradition, we\nrefer to \\(\\Delta_0\\) as the agent’s (starting) knowledge\nbase. (This terminology, given that we are talking about the\nagent’s beliefs, is known to be peculiar, but it\npersists.) Next, the agent ADJUSTS its knowlege base to produce\na new one, \\(\\Delta_1\\). We say that adjustment is carried out by way\nof an operation \\(\\mathcal{A}\\); so\n\\(\\mathcal{A}[\\Delta_0]=\\Delta_1\\). How does the adjustment process,\n\\(\\mathcal{A}\\), work? There are many possibilities. Unfortunately,\nmany believe that the simplest possibility (viz.,\n\\(\\mathcal{A}[\\Delta_i]\\) equals the set of all formulas that can be\ndeduced in some elementary manner from \\(\\Delta_i\\)) exhausts\nall the possibilities. The reality is that adjustment, as\nindicated above, can come by way of any mode of reasoning\n– induction, abduction, and yes, various forms of deduction\ncorresponding to the logical system in play. For present purposes,\nit’s not important that we carefully enumerate all the options.\n \nThe cycle continues when the agent ACTS on the environment, in\nan attempt to secure its goals. Acting, of course, can cause changes\nto the environment. At this point, the agent SENSES the\nenvironment, and this new information \\(\\Gamma_1\\) factors into the\nprocess of adjustment, so that\n\\(\\mathcal{A}[\\Delta_1\\cup\\Gamma_1]=\\Delta_2\\). The cycle of SENSES\n\\(\\Rightarrow\\) ADJUSTS \\(\\Rightarrow\\) ACTS continues to produce\nthe life \\(\\Delta_0,\\Delta_1,\\Delta_2,\\Delta_3,\\ldots,\\) … of\nour agent.  \nIt may strike you as preposterous that logicist AI be touted as an\napproach taken to replicate all of cognition. Reasoning over\nformulae in some logical system might be appropriate for\ncomputationally capturing high-level tasks like trying to solve a math\nproblem (or devising an outline for an entry in the Stanford\nEncyclopedia of Philosophy), but how could such reasoning apply to\ntasks like those a hawk tackles when swooping down to capture\nscurrying prey? In the human sphere, the task successfully negotiated\nby athletes would seem to be in the same category. Surely, some will\ndeclare, an outfielder chasing down a fly ball doesn’t prove\ntheorems to figure out how to pull off a diving catch to save the\ngame! Two brutally reductionistic arguments can be given in support of\nthis “logicist theory of everything” approach towards\ncognition. The first stems from the fact that a complete proof\ncalculus for just first-order logic can simulate all of Turing-level\ncomputation (Chapter 11, Boolos et al. 2007). The second justification\ncomes from the role logic plays in foundational theories of\nmathematics and mathematical reasoning. Not only are foundational\ntheories of mathematics cast in logic (Potter 2004), but there have\nbeen successful projects resulting in machine verification of ordinary\nnon-trivial theorems, e.g., in the\n Mizar project\n alone around 50,000 theorems have been verified (Naumowicz and\nKornilowicz 2009). The argument goes that if any approach to AI can be\ncast mathematically, then it can be cast in a logicist form.  \nNeedless to say, such a declaration has been carefully considered by\nlogicists beyond the reductionistic argument given above. For example,\nRosenschein and Kaelbling (1986) describe a method in which logic is\nused to specify finite state machines. These machines are used at\n“run time” for rapid, reactive processing. In this\napproach, though the finite state machines contain no logic in the\ntraditional sense, they are produced by logic and inference. Real\nrobot control via first-order theorem proving has been demonstrated by\nAmir and Maynard-Reid (1999, 2000, 2001). In fact, you can\n download\n version 2.0 of the software that makes this approach real for a Nomad\n200 mobile robot in an office environment. Of course, negotiating an\noffice environment is a far cry from the rapid adjustments an\noutfielder for the Yankees routinely puts on display, but certainly\nit’s an open question as to whether future machines will be able\nto mimic such feats through rapid reasoning. The question is open if\nfor no other reason than that all must concede that the constant\nincrease in reasoning speed of first-order theorem provers is\nbreathtaking. (For up-to-date news on this increase, visit and monitor\nthe\n TPTP site.)\n There is no known reason why the software engineering in question\ncannot continue to produce speed gains that would eventually allow an\nartificial creature to catch a fly ball by processing information in\npurely logicist fashion.  \nNow we come to the second topic related to logicist AI that warrants\nmention herein: common logic and the intensifying quest for\ninteroperability between logic-based systems using different logics.\nOnly a few brief comments are\n offered.[29]\n Readers wanting more can explore the links provided in the course of\nthe summary.  \nOne standardization is through what is known as\n Common Logic\n (CL), and variants thereof. (CL is published as an\n ISO standard\n – ISO is the International Standards Organization.)\nPhilosophers interested in logic, and of course logicians, will find\nCL to be quite fascinating. From an historical perspective, the advent\nof CL is interesting in no small part because the person spearheading\nit is none other than Pat Hayes, the same Hayes who, as we have seen,\nworked with McCarthy to establish logicist AI in the 1960s. Though\nHayes was not at the original 1956 Dartmouth conference, he certainly\nmust be regarded as one of the founders of contemporary AI.) One of\nthe interesting things about CL, at least as we see it, is that it\nsignifies a trend toward the marriage of logics, and programming\nlanguages and environments. Another system that is a logic/programming\nhybrid is\n Athena,\n which can be used as a programming language, and is at the same time\na form of MSL. Athena is based on formal systems known as\ndenotational proof languages (Arkoudas 2000).  \nHow is interoperability between two systems to be enabled by CL?\nSuppose one of these systems is based on logic \\(L\\), and the other on\n\\(L'\\). (To ease exposition, assume that both logics are first-order.)\nThe idea is that a theory \\(\\Phi_L\\), that is, a set of formulae in\n\\(L\\), can be translated into CL, producing \\(\\Phi_{CL}\\), and then\nthis theory can be translated into \\(\\Phi_L'\\). CL thus becomes an\ninter lingua. Note that what counts as a well-formed formula in\n\\(L\\) can be different than what counts as one in \\(L'\\). The two\nlogics might also have different proof theories. For example,\ninference in \\(L\\) might be based on resolution, while inference in\n\\(L'\\) is of the natural deduction variety. Finally, the symbol sets\nwill be different. Despite these differences, courtesy of the\ntranslations, desired behavior can be produced across the translation.\nThat, at any rate, is the hope. The technical challenges here are\nimmense, but federal monies are increasingly available for attacks on\nthe problem of interoperability. \nNow for the third topic in this section: what can be called\nencoding down. The technique is easy to understand. Suppose\nthat we have on hand a set \\(\\Phi\\) of first-order axioms. As is\nwell-known, the problem of deciding, for arbitrary formula \\(\\phi\\),\nwhether or not it’s deducible from \\(\\Phi\\) is\nTuring-undecidable: there is no Turing machine or equivalent that can\ncorrectly return “Yes” or “No” in the general\ncase. However, if the domain in question is finite, we can encode this\nproblem down to the propositional calculus. An assertion that all\nthings have \\(F\\) is of course equivalent to the assertion that\n\\(Fa\\), \\(Fb\\), \\(Fc\\), as long as the domain contains only these\nthree objects. So here a first-order quantified formula becomes a\nconjunction in the propositional calculus. Determining whether such\nconjunctions are provable from axioms themselves expressed in the\npropositional calculus is Turing-decidable, and in addition, in\ncertain clusters of cases, the check can be done very quickly in the\npropositional case; very quickly. Readers interested in\nencoding down to the propositional calculus should consult recent\n DARPA-sponsored work by Bart Selman.\n Please note that the target of encoding down doesn’t need to be\nthe propositional calculus. Because it’s generally harder for\nmachines to find proofs in an intensional logic than in straight\nfirst-order logic, it is often expedient to encode down the former to\nthe latter. For example, propositional modal logic can be encoded in\nmulti-sorted logic (a variant of FOL); see (Arkoudas & Bringsjord\n2005). Prominent usage of such an encoding down can be found in a set\nof systems known as Description Logics, which are a set of\nlogics less expressive than first-order logic but more expressive than\npropositional logic (Baader et al. 2003). Description logics are used\nto reason about ontologies in a given domain and have been\nsuccessfully used, for example, in the biomedical domain (Smith et al.\n2007).  \nIt’s tempting to define non-logicist AI by negation: an approach\nto building intelligent agents that rejects the distinguishing\nfeatures of logicist AI. Such a shortcut would imply that the agents\nengineered by non-logicist AI researchers and developers, whatever the\nvirtues of such agents might be, cannot be said to know that \\(\\phi\\);\n– for the simple reason that, by negation, the non-logicist\nparadigm would have not even a single declarative proposition that is\na candidate for \\(\\phi\\);. However, this isn’t a particularly\nenlightening way to define non-symbolic AI. A more productive approach\nis to say that non-symbolic AI is AI carried out on the basis of\nparticular formalisms other than logical systems, and to then\nenumerate those formalisms. It will turn out, of course, that these\nformalisms fail to include knowledge in the normal sense. (In\nphilosophy, as is well-known, the normal sense is one according to\nwhich if \\(p\\) is known, \\(p\\) is a declarative statement.)  \nFrom the standpoint of formalisms other than logical systems,\nnon-logicist AI can be partitioned into symbolic but non-logicist\napproaches, and connectionist/neurocomputational approaches. (AI\ncarried out on the basis of symbolic, declarative structures that, for\nreadability and ease of use, are not treated directly by researchers\nas elements of formal logics, does not count. In this category fall\ntraditional semantic networks, Schank’s (1972) conceptual\ndependency scheme, frame-based schemes, and other such schemes.) The\nformer approaches, today, are probabilistic, and are based on the\nformalisms (Bayesian networks) covered\n below.\n The latter approaches are based, as we have noted, on formalisms that\ncan be broadly termed “neurocomputational.” Given our\nspace constraints, only one of the formalisms in this category is\ndescribed here (and briefly at that): the aforementioned artificial\nneural\n networks.[30].\n Though artificial neural networks, with an appropriate architecture,\ncould be used for arbitrary computation, they are almost exclusively\nused for building learning systems.  \nNeural nets are composed of units or nodes designed to\nrepresent neurons, which are connected by links designed to\nrepresent dendrites, each of which has a numeric weight. \nA “Neuron” Within an Artificial Neural Network (from\nAIMA3e) \nIt is usually assumed that some of the units work in symbiosis with\nthe external environment; these units form the sets of input\nand output units. Each unit has a current activation\nlevel, which is its output, and can compute, based on its inputs\nand weights on those inputs, its activation level at the next moment\nin time. This computation is entirely local: a unit takes account of\nbut its neighbors in the net. This local computation is calculated in\ntwo stages. First, the input function, \\(in_i\\), gives the\nweighted sum of the unit’s input values, that is, the sum of the\ninput activations multiplied by their weights:  \nIn the second stage, the activation function, \\(g\\), takes the\ninput from the first stage as argument and generates the output, or\nactivation level, \\(a_i\\):  \nOne common (and confessedly elementary) choice for the activation\nfunction (which usually governs all units in a given net) is the step\nfunction, which usually has a threshold \\(t\\) that sees to it that a 1\nis output when the input is greater than \\(t\\), and that 0 is output\notherwise. This is supposed to be “brain-like” to some\ndegree, given that 1 represents the firing of a pulse from a neuron\nthrough an axon, and 0 represents no firing. A simple three-layer\nneural net is shown in the following picture. \nA Simple Three-Layer Artificial Neural Network (from\nAIMA3e) \nAs you might imagine, there are many different kinds of neural\nnetworks. The main distinction is between feed-forward and\nrecurrent networks. In feed-forward networks like the one\npictured immediately above, as their name suggests, links move\ninformation in one direction, and there are no cycles; recurrent\nnetworks allow for cycling back, and can become rather complicated.\nFor a more detailed presentation, see the \n\n Supplement on Neural Nets.\n  \nNeural networks were fundamentally plagued by the fact that while they\nare simple and have theoretically efficient learning algorithms, when\nthey are multi-layered and thus sufficiently expressive to represent\nnon-linear functions, they were very hard to train in practice. This\nchanged in the mid 2000s with the advent of methods that exploit\nstate-of-the-art hardware better (Rajat et al. 2009). The\nbackpropagation method for training multi-layered neural networks can\nbe translated into a sequence of repeated simple arithmetic operations\non a large set of numbers. The general trend in computing hardware has\nfavored algorithms that are able to do a large of number of simple\noperations that are not that dependent on each other, versus a small\nof number of complex and intricate operations. \nAnother key recent observation is that deep neural networks can be\npre-trained first in an unsupervised phase where they are just fed\ndata without any labels for the data. Each hidden layer is forced to\nrepresent the outputs of the layer below. The outcome of this training\nis a series of layers which represent the input domain with increasing\nlevels of abstraction. For example, if we pre-train the network with\nimages of faces, we would get a first layer which is good at detecting\nedges in images, a second layer which can combine edges to form facial\nfeatures such as eyes, noses etc., a third layer which responds to\ngroups of features, and so on (LeCun et al. 2015). \nPerhaps the best technique for teaching students about neural networks\nin the context of other statistical learning formalisms and methods is\nto focus on a specific problem, preferably one that seems unnatural to\ntackle using logicist techniques. The task is then to seek to engineer\na solution to the problem, using any and all techniques\navailable. One nice problem is handwriting recognition (which\nalso happens to have a rich philosophical dimension; see e.g.\nHofstadter & McGraw 1995). For example, consider the problem of\nassigning, given as input a handwritten digit \\(d\\), the correct\ndigit, 0 through 9. Because there is a database of 60,000 labeled\ndigits available to researchers (from the National Institute of\nScience and Technology), this problem has evolved into a benchmark\nproblem for comparing learning algorithms. It turns out that neural\nnetworks currently reign as the best approach to the problem according\nto a recent ranking by Benenson (2016). \nReaders interested in AI (and computational cognitive science) pursued\nfrom an overtly brain-based orientation are encouraged to explore the\nwork of Rick Granger (2004a, 2004b) and researchers in his\n Brain Engineering Laboratory\n and\n W. H. Neukom Institute for Computational Sciences.\n The contrast between the “dry”, logicist AI started at\nthe original 1956 conference, and the approach taken here by Granger\nand associates (in which brain circuitry is directly modeled) is\nremarkable. For those interested in computational properties of neural\nnetworks, Hornik et al. (1989) address the general representation\ncapability of neural networks independent of learning.  \nAt this point the reader has been exposed to the chief formalisms in\nAI, and may wonder about heterogeneous approaches that bridge them. Is\nthere such research and development in AI? Yes. From an\nengineering standpoint, such work makes irresistibly good\nsense. There is now an understanding that, in order to build\napplications that get the job done, one should choose from a toolbox\nthat includes logicist, probabilistic/Bayesian, and neurocomputational\ntechniques. Given that the original top-down logicist paradigm is\nalive and thriving (e.g., see Brachman & Levesque 2004, Mueller\n2006), and that, as noted, a resurgence of Bayesian and\nneurocomputational approaches has placed these two paradigms on solid,\nfertile footing as well, AI now moves forward, armed with this\nfundamental triad, and it is a virtual certainty that applications\n(e.g., robots) will be engineered by drawing from elements of all\nthree. Watson’s DeepQA architecture is one recent example of an\nengineering system that leverages multiple paradigms. For a detailed\ndiscussion, see the \n\n Supplement on Watson’s DeepQA Architecture.\n  \nGoogle DeepMind’s AlphaGo is another example of a multi-paradigm\nsystem, although in a much narrower form than Watson. The central\nalgorithmic problem in games such as Go or Chess is to search through\na vast sequence of valid moves. For most non-trivial games, this is\nnot feasible to do so exhaustively. The Monte Carlo tree search (MCTS)\nalgorithm gets around this obstacle by searching through an enormous\nspace of valid moves in a statistical fashion (Browne et al. 2012).\nWhile MCTS is the central algorithm in AlpaGo, there are two neural\nnetworks which help evaluate states in the game and help model how\nexpert opponents play (Silver et al. 2016). It should be noted that\nMCTS is behind almost all the winning submissions in general game\nplaying (Finnsson 2012). \nWhat, though, about deep, theoretical integration of the main\nparadigms in AI? Such integration is at present only a possibility for\nthe future, but readers are directed to the research of some striving\nfor such integration. For example: Sun (1994, 2002) has been working\nto demonstrate that human cognition that is on its face symbolic in\nnature (e.g., professional philosophizing in the analytic tradition,\nwhich deals explicitly with arguments and definitions carefully\nsymbolized) can arise from cognition that is neurocomputational in\nnature. Koller (1997) has investigated the marriage between\nprobability theory and logic. And, in general, the very recent arrival\nof so-called human-level AI is being led by theorists seeking\nto genuinely integrate the three paradigms set out above (e.g.,\nCassimatis 2006). \nFinally, we note that cognitive architectures such as Soar\n(Laird 2012) and PolyScheme (Cassimatis 2006) are another area where\nintegration of different fields of AI can be found. For example, one\nsuch endeavor striving to build human-level AI is the Companions\nproject (Forbus and Hinrichs 2006). Companions are long-lived systems\nthat strive to be human-level AI systems that function as\ncollaborators with humans. The Companions architecture tries to solve\nmultiple AI problems such as reasoning and learning, interactivity,\nand longevity in one unifying system. \nAs we noted above, work on AI has mushroomed over the past couple of\ndecades. Now that we have looked a bit at the content that composes\nAI, we take a quick look at the explosive growth of AI. \nFirst, a point of clarification. The growth of which we speak is not a\nshallow sort correlated with amount of funding provided for a given\nsub-field of AI. That kind of thing happens all the time in all\nfields, and can be triggered by entirely political and financial\nchanges designed to grow certain areas, and diminish others. Along the\nsame line, the growth of which we speak is not correlated with the\namount of industrial activity revolving around AI (or a sub-field\nthereof); for this sort of growth too can be driven by forces quite\noutside an expansion in the scientific breadth of\n AI.[31]\n Rather, we are speaking of an explosion of deep content: new\nmaterial which someone intending to be conversant with the field needs\nto know. Relative to other fields, the size of the explosion may or\nmay not be unprecedented. (Though it should perhaps be noted that an\nanalogous increase in philosophy would be marked by the development of\nentirely new formalisms for reasoning, reflected in the fact that,\nsay, longstanding philosophy textbooks like Copi’s (2004)\nIntroduction to Logic are dramatically rewritten and enlarged\nto include these formalisms, rather than remaining anchored to\nessentially immutable core formalisms, with incremental refinement\naround the edges through the years.) But it certainly appears to be\nquite remarkable, and is worth taking note of here, if for no other\nreason than that AI’s near-future will revolve in significant\npart around whether or not the new content in question forms a\nfoundation for new long-lived research and development that would not\notherwise\n obtain.[32] \nAI has also witnessed an explosion in its usage in various artifacts\nand applications. While we are nowhere near building a machine with\ncapabilities of a human or one that acts rationally in all scenarios\naccording to the Russell/Hutter definition above, algorithms that have\ntheir origins in AI research are now widely deployed for many tasks in\na variety of domains.  \nA huge part of AI’s growth in applications has been made\npossible through invention of new algorithms in the subfield of\nmachine learning. Machine learning is concerned with building\nsystems that improve their performance on a task when given examples\nof ideal performance on the task, or improve their performance with\nrepeated experience on the task. Algorithms from machine learning have\nbeen used in speech recognition systems, spam filters, online\nfraud-detection systems, product-recommendation systems, etc. The\ncurrent state-of-the-art in machine learning can be divided into three\nareas (Murphy 2013, Alpaydin 2014): \nIn addition to being used in domains that are traditionally the ken of\nAI, machine-learning algorithms have also been used in all stages of\nthe scientific process. For example, machine-learning techniques are\nnow routinely applied to analyze large volumes of data generated from\nparticle accelerators. CERN, for instance, generates a petabyte\n(\\(10^{15}\\) bytes) per second, and statistical algorithms that have\ntheir origins in AI are used to filter and analyze this data. Particle\naccelerators are used in fundamental experimental research in physics\nto probe the structure of our physical universe. They work by\ncolliding larger particles together to create much finer particles.\nNot all such events are fruitful. Machine-learning methods have been\nused to select events which are then analyzed further (Whiteson &\nWhiteson 2009 and Baldi et al. 2014). More recently, researchers at\nCERN launched a machine learning\n  competition\n to aid in the analysis of the Higgs Boson. The goal of this challenge\nwas to develop algorithms that separate meaningful events from\nbackground noise given data from the Large Hadron Collider, a particle\naccelerator at CERN. \nIn the past few decades, there has been an explosion in data that does\nnot have any explicit semantics attached to it. This data is generated\nby both humans and machines. Most of this data is not easily\nmachine-processable; for example, images, text, video (as opposed to\ncarefully curated data in a knowledge- or data-base). This has given\nrise to a huge industry that applies AI techniques to get usable\ninformation from such enormous data. This field of applying techniques\nderived from AI to large volumes of data goes by names such as\n“data mining,” “big data,”\n“analytics,” etc. This field is too vast to even\nmoderately cover in the present article, but we note that there is no\nfull agreement on what constitutes such a “big-data”\nproblem. One definition, from Madden (2012), is that big data differs\nfrom traditional machine-processable data in that it is too big (for\nmost of the existing state-of-the-art hardware), too quick (generated\nat a fast rate, e.g. online email transactions), or too hard. It is in\nthe too-hard part that AI techniques work quite well. While this\nuniverse is quite varied, we use the Watson’s system later in\nthis article as an AI-relevant exemplar. As we will see later, while\nmost of this new explosion is powered by learning, it isn’t\nentirely limited to just learning. This bloom in learning algorithms\nhas been supported by both a resurgence in neurocomputational\ntechniques and probabilistic techniques. \nOne of the remarkable aspects of (Charniak & McDermott 1985) is\nthis: The authors say the central dogma of AI is that “What the\nbrain does may be thought of at some level as a kind of\ncomputation” (p. 6). And yet nowhere in the book is brain-like\ncomputation discussed. In fact, you will search the index in vain for\nthe term ‘neural’ and its variants. Please note that the\nauthors are not to blame for this. A large part of AI’s growth\nhas come from formalisms, tools, and techniques that are, in some\nsense, brain-based, not logic-based. A paper that conveys the\nimportance and maturity of neurocomputation is (Litt et al. 2006).\n(Growth has also come from a return of probabilistic techniques that\nhad withered by the mid-70s and 80s. More about that momentarily, in\nthe next “resurgence”\n section.)\n  \nOne very prominent class of non-logicist formalism does make an\nexplicit nod in the direction of the brain: viz., artificial neural\nnetworks (or as they are often simply called, neural\nnetworks, or even just neural nets). (The structure of\nneural networks and more recent developments are discussed\n above).\n Because Minsky and Pappert’s (1969) Perceptrons led many\n(including, specifically, many sponsors of AI research and\ndevelopment) to conclude that neural networks didn’t have\nsufficient information-processing power to model human cognition, the\nformalism was pretty much universally dropped from AI. However, Minsky\nand Pappert had only considered very limited neural networks.\nConnectionism, the view that intelligence consists not in\nsymbolic processing, but rather non-symbolic processing at\nleast somewhat like what we find in the brain (at least at the\ncellular level), approximated specifically by artificial neural\nnetworks, came roaring back in the early 1980s on the strength of more\nsophisticated forms of such networks, and soon the situation was (to\nuse a metaphor introduced by John McCarthy) that of two horses in a\nrace toward building truly intelligent agents. \nIf one had to pick a year at which connectionism was resurrected, it\nwould certainly be 1986, the year Parallel Distributed\nProcessing (Rumelhart & McClelland 1986) appeared in print.\nThe rebirth of connectionism was specifically fueled by the\nback-propagation (backpropagation) algorithm over neural networks,\nnicely covered in Chapter 20 of AIMA. The\nsymbolicist/connectionist race led to a spate of lively debate in the\nliterature (e.g., Smolensky 1988, Bringsjord 1991), and some AI\nengineers have explicitly championed a methodology marked by a\nrejection of knowledge representation and reasoning. For example,\nRodney Brooks was such an engineer; he wrote the well-known\n“Intelligence Without Representation” (1991), and his Cog\nProject, to which we referred above, is arguably an incarnation of the\npremeditatedly non-logicist approach. Increasingly, however, those in\nthe business of building sophisticated systems find that both\nlogicist and more neurocomputational techniques are required (Wermter\n& Sun\n 2001).[33]\n In addition, the neurocomputational paradigm today includes\nconnectionism only as a proper part, in light of the fact that some of\nthose working on building intelligent systems strive to do so by\nengineering brain-based computation outside the neural network-based\napproach (e.g., Granger 2004a, 2004b).  \nAnother recent resurgence in neurocomputational techniques has\noccurred in machine learning. The modus operandi in machine learning\nis that given a problem, say recognizing handwritten digits\n\\(\\{0,1,\\ldots,9\\}\\) or faces, from a 2D matrix representing an image\nof the digits or faces, a machine learning or a domain expert would\nconstruct a feature vector representation function for the\ntask. This function is a transformation of the input into a format\nthat tries to throw away irrelevant information in the input and keep\nonly information useful for the task. Inputs transformed by \\(\\rr\\)\nare termed features. For recognizing faces, irrelevant\ninformation could be the amount of lighting in the scene and relevant\ninformation could be information about facial features. The machine is\nthen fed a sequence of inputs represented by the features and the\nideal or ground truth output values for those inputs. This converts\nthe learning challenge from that of having to learn the function\n\\(\\ff\\) from the examples: \\(\\left\\{\\left\\langle x_1,\n\\ff(x_1)\\right\\rangle,\\left\\langle x_2, \\ff(x_2)\\right\\rangle, \\ldots,\n\\left\\langle x_n, \\ff(x_n)\\right\\rangle \\right\\}\\) to having to learn\nfrom possibly easier data: \\(\\left\\{\\left\\langle \\rr(x_1),\n\\ff(x_1)\\right\\rangle,\\left\\langle \\rr(x_2), \\ff(x_2)\\right\\rangle,\n\\ldots, \\left\\langle \\rr(x_n), \\ff(x_n)\\right\\rangle \\right\\}\\). Here\nthe function \\(\\rr\\) is the function that computes the feature vector\nrepresentation of the input. Formally, \\(\\ff\\) is assumed to be a\ncomposition of the functions \\(\\gg\\) and \\(\\rr\\). That is, for any\ninput \\(x\\), \\(f(x) = \\gg\\left(\\rr\\left(x\\right)\\right)\\). This is\ndenoted by \\(\\ff=\\gg\\circ \\rr\\). For any input, the features are first\ncomputed, and then the function \\(\\gg\\) is applied. If the feature\nrepresentation \\(\\rr\\) is provided by the domain expert, the learning\nproblem becomes simpler to the extent the feature representation takes\non the difficulty of the task. At one extreme, the feature vector\ncould hide an easily extractable form of the answer in the input and\nin the other extreme the feature representation could be just the\nplain input. \nFor non-trivial problems, choosing the right representation is vital.\nFor instance, one of the drastic changes in the AI landscape was due\nto Minsky and Papert’s (1969) demonstration that the perceptron\ncannot learn even the binary XOR function, but this function\ncan be learnt by the perceptron if we have the right representation.\nFeature engineering has grown to be one of the most labor intensive\ntasks of machine learning, so much so that it is considered to be one\nof the “black arts” of machine learning. The other\nsignificant black art of learning methods is choosing the right\nparameters. These black arts require significant human expertise and\nexperience, which can be quite difficult to obtain without significant\napprenticeship (Domingos 2012). Another bigger issue is that the task\nof feature engineering is just knowledge representation in a new skin.\n \nGiven this state of affairs, there has been a recent resurgence in\nmethods for automatically learning a feature representation function\n\\(\\rr\\); such methods potentially bypass a large part of human labor\nthat is traditionally required. Such methods are based mostly on what\nare now termed deep neural networks. Such networks are simply\nneural networks with two or more hidden layers. These networks allow\nus to learn a feature function \\(\\rr\\) by using one or more of the\nhidden layers to learn \\(\\rr\\). The general form of learning in which\none learns from the raw sensory data without much hand-based feature\nengineering has now its own term: deep learning. A general and\nyet concise definition (Bengio et al. 2015) is:  \nThough the idea has been around for decades, recent innovations\nleading to more efficient learning techniques have made the approach\nmore feasible (Bengio et al. 2013). Deep-learning methods have\nrecently produced state-of-the-art results in image recognition (given\nan image containing various objects, label the objects from a given\nset of labels), speech recognition (from audio input, generate a\ntextual representation), and the analysis of data from particle\naccelerators (LeCun et al. 2015). Despite impressive results in tasks\nsuch as these, minor and major issues remain unresolved. A minor issue\nis that significant human expertise is still needed to choose an\narchitecture and set up the right parameters for the architecture; a\nmajor issue is the existence of so-called adversarial inputs,\nwhich are indistinguishable from normal inputs to humans but are\ncomputed in a special manner that makes a neural network regard them\nas different than similar inputs in the training data. The existence\nof such adversarial inputs, which remain stable across training data,\nhas raised doubts about how well performance on benchmarks can\ntranslate into performance in real-world systems with sensory noise\n(Szegedy et al. 2014). \nThere is a second dimension to the explosive growth of AI: the\nexplosion in popularity of probabilistic methods that aren’t\nneurocomputational in nature, in order to formalize and mechanize a\nform of non-logicist reasoning in the face of uncertainty.\nInterestingly enough, it is Eugene Charniak himself who can be safely\nconsidered one of the leading proponents of an explicit, premeditated\nturn away from logic to statistical techniques. His area of\nspecialization is natural language processing, and whereas his\nintroductory textbook of 1985 gave an accurate sense of his approach\nto parsing at the time (as we have seen, write computer programs that,\ngiven English text as input, ultimately infer meaning expressed in\nFOL), this approach was abandoned in favor of purely statistical\napproaches (Charniak 1993). At the\n AI@50\n conference, Charniak boldly proclaimed, in a talk tellingly entitled\n“Why Natural Language Processing is Now Statistical Natural\nLanguage Processing,” that logicist AI is moribund, and that the\nstatistical approach is the only promising game in town – for\nthe next 50\n years.[34] \nThe chief source of energy and debate at the conference flowed from\nthe clash between Charniak’s probabilistic orientation, and the\noriginal logicist orientation, upheld at the conference in question by\nJohn McCarthy and others. \nAI’s use of probability theory grows out of the standard form of\nthis theory, which grew directly out of technical philosophy and\nlogic. This form will be familiar to many philosophers, but\nlet’s review it quickly now, in order to set a firm stage for\nmaking points about the new probabilistic techniques that have\nenergized AI.  \nJust as in the case of FOL, in probability theory we are concerned\nwith declarative statements, or propositions, to which degrees\nof belief are applied; we can thus say that both logicist and\nprobabilistic approaches are symbolic in nature. Both approaches also\nagree that statements can either be true or false in the world. In\nbuilding agents, a simplistic logic-based approach requires agents to\nknow the truth-value of all possible statements. This is not\nrealistic, as an agent may not know the truth-value of some\nproposition \\(p\\) due to either ignorance, non-determinism in the\nphysical world, or just plain vagueness in the meaning of the\nstatement. More specifically, the fundamental proposition in\nprobability theory is a random variable, which can be conceived\nof as an aspect of the world whose status is initially unknown to the\nagent. We usually capitalize the names of random variables, though we\nreserve \\(p,q,r, \\ldots\\) as such names as well. For example, in a\nparticular murder investigation centered on whether or not Mr. Barolo\ncommitted the crime, the random variable \\(Guilty\\) might be of\nconcern. The detective may be interested as well in whether or not the\nmurder weapon – a particular knife, let us assume –\nbelongs to Barolo. In light of this, we might say that \\(\\Weapon =\n\\true\\) if it does, and \\(\\Weapon = \\false\\) if it doesn’t. As a\nnotational convenience, we can write \\(weapon\\) and \\(\\lnot weapon\\)\nand for these two cases, respectively; and we can use this convention\nfor other variables of this type.  \nThe kind of variables we have described so far are\n\\(\\mathbf{Boolean}\\), because their \\(\\mathbf{domain}\\) is simply\n\\(\\{true,false\\}.\\) But we can generalize and allow\n\\(\\mathbf{discrete}\\) random variables, whose values are from any\ncountable domain. For example, \\(\\PriceTChina\\) might be a variable\nfor the price of (a particular, presumably) tea in China, and its\ndomain might be \\(\\{1,2,3,4,5\\}\\), where each number here is in US\ndollars. A third type of variable is \\(\\mathbf{continous}\\); its\ndomain is either the reals, or some subset thereof.  \nWe say that an atomic event is an assignment of particular\nvalues from the appropriate domains to all the variables composing the\n(idealized) world. For example, in the simple murder investigation\nworld introduced just above, we have two Boolean variables,\n\\(\\Guilty\\) and \\(\\Weapon\\), and there are just four atomic events.\nNote that atomic events have some obvious properties. For example,\nthey are mutually exclusive, exhaustive, and logically entail the\ntruth or falsity of every proposition. Usually not obvious to\nbeginning students is a fourth property, namely, any proposition is\nlogically equivalent to the disjunction of all atomic events that\nentail that proposition.  \nPrior probabilities correspond to a degree of belief accorded to a\nproposition in the complete absence of any other information. For\nexample, if the prior probability of Barolo’s guilt is \\(0.2\\),\nwe write \n\n    \\[\n    P\\left(\\Guilty=true\\right)=0.2\n    \\]\n\n  \nor simply \\(\\P(guilty)=0.2\\). It is often convenient to have a\nnotation allowing one to refer economically to the probabilities of\nall the possible values for a random variable. For example,\nwe can write \n\n    \\[\n    \\P\\left(\\PriceTChina\\right)\n    \\]\n\n  \nas an abbreviation for the five equations listing all the possible\nprices for tea in China. We can also write \n\n    \\[\n    \\P\\left(\\PriceTChina\\right)=\\langle 1,2,3,4,5\\rangle\n    \\]\n\n  \nIn addition, as further convenient notation, we can write \\(\n\\mathbf{P}\\left(\\Guilty, \\Weapon\\right)\\) to denote the probabilities\nof all combinations of values of the relevant set of random variables.\nThis is referred to as the joint probability distribution of\n\\(\\Guilty\\) and \\(\\Weapon\\). The full joint probability\ndistribution covers the distribution for all the random variables used\nto describe a world. Given our simple murder world, we have 20 atomic\nevents summed up in the equation \n\n    \\[\n    \\mathbf{P}\\left(\\Guilty, \\Weapon, \\PriceTChina\\right)\n    \\]\n\n  \nThe final piece of the basic language of probability theory\ncorresponds to conditional probabilities. Where \\(p\\) and \\(q\\)\nare any propositions, the relevant expression is \\(P\\!\\left(p\\given\nq\\right)\\), which can be interpreted as “the probability of\n\\(p\\), given that all we know is \\(q\\).” For example,\n\n    \\[\n    P\\left(guilty\\ggiven weapon\\right)=0.7\n    \\]\n\n  \nsays that if the murder weapon belongs to Barolo, and no other\ninformation is available, the probability that Barolo is guilty is\n\\(0.7.\\)  \nAndrei Kolmogorov showed how to construct probability theory from\nthree axioms that make use of the machinery now introduced, viz.,  \nThese axioms are clearly at bottom logicist. The remainder of\nprobability theory can be erected from this foundation (conditional\nprobabilities are easily defined in terms of prior probabilities). We\ncan thus say that logic is in some fundamental sense still being used\nto characterize the set of beliefs that a rational agent can have. But\nwhere does probabilistic inference enter the picture on this\naccount, since traditional deduction is not used for inference in\nprobability theory? \nProbabilistic inference consists in computing, from observed evidence\nexpressed in terms of probability theory, posterior probabilities of\npropositions of interest. For a good long while, there have been\nalgorithms for carrying out such computation. These algorithms precede\nthe resurgence of probabilistic techniques in the 1990s. (Chapter 13\nof AIMA presents a number of them.) For example, given the\nKolmogorov axioms, here is a straightforward way of computing the\nprobability of any proposition, using the full joint distribution\ngiving the probabilities of all atomic events: Where \\(p\\) is some\nproposition, let \\(\\alpha(p)\\) be the disjunction of all atomic events\nin which \\(p\\) holds. Since the probability of a proposition (i.e.,\n\\(P(p)\\)) is equal to the sum of the probabilities of the atomic\nevents in which it holds, we have an equation that provides a method\nfor computing the probability of any proposition \\(p\\), viz.,  \nUnfortunately, there were two serious problems infecting this original\nprobabilistic approach: One, the processing in question needed to take\nplace over paralyzingly large amounts of information (enumeration over\nthe entire distribution is required). And two, the expressivity of the\napproach was merely propositional. (It was by the way the philosopher\nHilary Putnam (1963) who pointed out that there was a price to pay in\nmoving to the first-order level. The issue is not discussed herein.)\nEverything changed with the advent of a new formalism that marks the\nmarriage of probabilism and graph theory: Bayesian networks\n(also called belief nets). The pivotal text was (Pearl 1988).\nFor a more detailed discussion, see the \n\n Supplement on Bayesian Networks.\n  \nBefore concluding this section, it is probably worth noting that, from\nthe standpoint of philosophy, a situation such as the murder\ninvestigation we have exploited above would often be analyzed into\narguments, and strength factors, not into numbers to be\ncrunched by purely arithmetical procedures. For example, in the\nepistemology of Roderick Chisholm, as presented his Theory of\nKnowledge (1966, 1977), Detective Holmes might classify a\nproposition like  Barolo committed the murder. as\ncounterbalanced if he was unable to find a compelling argument\neither way, or perhaps probable if the murder weapon turned out\nto belong to Barolo. Such categories cannot be found on a continuum\nfrom 0 to 1, and they are used in articulating arguments for or\nagainst Barolo’s guilt. Argument-based approaches to uncertain\nand defeasible reasoning are virtually non-existent in AI. One\nexception is Pollock’s approach, covered below. This approach is\nChisholmian in nature.  \nIt should also be noted that there have been well-established\nformalisms for dealing with probabilistic reasoning as an instance of\nlogic-based reasoning. E.g., the activity a researcher in\nprobabilistic reasoning undertakes when she proves a theorem \\(\\phi\\)\nabout their domain (e.g. any theorem in (Pearl 1988)) is purely within\nthe realm of traditional logic. Readers interested in logic-flavored\napproaches to probabilistic reasoning can consult (Adams 1996,\nHailperin 1996 & 2010, Halpern 1998). Formalisms marrying\nprobability theory, induction and deductive reasoning, placing them on\nan equal footing, have been on the rise, with Markov logic (Richardson\nand Domingos 2006) being salient among these approaches.  \nProbabilistic Machine Learning \nMachine learning, in the sense given\n above,\n has been associated with probabilistic techniques. Probabilistic\ntechniques have been associated with both the learning of functions\n(e.g. Naive Bayes classification) and the modeling of theoretical\nproperties of learning algorithms. For example, a standard\nreformulation of supervised learning casts it as a Bayesian\nproblem. Assume that we are looking at recognizing digits\n\\([0{-}9]\\) from a given image. One way to cast this problem is to ask\nwhat the probability that the hypothesis \\(H_x\\): “the digit\nis \\(x\\)” is true given the image \\(d\\) from a sensor. Bayes\ntheorem gives us:  \n\\(P(d\\given H_x)\\) and \\(P(H_x)\\) can be estimated from the given\ntraining dataset. Then the hypothesis with the highest posterior\nprobability is then given as the answer and is given by:\n\\(\\argmax_{x}P\\left(d\\ggiven H_x\\right)*P\\left(H_x\\right) \\) In\naddition to probabilistic methods being used to build algorithms,\nprobability theory has also been used to analyze algorithms which\nmight not have an overt probabilistic or logical formulation. For\nexample, one of the central classes of meta-theorems in learning,\nprobably approximately correct (PAC) theorems, are cast in\nterms of lower bounds of the probability that the mismatch between the\ninduced/learnt fL function and the true function\nfT being less than a certain amount, given that the\nlearnt function fL works well for a certain number\nof cases (see Chapter 18, AIMA).  \nFrom at least its modern inception, AI has always been connected to\ngadgets, often ones produced by corporations, and it would be remiss\nof us not to say a few words about this phenomenon. While there have\nbeen a large number of commercial in-the-wild success stories for AI\nand its sister fields, such as optimization and decision-making, some\napplications are more visible and have been thoroughly battle-tested\nin the wild. In 2014, one of the most visible such domains (one in\nwhich AI has been strikingly successful) is information retrieval,\nincarnated as web search. Another recent success story is pattern\nrecognition. The state-of-the-art in applied pattern recognition\n(e.g., fingerprint/face verification, speech recognition, and\nhandwriting recognition) is robust enough to allow\n“high-stakes” deployment outside the laboratory. As of mid\n2018, several corporations and research laboratories have begun\ntesting autonomous vehicles on public roads, with even a handful of\njurisdictions making self-driving cars legal to operate. For example,\nGoogle’s autonomous cars have navigated hundreds of thousands of\nmiles in California with minimal human help under non-trivial\nconditions (Guizzo 2011).  \nComputer games provide a robust test bed for AI techniques as they can\ncapture important parts that might be necessary to test an AI\ntechnique while abstracting or removing details that might beyond the\nscope of core AI research, for example, designing better hardware or\ndealing with legal issues (Laird and VanLent 2001). One subclass of\ngames that has seen quite fruitful for commercial deployment of AI is\nreal-time strategy games. Real-time strategy games are games in which\nplayers manage an army given limited resources. One objective is to\nconstantly battle other players and reduce an opponent’s forces.\nReal-time strategy games differ from strategy games in that players\nplan their actions simultaneously in real-time and do not have to take\nturns playing. Such games have a number of challenges that are\ntantalizing within the grasp of the state-of-the-art. This makes such\ngames an attractive venue in which to deploy simple AI agents. An\noverview of AI used in real-time strategy games can be found in\n(Robertson and Watson 2015).  \nSome other ventures in AI, despite significant success, have been only\nchugging slowly and humbly along, quietly. For instance, AI-related\nmethods have achieved triumphs in solving open problems in mathematics\nthat have resisted any solution for decades. The most noteworthy\ninstance of such a problem is perhaps a proof of the statement that\n“All Robbins algebras are Boolean algebras.” This\nwas conjectured in the 1930s, and the proof was finally discovered by\nthe Otter automatic theorem-prover in 1996 after just a few months of\neffort (Kolata 1996, Wos 2013). Sister fields like formal verification\nhave also bloomed to the extent that it is now not too difficult to\nsemi-automatically verify vital hardware/software components (Kaufmann\net al. 2000 and Chajed et al. 2017). \nOther related areas, such as (natural) language translation, still\nhave a long way to go, but are good enough to let us use them under\nrestricted conditions. The jury is out on tasks such as machine\ntranslation, which seems to require both statistical methods (Lopez\n2008) and symbolic methods (España-Bonet 2011). Both\nmethods now have comparable but limited success in the wild. A\ndeployed translation system at Ford that was initially developed for\ntranslating manufacturing process instructions from English to other\nlanguages initially started out as rule-based system with Ford and\ndomain-specific vocabulary and language. This system then evolved to\nincorporate statistical techniques along with rule-based techniques as\nit gained new uses beyond translating manuals, for example, lay users\nwithin Ford translating their own documents (Rychtyckyj and Plesco\n2012).  \nAI’s great achievements mentioned above so far have all been in\nlimited, narrow domains. This lack of any success in the unrestricted\ngeneral case has caused a small set of researchers to break away into\nwhat is now called\n  artificial general intelligence\n (Goertzel and Pennachin 2007). The stated goals of this movement\ninclude shifting the focus again to building artifacts that are\ngenerally intelligent and not just capable in one narrow domain.  \nComputer Ethics has been around for a long time. In this\nsub-field, typically one would consider how one ought to act in a\ncertain class of situations involving computer technology, where the\n“one” here refers to a human being (Moor 1985). So-called\n“robot ethics” is different. In this sub-field (which goes\nby names such as “moral AI,” “ethical AI,”\n“machine ethics,” “moral robots,” etc.) one is\nconfronted with such prospects as robots being able to make autonomous\nand weighty decisions – decisions that might or might not be\nmorally permissible (Wallach & Allen 2010). If one were to attempt\nto engineer a robot with a capacity for sophisticated ethical\nreasoning and decision-making, one would also be doing Philosophical\nAI, as that concept is characterized\n elsewhere\n in the present entry. There can be many different flavors of\napproaches toward Moral AI. Wallach and Allen (2010) provide a\nhigh-level overview of the different approaches. Moral reasoning is\nobviously needed in robots that have the capability for lethal action.\nArkin (2009) provides an introduction to how we can control and\nregulate machines that have the capacity for lethal behavior. Moral AI\ngoes beyond obviously lethal situations, and we can have a spectrum of\nmoral machines. Moor (2006) provides one such spectrum of possible\nmoral agents. An example of a non-lethal but ethically-charged machine\nwould be a lying machine. Clark (2010) uses a computational theory\nof the mind, the ability to represent and reason about other\nagents, to build a lying machine that successfully persuades people\ninto believing falsehoods. Bello & Bringsjord (2013) give a\ngeneral overview of what might be required to build a moral machine,\none of the ingredients being a theory of mind.  \nThe most general framework for building machines that can reason\nethically consists in endowing the machines with a moral code.\nThis requires that the formal framework used for reasoning by the\nmachine be expressive enough to receive such codes. The field of Moral\nAI, for now, is not concerned with the source or provenance of such\ncodes. The source could be humans, and the machine could receive the\ncode directly (via explicit encoding) or indirectly (reading). Another\npossibility is that the code is inferred by the machine from a more\nbasic set of laws. We assume that the robot has access to some such\ncode, and we then try to engineer the robot to follow that code under\nall circumstances while making sure that the moral code and its\nrepresentation do not lead to unintended consequences. Deontic\nlogics are a class of formal logics that have been studied the\nmost for this purpose. Abstractly, such logics are concerned mainly\nwith what follows from a given moral code. Engineering then studies\nthe match of a given deontic logic to a moral code (i.e., is the logic\nexpressive enough) which has to be balanced with the ease of\nautomation. Bringsjord et al. (2006) provide a blueprint for using\ndeontic logics to build systems that can perform actions in accordance\nwith a moral code. The role deontic logics play in the framework\noffered by Bringsjord et al (which can be considered to be\nrepresentative of the field of deontic logic for moral AI) can be best\nunderstood as striving towards Leibniz’s dream of a universal\nmoral calculus:  \nDeontic logic-based frameworks can also be used in a fashion that is\nanalogous to moral self-reflection. In this mode, logic-based\nverification of the robot’s internal modules can done before the\nrobot ventures out into the real world. Govindarajulu and Bringsjord\n(2015) present an approach, drawing from formal-program\nverification, in which a deontic-logic based system could be used\nto verify that a robot acts in a certain ethically-sanctioned manner\nunder certain conditions. Since formal-verification approaches can be\nused to assert statements about an infinite number of situations and\nconditions, such approaches might be preferred to having the robot\nroam around in an ethically-charged test environment and make a finite\nset of decisions that are then judged for their ethical correctness.\nMore recently, Govindarajulu and Bringsjord (2017) use a deontic logic\nto present a computational model of the\n Doctrine of Double Effect,\n an ethical principle for moral dilemmas that has been studied\nempirically and analyzed extensively by\n philosophers.[35]\n The principle is usually presented and motivated via dilemmas using\ntrolleys and was first presented in this fashion by Foot (1967).  \nWhile there has been substantial theoretical and philosophical work,\nthe field of machine ethics is still in its infancy. There has been\nsome embryonic work in building ethical machines. One recent such\nexample would be Pereira and Saptawijaya (2016) who use logic\nprogramming and base their work in machine ethics on the ethical\ntheory known as contractualism, set out by Scanlon (1982). And\nwhat about the future? Since artificial agents are bound to get\nsmarter and smarter, and to have more and more autonomy and\nresponsibility, robot ethics is almost certainly going to grow in\nimportance. This endeavor might not be a straightforward application\nof classical ethics. For example, experimental results suggest that\nhumans hold robots to different ethical standards than they expect\nfrom humans under similar conditions (Malle et al.\n 2015).[36] \nNotice that the heading for this section isn’t Philosophy\nof AI. We’ll get to that category momentarily. (For now\nit can be identified with the attempt to answer such questions as\nwhether artificial agents created in AI can ever reach the full\nheights of human intelligence.) Philosophical AI is AI, not\nphilosophy; but it’s AI rooted in and flowing from, philosophy.\nFor example, one could engage, using the tools and techniques of\nphilosophy, a paradox, work out a proposed solution, and then proceed\nto a step that is surely optional for philosophers: expressing the\nsolution in terms that can be translated into a computer program that,\nwhen executed, allows an artificial agent to surmount concrete\ninstances of the original\n paradox.[37]\n Before we ostensively characterize Philosophical AI of this sort\ncourtesy of a particular research program, let us consider first the\nview that AI is in fact simply philosophy, or a part thereof.  \nDaniel Dennett (1979) has famously claimed not just that there are\nparts of AI intimately bound up with philosophy, but that AI\nis philosophy (and psychology, at least of the cognitive\nsort). (He has made a parallel claim about Artificial Life (Dennett\n1998)). This view will turn out to be incorrect, but the reasons why\nit’s wrong will prove illuminating, and our discussion will pave\nthe way for a discussion of Philosophical AI.  \nWhat does Dennett say, exactly? This:  \nElsewhere he says his view is that AI should be viewed “as a\nmost abstract inquiry into the possibility of intelligence or\nknowledge” (Dennett 1979, 64).  \nIn short, Dennett holds that AI is the attempt to explain\nintelligence, not by studying the brain in the hopes of identifying\ncomponents to which cognition can be reduced, and not by engineering\nsmall information-processing units from which one can build in\nbottom-up fashion to high-level cognitive processes, but rather by\n– and this is why he says the approach is top-down\n– designing and implementing abstract algorithms that capture\ncognition. Leaving aside the fact that, at least starting in the early\n1980s, AI includes an approach that is in some sense bottom-up (see\nthe neurocomputational paradigm discussed above, in\n Non-Logicist AI: A Summary;\n and see, specifically, Granger’s (2004a, 2004b) work,\nhyperlinked in text immediately above, a specific counterexample), a\nfatal flaw infects Dennett’s view. Dennett sees the potential\nflaw, as reflected in:  \nDennett has a ready answer to this objection. He writes:  \nUnfortunately, this is acutely problematic; and examination of the\nproblems throws light on the nature of AI.  \nFirst, insofar as philosophy and psychology are concerned with the\nnature of mind, they aren’t in the least trammeled by the\npresupposition that mentation consists in computation. AI, at least of\nthe “Strong” variety (we’ll discuss\n“Strong” versus “Weak” AI\n below)\n is indeed an attempt to substantiate, through engineering certain\nimpressive artifacts, the thesis that intelligence is at bottom\ncomputational (at the level of Turing machines and their equivalents,\ne.g., Register machines). So there is a philosophical claim, for sure.\nBut this doesn’t make AI philosophy, any more than some of the\ndeeper, more aggressive claims of some physicists (e.g., that the\n universe is ultimately digital in nature)\n make their field philosophy. Philosophy of physics certainly\nentertains the proposition that the physical universe can be\nperfectly modeled in digital terms (in a series of cellular automata,\ne.g.), but of course philosophy of physics can’t be\nidentified with this doctrine.  \nSecond, we now know well (and those familiar with the relevant formal\nterrain knew at the time of Dennett’s writing) that information\nprocessing can exceed standard computation, that is, can exceed\ncomputation at and below the level of what a Turing machine can muster\n(Turing-computation, we shall say). (Such information\nprocessing is known as hypercomputation, a term coined by\nphilosopher Jack Copeland, who has himself defined such machines\n(e.g., Copeland 1998). The first machines capable of hypercomputation\nwere trial-and-error machines, introduced in the same famous\nissue of the Journal of Symbolic Logic (Gold 1965; Putnam\n1965). A new hypercomputer is the infinite time Turing machine\n(Hamkins & Lewis 2000).) Dennett’s appeal to Church’s\nthesis thus flies in the face of the mathematical facts: some\nvarieties of information processing exceed standard computation (or\nTuring-computation). Church’s thesis, or more precisely, the\nChurch-Turing thesis, is the view that a function \\(f\\) is effectively\ncomputable if and only if \\(f\\) is Turing-computable (i.e., some\nTuring machine can compute \\(f\\)). Thus, this thesis has nothing to\nsay about information processing that is more demanding than what a\nTuring machine can achieve. (Put another way, there is no\ncounter-example to CTT to be automatically found in an\ninformation-processing device capable of feats beyond the reach of\nTMs.) For all philosophy and psychology know, intelligence, even if\ntied to information processing, exceeds what is Turing-computational\nor\n Turing-mechanical.[38]\n This is especially true because philosophy and psychology, unlike AI,\nare in no way fundamentally charged with engineering artifacts, which\nmakes the physical realizability of hypercomputation irrelevant from\ntheir perspectives. Therefore, contra Dennett, to consider AI\nas psychology or philosophy is to commit a serious error, precisely\nbecause so doing would box these fields into only a speck of the\nentire space of functions from the natural numbers (including tuples\ntherefrom) to the natural numbers. (Only a tiny portion of the\nfunctions in this space are Turing-computable.) AI is without question\nmuch, much narrower than this pair of fields. Of course, it’s\npossible that AI could be replaced by a field devoted not to building\ncomputational artifacts by writing computer programs and running them\non embodied Turing machines. But this new field, by definition, would\nnot be AI. Our exploration of AIMA and other textbooks provide\ndirect empirical confirmation of this.  \nThird, most AI researchers and developers, in point of fact, are\nsimply concerned with building useful, profitable artifacts, and\ndon’t spend much time reflecting upon the kinds of abstract\ndefinitions of intelligence explored in this entry (e.g.,\n What Exactly is AI?).\n  \nThough AI isn’t philosophy, there are certainly ways of doing\nreal implementation-focussed AI of the highest caliber that are\nintimately bound up with philosophy. The best way to demonstrate this\nis to simply present such research and development, or at least a\nrepresentative example thereof. While there have been many examples of\nsuch work, the most prominent example in AI is John Pollock’s\nOSCAR project, which stretched over a considerable portion of his\nlifetime. For a detailed presentation and further discussion, see\nthe \n\n Supplement on the OSCAR Project.\n  \nIt’s important to note at this juncture that the OSCAR project,\nand the information processing that underlies it, are without question\nat once philosophy and technical AI. Given that the work in\nquestion has appeared in the pages of Artificial Intelligence,\na first-rank journal devoted to that field, and not to philosophy,\nthis is undeniable (see, e.g., Pollock 2001, 1992). This point is\nimportant because while it’s certainly appropriate, in the\npresent venue, to emphasize connections between AI and philosophy,\nsome readers may suspect that this emphasis is contrived: they may\nsuspect that the truth of the matter is that page after page of AI\njournals are filled with narrow, technical content far from\nphilosophy. Many such papers do exist. But we must distinguish between\nwritings designed to present the nature of AI, and its core methods\nand goals, versus writings designed to present progress on specific\ntechnical issues.  \nWritings in the latter category are more often than not quite narrow,\nbut, as the example of Pollock shows, sometimes these specific issues\nare inextricably linked to philosophy. And of course Pollock’s\nwork is a representative example (albeit the most substantive one).\nOne could just as easily have selected work by folks who don’t\nhappen to also produce straight philosophy. For example, for an entire\nbook written within the confines of AI and computer science, but which\nis epistemic logic in action in many ways, suitable for use in\nseminars on that topic, see (Fagin et al. 2004). (It is hard to find\ntechnical work that isn’t bound up with philosophy in some\ndirect way. E.g., AI research on learning is all intimately bound up\nwith philosophical treatments of induction, of how genuinely new\nconcepts not simply defined in terms of prior ones can be learned. One\npossible partial answer offered by AI is inductive logic\nprogramming, discussed in Chapter 19 of AIMA.)  \nWhat of writings in the former category? Writings in this category,\nwhile by definition in AI venues, not philosophy ones, are nonetheless\nphilosophical. Most textbooks include plenty of material that falls\ninto this latter category, and hence they include discussion of the\nphilosophical nature of AI (e.g., that AI is aimed at building\nartificial intelligences, and that’s why, after all, it’s\ncalled ‘AI’).  \nRecall that we earlier discussed proposed definitions of AI, and\nrecall specifically that these proposals were couched in terms of the\ngoals of the field. We can follow this pattern here: We can\ndistinguish between “Strong” and “Weak” AI by\ntaking note of the different goals that these two versions of AI\nstrive to reach. “Strong” AI seeks to create artificial\npersons: machines that have all the mental powers we have, including\nphenomenal consciousness. “Weak” AI, on the other hand,\nseeks to build information-processing machines that appear to\nhave the full mental repertoire of human persons (Searle 1997).\n“Weak” AI can also be defined as the form of AI that aims\nat a system able to pass not just the Turing Test (again, abbreviated\nas TT), but the Total Turing Test (Harnad 1991). In TTT, a\nmachine must muster more than linguistic indistinguishability: it must\npass for a human in all behaviors – throwing a baseball, eating,\nteaching a class, etc.  \nIt would certainly seem to be exceedingly difficult for philosophers\nto overthrow “Weak” AI (Bringsjord and Xiao 2000). After\nall, what philosophical reason stands in the way of AI\nproducing artifacts that appear to be animals or even humans?\nHowever, some philosophers have aimed to do in “Strong”\nAI, and we turn now to the most prominent case in point.  \nWithout question, the most famous argument in the philosophy of AI is\nJohn Searle’s (1980) Chinese Room Argument (CRA), designed to\noverthrow “Strong” AI. We present a quick summary here and\na “report from the trenches” as to how AI practitioners\nregard the argument. Readers wanting to further study CRA will find an\nexcellent next step in the entry on\n the Chinese Room Argument\n and (Bishop & Preston 2002).  \nCRA is based on a thought-experiment in which Searle himself stars. He\nis inside a room; outside the room are native Chinese speakers who\ndon’t know that Searle is inside it. Searle-in-the-box, like\nSearle-in-real-life, doesn’t know any Chinese, but is fluent in\nEnglish. The Chinese speakers send cards into the room through a slot;\non these cards are written questions in Chinese. The box, courtesy of\nSearle’s secret work therein, returns cards to the native\nChinese speakers as output. Searle’s output is produced by\nconsulting a rulebook: this book is a lookup table that tells him what\nChinese to produce based on what is sent in. To Searle, the Chinese is\nall just a bunch of – to use Searle’s language –\nsquiggle-squoggles. The following schematic picture sums up the\nsituation. The labels should be obvious. \\(O\\) denotes the outside\nobservers, in this case the Chinese speakers. Input is denoted by\n\\(i\\) and output by \\(o\\). As you can see, there is an icon for the\nrulebook, and Searle himself is denoted by \\(P\\). \nThe Chinese Room, Schematic View \nNow, what is the argument based on this thought-experiment? Even if\nyou’ve never heard of CRA before, you doubtless can see the\nbasic idea: that Searle (in the box) is supposed to be everything a\ncomputer can be, and because he doesn’t understand Chinese, no\ncomputer could have such understanding. Searle is mindlessly moving\nsquiggle-squoggles around, and (according to the argument)\nthat’s all computers do,\n fundamentally.[39] \nWhere does CRA stand today? As we’ve already indicated, the\nargument would still seem to be alive and well; witness (Bishop &\nPreston 2002). However, there is little doubt that at least among AI\npractitioners, CRA is generally rejected. (This is of course\nthoroughly unsurprising.) Among these practitioners, the philosopher\nwho has offered the most formidable response out of AI itself is\nRapaport (1988), who argues that while AI systems are indeed\nsyntactic, the right syntax can constitute semantics. It should be\nsaid that a common attitude among proponents of “Strong”\nAI is that CRA is not only unsound, but silly, based as it is on a\nfanciful story (CR) far removed from the practice of AI\n– practice which is year by year moving ineluctably toward\nsophisticated robots that will once and for all silence CRA and its\nproponents. For example, John Pollock (as we’ve noted,\nphilosopher and practitioner of AI) writes:  \nTo wrap up discussion of CRA, we make two quick points, to wit:  \nReaders may wonder if there are philosophical debates that AI\nresearchers engage in, in the course of working in their field (as\nopposed to when they might attend a philosophy conference). Surely, AI\nresearchers have philosophical discussions amongst themselves, right?\n \nGenerally, one finds that AI researchers do discuss among themselves\ntopics in philosophy of AI, and these topics are usually the very same\nones that occupy philosophers of AI. However, the attitude reflected\nin the quote from Pollock immediately above is by far the dominant\none. That is, in general, the attitude of AI researchers is that\nphilosophizing is sometimes fun, but the upward march of AI\nengineering cannot be stopped, will not fail, and will eventually\nrender such philosophizing otiose.  \nWe will return to the issue of the future of AI in the\n final section\n of this entry.  \nFour decades ago, J.R. Lucas (1964) argued that Gödel’s\nfirst incompleteness theorem entails that no machine can ever reach\nhuman-level intelligence. His argument has not proved to be\ncompelling, but Lucas initiated a debate that has produced more\nformidable arguments. One of Lucas’ indefatigable defenders is\nthe physicist Roger Penrose, whose first attempt to vindicate Lucas\nwas a Gödelian attack on “Strong” AI articulated in\nhis The Emperor’s New Mind (1989). This first attempt\nfell short, and Penrose published a more elaborate and more fastidious\nGödelian case, expressed in Chapters 2 and 3 of his Shadows of\nthe Mind (1994).  \nIn light of the fact that readers can turn to the\n entry on the Gödel’s Incompleteness Theorems,\n a full review here is not needed. Instead, readers will be given a\ndecent sense of the argument by turning to an online paper in which\nPenrose, writing in response to critics (e.g., the philosopher David\nChalmers, the logician Solomon Feferman, and the computer scientist\nDrew McDermott) of his Shadows of the Mind, distills the\nargument to a couple of\n paragraphs.[40]\n Indeed, in this paper Penrose gives what he takes to be the perfected\nversion of the core Gödelian case given in SOTM. Here is\nthis version, verbatim:  \nDoes this argument succeed? A firm answer to this question is not\nappropriate to seek in the present entry. Interested readers are\nencouraged to consult four full-scale treatments of the argument\n(LaForte et. al 1998; Bringsjord and Xiao 2000; Shapiro 2003; Bowie\n1982). \nIn addition to the Gödelian and Searlean arguments covered\nbriefly above, a third attack on “Strong” AI (of the\nsymbolic variety) has been widely discussed (though with the rise of\nstatistical machine learning has come a corresponding decrease in the\nattention paid to it), namely, one given by the philosopher Hubert\nDreyfus (1972, 1992), some incarnations of which have been\nco-articulated with his brother, Stuart Dreyfus (1987), a computer\nscientist. Put crudely, the core idea in this attack is that human\nexpertise is not based on the explicit, disembodied, mechanical\nmanipulation of symbolic information (such as formulae in some logic,\nor probabilities in some Bayesian network), and that AI’s\nefforts to build machines with such expertise are doomed if based on\nthe symbolic paradigm. The genesis of the Dreyfusian attack was a\nbelief that the critique of (if you will) symbol-based philosophy\n(e.g., philosophy in the logic-based, rationalist tradition, as\nopposed to what is called the Continental tradition) from such\nthinkers as Heidegger and Merleau-Ponty could be made against the\nrationalist tradition in AI. After further reading and study of\nDreyfus’ writings, readers may judge whether this critique is\ncompelling, in an information-driven world increasingly managed by\nintelligent agents that carry out symbolic reasoning (albeit not even\nclose to the human level).  \nFor readers interested in exploring philosophy of AI beyond what Jim\nMoor (in a recent address – “The Next Fifty Years of AI:\nFuture Scientific Research vs. Past Philosophical Criticisms”\n– as the 2006 Barwise Award winner at the annual eastern\nAmerican Philosophical Association meeting) has called the “the\nbig three” criticisms of AI, there is no shortage of additional\nmaterial, much of it available on the Web. The last chapter of\nAIMA provides a compressed overview of some additional\narguments against “Strong” AI, and is in general not a bad\nnext step. Needless to say, Philosophy of AI today involves much more\nthan the three well-known arguments discussed above, and, inevitably,\nPhilosophy of AI tomorrow will include new debates and problems we\ncan’t see now. Because machines, inevitably, will get smarter\nand smarter (regardless of just how smart they get),\nPhilosophy of AI, pure and simple, is a growth industry. With every\nhuman activity that machines match, the “big” questions\nwill only attract more attention.  \nIf past predictions are any indication, the only thing we know today\nabout tomorrow’s science and technology is that it will be\nradically different than whatever we predict it will be like.\nArguably, in the case of AI, we may also specifically know today that\nprogress will be much slower than what most expect. After all, at the\n1956 kickoff conference (discussed at the start of this entry), Herb\nSimon predicted that thinking machines able to match the human mind\nwere “just around the corner” (for the relevant quotes and\ninformative discussion, see the first chapter of AIMA). As it\nturned out, the new century would arrive without a single machine able\nto converse at even the toddler level. (Recall that when it comes to\nthe building of machines capable of displaying human-level\nintelligence, Descartes, not Turing, seems today to be the better\nprophet.) Nonetheless, astonishing though it may be, serious thinkers\nin the late 20th century have continued to issue incredibly optimistic\npredictions regarding the progress of AI. For example, Hans Moravec\n(1999), in his Robot: Mere Machine to Transcendent Mind,\ninforms us that because the speed of computer hardware doubles every\n18 months (in accordance with Moore’s Law, which has\napparently held in the past), “fourth generation”\nrobots will soon enough exceed humans in all respects, from running\ncompanies to writing novels. These robots, so the story goes, will\nevolve to such lofty cognitive heights that we will stand to them as\nsingle-cell organisms stand to us\n today.[41] \nMoravec is by no means singularly Pollyannaish: Many others in AI\npredict the same sensational future unfolding on about the same rapid\nschedule. In fact, at the aforementioned AI@50 conference, Jim Moor\nposed the question “Will human-level AI be achieved within the\nnext 50 years?” to five thinkers who attended the original 1956\nconference: John McCarthy, Marvin Minsky, Oliver Selfridge, Ray\nSolomonoff, and Trenchard Moore. McCarthy and Minsky gave firm,\nunhesitating affirmatives, and Solomonoff seemed to suggest that AI\nprovided the one ray of hope in the face of fact that our species\nseems bent on destroying itself. (Selfridge’s reply was a bit\ncryptic. Moore returned a firm, unambiguous negative, and declared\nthat once his computer is smart enough to interact with him\nconversationally about mathematical problems, he might take this whole\nenterprise more seriously.) It is left to the reader to judge the\naccuracy of such risky predictions as have been given by Moravec,\nMcCarthy, and\n Minsky.[42] \nThe judgment of the reader in this regard ought to factor in the\nstunning resurgence, very recently, of serious reflection on what is\nknown as “The Singularity,” (denoted by us simply as\nS) the future point at which artificial intelligence exceeds\nhuman intelligence, whereupon immediately thereafter (as the story\ngoes) the machines make themselves rapidly smarter and smarter and\nsmarter, reaching a superhuman level of intelligence that, stuck as we\nare in the mud of our limited mentation, we can’t fathom. For\nextensive, balanced analysis of S, see Eden et al. (2013). \nReaders unfamiliar with the literature on S may be quite\nsurprised to learn the degree to which, among learned folks, this\nhypothetical event is not only taken seriously, but has in fact become\na target for extensive and frequent philosophizing [for a mordant tour\nof the recent thought in question, see Floridi (2015)]. What\narguments support the belief that S is in our future?\nThere are two main arguments at this point: the familiar\nhardware-based one [championed by Moravec, as noted above, and again\nmore recently by Kurzweil (2006)]; and the – as far as we know\n– original argument given by mathematician I. J. Good (1965). In\naddition, there is a recent and related doomsayer argument advanced by\nBostrom (2014), which seems to presuppose that S will occur.\nGood’s argument, nicely amplified and adjusted by Chalmers\n(2010), who affirms the tidied-up version of the argument, runs as\nfollows: \nIn this argument, ‘AI’ is artificial intelligence at the\nlevel of, and created by, human persons, ‘AI\\(^+\\)’\nartificial intelligence above the level of human persons, and\n‘AI\\(^{++}\\)’ super-intelligence constitutive of S.\nThe key process is presumably the creation of one class of\nmachine by another. We have added for convenience ‘HI’ for\nhuman intelligence; the central idea is then: HI will create AI, the\nlatter at the same level of intelligence as the former; AI will create\nAI\\(^+\\); AI\\(^+\\) will create AI\\(^{++}\\); with the ascension\nproceeding perhaps forever, but at any rate proceeding long enough for\nus to be as ants outstripped by gods.  \nThe argument certainly appears to be formally valid. Are its three\npremises true? Taking up such a question would fling us far beyond the\nscope of this entry. We point out only that the concept of one class\nof machines creating another, more powerful class of machines is not a\ntransparent one, and neither Good nor Chalmers provides a rigorous\naccount of the concept, which is ripe for philosophical analysis. (As\nto mathematical analysis, some exists, of course. It is for example\nwell-known that a computing machine at level \\(L\\) cannot possibly\ncreate another machine at a higher level \\(L'\\). For instance, a\nlinear-bounded automaton can’t create a Turing machine.) \nThe Good-Chalmers argument has a rather clinical air about it; the\nargument doesn’t say anything regarding whether machines in\nthe AI\\(^{++}\\) category will be benign, malicious, or munificent.\nMany others gladly fill this gap with dark, dark pessimism. The\nlocus classicus here is without question a widely read paper by\nBill Joy (2000): “Why The Future Doesn’t Need Us.”\nJoy believes that the human race is doomed, in no small part because\nit’s busy building smart machines. He writes:  \nThe 21st-century technologies – genetics, nanotechnology, and\nrobotics (GNR) – are so powerful that they can spawn whole new\nclasses of accidents and abuses. Most dangerously, for the first time,\nthese accidents and abuses are widely within the reach of individuals\nor small groups. They will not require large facilities or rare raw\nmaterials. Knowledge alone will enable the use of them. \nThus we have the possibility not just of weapons of mass destruction\nbut of knowledge-enabled mass destruction (KMD), this destructiveness\nhugely amplified by the power of self-replication.  \nI think it is no exaggeration to say we are on the cusp of the further\nperfection of extreme evil, an evil whose possibility spreads well\nbeyond that which weapons of mass destruction bequeathed to the\nnation-states, on to a surprising and terrible empowerment of extreme\n individuals.[43] \nPhilosophers would be most interested in arguments for this\nview. What are Joy’s? Well, no small reason for the attention\nlavished on his paper is that, like Raymond Kurzweil (2000), Joy\nrelies heavily on an argument given by none other than the Unabomber\n(Theodore Kaczynski). The idea is that, assuming we succeed in\nbuilding intelligent machines, we will have them do most (if not all)\nwork for us. If we further allow the machines to make decisions for us\n– even if we retain oversight over the machines –, we will\neventually depend on them to the point where we must simply accept\ntheir decisions. But even if we don’t allow the machines to make\ndecisions, the control of such machines is likely to be held by a\nsmall elite who will view the rest of humanity as unnecessary –\nsince the machines can do any needed work (Joy 2000). \nThis isn’t the place to assess this argument. (Having said that,\nthe pattern pushed by the Unabomber and his supporters certainly\nappears to be flatly\n invalid.[44])\n In fact, many readers will doubtless feel that no such place exists\nor will exist, because the reasoning here is amateurish. So then, what\nabout the reasoning of professional philosophers on the matter?  \nBostrom has recently painted an exceedingly dark picture of a possible\nfuture. He points out that the “first superintelligence”\ncould have the capability \n Clearly, the most vulnerable premise in this sort of argument\nis that the “first superintelligence” will arrive indeed\narrive. Here perhaps the Good-Chalmers argument provides a basis.\n \nSearle (2014) thinks Bostrom’s book is misguided and\nfundamentally mistaken, and that we needn’t worry. His rationale\nis dirt-simple: Machines aren’t conscious; Bostrom is alarmed at\nthe prospect of malicious machines who do us in; a malicious machine\nis by definition a conscious machine; ergo, Bostrom’s argument\ndoesn’t work. Searle writes:  \nThe positively remarkable thing here, it seems to us, is that Searle\nappears to be unaware of the brute fact that most AI engineers are\nperfectly content to build machines on the basis of the AIMA\nview of AI we presented and explained above: the view according to\nwhich machines simply map percepts to actions. On this view, it\ndoesn’t matter whether the machine really has desires;\nwhat matters is whether it acts suitably on the basis of how AI\nscientists engineer formal correlates to desire. An\nautonomous machine with overwhelming destructive power that\nnon-consciously “decides” to kill doesn’t become\njust a nuisance because genuine, human-level, subjective desire is\nabsent from the machine. If an AI can play the game of chess, and the\ngame of Jeopardy!, it can certainly play the game of war. Just\nas it does little good for a human loser to point out that the\nvictorious machine in a game of chess isn’t conscious, it will\ndo little good for humans being killed by machines to point out that\nthese machines aren’t conscious. (It is interesting to note that\nthe genesis of Joy’s paper was an informal conversation with\nJohn Searle and Raymond Kurzweil. According to Joy, Searle\ndidn’t think there was much to worry about, since he was (and\nis) quite confident that tomorrow’s robots can’t be\n conscious.[45])\n  \nThere are some things we can safely say about tomorrow.\nCertainly, barring some cataclysmic events (nuclear or biological\nwarfare, global economic depression, a meteorite smashing into Earth,\netc.), we now know that AI will succeed in producing artificial\nanimals. Since even some natural animals (mules, e.g.) can be\neasily trained to work for humans, it stands to reason that artificial\nanimals, designed from scratch with our purposes in mind, will be\ndeployed to work for us. In fact, many jobs currently done by humans\nwill certainly be done by appropriately programmed artificial animals.\nTo pick an arbitrary example, it is difficult to believe that\ncommercial drivers won’t be artificial in the future. (Indeed,\nDaimler is already running commercials in which they tout the ability\nof their automobiles to drive “autonomously,” allowing\nhuman occupants of these vehicles to ignore the road and read.) Other\nexamples would include: cleaners, mail carriers, clerical workers,\nmilitary scouts, surgeons, and pilots. (As to cleaners, probably a\nsignificant number of readers, at this very moment, have robots from\niRobot cleaning the carpets in their homes.) It is hard to see how\nsuch jobs are inseparably bound up with the attributes often taken to\nbe at the core of personhood – attributes that would be the most\ndifficult for AI to\n replicate.[46] \nAndy Clark (2003) has another prediction: Humans will gradually\nbecome, at least to an appreciable degree, cyborgs, courtesy of\nartificial limbs and sense organs, and implants. The main driver of\nthis trend will be that while standalone AIs are often desirable, they\nare hard to engineer when the desired level of intelligence is high.\nBut to let humans “pilot” less intelligent machines is a\ngood deal easier, and still very attractive for concrete reasons.\nAnother related prediction is that AI would play the role of a\ncognitive prosthesis for humans (Ford et al. 1997; Hoffman et al.\n2001). The prosthesis view sees AI as a “great equalizer”\nthat would lead to less stratification in society, perhaps similar to\nhow the Hindu-Arabic numeral system made arithmetic available to the\nmasses, and to how the Guttenberg press contributed to literacy\nbecoming more universal.  \nEven if the argument is formally invalid, it leaves us with a question\n– the cornerstone question about AI and the future: Will AI\nproduce artificial creatures that replicate and exceed human cognition\n(as Kurzweil and Joy believe)? Or is this merely an interesting\nsupposition?  \nThis is a question not just for scientists and engineers; it is also a\nquestion for philosophers. This is so for two reasons. One, research\nand development designed to validate an affirmative answer must\ninclude philosophy – for reasons rooted in earlier parts of the\npresent entry. (E.g., philosophy is the place to turn to for robust\nformalisms to model human propositional attitudes in machine terms.)\nTwo, philosophers might well be able to provide arguments that answer\nthe cornerstone question now, definitively. If a version of either of\nthe three arguments against “Strong” AI alluded to above\n(Searle’s CRA; the Gödelian attack; the Dreyfus argument)\nare sound, then of course AI will not manage to produce machines\nhaving the mental powers of persons. No doubt the future holds not\nonly ever-smarter machines, but new arguments pro and con on the\nquestion of whether this progress can reach the human level that\nDescartes declared to be unreachable. ","contact.mail":"Selmer.Bringsjord@gmail.com","contact.domain":"gmail.com"},{"date.published":"2018-07-12","url":"https://plato.stanford.edu/entries/artificial-intelligence/","author1":"Selmer Bringsjord","author2":"Naveen Sundar Govindarajulu","author1.info":"http://www.rpi.edu/~brings","entry":"artificial-intelligence","body.text":"\n\n\nArtificial intelligence (AI) is the field devoted to building\nartificial animals (or at least artificial creatures that – in\nsuitable contexts – appear to be animals) and, for\nmany, artificial persons (or at least artificial creatures that\n– in suitable contexts – appear to be\n persons).[1]\n Such goals immediately ensure that AI is a discipline of considerable\ninterest to many philosophers, and this has been confirmed (e.g.) by\nthe energetic attempt, on the part of numerous philosophers, to show\nthat these goals are in fact un/attainable. On the constructive side,\nmany of the core formalisms and techniques used in AI come out of, and\nare indeed still much used and refined in, philosophy: first-order\nlogic and its extensions; intensional logics suitable for the modeling\nof doxastic attitudes and deontic reasoning; inductive logic,\nprobability theory, and probabilistic reasoning; practical reasoning\nand planning, and so on. In light of this, some philosophers conduct\nAI research and development as philosophy. \n\n\nIn the present entry, the history of AI is briefly recounted, proposed\ndefinitions of the field are discussed, and an overview of the field\nis provided. In addition, both philosophical AI (AI pursued as and out\nof philosophy) and philosophy of AI are discussed, via\nexamples of both. The entry ends with some de rigueur\nspeculative commentary regarding the future of AI. \n\nThe field of artificial intelligence (AI) officially started in 1956,\nlaunched by a small but now-famous\n DARPA-sponsored\n summer conference at Dartmouth College, in Hanover, New Hampshire.\n(The 50-year celebration of this conference,\n AI@50,\n was held in July 2006 at Dartmouth, with five of the original\nparticipants making it\n back.[2]\n What happened at this historic conference figures in the final\nsection of this entry.) Ten thinkers attended, including John McCarthy\n(who was working at Dartmouth in 1956), Claude Shannon, Marvin Minsky,\nArthur Samuel, Trenchard Moore (apparently the lone note-taker at the\noriginal conference), Ray Solomonoff, Oliver Selfridge, Allen Newell,\nand Herbert Simon. From where we stand now, into the start of the new\nmillennium, the Dartmouth conference is memorable for many reasons,\nincluding this pair: one, the term ‘artificial\nintelligence’ was coined there (and has long been firmly\nentrenched, despite being disliked by some of the attendees, e.g.,\nMoore); two, Newell and Simon revealed a program – Logic\nTheorist (LT) – agreed by the attendees (and, indeed, by nearly\nall those who learned of and about it soon after the conference) to be\na remarkable achievement. LT was capable of proving elementary\ntheorems in the propositional\n calculus.[3][4] \nThough the term ‘artificial intelligence’ made\nits advent at the 1956 conference, certainly the field of AI,\noperationally defined (defined, i.e., as a field constituted by\npractitioners who think and act in certain ways), was in operation\nbefore 1956. For example, in a famous Mind paper of 1950,\nAlan Turing argues that the question “Can a machine\nthink?” (and here Turing is talking about standard computing\nmachines: machines capable of computing functions from the natural\nnumbers (or pairs, triples, … thereof) to the natural numbers\nthat a Turing machine or equivalent can handle) should be replaced\nwith the question “Can a machine be linguistically\nindistinguishable from a human?.” Specifically, he proposes a\ntest, the\n “Turing Test”\n (TT) as it’s now known. In the TT, a woman and a computer are\nsequestered in sealed rooms, and a human judge, in the dark as to\nwhich of the two rooms contains which contestant, asks questions by\nemail (actually, by teletype, to use the original term) of the\ntwo. If, on the strength of returned answers, the judge can do no\nbetter than 50/50 when delivering a verdict as to which room houses\nwhich player, we say that the computer in question has passed\nthe TT. Passing in this sense operationalizes linguistic\nindistinguishability. Later, we shall discuss the role that TT has\nplayed, and indeed continues to play, in attempts to define AI. At the\nmoment, though, the point is that in his paper, Turing explicitly lays\ndown the call for building machines that would provide an existence\nproof of an affirmative answer to his question. The call even includes\na suggestion for how such construction should proceed. (He suggests\nthat “child machines” be built, and that these machines\ncould then gradually grow up on their own to learn to communicate in\nnatural language at the level of adult humans. This suggestion has\narguably been followed by Rodney Brooks and the philosopher Daniel\nDennett (1994) in the Cog Project. In addition, the Spielberg/Kubrick\nmovie A.I. is at least in part a cinematic exploration of\nTuring’s\n suggestion.[5])\n The TT continues to be at the heart of AI and discussions of its\nfoundations, as confirmed by the appearance of (Moor 2003). In fact,\nthe TT continues to be used to define the field, as in\nNilsson’s (1998) position, expressed in his textbook for the\nfield, that AI simply is the field devoted to building an artifact\nable to negotiate this test. Energy supplied by the dream of\nengineering a computer that can pass TT, or by controversy surrounding\nclaims that it has already been passed, is if anything\nstronger than ever, and the reader has only to do an internet search\nvia the string  \nturing test passed  \nto find up-to-the-minute attempts at reaching this dream, and attempts\n(sometimes made by philosophers) to debunk claims that some such\nattempt has succeeded. \nReturning to the issue of the historical record, even if one bolsters\nthe claim that AI started at the 1956 conference by adding the proviso\nthat ‘artificial intelligence’ refers to a nuts-and-bolts\nengineering pursuit (in which case Turing’s\nphilosophical discussion, despite calls for a child machine,\nwouldn’t exactly count as AI per se), one must confront the fact\nthat Turing, and indeed many predecessors, did attempt to build\nintelligent artifacts. In Turing’s case, such building was\nsurprisingly well-understood before the advent of programmable\ncomputers: Turing wrote a program for playing chess before there were\ncomputers to run such programs on, by slavishly following the code\nhimself. He did this well before 1950, and long before Newell (1973)\ngave thought in print to the possibility of a sustained, serious\nattempt at building a good chess-playing\n computer.[6] \nFrom the perspective of philosophy, which views the systematic\ninvestigation of mechanical intelligence as meaningful and productive\nseparate from the specific logicist formalisms (e.g., first-order\nlogic) and problems (e.g., the Entscheidungsproblem) that gave\nbirth to computer science, neither the 1956 conference, nor\nTuring’s Mind paper, come close to marking the start of\nAI. This is easy enough to see. For example, Descartes proposed TT\n(not the TT by name, of course) long before Turing was\n born.[7]\n Here’s the relevant passage:  \nAt the moment, Descartes is certainly carrying the\n day.[8]\n Turing predicted that his test would be passed by 2000, but the\nfireworks across the globe at the start of the new millennium have\nlong since died down, and the most articulate of computers still\ncan’t meaningfully debate a sharp toddler. Moreover, while in\ncertain focussed areas machines out-perform minds (IBM’s famous\nDeep Blue prevailed in chess over Gary Kasparov, e.g.; and more\nrecently, AI systems have prevailed in other games, e.g.\nJeopardy! and Go, about which more will momentarily be said),\nminds have a (Cartesian) capacity for cultivating their expertise in\nvirtually any sphere. (If it were announced to Deep Blue, or\nany current successor, that chess was no longer to be the game of\nchoice, but rather a heretofore unplayed variant of chess, the machine\nwould be trounced by human children of average intelligence having no\nchess expertise.) AI simply hasn’t managed to create\ngeneral intelligence; it hasn’t even managed to produce\nan artifact indicating that eventually it will create such a\nthing.  \nBut what about IBM Watson’s famous nail-biting victory in the\nJeopardy! game-show\n contest?[9]\n That certainly seems to be a machine triumph over humans on their\n“home field,” since Jeopardy! delivers a\nhuman-level linguistic challenge ranging across many domains. Indeed,\namong many AI cognoscenti, Watson’s success is considered to be\nmuch more impressive than Deep Blue’s, for numerous reasons. One\nreason is that while chess is generally considered to be\nwell-understood from the formal-computational perspective (after all,\nit’s well-known that there exists a perfect strategy for playing\nchess), in open-domain question-answering (QA), as in any\nsignificant natural-language processing task, there is no consensus as\nto what problem, formally speaking, one is trying to solve. Briefly,\nquestion-answering (QA) is what the reader would think it is: one asks\na question of a machine, and gets an answer, where the answer has to\nbe produced via some “significant” computational process.\n(See Strzalkowski & Harabagiu (2006) for an overview of what QA,\nhistorically, has been as a field.) A bit more precisely, there is no\nagreement as to what underlying function, formally speaking,\nquestion-answering capability computes. This lack of agreement stems\nquite naturally from the fact that there is of course no consensus as\nto what natural languages are, formally\n speaking.[10]\n Despite this murkiness, and in the face of an almost universal belief\nthat open-domain question-answering would remain unsolved for a decade\nor more, Watson decisively beat the two top human Jeopardy!\nchampions on the planet. During the contest, Watson had to answer\nquestions that required not only command of simple factoids\n(Question1), but also of some amount of rudimentary\nreasoning (in the form of temporal reasoning) and commonsense\n(Question2): \nQuestion1: The only two consecutive U.S. presidents\nwith the same first name.  \nQuestion2: In May 1898, Portugal celebrated the\n400th anniversary of this explorer’s arrival in India.  \nWhile Watson is demonstrably better than humans in\nJeopardy!-style quizzing (a new human Jeopardy! master\ncould arrive on the scene, but as for chess, AI now assumes that a\nsecond round of IBM-level investment would vanquish the new human\nopponent), this approach does not work for the kind of NLP challenge\nthat Descartes described; that is, Watson can’t converse on the\nfly. After all, some questions don’t hinge on sophisticated\ninformation retrieval and machine learning over pre-existing data, but\nrather on intricate reasoning right on the spot. Such questions may\nfor instance involve anaphora resolution, which require even deeper\ndegrees of commonsensical understanding of time, space, history, folk\npsychology, and so on. Levesque (2013) has catalogued some alarmingly\nsimple questions which fall in this category. (Marcus, 2013, gives an\naccount of Levesque’s challenges that is accessible to a wider\naudience.) The other class of question-answering tasks on which Watson\nfails can be characterized as dynamic question-answering. These\nare questions for which answers may not be recorded in textual form\nanywhere at the time of questioning, or for which answers are\ndependent on factors that change with time. Two questions that fall in\nthis category are given below (Govindarajulu et al. 2013): \nQuestion3: If I have 4 foos and 5 bars, and if foos\nare not the same as bars, how many foos will I have if I get 3 bazes\nwhich just happen to be foos? \nQuestion4: What was IBM’s Sharpe ratio in the\nlast 60 days of trading?  \nClosely following Watson’s victory, in March 2016,\n Google DeepMind’s AlphaGo\n defeated one of Go’s top-ranked players, Lee Seedol, in four\nout of five matches. This was considered a landmark achievement within\nAI, as it was widely believed in the AI community that computer\nvictory in Go was at least a few decades away, partly due to the\nenormous number of valid sequences of moves in Go compared to that in\n Chess.[11]\n While this is a remarkable achievement, it should be noted that,\ndespite breathless coverage in the popular\n press,[12]\n AlphaGo, while indisputably a great Go player, is just that. For\nexample, neither AlphaGo nor Watson can understand the rules of Go\nwritten in plain-and-simple English and produce a computer program\nthat can play the game. It’s interesting that there is one\nendeavor in AI that tackles a narrow version of this very problem: In\ngeneral game playing, a machine is given a description of a\nbrand new game just before it has to play the game (Genesereth et al.\n2005). However, the description in question is expressed in a formal\nlanguage, and the machine has to manage to play the game from this\ndescription. Note that this is still far from understanding even a\nsimple description of a game in English well enough to play it.  \nBut what if we consider the history of AI not from the perspective of\nphilosophy, but rather from the perspective of the field with which,\ntoday, it is most closely connected? The reference here is to computer\nscience. From this perspective, does AI run back to well before\nTuring? Interestingly enough, the results are the same: we find that\nAI runs deep into the past, and has always had philosophy in its\nveins. This is true for the simple reason that computer science grew\nout of logic and probability\n theory,[13]\n which in turn grew out of (and is still intertwined with) philosophy.\nComputer science, today, is shot through and through with logic; the\ntwo fields cannot be separated. This phenomenon has become an object\nof study unto itself (Halpern et al. 2001). The situation is no\ndifferent when we are talking not about traditional logic, but rather\nabout probabilistic formalisms, also a significant component of\nmodern-day AI: These formalisms also grew out of philosophy, as nicely\nchronicled, in part, by Glymour (1992). For example, in the one mind\nof Pascal was born a method of rigorously calculating probabilities,\nconditional probability (which plays a particularly large role in AI,\ncurrently), and such fertile philosophico-probabilistic arguments as\n Pascal’s wager,\n according to which it is irrational not to become a Christian.  \nThat modern-day AI has its roots in philosophy, and in fact that these\nhistorical roots are temporally deeper than even Descartes’\ndistant day, can be seen by looking to the clever, revealing cover of\nthe second edition (the third edition is the current one) of the\ncomprehensive textbook\n Artificial Intelligence: A Modern Approach\n (known in the AI community as simply AIMA2e for Russell &\nNorvig, 2002). \nCover of AIMA2e (Russell & Norvig 2002) \nWhat you see there is an eclectic collection of memorabilia that might\nbe on and around the desk of some imaginary AI researcher. For\nexample, if you look carefully, you will specifically see: a picture\nof Turing, a view of Big Ben through a window (perhaps R&N are\naware of the fact that Turing famously held at one point that a\nphysical machine with the power of a universal Turing machine is\nphysically impossible: he quipped that it would have to be the size of\nBig Ben), a planning algorithm described in Aristotle’s De\nMotu Animalium,\n Frege’s fascinating notation for first-order logic,\n a glimpse of Lewis Carroll’s (1958) pictorial representation of\nsyllogistic reasoning, Ramon Lull’s concept-generating wheel\nfrom his 13th-century Ars Magna, and a number of\nother pregnant items (including, in a clever, recursive, and\nbordering-on-self-congratulatory touch, a copy of AIMA itself).\nThough there is insufficient space here to make all the historical\nconnections, we can safely infer from the appearance of these items\n(and here we of course refer to the ancient ones: Aristotle conceived\nof planning as information-processing over two-and-a-half millennia\nback; and in addition, as Glymour (1992) notes, Artistotle can also be\ncredited with devising the first knowledge-bases and ontologies, two\ntypes of representation schemes that have long been central to AI)\nthat AI is indeed very, very old. Even those who insist that AI is at\nleast in part an artifact-building enterprise must concede that, in\nlight of these objects, AI is ancient, for it isn’t just\ntheorizing from the perspective that intelligence is at bottom\ncomputational that runs back into the remote past of human history:\nLull’s wheel, for example, marks an attempt to capture\nintelligence not only in computation, but in a physical artifact that\nembodies that\n computation.[14] \nAIMA has now reached its the third edition, and those interested in\nthe history of AI, and for that matter the history of philosophy of\nmind, will not be disappointed by examination of the cover of the\nthird installment (the cover of the second edition is almost exactly\nlike the first edition). (All the elements of the cover, separately\nlisted and annotated, can be found\n online.)\n One significant addition to the cover of the third edition is a\ndrawing of Thomas Bayes; his appearance reflects the recent rise in\nthe popularity of probabilistic techniques in AI, which we discuss\nlater. \nOne final point about the history of AI seems worth making.  \nIt is generally assumed that the birth of modern-day AI in the 1950s\ncame in large part because of and through the advent of the modern\nhigh-speed digital computer. This assumption accords with\ncommon-sense. After all, AI (and, for that matter, to some degree its\ncousin, cognitive science, particularly computational cognitive\nmodeling, the sub-field of cognitive science devoted to producing\ncomputational simulations of human cognition) is aimed at implementing\nintelligence in a computer, and it stands to reason that such a goal\nwould be inseparably linked with the advent of such devices. However,\nthis is only part of the story: the part that reaches back but to\nTuring and others (e.g., von Neuman) responsible for the first\nelectronic computers. The other part is that, as already mentioned, AI\nhas a particularly strong tie, historically speaking, to reasoning\n(logic-based and, in the need to deal with uncertainty,\ninductive/probabilistic reasoning). In this story, nicely told by\nGlymour (1992), a search for an answer to the question “What is\na proof?” eventually led to an answer based on Frege’s\nversion of first-order logic (FOL): a (finitary) mathematical proof\nconsists in a series of step-by-step inferences from one formula of\nfirst-order logic to the next. The obvious extension of this answer\n(and it isn’t a complete answer, given that lots of classical\nmathematics, despite conventional wisdom, clearly can’t be\nexpressed in FOL; even the Peano Axioms, to be expressed as a finite\nset of formulae, require SOL) is to say that not only\nmathematical thinking, but thinking, period, can be expressed in FOL.\n(This extension was entertained by many logicians long before the\nstart of information-processing psychology and cognitive science\n– a fact some cognitive psychologists and cognitive scientists\noften seem to forget.) Today, logic-based AI is only part of\nAI, but the point is that this part still lives (with help from logics\nmuch more powerful, but much more complicated, than FOL), and it can\nbe traced all the way back to Aristotle’s theory of the\n syllogism.[15]\n In the case of uncertain reasoning, the question isn’t\n“What is a proof?”, but rather questions such as\n“What is it rational to believe, in light of certain\nobservations and probabilities?” This is a question posed and\ntackled long before the arrival of digital computers.  \nSo far we have been proceeding as if we have a firm and precise grasp\nof the nature of AI. But what exactly is AI? Philosophers\narguably know better than anyone that precisely defining a particular\ndiscipline to the satisfaction of all relevant parties (including\nthose working in the discipline itself) can be acutely challenging.\nPhilosophers of science certainly have proposed credible accounts of\nwhat constitutes at least the general shape and texture of a given\nfield of science and/or engineering, but what exactly is the\nagreed-upon definition of physics? What about biology? What, for that\nmatter, is philosophy, exactly? These are remarkably difficult, maybe\neven eternally unanswerable, questions, especially if the target is a\nconsensus definition. Perhaps the most prudent course we can\nmanage here under obvious space constraints is to present in\nencapsulated form some proposed definitions of AI. We do\ninclude a glimpse of recent attempts to define AI in detailed,\nrigorous fashion (and we suspect that such attempts will be of\ninterest to philosophers of science, and those interested in this\nsub-area of philosophy).  \nRussell and Norvig (1995, 2002, 2009), in their aforementioned\nAIMA text, provide a set of possible answers to the “What\nis AI?” question that has considerable currency in the field\nitself. These answers all assume that AI should be defined in terms of\nits goals: a candidate definition thus has the form “AI is the\nfield that aims at building …” The answers all fall under\na quartet of types placed along two dimensions. One dimension is\nwhether the goal is to match human performance, or, instead, ideal\nrationality. The other dimension is whether the goal is to build\nsystems that reason/think, or rather systems that act. The situation\nis summed up in this table: \nFour Possible Goals for AI According to AIMA  \nPlease note that this quartet of possibilities does reflect (at least\na significant portion of) the relevant literature. For example,\nphilosopher John Haugeland (1985) falls into the Human/Reasoning\nquadrant when he says that AI is “The exciting new effort to\nmake computers think … machines with minds, in the\nfull and literal sense.” (By far, this is the quadrant that most\npopular narratives affirm and explore. The recent\n Westworld\n TV series is a powerful case in point.) Luger and Stubblefield (1993)\nseem to fall into the Ideal/Act quadrant when they write: “The\nbranch of computer science that is concerned with the automation of\nintelligent behavior.” The Human/Act position is occupied most\nprominently by Turing, whose test is passed only by those systems able\nto act sufficiently like a human. The “thinking\nrationally” position is defended (e.g.) by Winston (1992). While\nit might not be entirely uncontroversial to assert that the four bins\ngiven here are exhaustive, such an assertion appears to be quite\nplausible, even when the literature up to the present moment is\ncanvassed.  \nIt’s important to know that the contrast between the focus on\nsystems that think/reason versus systems that act, while found, as we\nhave seen, at the heart of the AIMA texts, and at the heart of\nAI itself, should not be interpreted as implying that AI researchers\nview their work as falling all and only within one of these two\ncompartments. Researchers who focus more or less exclusively on\nknowledge representation and reasoning, are also quite prepared to\nacknowledge that they are working on (what they take to be) a central\ncomponent or capability within any one of a family of larger systems\nspanning the reason/act distinction. The clearest case may come from\nthe work on planning – an AI area traditionally making central\nuse of representation and reasoning. For good or ill, much of this\nresearch is done in abstraction (in vitro, as opposed to in vivo), but\nthe researchers involved certainly intend or at least hope that the\nresults of their work can be embedded into systems that actually do\nthings, such as, for example, execute the plans.  \nWhat about Russell and Norvig themselves? What is their answer to the\nWhat is AI? question? They are firmly in the the “acting\nrationally” camp. In fact, it’s safe to say both that they\nare the chief proponents of this answer, and that they have been\nremarkably successful evangelists. Their extremely influential\nAIMA series can be viewed as a book-length defense and\nspecification of the Ideal/Act category. We will look a bit later at\nhow Russell and Norvig lay out all of AI in terms of intelligent\nagents, which are systems that act in accordance with various\nideal standards for rationality. But first let’s look a bit\ncloser at the view of intelligence underlying the AIMA text. We\ncan do so by turning to Russell (1997). Here Russell recasts the\n“What is AI?” question as the question “What is\nintelligence?” (presumably under the assumption that we have a\ngood grasp of what an artifact is), and then he identifies\nintelligence with rationality. More specifically, Russell sees\nAI as the field devoted to building intelligent agents, which\nare functions taking as input tuples of percepts from the external\nenvironment, and producing behavior (actions) on the basis of these\npercepts. Russell’s overall picture is this one: \nThe Basic Picture Underlying Russell’s Account of\nIntelligence/Rationality  \nLet’s unpack this diagram a bit, and take a look, first, at the\naccount of perfect rationality that can be derived from it. The\nbehavior of the agent in the environment \\(E\\) (from a class \\(\\bE\\)\nof environments) produces a sequence of states or snapshots of that\nenvironment. A performance measure \\(U\\) evaluates this sequence;\nnotice the box labeled “Performance Measure” in the above\nfigure. We let \\(V(f,\\bE,U)\\) denote the expected utility\naccording to \\(U\\) of the agent function \\(f\\) operating on\n \\(\\bE\\).[16]\n Now we identify a perfectly rational agent with the agent function:\n \nAccording to the above equation, a perfectly rational agent can be\ntaken to be the function \\(f_{opt}\\) which produces the maximum\nexpected utility in the environment under consideration. Of course, as\nRussell points out, it’s usually not possible to actually build\nperfectly rational agents. For example, though it’s easy enough\nto specify an algorithm for playing invincible chess, it’s not\nfeasible to implement this algorithm. What traditionally happens in AI\nis that programs that are – to use Russell’s apt\nterminology – calculatively rational are constructed\ninstead: these are programs that, if executed infinitely fast,\nwould result in perfectly rational behavior. In the case of chess,\nthis would mean that we strive to write a program that runs an\nalgorithm capable, in principle, of finding a flawless move, but we\nadd features that truncate the search for this move in order to play\nwithin intervals of digestible duration.  \nRussell himself champions a new brand of intelligence/rationality for\nAI; he calls this brand bounded optimality. To understand\nRussell’s view, first we follow him in introducing a\ndistinction: We say that agents have two components: a program, and a\nmachine upon which the program runs. We write \\(Agent(P, M)\\) to\ndenote the agent function implemented by program \\(P\\) running on\nmachine \\(M\\). Now, let \\(\\mathcal{P}(M)\\) denote the set of all\nprograms \\(P\\) that can run on machine \\(M\\). The bounded\noptimal program \\(P_{\\opt,M}\\) then is: \nYou can understand this equation in terms of any of the mathematical\nidealizations for standard computation. For example, machines can be\nidentified with Turing machines minus instructions (i.e., TMs are here\nviewed architecturally only: as having tapes divided into squares upon\nwhich symbols can be written, read/write heads capable of moving up\nand down the tape to write and erase, and control units which are in\none of a finite number of states at any time), and programs can be\nidentified with instructions in the Turing-machine model (telling the\nmachine to write and erase symbols, depending upon what state the\nmachine is in). So, if you are told that you must\n“program” within the constraints of a 22-state Turing\nmachine, you could search for the “best” program given\nthose constraints. In other words, you could strive to find the\noptimal program within the bounds of the 22-state architecture.\nRussell’s (1997) view is thus that AI is the field devoted to\ncreating optimal programs for intelligent agents, under time and space\nconstraints on the machines implementing these\n programs.[17] \nThe reader must have noticed that in the equation for \\(P_{\\opt,M}\\)\nwe have not elaborated on \\(\\bE\\) and \\(U\\) and how equation\n\\eqref{eq1} might be used to construct an agent if the class of\nenvironments \\(\\bE\\) is quite general, or if the true environment\n\\(E\\) is simply unknown. Depending on the task for which one is\nconstructing an artificial agent, \\(E\\) and \\(U\\) would vary. The\nmathematical form of the environment \\(E\\) and the utility function\n\\(U\\) would vary wildly from, say, chess to Jeopardy!. Of\ncourse, if we were to design a globally intelligent agent, and not\njust a chess-playing agent, we could get away with having just one\npair of \\(E\\) and \\(U\\). What would \\(E\\) look like if we were\nbuilding a generally intelligent agent and not just an agent that is\ngood at a single task? \\(E\\) would be a model of not just a single\ngame or a task, but the entire physical-social-virtual universe\nconsisting of many games, tasks, situations, problems, etc. This\nproject is (at least currently) hopelessly difficult as, obviously, we\nare nowhere near to having such a comprehensive theory-of-everything\nmodel. For further discussion of a theoretical architecture put\nforward for this problem, see the\n Supplement on the AIXI architecture.\n  \nIt should be mentioned that there is a different, much more\nstraightforward answer to the “What is AI?” question. This\nanswer, which goes back to the days of the original Dartmouth\nconference, was expressed by, among others, Newell (1973), one of the\ngrandfathers of modern-day AI (recall that he attended the 1956\nconference); it is:  \nThe above definition can be seen as fully specifying a concrete\nversion of Russell and Norvig’s four possible goals. Though few\nare aware of this now, this answer was taken quite seriously for a\nwhile, and in fact underlied one of the most famous programs in the\nhistory of AI: the ANALOGY program of Evans (1968), which solved\ngeometric analogy problems of a type seen in many intelligence tests.\nAn attempt to rigorously define this forgotten form of AI (as what\nthey dub Psychometric AI), and to resurrect it from the days of\nNewell and Evans, is provided by Bringsjord and Schimanski (2003) [see\nalso e.g. (Bringsjord 2011)]. A sizable private investment has been\nmade in the ongoing attempt, now known as\n Project Aristo,\n to build a “digital Aristotle”, in the form of a machine\nable to excel on standardized tests such at the AP exams tackled by US\nhigh school students (Friedland et al. 2004). (Vibrant work in this\ndirection continues today at the\n Allen Institute for Artificial Intelligence.)[18]\n In addition, researchers at Northwestern have forged a connection\nbetween AI and tests of mechanical ability (Klenk et al. 2005).  \nIn the end, as is the case with any discipline, to really know\nprecisely what that discipline is requires you to, at least to some\ndegree, dive in and do, or at least dive in and read. Two decades ago\nsuch a dive was quite manageable. Today, because the content that has\ncome to constitute AI has mushroomed, the dive (or at least the swim\nafter it) is a bit more demanding. \nThere are a number of ways of “carving up” AI. By far the\nmost prudent and productive way to summarize the field is to turn yet\nagain to the AIMA text given its comprehensive overview of the\nfield.  \nAs Russell and Norvig (2009) tell us in the Preface of AIMA:\n \nThe basic picture is thus summed up in this figure: \nImpressionistic Overview of an Intelligent Agent \nThe content of AIMA derives, essentially, from fleshing out\nthis picture; that is, the above figure corresponds to the different\nways of representing the overall function that intelligent agents\nimplement. And there is a progression from the least powerful agents\nup to the more powerful ones. The following figure gives a high-level\nview of a simple kind of agent discussed early in the book. (Though\nsimple, this sort of agent corresponds to the architecture of\nrepresentation-free agents designed and implemented by Rodney Brooks,\n1991.) \nA Simple Reflex Agent \nAs the book progresses, agents get increasingly sophisticated, and the\nimplementation of the function they represent thus draws from more and\nmore of what AI can currently muster. The following figure gives an\noverview of an agent that is a bit smarter than the simple reflex\nagent. This smarter agent has the ability to internally model the\noutside world, and is therefore not simply at the mercy of what can at\nthe moment be directly sensed. \nA More Sophisticated Reflex Agent \nThere are seven parts to AIMA. As the reader passes through\nthese parts, she is introduced to agents that take on the powers\ndiscussed in each part. Part I is an introduction to the agent-based\nview. Part II is concerned with giving an intelligent agent the\ncapacity to think ahead a few steps in clearly defined environments.\nExamples here include agents able to successfully play games of\nperfect information, such as chess. Part III deals with agents that\nhave declarative knowledge and can reason in ways that will be quite\nfamiliar to most philosophers and logicians (e.g., knowledge-based\nagents deduce what actions should be taken to secure their goals).\nPart IV of the book outfits agents with the power to handle\nuncertainty by reasoning in probabilistic\n fashion.[19]\n In Part V, agents are given a capacity to learn. The following figure\nshows the overall structure of a learning agent. \nA Learning Agent \nThe final set of powers agents are given allow them to communicate.\nThese powers are covered in Part VI.  \nPhilosophers who patiently travel the entire progression of\nincreasingly smart agents will no doubt ask, when reaching the end of\nPart VII, if anything is missing. Are we given enough, in general, to\nbuild an artificial person, or is there enough only to build a mere\nanimal? This question is implicit in the following from Charniak and\nMcDermott (1985):  \nTo their credit, Russell & Norvig, in AIMA’s Chapter\n27, “AI: Present and Future,” consider this question, at\nleast to some\n degree.[]\n They do so by considering some challenges to AI that have hitherto\nnot been met. One of these challenges is described by R&N as\nfollows:  \nWhile there has seen some advances in addressing this challenge (in\nthe form of deep learning or representation learning),\nthis specific challenge is actually merely a foothill before a range\nof dizzyingly high mountains that AI must eventually somehow manage to\nclimb. One of those mountains, put simply, is\n reading.[21]\n Despite the fact that, as noted, Part V of AIMA is devoted to\nmachine learning, AI, as it stands, offers next to nothing in the way\nof a mechanization of learning by reading. Yet when you think about\nit, reading is probably the dominant way you learn at this stage in\nyour life. Consider what you’re doing at this very moment.\nIt’s a good bet that you are reading this sentence because,\nearlier, you set yourself the goal of learning about the field of AI.\n Yet\n the formal models of learning provided in AIMA’s Part IV\n(which are all and only the models at play in AI) cannot be applied to\nlearning by\n reading.[22]\n These models all start with a function-based view of learning.\nAccording to this view, to learn is almost invariably to produce an\nunderlying function \\(\\ff\\) on the basis of a restricted set of\npairs \nFor example, consider receiving inputs consisting of 1, 2, 3, 4, and\n5, and corresponding range values of 1, 4, 9, 16, and 25; the goal is\nto “learn” the underlying mapping from natural numbers to\nnatural numbers. In this case, assume that the underlying function is\n\\(n^2\\), and that you do “learn” it. While this narrow\nmodel of learning can be productively applied to a number of\nprocesses, the process of reading isn’t one of them. Learning by\nreading cannot (at least for the foreseeable future) be modeled as\ndivining a function that produces argument-value pairs. Instead, your\nreading about AI can pay dividends only if your knowledge has\nincreased in the right way, and if that knowledge leaves you\npoised to be able to produce behavior taken to confirm sufficient\nmastery of the subject area in question. This behavior can range from\ncorrectly answering and justifying test questions regarding AI, to\nproducing a robust, compelling presentation or paper that signals your\nachievement.  \nTwo points deserve to be made about machine reading. First, it may not\nbe clear to all readers that reading is an ability that is central to\nintelligence. The centrality derives from the fact that intelligence\nrequires vast knowledge. We have no other means of getting systematic\nknowledge into a system than to get it in from text, whether text on\nthe web, text in libraries, newspapers, and so on. You might even say\nthat the big problem with AI has been that machines really don’t\nknow much compared to humans. That can only be because of the fact\nthat humans read (or hear: illiterate people can listen to text being\nuttered and learn that way). Either machines gain knowledge by humans\nmanually encoding and inserting knowledge, or by reading and\nlistening. These are brute facts. (We leave aside supernatural\ntechniques, of course. Oddly enough, Turing didn’t: he seemed to\nthink ESP should be discussed in connection with the powers of minds\nand machines. See Turing,\n 1950.)[23] \nNow for the second point. Humans able to read have invariably also\nlearned a language, and learning languages has been modeled in\nconformity to the function-based approach adumbrated just above\n(Osherson et al. 1986). However, this doesn’t entail that an\nartificial agent able to read, at least to a significant degree, must\nhave really and truly learned a natural language. AI is first and\nforemost concerned with engineering computational artifacts that\nmeasure up to some test (where, yes, sometimes that test is from the\nhuman sphere), not with whether these artifacts process information in\nways that match those present in the human case. It may or may not be\nnecessary, when engineering a machine that can read, to imbue that\nmachine with human-level linguistic competence. The issue is\nempirical, and as time unfolds, and the engineering is pursued, we\nshall no doubt see the issue settled.  \nTwo additional high mountains facing AI are subjective consciousness\nand creativity, yet it would seem that these great challenges are ones\nthe field apparently hasn’t even come to grips with. Mental\nphenomena of paramount importance to many philosophers of mind and\nneuroscience are simply missing from AIMA. For example,\nconsciousness is only mentioned in passing in AIMA, but\nsubjective consciousness is the most important thing in our lives\n– indeed we only desire to go on living because we wish to go on\nenjoying subjective states of certain types. Moreover, if human minds\nare the product of evolution, then presumably phenomenal consciousness\nhas great survival value, and would be of tremendous help to a robot\nintended to have at least the behavioral repertoire of the first\ncreatures with brains that match our own (hunter-gatherers; see Pinker\n1997). Of course, subjective consciousness is largely missing from the\nsister fields of cognitive psychology and computational cognitive\nmodeling as well. We discuss some of these challenges in the\n Philosophy of Artificial Intelligence\n section below. For a list of similar challenges to cognitive science,\nsee the relevant\n section of the entry on cognitive science.[24] \nTo some readers, it might seem in the very least tendentious to point\nto subjective consciousness as a major challenge to AI that it has yet\nto address. These readers might be of the view that pointing to this\nproblem is to look at AI through a distinctively philosophical prism,\nand indeed a controversial philosophical standpoint.  \nBut as its literature makes clear, AI measures itself by looking to\nanimals and humans and picking out in them remarkable mental powers,\nand by then seeing if these powers can be mechanized. Arguably the\npower most important to humans (the capacity to experience) is nowhere\nto be found on the target list of most AI researchers. There may be a\ngood reason for this (no formalism is at hand, perhaps), but there is\nno denying the state of affairs in question obtains, and that, in\nlight of how AI measures itself, that it’s worrisome.  \nAs to creativity, it’s quite remarkable that the power we most\npraise in human minds is nowhere to be found in AIMA. Just as\nin (Charniak & McDermott 1985) one cannot find\n‘neural’ in the index, ‘creativity’\ncan’t be found in the index of AIMA. This is particularly\nodd because many AI researchers have in fact worked on creativity\n(especially those coming out of philosophy; e.g., Boden 1994,\nBringsjord & Ferrucci 2000).  \nAlthough the focus has been on AIMA, any of its counterparts\ncould have been used. As an example, consider Artificial\nIntelligence: A New Synthesis, by Nils Nilsson. As in the case of\nAIMA, everything here revolves around a gradual progression\nfrom the simplest of agents (in Nilsson’s case, reactive\nagents), to ones having more and more of those powers that\ndistinguish persons. Energetic readers can verify that there is a\nstriking parallel between the main sections of Nilsson’s book\nand AIMA. In addition, Nilsson, like Russell and Norvig,\nignores phenomenal consciousness, reading, and creativity. None of the\nthree are even mentioned. Likewise, a recent comprehensive AI textbook\nby Luger (2008) follows the same pattern.  \nA final point to wrap up this section. It seems quite plausible to\nhold that there is a certain inevitability to the structure of an AI\ntextbook, and the apparent reason is perhaps rather interesting. In\npersonal conversation, Jim Hendler, a well-known AI researcher who is\none of the main innovators behind Semantic Web (Berners-Lee, Hendler,\nLassila 2001), an under-development “AI-ready” version of\nthe World Wide Web, has said that this inevitability can be rather\neasily displayed when teaching Introduction to AI; here’s how.\nBegin by asking students what they think AI is. Invariably, many\nstudents will volunteer that AI is the field devoted to building\nartificial creatures that are intelligent. Next, ask for examples of\nintelligent creatures. Students always respond by giving examples\nacross a continuum: simple multi-cellular organisms, insects, rodents,\nlower mammals, higher mammals (culminating in the great apes), and\nfinally human persons. When students are asked to describe the\ndifferences between the creatures they have cited, they end up\nessentially describing the progression from simple agents to ones\nhaving our (e.g.) communicative powers. This progression gives the\nskeleton of every comprehensive AI textbook. Why does this happen? The\nanswer seems clear: it happens because we can’t resist\nconceiving of AI in terms of the powers of extant creatures with which\nwe are familiar. At least at present, persons, and the creatures who\nenjoy only bits and pieces of personhood, are – to repeat\n– the measure of\n AI.[25] \nReasoning based on classical deductive logic is monotonic; that is, if\n\\(\\Phi\\vdash\\phi\\), then for all \\(\\psi\\), \\(\\Phi\\cup\n\\{\\psi\\}\\vdash\\phi\\). Commonsense reasoning is not monotonic. While\nyou may currently believe on the basis of reasoning that your house is\nstill standing, if while at work you see on your computer screen that\na vast tornado is moving through the location of your house, you will\ndrop this belief. The addition of new information causes previous\ninferences to fail. In the simpler example that has become an AI\nstaple, if I tell you that Tweety is a bird, you will infer that\nTweety can fly, but if I then inform you that Tweety is a penguin, the\ninference evaporates, as well it should. Nonmonotonic (or defeasible)\nlogic includes formalisms designed to capture the mechanisms\nunderlying these kinds of examples. See the separate entry on\n logic and artificial intelligence,\n which is focused on nonmonotonic reasoning, and reasoning about time\nand change. It also provides a history of the early days of\nlogic-based AI, making clear the contributions of those who founded\nthe tradition (e.g., John McCarthy and Pat Hayes; see their seminal\n1969 paper).  \nThe formalisms and techniques of logic-based AI have reached a level\nof impressive maturity – so much so that in various academic and\ncorporate laboratories, implementations of these formalisms and\ntechniques can be used to engineer robust, real-world software. It is\nstrongly recommend that readers who have an interest to learn where AI\nstands in these areas consult (Mueller 2006), which provides, in one\nvolume, integrated coverage of nonmonotonic reasoning (in the form,\nspecifically, of circumscription), and reasoning about time and change\nin the situation and event calculi. (The former calculus is also\nintroduced by Thomason. In the second, timepoints are included, among\nother things.) The other nice thing about (Mueller 2006) is that the\nlogic used is multi-sorted first-order logic (MSL), which has\nunificatory power that will be known to and appreciated by many\ntechnical philosophers and logicians (Manzano 1996).  \nWe now turn to three further topics of importance in AI. They are: \nThis trio is covered in order, beginning with the first. \nDetailed accounts of logicist AI that fall under the agent-based\nscheme can be found in (Nilsson 1991, Bringsjord & Ferrucci\n 1998).[26].\n The core idea is that an intelligent agent receives percepts from the\nexternal world in the form of formulae in some logical system (e.g.,\nfirst-order logic), and infers, on the basis of these percepts and its\nknowledge base, what actions should be performed to secure the\nagent’s goals. (This is of course a barbaric simplification.\nInformation from the external world is encoded in formulae,\nand transducers to accomplish this feat may be components of the\nagent.)  \nTo clarify things a bit, we consider, briefly, the logicist view in\nconnection with arbitrary logical systems\n \\(\\mathcal{L}_{X}\\).[27]\n We obtain a particular logical system by setting \\(X\\) in the\nappropriate way. Some examples: If \\(X=I\\), then we have a system at\nthe level of FOL [following the standard notation from model theory;\nsee e.g. (Ebbinghaus et al. 1984)]. \\(\\mathcal{L}_{II}\\) is\nsecond-order logic, and \\(\\mathcal{L}_{\\omega_I\\omega}\\) is a\n“small system” of infinitary logic (countably infinite\nconjunctions and disjunctions are permitted). These logical systems\nare all extensional, but there are intensional ones as\nwell. For example, we can have logical systems corresponding to those\nseen in standard propositional modal logic (Chellas 1980). One\npossibility, familiar to many philosophers, would be propositional\nKT45, or\n \\(\\mathcal{L}_{KT45}\\).[28]\n In each case, the system in question includes a relevant alphabet\nfrom which well-formed formulae are constructed by way of a formal\ngrammar, a reasoning (or proof) theory, a formal semantics, and at\nleast some meta-theoretical results (soundness, completeness, etc.).\nTaking off from standard notation, we can thus say that a set of\nformulas in some particular logical system \\(\\mathcal{L}_X\\),\n\\(\\Phi_{\\mathcal{L}_X}\\), can be used, in conjunction with some\nreasoning theory, to infer some particular formula\n\\(\\phi_{\\mathcal{L}_X}\\). (The reasoning may be deductive, inductive,\nabductive, and so on. Logicist AI isn’t in the least restricted\nto any particular mode of reasoning.) To say that such a situation\nholds, we write \n\n    \\[\n    \\Phi_{\\mathcal{L}_X} \\vdash_{\\mathcal{L}_X} \\phi_{\\mathcal{L}_X}\n    \\]\n\n  \nWhen the logical system referred to is clear from context, or when we\ndon’t care about which logical system is involved, we can simply\nwrite \n\n    \\[\n    \\Phi \\vdash \\phi\n    \\]\n\n  \nEach logical system, in its formal semantics, will include objects\ndesigned to represent ways the world pointed to by formulae in this\nsystem can be. Let these ways be denoted by \\(W^i_{{\\mathcal{L}_X}}\\).\nWhen we aren’t concerned with which logical system is involved,\nwe can simply write \\(W^i\\). To say that such a way models a formula\n\\(\\phi\\) we write \n\n    \\[\n    W_i \\models \\phi\n    \\]\n\n  \nWe extend this to a set of formulas in the natural way:\n\\(W^i\\models\\Phi\\) means that all the elements of \\(\\Phi\\) are true on\n\\(W^i\\). Now, using the simple machinery we’ve established, we\ncan describe, in broad strokes, the life of an intelligent agent that\nconforms to the logicist point of view. This life conforms to the\nbasic cycle that undergirds intelligent agents in the AIMA\nsense.  \nTo begin, we assume that the human designer, after studying the world,\nuses the language of a particular logical system to give to our agent\nan initial set of beliefs \\(\\Delta_0\\) about what this world is like.\nIn doing so, the designer works with a formal model of this world,\n\\(W\\), and ensures that \\(W\\models\\Delta_0\\). Following tradition, we\nrefer to \\(\\Delta_0\\) as the agent’s (starting) knowledge\nbase. (This terminology, given that we are talking about the\nagent’s beliefs, is known to be peculiar, but it\npersists.) Next, the agent ADJUSTS its knowlege base to produce\na new one, \\(\\Delta_1\\). We say that adjustment is carried out by way\nof an operation \\(\\mathcal{A}\\); so\n\\(\\mathcal{A}[\\Delta_0]=\\Delta_1\\). How does the adjustment process,\n\\(\\mathcal{A}\\), work? There are many possibilities. Unfortunately,\nmany believe that the simplest possibility (viz.,\n\\(\\mathcal{A}[\\Delta_i]\\) equals the set of all formulas that can be\ndeduced in some elementary manner from \\(\\Delta_i\\)) exhausts\nall the possibilities. The reality is that adjustment, as\nindicated above, can come by way of any mode of reasoning\n– induction, abduction, and yes, various forms of deduction\ncorresponding to the logical system in play. For present purposes,\nit’s not important that we carefully enumerate all the options.\n \nThe cycle continues when the agent ACTS on the environment, in\nan attempt to secure its goals. Acting, of course, can cause changes\nto the environment. At this point, the agent SENSES the\nenvironment, and this new information \\(\\Gamma_1\\) factors into the\nprocess of adjustment, so that\n\\(\\mathcal{A}[\\Delta_1\\cup\\Gamma_1]=\\Delta_2\\). The cycle of SENSES\n\\(\\Rightarrow\\) ADJUSTS \\(\\Rightarrow\\) ACTS continues to produce\nthe life \\(\\Delta_0,\\Delta_1,\\Delta_2,\\Delta_3,\\ldots,\\) … of\nour agent.  \nIt may strike you as preposterous that logicist AI be touted as an\napproach taken to replicate all of cognition. Reasoning over\nformulae in some logical system might be appropriate for\ncomputationally capturing high-level tasks like trying to solve a math\nproblem (or devising an outline for an entry in the Stanford\nEncyclopedia of Philosophy), but how could such reasoning apply to\ntasks like those a hawk tackles when swooping down to capture\nscurrying prey? In the human sphere, the task successfully negotiated\nby athletes would seem to be in the same category. Surely, some will\ndeclare, an outfielder chasing down a fly ball doesn’t prove\ntheorems to figure out how to pull off a diving catch to save the\ngame! Two brutally reductionistic arguments can be given in support of\nthis “logicist theory of everything” approach towards\ncognition. The first stems from the fact that a complete proof\ncalculus for just first-order logic can simulate all of Turing-level\ncomputation (Chapter 11, Boolos et al. 2007). The second justification\ncomes from the role logic plays in foundational theories of\nmathematics and mathematical reasoning. Not only are foundational\ntheories of mathematics cast in logic (Potter 2004), but there have\nbeen successful projects resulting in machine verification of ordinary\nnon-trivial theorems, e.g., in the\n Mizar project\n alone around 50,000 theorems have been verified (Naumowicz and\nKornilowicz 2009). The argument goes that if any approach to AI can be\ncast mathematically, then it can be cast in a logicist form.  \nNeedless to say, such a declaration has been carefully considered by\nlogicists beyond the reductionistic argument given above. For example,\nRosenschein and Kaelbling (1986) describe a method in which logic is\nused to specify finite state machines. These machines are used at\n“run time” for rapid, reactive processing. In this\napproach, though the finite state machines contain no logic in the\ntraditional sense, they are produced by logic and inference. Real\nrobot control via first-order theorem proving has been demonstrated by\nAmir and Maynard-Reid (1999, 2000, 2001). In fact, you can\n download\n version 2.0 of the software that makes this approach real for a Nomad\n200 mobile robot in an office environment. Of course, negotiating an\noffice environment is a far cry from the rapid adjustments an\noutfielder for the Yankees routinely puts on display, but certainly\nit’s an open question as to whether future machines will be able\nto mimic such feats through rapid reasoning. The question is open if\nfor no other reason than that all must concede that the constant\nincrease in reasoning speed of first-order theorem provers is\nbreathtaking. (For up-to-date news on this increase, visit and monitor\nthe\n TPTP site.)\n There is no known reason why the software engineering in question\ncannot continue to produce speed gains that would eventually allow an\nartificial creature to catch a fly ball by processing information in\npurely logicist fashion.  \nNow we come to the second topic related to logicist AI that warrants\nmention herein: common logic and the intensifying quest for\ninteroperability between logic-based systems using different logics.\nOnly a few brief comments are\n offered.[29]\n Readers wanting more can explore the links provided in the course of\nthe summary.  \nOne standardization is through what is known as\n Common Logic\n (CL), and variants thereof. (CL is published as an\n ISO standard\n – ISO is the International Standards Organization.)\nPhilosophers interested in logic, and of course logicians, will find\nCL to be quite fascinating. From an historical perspective, the advent\nof CL is interesting in no small part because the person spearheading\nit is none other than Pat Hayes, the same Hayes who, as we have seen,\nworked with McCarthy to establish logicist AI in the 1960s. Though\nHayes was not at the original 1956 Dartmouth conference, he certainly\nmust be regarded as one of the founders of contemporary AI.) One of\nthe interesting things about CL, at least as we see it, is that it\nsignifies a trend toward the marriage of logics, and programming\nlanguages and environments. Another system that is a logic/programming\nhybrid is\n Athena,\n which can be used as a programming language, and is at the same time\na form of MSL. Athena is based on formal systems known as\ndenotational proof languages (Arkoudas 2000).  \nHow is interoperability between two systems to be enabled by CL?\nSuppose one of these systems is based on logic \\(L\\), and the other on\n\\(L'\\). (To ease exposition, assume that both logics are first-order.)\nThe idea is that a theory \\(\\Phi_L\\), that is, a set of formulae in\n\\(L\\), can be translated into CL, producing \\(\\Phi_{CL}\\), and then\nthis theory can be translated into \\(\\Phi_L'\\). CL thus becomes an\ninter lingua. Note that what counts as a well-formed formula in\n\\(L\\) can be different than what counts as one in \\(L'\\). The two\nlogics might also have different proof theories. For example,\ninference in \\(L\\) might be based on resolution, while inference in\n\\(L'\\) is of the natural deduction variety. Finally, the symbol sets\nwill be different. Despite these differences, courtesy of the\ntranslations, desired behavior can be produced across the translation.\nThat, at any rate, is the hope. The technical challenges here are\nimmense, but federal monies are increasingly available for attacks on\nthe problem of interoperability. \nNow for the third topic in this section: what can be called\nencoding down. The technique is easy to understand. Suppose\nthat we have on hand a set \\(\\Phi\\) of first-order axioms. As is\nwell-known, the problem of deciding, for arbitrary formula \\(\\phi\\),\nwhether or not it’s deducible from \\(\\Phi\\) is\nTuring-undecidable: there is no Turing machine or equivalent that can\ncorrectly return “Yes” or “No” in the general\ncase. However, if the domain in question is finite, we can encode this\nproblem down to the propositional calculus. An assertion that all\nthings have \\(F\\) is of course equivalent to the assertion that\n\\(Fa\\), \\(Fb\\), \\(Fc\\), as long as the domain contains only these\nthree objects. So here a first-order quantified formula becomes a\nconjunction in the propositional calculus. Determining whether such\nconjunctions are provable from axioms themselves expressed in the\npropositional calculus is Turing-decidable, and in addition, in\ncertain clusters of cases, the check can be done very quickly in the\npropositional case; very quickly. Readers interested in\nencoding down to the propositional calculus should consult recent\n DARPA-sponsored work by Bart Selman.\n Please note that the target of encoding down doesn’t need to be\nthe propositional calculus. Because it’s generally harder for\nmachines to find proofs in an intensional logic than in straight\nfirst-order logic, it is often expedient to encode down the former to\nthe latter. For example, propositional modal logic can be encoded in\nmulti-sorted logic (a variant of FOL); see (Arkoudas & Bringsjord\n2005). Prominent usage of such an encoding down can be found in a set\nof systems known as Description Logics, which are a set of\nlogics less expressive than first-order logic but more expressive than\npropositional logic (Baader et al. 2003). Description logics are used\nto reason about ontologies in a given domain and have been\nsuccessfully used, for example, in the biomedical domain (Smith et al.\n2007).  \nIt’s tempting to define non-logicist AI by negation: an approach\nto building intelligent agents that rejects the distinguishing\nfeatures of logicist AI. Such a shortcut would imply that the agents\nengineered by non-logicist AI researchers and developers, whatever the\nvirtues of such agents might be, cannot be said to know that \\(\\phi\\);\n– for the simple reason that, by negation, the non-logicist\nparadigm would have not even a single declarative proposition that is\na candidate for \\(\\phi\\);. However, this isn’t a particularly\nenlightening way to define non-symbolic AI. A more productive approach\nis to say that non-symbolic AI is AI carried out on the basis of\nparticular formalisms other than logical systems, and to then\nenumerate those formalisms. It will turn out, of course, that these\nformalisms fail to include knowledge in the normal sense. (In\nphilosophy, as is well-known, the normal sense is one according to\nwhich if \\(p\\) is known, \\(p\\) is a declarative statement.)  \nFrom the standpoint of formalisms other than logical systems,\nnon-logicist AI can be partitioned into symbolic but non-logicist\napproaches, and connectionist/neurocomputational approaches. (AI\ncarried out on the basis of symbolic, declarative structures that, for\nreadability and ease of use, are not treated directly by researchers\nas elements of formal logics, does not count. In this category fall\ntraditional semantic networks, Schank’s (1972) conceptual\ndependency scheme, frame-based schemes, and other such schemes.) The\nformer approaches, today, are probabilistic, and are based on the\nformalisms (Bayesian networks) covered\n below.\n The latter approaches are based, as we have noted, on formalisms that\ncan be broadly termed “neurocomputational.” Given our\nspace constraints, only one of the formalisms in this category is\ndescribed here (and briefly at that): the aforementioned artificial\nneural\n networks.[30].\n Though artificial neural networks, with an appropriate architecture,\ncould be used for arbitrary computation, they are almost exclusively\nused for building learning systems.  \nNeural nets are composed of units or nodes designed to\nrepresent neurons, which are connected by links designed to\nrepresent dendrites, each of which has a numeric weight. \nA “Neuron” Within an Artificial Neural Network (from\nAIMA3e) \nIt is usually assumed that some of the units work in symbiosis with\nthe external environment; these units form the sets of input\nand output units. Each unit has a current activation\nlevel, which is its output, and can compute, based on its inputs\nand weights on those inputs, its activation level at the next moment\nin time. This computation is entirely local: a unit takes account of\nbut its neighbors in the net. This local computation is calculated in\ntwo stages. First, the input function, \\(in_i\\), gives the\nweighted sum of the unit’s input values, that is, the sum of the\ninput activations multiplied by their weights:  \nIn the second stage, the activation function, \\(g\\), takes the\ninput from the first stage as argument and generates the output, or\nactivation level, \\(a_i\\):  \nOne common (and confessedly elementary) choice for the activation\nfunction (which usually governs all units in a given net) is the step\nfunction, which usually has a threshold \\(t\\) that sees to it that a 1\nis output when the input is greater than \\(t\\), and that 0 is output\notherwise. This is supposed to be “brain-like” to some\ndegree, given that 1 represents the firing of a pulse from a neuron\nthrough an axon, and 0 represents no firing. A simple three-layer\nneural net is shown in the following picture. \nA Simple Three-Layer Artificial Neural Network (from\nAIMA3e) \nAs you might imagine, there are many different kinds of neural\nnetworks. The main distinction is between feed-forward and\nrecurrent networks. In feed-forward networks like the one\npictured immediately above, as their name suggests, links move\ninformation in one direction, and there are no cycles; recurrent\nnetworks allow for cycling back, and can become rather complicated.\nFor a more detailed presentation, see the \n\n Supplement on Neural Nets.\n  \nNeural networks were fundamentally plagued by the fact that while they\nare simple and have theoretically efficient learning algorithms, when\nthey are multi-layered and thus sufficiently expressive to represent\nnon-linear functions, they were very hard to train in practice. This\nchanged in the mid 2000s with the advent of methods that exploit\nstate-of-the-art hardware better (Rajat et al. 2009). The\nbackpropagation method for training multi-layered neural networks can\nbe translated into a sequence of repeated simple arithmetic operations\non a large set of numbers. The general trend in computing hardware has\nfavored algorithms that are able to do a large of number of simple\noperations that are not that dependent on each other, versus a small\nof number of complex and intricate operations. \nAnother key recent observation is that deep neural networks can be\npre-trained first in an unsupervised phase where they are just fed\ndata without any labels for the data. Each hidden layer is forced to\nrepresent the outputs of the layer below. The outcome of this training\nis a series of layers which represent the input domain with increasing\nlevels of abstraction. For example, if we pre-train the network with\nimages of faces, we would get a first layer which is good at detecting\nedges in images, a second layer which can combine edges to form facial\nfeatures such as eyes, noses etc., a third layer which responds to\ngroups of features, and so on (LeCun et al. 2015). \nPerhaps the best technique for teaching students about neural networks\nin the context of other statistical learning formalisms and methods is\nto focus on a specific problem, preferably one that seems unnatural to\ntackle using logicist techniques. The task is then to seek to engineer\na solution to the problem, using any and all techniques\navailable. One nice problem is handwriting recognition (which\nalso happens to have a rich philosophical dimension; see e.g.\nHofstadter & McGraw 1995). For example, consider the problem of\nassigning, given as input a handwritten digit \\(d\\), the correct\ndigit, 0 through 9. Because there is a database of 60,000 labeled\ndigits available to researchers (from the National Institute of\nScience and Technology), this problem has evolved into a benchmark\nproblem for comparing learning algorithms. It turns out that neural\nnetworks currently reign as the best approach to the problem according\nto a recent ranking by Benenson (2016). \nReaders interested in AI (and computational cognitive science) pursued\nfrom an overtly brain-based orientation are encouraged to explore the\nwork of Rick Granger (2004a, 2004b) and researchers in his\n Brain Engineering Laboratory\n and\n W. H. Neukom Institute for Computational Sciences.\n The contrast between the “dry”, logicist AI started at\nthe original 1956 conference, and the approach taken here by Granger\nand associates (in which brain circuitry is directly modeled) is\nremarkable. For those interested in computational properties of neural\nnetworks, Hornik et al. (1989) address the general representation\ncapability of neural networks independent of learning.  \nAt this point the reader has been exposed to the chief formalisms in\nAI, and may wonder about heterogeneous approaches that bridge them. Is\nthere such research and development in AI? Yes. From an\nengineering standpoint, such work makes irresistibly good\nsense. There is now an understanding that, in order to build\napplications that get the job done, one should choose from a toolbox\nthat includes logicist, probabilistic/Bayesian, and neurocomputational\ntechniques. Given that the original top-down logicist paradigm is\nalive and thriving (e.g., see Brachman & Levesque 2004, Mueller\n2006), and that, as noted, a resurgence of Bayesian and\nneurocomputational approaches has placed these two paradigms on solid,\nfertile footing as well, AI now moves forward, armed with this\nfundamental triad, and it is a virtual certainty that applications\n(e.g., robots) will be engineered by drawing from elements of all\nthree. Watson’s DeepQA architecture is one recent example of an\nengineering system that leverages multiple paradigms. For a detailed\ndiscussion, see the \n\n Supplement on Watson’s DeepQA Architecture.\n  \nGoogle DeepMind’s AlphaGo is another example of a multi-paradigm\nsystem, although in a much narrower form than Watson. The central\nalgorithmic problem in games such as Go or Chess is to search through\na vast sequence of valid moves. For most non-trivial games, this is\nnot feasible to do so exhaustively. The Monte Carlo tree search (MCTS)\nalgorithm gets around this obstacle by searching through an enormous\nspace of valid moves in a statistical fashion (Browne et al. 2012).\nWhile MCTS is the central algorithm in AlpaGo, there are two neural\nnetworks which help evaluate states in the game and help model how\nexpert opponents play (Silver et al. 2016). It should be noted that\nMCTS is behind almost all the winning submissions in general game\nplaying (Finnsson 2012). \nWhat, though, about deep, theoretical integration of the main\nparadigms in AI? Such integration is at present only a possibility for\nthe future, but readers are directed to the research of some striving\nfor such integration. For example: Sun (1994, 2002) has been working\nto demonstrate that human cognition that is on its face symbolic in\nnature (e.g., professional philosophizing in the analytic tradition,\nwhich deals explicitly with arguments and definitions carefully\nsymbolized) can arise from cognition that is neurocomputational in\nnature. Koller (1997) has investigated the marriage between\nprobability theory and logic. And, in general, the very recent arrival\nof so-called human-level AI is being led by theorists seeking\nto genuinely integrate the three paradigms set out above (e.g.,\nCassimatis 2006). \nFinally, we note that cognitive architectures such as Soar\n(Laird 2012) and PolyScheme (Cassimatis 2006) are another area where\nintegration of different fields of AI can be found. For example, one\nsuch endeavor striving to build human-level AI is the Companions\nproject (Forbus and Hinrichs 2006). Companions are long-lived systems\nthat strive to be human-level AI systems that function as\ncollaborators with humans. The Companions architecture tries to solve\nmultiple AI problems such as reasoning and learning, interactivity,\nand longevity in one unifying system. \nAs we noted above, work on AI has mushroomed over the past couple of\ndecades. Now that we have looked a bit at the content that composes\nAI, we take a quick look at the explosive growth of AI. \nFirst, a point of clarification. The growth of which we speak is not a\nshallow sort correlated with amount of funding provided for a given\nsub-field of AI. That kind of thing happens all the time in all\nfields, and can be triggered by entirely political and financial\nchanges designed to grow certain areas, and diminish others. Along the\nsame line, the growth of which we speak is not correlated with the\namount of industrial activity revolving around AI (or a sub-field\nthereof); for this sort of growth too can be driven by forces quite\noutside an expansion in the scientific breadth of\n AI.[31]\n Rather, we are speaking of an explosion of deep content: new\nmaterial which someone intending to be conversant with the field needs\nto know. Relative to other fields, the size of the explosion may or\nmay not be unprecedented. (Though it should perhaps be noted that an\nanalogous increase in philosophy would be marked by the development of\nentirely new formalisms for reasoning, reflected in the fact that,\nsay, longstanding philosophy textbooks like Copi’s (2004)\nIntroduction to Logic are dramatically rewritten and enlarged\nto include these formalisms, rather than remaining anchored to\nessentially immutable core formalisms, with incremental refinement\naround the edges through the years.) But it certainly appears to be\nquite remarkable, and is worth taking note of here, if for no other\nreason than that AI’s near-future will revolve in significant\npart around whether or not the new content in question forms a\nfoundation for new long-lived research and development that would not\notherwise\n obtain.[32] \nAI has also witnessed an explosion in its usage in various artifacts\nand applications. While we are nowhere near building a machine with\ncapabilities of a human or one that acts rationally in all scenarios\naccording to the Russell/Hutter definition above, algorithms that have\ntheir origins in AI research are now widely deployed for many tasks in\na variety of domains.  \nA huge part of AI’s growth in applications has been made\npossible through invention of new algorithms in the subfield of\nmachine learning. Machine learning is concerned with building\nsystems that improve their performance on a task when given examples\nof ideal performance on the task, or improve their performance with\nrepeated experience on the task. Algorithms from machine learning have\nbeen used in speech recognition systems, spam filters, online\nfraud-detection systems, product-recommendation systems, etc. The\ncurrent state-of-the-art in machine learning can be divided into three\nareas (Murphy 2013, Alpaydin 2014): \nIn addition to being used in domains that are traditionally the ken of\nAI, machine-learning algorithms have also been used in all stages of\nthe scientific process. For example, machine-learning techniques are\nnow routinely applied to analyze large volumes of data generated from\nparticle accelerators. CERN, for instance, generates a petabyte\n(\\(10^{15}\\) bytes) per second, and statistical algorithms that have\ntheir origins in AI are used to filter and analyze this data. Particle\naccelerators are used in fundamental experimental research in physics\nto probe the structure of our physical universe. They work by\ncolliding larger particles together to create much finer particles.\nNot all such events are fruitful. Machine-learning methods have been\nused to select events which are then analyzed further (Whiteson &\nWhiteson 2009 and Baldi et al. 2014). More recently, researchers at\nCERN launched a machine learning\n  competition\n to aid in the analysis of the Higgs Boson. The goal of this challenge\nwas to develop algorithms that separate meaningful events from\nbackground noise given data from the Large Hadron Collider, a particle\naccelerator at CERN. \nIn the past few decades, there has been an explosion in data that does\nnot have any explicit semantics attached to it. This data is generated\nby both humans and machines. Most of this data is not easily\nmachine-processable; for example, images, text, video (as opposed to\ncarefully curated data in a knowledge- or data-base). This has given\nrise to a huge industry that applies AI techniques to get usable\ninformation from such enormous data. This field of applying techniques\nderived from AI to large volumes of data goes by names such as\n“data mining,” “big data,”\n“analytics,” etc. This field is too vast to even\nmoderately cover in the present article, but we note that there is no\nfull agreement on what constitutes such a “big-data”\nproblem. One definition, from Madden (2012), is that big data differs\nfrom traditional machine-processable data in that it is too big (for\nmost of the existing state-of-the-art hardware), too quick (generated\nat a fast rate, e.g. online email transactions), or too hard. It is in\nthe too-hard part that AI techniques work quite well. While this\nuniverse is quite varied, we use the Watson’s system later in\nthis article as an AI-relevant exemplar. As we will see later, while\nmost of this new explosion is powered by learning, it isn’t\nentirely limited to just learning. This bloom in learning algorithms\nhas been supported by both a resurgence in neurocomputational\ntechniques and probabilistic techniques. \nOne of the remarkable aspects of (Charniak & McDermott 1985) is\nthis: The authors say the central dogma of AI is that “What the\nbrain does may be thought of at some level as a kind of\ncomputation” (p. 6). And yet nowhere in the book is brain-like\ncomputation discussed. In fact, you will search the index in vain for\nthe term ‘neural’ and its variants. Please note that the\nauthors are not to blame for this. A large part of AI’s growth\nhas come from formalisms, tools, and techniques that are, in some\nsense, brain-based, not logic-based. A paper that conveys the\nimportance and maturity of neurocomputation is (Litt et al. 2006).\n(Growth has also come from a return of probabilistic techniques that\nhad withered by the mid-70s and 80s. More about that momentarily, in\nthe next “resurgence”\n section.)\n  \nOne very prominent class of non-logicist formalism does make an\nexplicit nod in the direction of the brain: viz., artificial neural\nnetworks (or as they are often simply called, neural\nnetworks, or even just neural nets). (The structure of\nneural networks and more recent developments are discussed\n above).\n Because Minsky and Pappert’s (1969) Perceptrons led many\n(including, specifically, many sponsors of AI research and\ndevelopment) to conclude that neural networks didn’t have\nsufficient information-processing power to model human cognition, the\nformalism was pretty much universally dropped from AI. However, Minsky\nand Pappert had only considered very limited neural networks.\nConnectionism, the view that intelligence consists not in\nsymbolic processing, but rather non-symbolic processing at\nleast somewhat like what we find in the brain (at least at the\ncellular level), approximated specifically by artificial neural\nnetworks, came roaring back in the early 1980s on the strength of more\nsophisticated forms of such networks, and soon the situation was (to\nuse a metaphor introduced by John McCarthy) that of two horses in a\nrace toward building truly intelligent agents. \nIf one had to pick a year at which connectionism was resurrected, it\nwould certainly be 1986, the year Parallel Distributed\nProcessing (Rumelhart & McClelland 1986) appeared in print.\nThe rebirth of connectionism was specifically fueled by the\nback-propagation (backpropagation) algorithm over neural networks,\nnicely covered in Chapter 20 of AIMA. The\nsymbolicist/connectionist race led to a spate of lively debate in the\nliterature (e.g., Smolensky 1988, Bringsjord 1991), and some AI\nengineers have explicitly championed a methodology marked by a\nrejection of knowledge representation and reasoning. For example,\nRodney Brooks was such an engineer; he wrote the well-known\n“Intelligence Without Representation” (1991), and his Cog\nProject, to which we referred above, is arguably an incarnation of the\npremeditatedly non-logicist approach. Increasingly, however, those in\nthe business of building sophisticated systems find that both\nlogicist and more neurocomputational techniques are required (Wermter\n& Sun\n 2001).[33]\n In addition, the neurocomputational paradigm today includes\nconnectionism only as a proper part, in light of the fact that some of\nthose working on building intelligent systems strive to do so by\nengineering brain-based computation outside the neural network-based\napproach (e.g., Granger 2004a, 2004b).  \nAnother recent resurgence in neurocomputational techniques has\noccurred in machine learning. The modus operandi in machine learning\nis that given a problem, say recognizing handwritten digits\n\\(\\{0,1,\\ldots,9\\}\\) or faces, from a 2D matrix representing an image\nof the digits or faces, a machine learning or a domain expert would\nconstruct a feature vector representation function for the\ntask. This function is a transformation of the input into a format\nthat tries to throw away irrelevant information in the input and keep\nonly information useful for the task. Inputs transformed by \\(\\rr\\)\nare termed features. For recognizing faces, irrelevant\ninformation could be the amount of lighting in the scene and relevant\ninformation could be information about facial features. The machine is\nthen fed a sequence of inputs represented by the features and the\nideal or ground truth output values for those inputs. This converts\nthe learning challenge from that of having to learn the function\n\\(\\ff\\) from the examples: \\(\\left\\{\\left\\langle x_1,\n\\ff(x_1)\\right\\rangle,\\left\\langle x_2, \\ff(x_2)\\right\\rangle, \\ldots,\n\\left\\langle x_n, \\ff(x_n)\\right\\rangle \\right\\}\\) to having to learn\nfrom possibly easier data: \\(\\left\\{\\left\\langle \\rr(x_1),\n\\ff(x_1)\\right\\rangle,\\left\\langle \\rr(x_2), \\ff(x_2)\\right\\rangle,\n\\ldots, \\left\\langle \\rr(x_n), \\ff(x_n)\\right\\rangle \\right\\}\\). Here\nthe function \\(\\rr\\) is the function that computes the feature vector\nrepresentation of the input. Formally, \\(\\ff\\) is assumed to be a\ncomposition of the functions \\(\\gg\\) and \\(\\rr\\). That is, for any\ninput \\(x\\), \\(f(x) = \\gg\\left(\\rr\\left(x\\right)\\right)\\). This is\ndenoted by \\(\\ff=\\gg\\circ \\rr\\). For any input, the features are first\ncomputed, and then the function \\(\\gg\\) is applied. If the feature\nrepresentation \\(\\rr\\) is provided by the domain expert, the learning\nproblem becomes simpler to the extent the feature representation takes\non the difficulty of the task. At one extreme, the feature vector\ncould hide an easily extractable form of the answer in the input and\nin the other extreme the feature representation could be just the\nplain input. \nFor non-trivial problems, choosing the right representation is vital.\nFor instance, one of the drastic changes in the AI landscape was due\nto Minsky and Papert’s (1969) demonstration that the perceptron\ncannot learn even the binary XOR function, but this function\ncan be learnt by the perceptron if we have the right representation.\nFeature engineering has grown to be one of the most labor intensive\ntasks of machine learning, so much so that it is considered to be one\nof the “black arts” of machine learning. The other\nsignificant black art of learning methods is choosing the right\nparameters. These black arts require significant human expertise and\nexperience, which can be quite difficult to obtain without significant\napprenticeship (Domingos 2012). Another bigger issue is that the task\nof feature engineering is just knowledge representation in a new skin.\n \nGiven this state of affairs, there has been a recent resurgence in\nmethods for automatically learning a feature representation function\n\\(\\rr\\); such methods potentially bypass a large part of human labor\nthat is traditionally required. Such methods are based mostly on what\nare now termed deep neural networks. Such networks are simply\nneural networks with two or more hidden layers. These networks allow\nus to learn a feature function \\(\\rr\\) by using one or more of the\nhidden layers to learn \\(\\rr\\). The general form of learning in which\none learns from the raw sensory data without much hand-based feature\nengineering has now its own term: deep learning. A general and\nyet concise definition (Bengio et al. 2015) is:  \nThough the idea has been around for decades, recent innovations\nleading to more efficient learning techniques have made the approach\nmore feasible (Bengio et al. 2013). Deep-learning methods have\nrecently produced state-of-the-art results in image recognition (given\nan image containing various objects, label the objects from a given\nset of labels), speech recognition (from audio input, generate a\ntextual representation), and the analysis of data from particle\naccelerators (LeCun et al. 2015). Despite impressive results in tasks\nsuch as these, minor and major issues remain unresolved. A minor issue\nis that significant human expertise is still needed to choose an\narchitecture and set up the right parameters for the architecture; a\nmajor issue is the existence of so-called adversarial inputs,\nwhich are indistinguishable from normal inputs to humans but are\ncomputed in a special manner that makes a neural network regard them\nas different than similar inputs in the training data. The existence\nof such adversarial inputs, which remain stable across training data,\nhas raised doubts about how well performance on benchmarks can\ntranslate into performance in real-world systems with sensory noise\n(Szegedy et al. 2014). \nThere is a second dimension to the explosive growth of AI: the\nexplosion in popularity of probabilistic methods that aren’t\nneurocomputational in nature, in order to formalize and mechanize a\nform of non-logicist reasoning in the face of uncertainty.\nInterestingly enough, it is Eugene Charniak himself who can be safely\nconsidered one of the leading proponents of an explicit, premeditated\nturn away from logic to statistical techniques. His area of\nspecialization is natural language processing, and whereas his\nintroductory textbook of 1985 gave an accurate sense of his approach\nto parsing at the time (as we have seen, write computer programs that,\ngiven English text as input, ultimately infer meaning expressed in\nFOL), this approach was abandoned in favor of purely statistical\napproaches (Charniak 1993). At the\n AI@50\n conference, Charniak boldly proclaimed, in a talk tellingly entitled\n“Why Natural Language Processing is Now Statistical Natural\nLanguage Processing,” that logicist AI is moribund, and that the\nstatistical approach is the only promising game in town – for\nthe next 50\n years.[34] \nThe chief source of energy and debate at the conference flowed from\nthe clash between Charniak’s probabilistic orientation, and the\noriginal logicist orientation, upheld at the conference in question by\nJohn McCarthy and others. \nAI’s use of probability theory grows out of the standard form of\nthis theory, which grew directly out of technical philosophy and\nlogic. This form will be familiar to many philosophers, but\nlet’s review it quickly now, in order to set a firm stage for\nmaking points about the new probabilistic techniques that have\nenergized AI.  \nJust as in the case of FOL, in probability theory we are concerned\nwith declarative statements, or propositions, to which degrees\nof belief are applied; we can thus say that both logicist and\nprobabilistic approaches are symbolic in nature. Both approaches also\nagree that statements can either be true or false in the world. In\nbuilding agents, a simplistic logic-based approach requires agents to\nknow the truth-value of all possible statements. This is not\nrealistic, as an agent may not know the truth-value of some\nproposition \\(p\\) due to either ignorance, non-determinism in the\nphysical world, or just plain vagueness in the meaning of the\nstatement. More specifically, the fundamental proposition in\nprobability theory is a random variable, which can be conceived\nof as an aspect of the world whose status is initially unknown to the\nagent. We usually capitalize the names of random variables, though we\nreserve \\(p,q,r, \\ldots\\) as such names as well. For example, in a\nparticular murder investigation centered on whether or not Mr. Barolo\ncommitted the crime, the random variable \\(Guilty\\) might be of\nconcern. The detective may be interested as well in whether or not the\nmurder weapon – a particular knife, let us assume –\nbelongs to Barolo. In light of this, we might say that \\(\\Weapon =\n\\true\\) if it does, and \\(\\Weapon = \\false\\) if it doesn’t. As a\nnotational convenience, we can write \\(weapon\\) and \\(\\lnot weapon\\)\nand for these two cases, respectively; and we can use this convention\nfor other variables of this type.  \nThe kind of variables we have described so far are\n\\(\\mathbf{Boolean}\\), because their \\(\\mathbf{domain}\\) is simply\n\\(\\{true,false\\}.\\) But we can generalize and allow\n\\(\\mathbf{discrete}\\) random variables, whose values are from any\ncountable domain. For example, \\(\\PriceTChina\\) might be a variable\nfor the price of (a particular, presumably) tea in China, and its\ndomain might be \\(\\{1,2,3,4,5\\}\\), where each number here is in US\ndollars. A third type of variable is \\(\\mathbf{continous}\\); its\ndomain is either the reals, or some subset thereof.  \nWe say that an atomic event is an assignment of particular\nvalues from the appropriate domains to all the variables composing the\n(idealized) world. For example, in the simple murder investigation\nworld introduced just above, we have two Boolean variables,\n\\(\\Guilty\\) and \\(\\Weapon\\), and there are just four atomic events.\nNote that atomic events have some obvious properties. For example,\nthey are mutually exclusive, exhaustive, and logically entail the\ntruth or falsity of every proposition. Usually not obvious to\nbeginning students is a fourth property, namely, any proposition is\nlogically equivalent to the disjunction of all atomic events that\nentail that proposition.  \nPrior probabilities correspond to a degree of belief accorded to a\nproposition in the complete absence of any other information. For\nexample, if the prior probability of Barolo’s guilt is \\(0.2\\),\nwe write \n\n    \\[\n    P\\left(\\Guilty=true\\right)=0.2\n    \\]\n\n  \nor simply \\(\\P(guilty)=0.2\\). It is often convenient to have a\nnotation allowing one to refer economically to the probabilities of\nall the possible values for a random variable. For example,\nwe can write \n\n    \\[\n    \\P\\left(\\PriceTChina\\right)\n    \\]\n\n  \nas an abbreviation for the five equations listing all the possible\nprices for tea in China. We can also write \n\n    \\[\n    \\P\\left(\\PriceTChina\\right)=\\langle 1,2,3,4,5\\rangle\n    \\]\n\n  \nIn addition, as further convenient notation, we can write \\(\n\\mathbf{P}\\left(\\Guilty, \\Weapon\\right)\\) to denote the probabilities\nof all combinations of values of the relevant set of random variables.\nThis is referred to as the joint probability distribution of\n\\(\\Guilty\\) and \\(\\Weapon\\). The full joint probability\ndistribution covers the distribution for all the random variables used\nto describe a world. Given our simple murder world, we have 20 atomic\nevents summed up in the equation \n\n    \\[\n    \\mathbf{P}\\left(\\Guilty, \\Weapon, \\PriceTChina\\right)\n    \\]\n\n  \nThe final piece of the basic language of probability theory\ncorresponds to conditional probabilities. Where \\(p\\) and \\(q\\)\nare any propositions, the relevant expression is \\(P\\!\\left(p\\given\nq\\right)\\), which can be interpreted as “the probability of\n\\(p\\), given that all we know is \\(q\\).” For example,\n\n    \\[\n    P\\left(guilty\\ggiven weapon\\right)=0.7\n    \\]\n\n  \nsays that if the murder weapon belongs to Barolo, and no other\ninformation is available, the probability that Barolo is guilty is\n\\(0.7.\\)  \nAndrei Kolmogorov showed how to construct probability theory from\nthree axioms that make use of the machinery now introduced, viz.,  \nThese axioms are clearly at bottom logicist. The remainder of\nprobability theory can be erected from this foundation (conditional\nprobabilities are easily defined in terms of prior probabilities). We\ncan thus say that logic is in some fundamental sense still being used\nto characterize the set of beliefs that a rational agent can have. But\nwhere does probabilistic inference enter the picture on this\naccount, since traditional deduction is not used for inference in\nprobability theory? \nProbabilistic inference consists in computing, from observed evidence\nexpressed in terms of probability theory, posterior probabilities of\npropositions of interest. For a good long while, there have been\nalgorithms for carrying out such computation. These algorithms precede\nthe resurgence of probabilistic techniques in the 1990s. (Chapter 13\nof AIMA presents a number of them.) For example, given the\nKolmogorov axioms, here is a straightforward way of computing the\nprobability of any proposition, using the full joint distribution\ngiving the probabilities of all atomic events: Where \\(p\\) is some\nproposition, let \\(\\alpha(p)\\) be the disjunction of all atomic events\nin which \\(p\\) holds. Since the probability of a proposition (i.e.,\n\\(P(p)\\)) is equal to the sum of the probabilities of the atomic\nevents in which it holds, we have an equation that provides a method\nfor computing the probability of any proposition \\(p\\), viz.,  \nUnfortunately, there were two serious problems infecting this original\nprobabilistic approach: One, the processing in question needed to take\nplace over paralyzingly large amounts of information (enumeration over\nthe entire distribution is required). And two, the expressivity of the\napproach was merely propositional. (It was by the way the philosopher\nHilary Putnam (1963) who pointed out that there was a price to pay in\nmoving to the first-order level. The issue is not discussed herein.)\nEverything changed with the advent of a new formalism that marks the\nmarriage of probabilism and graph theory: Bayesian networks\n(also called belief nets). The pivotal text was (Pearl 1988).\nFor a more detailed discussion, see the \n\n Supplement on Bayesian Networks.\n  \nBefore concluding this section, it is probably worth noting that, from\nthe standpoint of philosophy, a situation such as the murder\ninvestigation we have exploited above would often be analyzed into\narguments, and strength factors, not into numbers to be\ncrunched by purely arithmetical procedures. For example, in the\nepistemology of Roderick Chisholm, as presented his Theory of\nKnowledge (1966, 1977), Detective Holmes might classify a\nproposition like  Barolo committed the murder. as\ncounterbalanced if he was unable to find a compelling argument\neither way, or perhaps probable if the murder weapon turned out\nto belong to Barolo. Such categories cannot be found on a continuum\nfrom 0 to 1, and they are used in articulating arguments for or\nagainst Barolo’s guilt. Argument-based approaches to uncertain\nand defeasible reasoning are virtually non-existent in AI. One\nexception is Pollock’s approach, covered below. This approach is\nChisholmian in nature.  \nIt should also be noted that there have been well-established\nformalisms for dealing with probabilistic reasoning as an instance of\nlogic-based reasoning. E.g., the activity a researcher in\nprobabilistic reasoning undertakes when she proves a theorem \\(\\phi\\)\nabout their domain (e.g. any theorem in (Pearl 1988)) is purely within\nthe realm of traditional logic. Readers interested in logic-flavored\napproaches to probabilistic reasoning can consult (Adams 1996,\nHailperin 1996 & 2010, Halpern 1998). Formalisms marrying\nprobability theory, induction and deductive reasoning, placing them on\nan equal footing, have been on the rise, with Markov logic (Richardson\nand Domingos 2006) being salient among these approaches.  \nProbabilistic Machine Learning \nMachine learning, in the sense given\n above,\n has been associated with probabilistic techniques. Probabilistic\ntechniques have been associated with both the learning of functions\n(e.g. Naive Bayes classification) and the modeling of theoretical\nproperties of learning algorithms. For example, a standard\nreformulation of supervised learning casts it as a Bayesian\nproblem. Assume that we are looking at recognizing digits\n\\([0{-}9]\\) from a given image. One way to cast this problem is to ask\nwhat the probability that the hypothesis \\(H_x\\): “the digit\nis \\(x\\)” is true given the image \\(d\\) from a sensor. Bayes\ntheorem gives us:  \n\\(P(d\\given H_x)\\) and \\(P(H_x)\\) can be estimated from the given\ntraining dataset. Then the hypothesis with the highest posterior\nprobability is then given as the answer and is given by:\n\\(\\argmax_{x}P\\left(d\\ggiven H_x\\right)*P\\left(H_x\\right) \\) In\naddition to probabilistic methods being used to build algorithms,\nprobability theory has also been used to analyze algorithms which\nmight not have an overt probabilistic or logical formulation. For\nexample, one of the central classes of meta-theorems in learning,\nprobably approximately correct (PAC) theorems, are cast in\nterms of lower bounds of the probability that the mismatch between the\ninduced/learnt fL function and the true function\nfT being less than a certain amount, given that the\nlearnt function fL works well for a certain number\nof cases (see Chapter 18, AIMA).  \nFrom at least its modern inception, AI has always been connected to\ngadgets, often ones produced by corporations, and it would be remiss\nof us not to say a few words about this phenomenon. While there have\nbeen a large number of commercial in-the-wild success stories for AI\nand its sister fields, such as optimization and decision-making, some\napplications are more visible and have been thoroughly battle-tested\nin the wild. In 2014, one of the most visible such domains (one in\nwhich AI has been strikingly successful) is information retrieval,\nincarnated as web search. Another recent success story is pattern\nrecognition. The state-of-the-art in applied pattern recognition\n(e.g., fingerprint/face verification, speech recognition, and\nhandwriting recognition) is robust enough to allow\n“high-stakes” deployment outside the laboratory. As of mid\n2018, several corporations and research laboratories have begun\ntesting autonomous vehicles on public roads, with even a handful of\njurisdictions making self-driving cars legal to operate. For example,\nGoogle’s autonomous cars have navigated hundreds of thousands of\nmiles in California with minimal human help under non-trivial\nconditions (Guizzo 2011).  \nComputer games provide a robust test bed for AI techniques as they can\ncapture important parts that might be necessary to test an AI\ntechnique while abstracting or removing details that might beyond the\nscope of core AI research, for example, designing better hardware or\ndealing with legal issues (Laird and VanLent 2001). One subclass of\ngames that has seen quite fruitful for commercial deployment of AI is\nreal-time strategy games. Real-time strategy games are games in which\nplayers manage an army given limited resources. One objective is to\nconstantly battle other players and reduce an opponent’s forces.\nReal-time strategy games differ from strategy games in that players\nplan their actions simultaneously in real-time and do not have to take\nturns playing. Such games have a number of challenges that are\ntantalizing within the grasp of the state-of-the-art. This makes such\ngames an attractive venue in which to deploy simple AI agents. An\noverview of AI used in real-time strategy games can be found in\n(Robertson and Watson 2015).  \nSome other ventures in AI, despite significant success, have been only\nchugging slowly and humbly along, quietly. For instance, AI-related\nmethods have achieved triumphs in solving open problems in mathematics\nthat have resisted any solution for decades. The most noteworthy\ninstance of such a problem is perhaps a proof of the statement that\n“All Robbins algebras are Boolean algebras.” This\nwas conjectured in the 1930s, and the proof was finally discovered by\nthe Otter automatic theorem-prover in 1996 after just a few months of\neffort (Kolata 1996, Wos 2013). Sister fields like formal verification\nhave also bloomed to the extent that it is now not too difficult to\nsemi-automatically verify vital hardware/software components (Kaufmann\net al. 2000 and Chajed et al. 2017). \nOther related areas, such as (natural) language translation, still\nhave a long way to go, but are good enough to let us use them under\nrestricted conditions. The jury is out on tasks such as machine\ntranslation, which seems to require both statistical methods (Lopez\n2008) and symbolic methods (España-Bonet 2011). Both\nmethods now have comparable but limited success in the wild. A\ndeployed translation system at Ford that was initially developed for\ntranslating manufacturing process instructions from English to other\nlanguages initially started out as rule-based system with Ford and\ndomain-specific vocabulary and language. This system then evolved to\nincorporate statistical techniques along with rule-based techniques as\nit gained new uses beyond translating manuals, for example, lay users\nwithin Ford translating their own documents (Rychtyckyj and Plesco\n2012).  \nAI’s great achievements mentioned above so far have all been in\nlimited, narrow domains. This lack of any success in the unrestricted\ngeneral case has caused a small set of researchers to break away into\nwhat is now called\n  artificial general intelligence\n (Goertzel and Pennachin 2007). The stated goals of this movement\ninclude shifting the focus again to building artifacts that are\ngenerally intelligent and not just capable in one narrow domain.  \nComputer Ethics has been around for a long time. In this\nsub-field, typically one would consider how one ought to act in a\ncertain class of situations involving computer technology, where the\n“one” here refers to a human being (Moor 1985). So-called\n“robot ethics” is different. In this sub-field (which goes\nby names such as “moral AI,” “ethical AI,”\n“machine ethics,” “moral robots,” etc.) one is\nconfronted with such prospects as robots being able to make autonomous\nand weighty decisions – decisions that might or might not be\nmorally permissible (Wallach & Allen 2010). If one were to attempt\nto engineer a robot with a capacity for sophisticated ethical\nreasoning and decision-making, one would also be doing Philosophical\nAI, as that concept is characterized\n elsewhere\n in the present entry. There can be many different flavors of\napproaches toward Moral AI. Wallach and Allen (2010) provide a\nhigh-level overview of the different approaches. Moral reasoning is\nobviously needed in robots that have the capability for lethal action.\nArkin (2009) provides an introduction to how we can control and\nregulate machines that have the capacity for lethal behavior. Moral AI\ngoes beyond obviously lethal situations, and we can have a spectrum of\nmoral machines. Moor (2006) provides one such spectrum of possible\nmoral agents. An example of a non-lethal but ethically-charged machine\nwould be a lying machine. Clark (2010) uses a computational theory\nof the mind, the ability to represent and reason about other\nagents, to build a lying machine that successfully persuades people\ninto believing falsehoods. Bello & Bringsjord (2013) give a\ngeneral overview of what might be required to build a moral machine,\none of the ingredients being a theory of mind.  \nThe most general framework for building machines that can reason\nethically consists in endowing the machines with a moral code.\nThis requires that the formal framework used for reasoning by the\nmachine be expressive enough to receive such codes. The field of Moral\nAI, for now, is not concerned with the source or provenance of such\ncodes. The source could be humans, and the machine could receive the\ncode directly (via explicit encoding) or indirectly (reading). Another\npossibility is that the code is inferred by the machine from a more\nbasic set of laws. We assume that the robot has access to some such\ncode, and we then try to engineer the robot to follow that code under\nall circumstances while making sure that the moral code and its\nrepresentation do not lead to unintended consequences. Deontic\nlogics are a class of formal logics that have been studied the\nmost for this purpose. Abstractly, such logics are concerned mainly\nwith what follows from a given moral code. Engineering then studies\nthe match of a given deontic logic to a moral code (i.e., is the logic\nexpressive enough) which has to be balanced with the ease of\nautomation. Bringsjord et al. (2006) provide a blueprint for using\ndeontic logics to build systems that can perform actions in accordance\nwith a moral code. The role deontic logics play in the framework\noffered by Bringsjord et al (which can be considered to be\nrepresentative of the field of deontic logic for moral AI) can be best\nunderstood as striving towards Leibniz’s dream of a universal\nmoral calculus:  \nDeontic logic-based frameworks can also be used in a fashion that is\nanalogous to moral self-reflection. In this mode, logic-based\nverification of the robot’s internal modules can done before the\nrobot ventures out into the real world. Govindarajulu and Bringsjord\n(2015) present an approach, drawing from formal-program\nverification, in which a deontic-logic based system could be used\nto verify that a robot acts in a certain ethically-sanctioned manner\nunder certain conditions. Since formal-verification approaches can be\nused to assert statements about an infinite number of situations and\nconditions, such approaches might be preferred to having the robot\nroam around in an ethically-charged test environment and make a finite\nset of decisions that are then judged for their ethical correctness.\nMore recently, Govindarajulu and Bringsjord (2017) use a deontic logic\nto present a computational model of the\n Doctrine of Double Effect,\n an ethical principle for moral dilemmas that has been studied\nempirically and analyzed extensively by\n philosophers.[35]\n The principle is usually presented and motivated via dilemmas using\ntrolleys and was first presented in this fashion by Foot (1967).  \nWhile there has been substantial theoretical and philosophical work,\nthe field of machine ethics is still in its infancy. There has been\nsome embryonic work in building ethical machines. One recent such\nexample would be Pereira and Saptawijaya (2016) who use logic\nprogramming and base their work in machine ethics on the ethical\ntheory known as contractualism, set out by Scanlon (1982). And\nwhat about the future? Since artificial agents are bound to get\nsmarter and smarter, and to have more and more autonomy and\nresponsibility, robot ethics is almost certainly going to grow in\nimportance. This endeavor might not be a straightforward application\nof classical ethics. For example, experimental results suggest that\nhumans hold robots to different ethical standards than they expect\nfrom humans under similar conditions (Malle et al.\n 2015).[36] \nNotice that the heading for this section isn’t Philosophy\nof AI. We’ll get to that category momentarily. (For now\nit can be identified with the attempt to answer such questions as\nwhether artificial agents created in AI can ever reach the full\nheights of human intelligence.) Philosophical AI is AI, not\nphilosophy; but it’s AI rooted in and flowing from, philosophy.\nFor example, one could engage, using the tools and techniques of\nphilosophy, a paradox, work out a proposed solution, and then proceed\nto a step that is surely optional for philosophers: expressing the\nsolution in terms that can be translated into a computer program that,\nwhen executed, allows an artificial agent to surmount concrete\ninstances of the original\n paradox.[37]\n Before we ostensively characterize Philosophical AI of this sort\ncourtesy of a particular research program, let us consider first the\nview that AI is in fact simply philosophy, or a part thereof.  \nDaniel Dennett (1979) has famously claimed not just that there are\nparts of AI intimately bound up with philosophy, but that AI\nis philosophy (and psychology, at least of the cognitive\nsort). (He has made a parallel claim about Artificial Life (Dennett\n1998)). This view will turn out to be incorrect, but the reasons why\nit’s wrong will prove illuminating, and our discussion will pave\nthe way for a discussion of Philosophical AI.  \nWhat does Dennett say, exactly? This:  \nElsewhere he says his view is that AI should be viewed “as a\nmost abstract inquiry into the possibility of intelligence or\nknowledge” (Dennett 1979, 64).  \nIn short, Dennett holds that AI is the attempt to explain\nintelligence, not by studying the brain in the hopes of identifying\ncomponents to which cognition can be reduced, and not by engineering\nsmall information-processing units from which one can build in\nbottom-up fashion to high-level cognitive processes, but rather by\n– and this is why he says the approach is top-down\n– designing and implementing abstract algorithms that capture\ncognition. Leaving aside the fact that, at least starting in the early\n1980s, AI includes an approach that is in some sense bottom-up (see\nthe neurocomputational paradigm discussed above, in\n Non-Logicist AI: A Summary;\n and see, specifically, Granger’s (2004a, 2004b) work,\nhyperlinked in text immediately above, a specific counterexample), a\nfatal flaw infects Dennett’s view. Dennett sees the potential\nflaw, as reflected in:  \nDennett has a ready answer to this objection. He writes:  \nUnfortunately, this is acutely problematic; and examination of the\nproblems throws light on the nature of AI.  \nFirst, insofar as philosophy and psychology are concerned with the\nnature of mind, they aren’t in the least trammeled by the\npresupposition that mentation consists in computation. AI, at least of\nthe “Strong” variety (we’ll discuss\n“Strong” versus “Weak” AI\n below)\n is indeed an attempt to substantiate, through engineering certain\nimpressive artifacts, the thesis that intelligence is at bottom\ncomputational (at the level of Turing machines and their equivalents,\ne.g., Register machines). So there is a philosophical claim, for sure.\nBut this doesn’t make AI philosophy, any more than some of the\ndeeper, more aggressive claims of some physicists (e.g., that the\n universe is ultimately digital in nature)\n make their field philosophy. Philosophy of physics certainly\nentertains the proposition that the physical universe can be\nperfectly modeled in digital terms (in a series of cellular automata,\ne.g.), but of course philosophy of physics can’t be\nidentified with this doctrine.  \nSecond, we now know well (and those familiar with the relevant formal\nterrain knew at the time of Dennett’s writing) that information\nprocessing can exceed standard computation, that is, can exceed\ncomputation at and below the level of what a Turing machine can muster\n(Turing-computation, we shall say). (Such information\nprocessing is known as hypercomputation, a term coined by\nphilosopher Jack Copeland, who has himself defined such machines\n(e.g., Copeland 1998). The first machines capable of hypercomputation\nwere trial-and-error machines, introduced in the same famous\nissue of the Journal of Symbolic Logic (Gold 1965; Putnam\n1965). A new hypercomputer is the infinite time Turing machine\n(Hamkins & Lewis 2000).) Dennett’s appeal to Church’s\nthesis thus flies in the face of the mathematical facts: some\nvarieties of information processing exceed standard computation (or\nTuring-computation). Church’s thesis, or more precisely, the\nChurch-Turing thesis, is the view that a function \\(f\\) is effectively\ncomputable if and only if \\(f\\) is Turing-computable (i.e., some\nTuring machine can compute \\(f\\)). Thus, this thesis has nothing to\nsay about information processing that is more demanding than what a\nTuring machine can achieve. (Put another way, there is no\ncounter-example to CTT to be automatically found in an\ninformation-processing device capable of feats beyond the reach of\nTMs.) For all philosophy and psychology know, intelligence, even if\ntied to information processing, exceeds what is Turing-computational\nor\n Turing-mechanical.[38]\n This is especially true because philosophy and psychology, unlike AI,\nare in no way fundamentally charged with engineering artifacts, which\nmakes the physical realizability of hypercomputation irrelevant from\ntheir perspectives. Therefore, contra Dennett, to consider AI\nas psychology or philosophy is to commit a serious error, precisely\nbecause so doing would box these fields into only a speck of the\nentire space of functions from the natural numbers (including tuples\ntherefrom) to the natural numbers. (Only a tiny portion of the\nfunctions in this space are Turing-computable.) AI is without question\nmuch, much narrower than this pair of fields. Of course, it’s\npossible that AI could be replaced by a field devoted not to building\ncomputational artifacts by writing computer programs and running them\non embodied Turing machines. But this new field, by definition, would\nnot be AI. Our exploration of AIMA and other textbooks provide\ndirect empirical confirmation of this.  \nThird, most AI researchers and developers, in point of fact, are\nsimply concerned with building useful, profitable artifacts, and\ndon’t spend much time reflecting upon the kinds of abstract\ndefinitions of intelligence explored in this entry (e.g.,\n What Exactly is AI?).\n  \nThough AI isn’t philosophy, there are certainly ways of doing\nreal implementation-focussed AI of the highest caliber that are\nintimately bound up with philosophy. The best way to demonstrate this\nis to simply present such research and development, or at least a\nrepresentative example thereof. While there have been many examples of\nsuch work, the most prominent example in AI is John Pollock’s\nOSCAR project, which stretched over a considerable portion of his\nlifetime. For a detailed presentation and further discussion, see\nthe \n\n Supplement on the OSCAR Project.\n  \nIt’s important to note at this juncture that the OSCAR project,\nand the information processing that underlies it, are without question\nat once philosophy and technical AI. Given that the work in\nquestion has appeared in the pages of Artificial Intelligence,\na first-rank journal devoted to that field, and not to philosophy,\nthis is undeniable (see, e.g., Pollock 2001, 1992). This point is\nimportant because while it’s certainly appropriate, in the\npresent venue, to emphasize connections between AI and philosophy,\nsome readers may suspect that this emphasis is contrived: they may\nsuspect that the truth of the matter is that page after page of AI\njournals are filled with narrow, technical content far from\nphilosophy. Many such papers do exist. But we must distinguish between\nwritings designed to present the nature of AI, and its core methods\nand goals, versus writings designed to present progress on specific\ntechnical issues.  \nWritings in the latter category are more often than not quite narrow,\nbut, as the example of Pollock shows, sometimes these specific issues\nare inextricably linked to philosophy. And of course Pollock’s\nwork is a representative example (albeit the most substantive one).\nOne could just as easily have selected work by folks who don’t\nhappen to also produce straight philosophy. For example, for an entire\nbook written within the confines of AI and computer science, but which\nis epistemic logic in action in many ways, suitable for use in\nseminars on that topic, see (Fagin et al. 2004). (It is hard to find\ntechnical work that isn’t bound up with philosophy in some\ndirect way. E.g., AI research on learning is all intimately bound up\nwith philosophical treatments of induction, of how genuinely new\nconcepts not simply defined in terms of prior ones can be learned. One\npossible partial answer offered by AI is inductive logic\nprogramming, discussed in Chapter 19 of AIMA.)  \nWhat of writings in the former category? Writings in this category,\nwhile by definition in AI venues, not philosophy ones, are nonetheless\nphilosophical. Most textbooks include plenty of material that falls\ninto this latter category, and hence they include discussion of the\nphilosophical nature of AI (e.g., that AI is aimed at building\nartificial intelligences, and that’s why, after all, it’s\ncalled ‘AI’).  \nRecall that we earlier discussed proposed definitions of AI, and\nrecall specifically that these proposals were couched in terms of the\ngoals of the field. We can follow this pattern here: We can\ndistinguish between “Strong” and “Weak” AI by\ntaking note of the different goals that these two versions of AI\nstrive to reach. “Strong” AI seeks to create artificial\npersons: machines that have all the mental powers we have, including\nphenomenal consciousness. “Weak” AI, on the other hand,\nseeks to build information-processing machines that appear to\nhave the full mental repertoire of human persons (Searle 1997).\n“Weak” AI can also be defined as the form of AI that aims\nat a system able to pass not just the Turing Test (again, abbreviated\nas TT), but the Total Turing Test (Harnad 1991). In TTT, a\nmachine must muster more than linguistic indistinguishability: it must\npass for a human in all behaviors – throwing a baseball, eating,\nteaching a class, etc.  \nIt would certainly seem to be exceedingly difficult for philosophers\nto overthrow “Weak” AI (Bringsjord and Xiao 2000). After\nall, what philosophical reason stands in the way of AI\nproducing artifacts that appear to be animals or even humans?\nHowever, some philosophers have aimed to do in “Strong”\nAI, and we turn now to the most prominent case in point.  \nWithout question, the most famous argument in the philosophy of AI is\nJohn Searle’s (1980) Chinese Room Argument (CRA), designed to\noverthrow “Strong” AI. We present a quick summary here and\na “report from the trenches” as to how AI practitioners\nregard the argument. Readers wanting to further study CRA will find an\nexcellent next step in the entry on\n the Chinese Room Argument\n and (Bishop & Preston 2002).  \nCRA is based on a thought-experiment in which Searle himself stars. He\nis inside a room; outside the room are native Chinese speakers who\ndon’t know that Searle is inside it. Searle-in-the-box, like\nSearle-in-real-life, doesn’t know any Chinese, but is fluent in\nEnglish. The Chinese speakers send cards into the room through a slot;\non these cards are written questions in Chinese. The box, courtesy of\nSearle’s secret work therein, returns cards to the native\nChinese speakers as output. Searle’s output is produced by\nconsulting a rulebook: this book is a lookup table that tells him what\nChinese to produce based on what is sent in. To Searle, the Chinese is\nall just a bunch of – to use Searle’s language –\nsquiggle-squoggles. The following schematic picture sums up the\nsituation. The labels should be obvious. \\(O\\) denotes the outside\nobservers, in this case the Chinese speakers. Input is denoted by\n\\(i\\) and output by \\(o\\). As you can see, there is an icon for the\nrulebook, and Searle himself is denoted by \\(P\\). \nThe Chinese Room, Schematic View \nNow, what is the argument based on this thought-experiment? Even if\nyou’ve never heard of CRA before, you doubtless can see the\nbasic idea: that Searle (in the box) is supposed to be everything a\ncomputer can be, and because he doesn’t understand Chinese, no\ncomputer could have such understanding. Searle is mindlessly moving\nsquiggle-squoggles around, and (according to the argument)\nthat’s all computers do,\n fundamentally.[39] \nWhere does CRA stand today? As we’ve already indicated, the\nargument would still seem to be alive and well; witness (Bishop &\nPreston 2002). However, there is little doubt that at least among AI\npractitioners, CRA is generally rejected. (This is of course\nthoroughly unsurprising.) Among these practitioners, the philosopher\nwho has offered the most formidable response out of AI itself is\nRapaport (1988), who argues that while AI systems are indeed\nsyntactic, the right syntax can constitute semantics. It should be\nsaid that a common attitude among proponents of “Strong”\nAI is that CRA is not only unsound, but silly, based as it is on a\nfanciful story (CR) far removed from the practice of AI\n– practice which is year by year moving ineluctably toward\nsophisticated robots that will once and for all silence CRA and its\nproponents. For example, John Pollock (as we’ve noted,\nphilosopher and practitioner of AI) writes:  \nTo wrap up discussion of CRA, we make two quick points, to wit:  \nReaders may wonder if there are philosophical debates that AI\nresearchers engage in, in the course of working in their field (as\nopposed to when they might attend a philosophy conference). Surely, AI\nresearchers have philosophical discussions amongst themselves, right?\n \nGenerally, one finds that AI researchers do discuss among themselves\ntopics in philosophy of AI, and these topics are usually the very same\nones that occupy philosophers of AI. However, the attitude reflected\nin the quote from Pollock immediately above is by far the dominant\none. That is, in general, the attitude of AI researchers is that\nphilosophizing is sometimes fun, but the upward march of AI\nengineering cannot be stopped, will not fail, and will eventually\nrender such philosophizing otiose.  \nWe will return to the issue of the future of AI in the\n final section\n of this entry.  \nFour decades ago, J.R. Lucas (1964) argued that Gödel’s\nfirst incompleteness theorem entails that no machine can ever reach\nhuman-level intelligence. His argument has not proved to be\ncompelling, but Lucas initiated a debate that has produced more\nformidable arguments. One of Lucas’ indefatigable defenders is\nthe physicist Roger Penrose, whose first attempt to vindicate Lucas\nwas a Gödelian attack on “Strong” AI articulated in\nhis The Emperor’s New Mind (1989). This first attempt\nfell short, and Penrose published a more elaborate and more fastidious\nGödelian case, expressed in Chapters 2 and 3 of his Shadows of\nthe Mind (1994).  \nIn light of the fact that readers can turn to the\n entry on the Gödel’s Incompleteness Theorems,\n a full review here is not needed. Instead, readers will be given a\ndecent sense of the argument by turning to an online paper in which\nPenrose, writing in response to critics (e.g., the philosopher David\nChalmers, the logician Solomon Feferman, and the computer scientist\nDrew McDermott) of his Shadows of the Mind, distills the\nargument to a couple of\n paragraphs.[40]\n Indeed, in this paper Penrose gives what he takes to be the perfected\nversion of the core Gödelian case given in SOTM. Here is\nthis version, verbatim:  \nDoes this argument succeed? A firm answer to this question is not\nappropriate to seek in the present entry. Interested readers are\nencouraged to consult four full-scale treatments of the argument\n(LaForte et. al 1998; Bringsjord and Xiao 2000; Shapiro 2003; Bowie\n1982). \nIn addition to the Gödelian and Searlean arguments covered\nbriefly above, a third attack on “Strong” AI (of the\nsymbolic variety) has been widely discussed (though with the rise of\nstatistical machine learning has come a corresponding decrease in the\nattention paid to it), namely, one given by the philosopher Hubert\nDreyfus (1972, 1992), some incarnations of which have been\nco-articulated with his brother, Stuart Dreyfus (1987), a computer\nscientist. Put crudely, the core idea in this attack is that human\nexpertise is not based on the explicit, disembodied, mechanical\nmanipulation of symbolic information (such as formulae in some logic,\nor probabilities in some Bayesian network), and that AI’s\nefforts to build machines with such expertise are doomed if based on\nthe symbolic paradigm. The genesis of the Dreyfusian attack was a\nbelief that the critique of (if you will) symbol-based philosophy\n(e.g., philosophy in the logic-based, rationalist tradition, as\nopposed to what is called the Continental tradition) from such\nthinkers as Heidegger and Merleau-Ponty could be made against the\nrationalist tradition in AI. After further reading and study of\nDreyfus’ writings, readers may judge whether this critique is\ncompelling, in an information-driven world increasingly managed by\nintelligent agents that carry out symbolic reasoning (albeit not even\nclose to the human level).  \nFor readers interested in exploring philosophy of AI beyond what Jim\nMoor (in a recent address – “The Next Fifty Years of AI:\nFuture Scientific Research vs. Past Philosophical Criticisms”\n– as the 2006 Barwise Award winner at the annual eastern\nAmerican Philosophical Association meeting) has called the “the\nbig three” criticisms of AI, there is no shortage of additional\nmaterial, much of it available on the Web. The last chapter of\nAIMA provides a compressed overview of some additional\narguments against “Strong” AI, and is in general not a bad\nnext step. Needless to say, Philosophy of AI today involves much more\nthan the three well-known arguments discussed above, and, inevitably,\nPhilosophy of AI tomorrow will include new debates and problems we\ncan’t see now. Because machines, inevitably, will get smarter\nand smarter (regardless of just how smart they get),\nPhilosophy of AI, pure and simple, is a growth industry. With every\nhuman activity that machines match, the “big” questions\nwill only attract more attention.  \nIf past predictions are any indication, the only thing we know today\nabout tomorrow’s science and technology is that it will be\nradically different than whatever we predict it will be like.\nArguably, in the case of AI, we may also specifically know today that\nprogress will be much slower than what most expect. After all, at the\n1956 kickoff conference (discussed at the start of this entry), Herb\nSimon predicted that thinking machines able to match the human mind\nwere “just around the corner” (for the relevant quotes and\ninformative discussion, see the first chapter of AIMA). As it\nturned out, the new century would arrive without a single machine able\nto converse at even the toddler level. (Recall that when it comes to\nthe building of machines capable of displaying human-level\nintelligence, Descartes, not Turing, seems today to be the better\nprophet.) Nonetheless, astonishing though it may be, serious thinkers\nin the late 20th century have continued to issue incredibly optimistic\npredictions regarding the progress of AI. For example, Hans Moravec\n(1999), in his Robot: Mere Machine to Transcendent Mind,\ninforms us that because the speed of computer hardware doubles every\n18 months (in accordance with Moore’s Law, which has\napparently held in the past), “fourth generation”\nrobots will soon enough exceed humans in all respects, from running\ncompanies to writing novels. These robots, so the story goes, will\nevolve to such lofty cognitive heights that we will stand to them as\nsingle-cell organisms stand to us\n today.[41] \nMoravec is by no means singularly Pollyannaish: Many others in AI\npredict the same sensational future unfolding on about the same rapid\nschedule. In fact, at the aforementioned AI@50 conference, Jim Moor\nposed the question “Will human-level AI be achieved within the\nnext 50 years?” to five thinkers who attended the original 1956\nconference: John McCarthy, Marvin Minsky, Oliver Selfridge, Ray\nSolomonoff, and Trenchard Moore. McCarthy and Minsky gave firm,\nunhesitating affirmatives, and Solomonoff seemed to suggest that AI\nprovided the one ray of hope in the face of fact that our species\nseems bent on destroying itself. (Selfridge’s reply was a bit\ncryptic. Moore returned a firm, unambiguous negative, and declared\nthat once his computer is smart enough to interact with him\nconversationally about mathematical problems, he might take this whole\nenterprise more seriously.) It is left to the reader to judge the\naccuracy of such risky predictions as have been given by Moravec,\nMcCarthy, and\n Minsky.[42] \nThe judgment of the reader in this regard ought to factor in the\nstunning resurgence, very recently, of serious reflection on what is\nknown as “The Singularity,” (denoted by us simply as\nS) the future point at which artificial intelligence exceeds\nhuman intelligence, whereupon immediately thereafter (as the story\ngoes) the machines make themselves rapidly smarter and smarter and\nsmarter, reaching a superhuman level of intelligence that, stuck as we\nare in the mud of our limited mentation, we can’t fathom. For\nextensive, balanced analysis of S, see Eden et al. (2013). \nReaders unfamiliar with the literature on S may be quite\nsurprised to learn the degree to which, among learned folks, this\nhypothetical event is not only taken seriously, but has in fact become\na target for extensive and frequent philosophizing [for a mordant tour\nof the recent thought in question, see Floridi (2015)]. What\narguments support the belief that S is in our future?\nThere are two main arguments at this point: the familiar\nhardware-based one [championed by Moravec, as noted above, and again\nmore recently by Kurzweil (2006)]; and the – as far as we know\n– original argument given by mathematician I. J. Good (1965). In\naddition, there is a recent and related doomsayer argument advanced by\nBostrom (2014), which seems to presuppose that S will occur.\nGood’s argument, nicely amplified and adjusted by Chalmers\n(2010), who affirms the tidied-up version of the argument, runs as\nfollows: \nIn this argument, ‘AI’ is artificial intelligence at the\nlevel of, and created by, human persons, ‘AI\\(^+\\)’\nartificial intelligence above the level of human persons, and\n‘AI\\(^{++}\\)’ super-intelligence constitutive of S.\nThe key process is presumably the creation of one class of\nmachine by another. We have added for convenience ‘HI’ for\nhuman intelligence; the central idea is then: HI will create AI, the\nlatter at the same level of intelligence as the former; AI will create\nAI\\(^+\\); AI\\(^+\\) will create AI\\(^{++}\\); with the ascension\nproceeding perhaps forever, but at any rate proceeding long enough for\nus to be as ants outstripped by gods.  \nThe argument certainly appears to be formally valid. Are its three\npremises true? Taking up such a question would fling us far beyond the\nscope of this entry. We point out only that the concept of one class\nof machines creating another, more powerful class of machines is not a\ntransparent one, and neither Good nor Chalmers provides a rigorous\naccount of the concept, which is ripe for philosophical analysis. (As\nto mathematical analysis, some exists, of course. It is for example\nwell-known that a computing machine at level \\(L\\) cannot possibly\ncreate another machine at a higher level \\(L'\\). For instance, a\nlinear-bounded automaton can’t create a Turing machine.) \nThe Good-Chalmers argument has a rather clinical air about it; the\nargument doesn’t say anything regarding whether machines in\nthe AI\\(^{++}\\) category will be benign, malicious, or munificent.\nMany others gladly fill this gap with dark, dark pessimism. The\nlocus classicus here is without question a widely read paper by\nBill Joy (2000): “Why The Future Doesn’t Need Us.”\nJoy believes that the human race is doomed, in no small part because\nit’s busy building smart machines. He writes:  \nThe 21st-century technologies – genetics, nanotechnology, and\nrobotics (GNR) – are so powerful that they can spawn whole new\nclasses of accidents and abuses. Most dangerously, for the first time,\nthese accidents and abuses are widely within the reach of individuals\nor small groups. They will not require large facilities or rare raw\nmaterials. Knowledge alone will enable the use of them. \nThus we have the possibility not just of weapons of mass destruction\nbut of knowledge-enabled mass destruction (KMD), this destructiveness\nhugely amplified by the power of self-replication.  \nI think it is no exaggeration to say we are on the cusp of the further\nperfection of extreme evil, an evil whose possibility spreads well\nbeyond that which weapons of mass destruction bequeathed to the\nnation-states, on to a surprising and terrible empowerment of extreme\n individuals.[43] \nPhilosophers would be most interested in arguments for this\nview. What are Joy’s? Well, no small reason for the attention\nlavished on his paper is that, like Raymond Kurzweil (2000), Joy\nrelies heavily on an argument given by none other than the Unabomber\n(Theodore Kaczynski). The idea is that, assuming we succeed in\nbuilding intelligent machines, we will have them do most (if not all)\nwork for us. If we further allow the machines to make decisions for us\n– even if we retain oversight over the machines –, we will\neventually depend on them to the point where we must simply accept\ntheir decisions. But even if we don’t allow the machines to make\ndecisions, the control of such machines is likely to be held by a\nsmall elite who will view the rest of humanity as unnecessary –\nsince the machines can do any needed work (Joy 2000). \nThis isn’t the place to assess this argument. (Having said that,\nthe pattern pushed by the Unabomber and his supporters certainly\nappears to be flatly\n invalid.[44])\n In fact, many readers will doubtless feel that no such place exists\nor will exist, because the reasoning here is amateurish. So then, what\nabout the reasoning of professional philosophers on the matter?  \nBostrom has recently painted an exceedingly dark picture of a possible\nfuture. He points out that the “first superintelligence”\ncould have the capability \n Clearly, the most vulnerable premise in this sort of argument\nis that the “first superintelligence” will arrive indeed\narrive. Here perhaps the Good-Chalmers argument provides a basis.\n \nSearle (2014) thinks Bostrom’s book is misguided and\nfundamentally mistaken, and that we needn’t worry. His rationale\nis dirt-simple: Machines aren’t conscious; Bostrom is alarmed at\nthe prospect of malicious machines who do us in; a malicious machine\nis by definition a conscious machine; ergo, Bostrom’s argument\ndoesn’t work. Searle writes:  \nThe positively remarkable thing here, it seems to us, is that Searle\nappears to be unaware of the brute fact that most AI engineers are\nperfectly content to build machines on the basis of the AIMA\nview of AI we presented and explained above: the view according to\nwhich machines simply map percepts to actions. On this view, it\ndoesn’t matter whether the machine really has desires;\nwhat matters is whether it acts suitably on the basis of how AI\nscientists engineer formal correlates to desire. An\nautonomous machine with overwhelming destructive power that\nnon-consciously “decides” to kill doesn’t become\njust a nuisance because genuine, human-level, subjective desire is\nabsent from the machine. If an AI can play the game of chess, and the\ngame of Jeopardy!, it can certainly play the game of war. Just\nas it does little good for a human loser to point out that the\nvictorious machine in a game of chess isn’t conscious, it will\ndo little good for humans being killed by machines to point out that\nthese machines aren’t conscious. (It is interesting to note that\nthe genesis of Joy’s paper was an informal conversation with\nJohn Searle and Raymond Kurzweil. According to Joy, Searle\ndidn’t think there was much to worry about, since he was (and\nis) quite confident that tomorrow’s robots can’t be\n conscious.[45])\n  \nThere are some things we can safely say about tomorrow.\nCertainly, barring some cataclysmic events (nuclear or biological\nwarfare, global economic depression, a meteorite smashing into Earth,\netc.), we now know that AI will succeed in producing artificial\nanimals. Since even some natural animals (mules, e.g.) can be\neasily trained to work for humans, it stands to reason that artificial\nanimals, designed from scratch with our purposes in mind, will be\ndeployed to work for us. In fact, many jobs currently done by humans\nwill certainly be done by appropriately programmed artificial animals.\nTo pick an arbitrary example, it is difficult to believe that\ncommercial drivers won’t be artificial in the future. (Indeed,\nDaimler is already running commercials in which they tout the ability\nof their automobiles to drive “autonomously,” allowing\nhuman occupants of these vehicles to ignore the road and read.) Other\nexamples would include: cleaners, mail carriers, clerical workers,\nmilitary scouts, surgeons, and pilots. (As to cleaners, probably a\nsignificant number of readers, at this very moment, have robots from\niRobot cleaning the carpets in their homes.) It is hard to see how\nsuch jobs are inseparably bound up with the attributes often taken to\nbe at the core of personhood – attributes that would be the most\ndifficult for AI to\n replicate.[46] \nAndy Clark (2003) has another prediction: Humans will gradually\nbecome, at least to an appreciable degree, cyborgs, courtesy of\nartificial limbs and sense organs, and implants. The main driver of\nthis trend will be that while standalone AIs are often desirable, they\nare hard to engineer when the desired level of intelligence is high.\nBut to let humans “pilot” less intelligent machines is a\ngood deal easier, and still very attractive for concrete reasons.\nAnother related prediction is that AI would play the role of a\ncognitive prosthesis for humans (Ford et al. 1997; Hoffman et al.\n2001). The prosthesis view sees AI as a “great equalizer”\nthat would lead to less stratification in society, perhaps similar to\nhow the Hindu-Arabic numeral system made arithmetic available to the\nmasses, and to how the Guttenberg press contributed to literacy\nbecoming more universal.  \nEven if the argument is formally invalid, it leaves us with a question\n– the cornerstone question about AI and the future: Will AI\nproduce artificial creatures that replicate and exceed human cognition\n(as Kurzweil and Joy believe)? Or is this merely an interesting\nsupposition?  \nThis is a question not just for scientists and engineers; it is also a\nquestion for philosophers. This is so for two reasons. One, research\nand development designed to validate an affirmative answer must\ninclude philosophy – for reasons rooted in earlier parts of the\npresent entry. (E.g., philosophy is the place to turn to for robust\nformalisms to model human propositional attitudes in machine terms.)\nTwo, philosophers might well be able to provide arguments that answer\nthe cornerstone question now, definitively. If a version of either of\nthe three arguments against “Strong” AI alluded to above\n(Searle’s CRA; the Gödelian attack; the Dreyfus argument)\nare sound, then of course AI will not manage to produce machines\nhaving the mental powers of persons. No doubt the future holds not\nonly ever-smarter machines, but new arguments pro and con on the\nquestion of whether this progress can reach the human level that\nDescartes declared to be unreachable. ","contact.mail":"Naveen.Sundar.G@gmail.com","contact.domain":"gmail.com"}]
