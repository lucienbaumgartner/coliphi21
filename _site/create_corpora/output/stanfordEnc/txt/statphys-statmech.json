[{"date.published":"2001-04-12","date.changed":"2015-07-24","url":"https://plato.stanford.edu/entries/statphys-statmech/","author1":"Lawrence Sklar","entry":"statphys-statmech","body.text":"\n\n\n\nStatistical mechanics was the first foundational physical theory in\nwhich probabilistic concepts and probabilistic explanation played a\nfundamental role. For the philosopher it provides a crucial test case\nin which to compare the philosophers' ideas about the meaning of\nprobabilistic assertions and the role of probability in explanation\nwith what actually goes on when probability enters a foundational\nphysical theory. The account offered by statistical mechanics of the\nasymmetry in time of physical processes also plays an important role in\nthe philosopher's attempt to understand the alleged asymmetries of\ncausation and of time itself. \n\n\n\nFrom the seventeenth century onward it was realized that material\nsystems could often be described by a small number of descriptive\nparameters that were related to one another in simple lawlike ways.\nThese parameters referred to geometric, dynamical and thermal\nproperties of matter. Typical of the laws was the ideal gas law that\nrelated product of pressure and volume of a gas to the temperature of\nthe gas. \n\nIt was soon realized that a fundamental concept was that of\nequilibrium. Left to themselves systems would change the values of\ntheir parameters until they reached a state where no further changes\nwere observed, the equilibrium state. Further, it became apparent that\nthis spontaneous approach to equilibrium was a time-asymmetric process.\nUneven temperatures, for example, changed until temperatures were\nuniform. This same “uniformization” process held for densities. \n\nProfound studies by S. Carnot of the ability to extract mechanical\nwork out of engines that ran by virtue of the temperature difference\nbetween boiler and condenser led to the introduction by R. Clausius of\none more important parameter describing a material system, its entropy.\nHow was the existence of this simple set of parameters for describing\nmatter and the lawlike regularities connecting them to be explained?\nWhat accounted for the approach to equilibrium and its time asymmetry?\nThat the heat content of a body was a form of energy, convertible to\nand from mechanical work formed one fundamental principle. The\ninability of an isolated system to spontaneously move to a more orderly\nstate, to lower its entropy, constituted another. But why were these\nlaws true? \n\nOne approach, that of P. Duhem and E. Mach and the “energeticists,”\nwas to insist that these principles were autonomous phenomenological\nlaws that needed no further grounding in some other physical\nprinciples. An alternative approach was to claim that the energy in a\nbody stored as heat content was an energy of motion of some kind of\nhidden, microscopic constituents of the body, and to insist that the\nlaws noted, the thermodynamic principles, needed to be accounted for\nout of the constitution of the macroscopic object out of its parts and\nthe fundamental dynamical laws governing the motion of those parts.\nThis is the kinetic theory of heat. \n\nEarly work on kinetic theory by W. Herepath and J. Waterston was\nvirtually ignored, but the work of A. Krönig made kinetic theory a\nlively topic in physics. J. C. Maxwell made a major advance by deriving\nfrom some simple postulates a law for the distribution of velocities of\nthe molecules of a gas when it was in equilibrium. Both Maxwell and L.\nBoltzmann went further, and in different, but related, ways derived an\nequation for the approach to equilibrium of a gas. The equilibrium\ndistribution earlier found by Maxwell could then be shown to be a\nstationary solution of this equation. \n\nThis early work met with vigorous objections. H. Poincaré had\nproven a recurrence theorem for bounded dynamical systems that seemed\nto contradict the monotonic approach to equilibrium demanded by\nthermodynamics. Poincaré's theorem showed that any appropriately\nbounded system in which energy was conserved would of necessity, over\nan infinite time, return an infinite number of times to states\narbitrarily close to the initial dynamical state in which the system\nwas started. J. Loschmidt argued that the time irreversibility of\nthermodynamics was incompatible with the symmetry under time reversal\nof the classical dynamics assumed to govern the motion of the molecular\nconstituents of the object. \n\nPartly driven by the need to deal with these objections explicitly\nprobabilistic notions began to be introduced into the theory by Maxwell\nand Boltzmann. Both realized that equilibrium values for quantities\ncould be calculated by imposing a probability distribution over the\nmicroscopic dynamical states compatible with the constraints placed on\nthe system, and identifying the observed macroscopic values with\naverages over quantities definable from the microscopic states using\nthat probability distribution. But what was the physical justification\nfor this procedure? \n\nBoth also argued that the evolution toward equilibrium demanded in\nthe non-equilibrium theory could also be understood probabilistically.\nMaxwell, introducing the notion of a “demon” who could manipulate the\nmicroscopic states of a system, argued that the law of entropic\nincrease was only probabilistically valid. Boltzmann offered a\nprobabilistic version of his equation describing the approach to\nequilibrium. Without considerable care, however, the Boltzmannian\npicture can still appear contrary to the objections from recurrence and\nreversibility interpreted in a probabilistic manner. \n\nLate in his life Boltzmann responded to the objections to the\nprobabilistic theory by offering a time-symmetric interpretation of the\ntheory. Systems were probabilistically almost always close to\nequilibrium. But transient fluctuations to non-equilibrium states could\nbe expected. Once in a non-equilibrium state it was highly likely that\nboth after and before that state the system was closer to equilibrium.\nWhy then did we live in a universe that was not close to equilibrium?\nPerhaps the universe was vast in space and time and we lived in a\n“small” non-equilibrium fluctuational part of it. We could only find\nourselves in such an “improbable” part, for only in such a region could\nsentient beings exist. Why did we find entropy increasing toward the\nfuture and not toward the past? Here the answer was that just as the\nlocal direction of gravity defined what we meant by the downward\ndirection of space, the local direction in time in which entropy was\nincreasing fixed what we took to be the future direction of time. \n\nIn an important work (listed in the bibliography), P. and T.\nEhrenfest also offered a reading of the Boltzmann equation of approach\nto equilibrium that avoided recurrence objections. Here the solution of\nthe equation was taken to describe not “the overwhelmingly probable\nevolution” of a system, but, instead, the sequence of states that would\nbe found overwhelmingly dominant at different times in a collection of\nsystems all started in the same non-equilibrium condition. Even if each\nindividual system approximately recurred to its initial conditions,\nthis “concentration curve” could still show monotonic change toward\nequilibrium from an initial non-equilibrium condition. \n\nMany of the philosophical issues in statistical mechanics center\naround the notion of probability as it appears in the theory. How are\nthese probabilities to be understood? What justified choosing one\nprobability distribution rather than another? How are the probabilities\nto be used in making predictions within the theory? How are they to be\nused to provide explanations of the observed phenomena? And how are the\nprobability distributions themselves to receive an explanatory account?\nThat is, what is the nature of the physical world that is responsible\nfor the correct probabilities playing the successful role that they do\nplay in the theory? \n\nPhilosophers concerned with the interpretation of probability are\nusually dealing with the following problem: Probability is\ncharacterized by a number of formal rules, the additivity of\nprobabilities for disjoint sets of possibilities being the most central\nof these. But what ought we to take the formal theory to be a theory\nof? Some interpretations are “objectivist,” taking probabilities to be,\npossibly, frequencies of outcomes, or idealized limits of such\nfrequencies or perhaps measures of “dispositions” or “propensities” of\noutcomes in specified test situations. \n\nOther interpretations are “subjectivist,” taking probabilities to be\nmeasures of “degrees of belief,” perhaps evidenced in behavior in\nsituations of risk by choices of available lotteries over outcomes.\nStill another interpretation reads probabilities as measures of a kind\nof “partial logical entailment” among propositions. \n\nAlthough subjectivist (or, rather, logical) interpretations of\nprobability in statistical mechanics have been proffered (by E. Jaynes,\nfor example), most interpreters of the theory opt for an objectivist\ninterpretation of probability. This still leaves open, however,\nimportant questions about just what “objective” feature the posited\nprobabilities of the theory are and how nature contrives to have such\nprobabilities evinced in its behavior. \n\nPhilosophers dealing with statistical explanation have generally\nfocussed on everyday uses of probability in explanation, or the use of\nprobabilistic explanations in such disciplines as the social sciences.\nSometimes it has been suggested that to probabilistically explain an\noutcome is to show it likely to have occurred given the background\nfacts of the world. In other cases it is suggested that to explain an\noutcome probabilistically is to produce facts which raise the\nprobability of that outcome over what it would have been those facts\nbeing ignored. Still others suggest that probabilistic explanation is\nshowing an event to have been the causal outcome of some feature of the\nworld characterized by a probabilistic causal disposition. \n\nThe explanatory patterns of non-equilibrium statistical mechanics\nplace the evolution of the macroscopic features of matter in a pattern\nof probabilities over possible microscopic evolutions. Here the types\nof explanation offered do fit the traditional philosophical models. The\nmain open questions concern the explanatory grounds behind the posited\nprobabilities. In equilibrium theory, as we shall see, the statistical\nexplanatory pattern has a rather different nature. \n\nThe standard method for calculating the properties of an\nenergetically isolated system in equilibrium was initiated by Maxwell\nand Boltzmann and developed by J. Gibbs as the microcanonical ensemble.\nHere a probability distribution is imposed over the set of microscopic\nstates compatible with the external constraints imposed on the system.\nUsing this probability distribution, average values of specified\nfunctions of the microscopic conditions of the gas (phase averages) are\ncalculated. These are identified with the macroscopic conditions. But a\nnumber of questions arise: Why this probability distribution? Why\naverage values for macroscopic conditions? How do phase averages\nrelated to measured features of the macroscopic system? \n\nBoltzmann thought of the proper average values to identify with\nmacroscopic features as being averages over time of quantities\ncalculable from microscopic states. He wished to identify the phase\naverages with such time averages. He realized that this could be done\nif a system started in any microscopic state eventually went through\nall the possible microscopic states. That this was so became known as\nthe ergodic hypothesis. But it is provably false on topological and\nmeasure theoretic grounds. A weaker claim, that a system started in any\nstate would go arbitrarily close to each other microscopic state is\nalso false, and even if true would not do the job needed. \n\nThe mathematical discipline of ergodic theory developed out of these\nearly ideas. When can a phase average be identified with a time average\nover infinite time? G. Birkhoff (with earlier results by J. von\nNeumann) showed that this would be so for all but perhaps a set of\nmeasure zero of the trajectories (in the standard measure used to\ndefine the probability function) if the set of phase points was\nmetrically indecomposable, that is if it could not be divided into more\nthan one piece such that each piece had measure greater than zero and\nsuch that a system started in one piece always evolved to a system in\nthat piece. \n\nBut did a realistic model of a system ever meet the condition of\nmetric indecomposability? What is needed to derive metric\nindecomposability is sufficient instability of the trajectories so that\nthe trajectories do not form groups of non-zero measure which fail to\nwander sufficiently over the entire phase region. The existence of a\nhidden constant of motion would violate metric indecomposability. After\nmuch arduous work, culminating in that of Ya. Sinai, it was shown that\nsome “realistic” models of systems, such as the model of a gas as “hard\nspheres in a box,” conformed to metric indecomposability. On the other\nhand another result of dynamical theory, the Kolmogorov-Arnold-Moser\n(KAM) theorem shows that more realistic models (say of molecules\ninteracting by means of “soft” potentials) are likely not to obey\nergodicity in a strict sense. In these cases more subtle reasoning\n(relying on the many degrees of freedom in a system composed of a vast\nnumber of constituents) is also needed. \n\nIf ergodicity holds what can be shown? It can be shown that for all\nbut a set of measure zero of initial points, the time average of a\nphase quantity over infinite time will equal its phase average. It can\nbe shown that for any measurable region the average time the system\nspends in that region will be proportional to the region's size (as\nmeasured by the probability measure used in the microcanonical\nensemble). A solution to a further problem is also advanced. Boltzmann\nknew that the standard probability distribution was invariant under\ntime evolution given the dynamics of the systems. But how could we know\nthat it was the only such invariant measure? With ergodicity we can\nshow that the standard probability distribution is the only one that is\nso invariant, at least if we confine ourselves to probability measures\nthat assign probability zero to every set assigned zero by the standard\nmeasure. \n\nWe have, then, a kind of “transcendental deduction” of the standard\nprobability assigned over microscopic states in the case of\nequilibrium. Equilibrium is a time-unchanging state. So we demand that\nthe probability measure by which equilibrium quantities are to be\ncalculated be stationary in time as well. If we assume that probability\nmeasures assigning non-zero probability to sets of states assigned zero\nby the usual measure can be ignored, then we can show that the standard\nprobability is the only such time invariant probability under the\ndynamics that drives the individual systems from one microscopic state\nto another. \n\nAs a full “rationale” for standard equilibrium statistical\nmechanics, however, much remains questionable. There is the problem\nthat strict ergodicity is not true of realistic systems. There are many\nproblems encountered if one tries to use the rationale as Boltzmann\nhoped to identify phase averages with measured quantities relying on\nthe fact that macroscopic measurements take “long times” on a molecular\nscale. There are the problems introduced by the fact that all of the\nmathematically legitimate ergodic results are qualified by exceptions\nfor “sets of measure zero.” What is it physically that makes it\nlegitimate to ignore a set of trajectories just because it has measure\nzero in the standard measure? After all, such neglect leads to\ncatastrophically wrong predictions when there really are hidden, global\nconstants of motion. In proving the standard measure uniquely\ninvariant, why are we entitled to ignore probability measures that\nassign non-zero probabilities to sets of conditions assigned\nprobability zero in the standard measure? After all, it was just the\nuse of that standard measure that we were trying to justify in the\nfirst place. \n\nIn any case, equilibrium theory as an autonomous discipline is\nmisleading. What we want, after all, is a treatment of equilibrium in\nthe non-equilibrium context. We would like to understand how and why\nsystems evolve from any initially fixed macroscopic state, taking\nequilibrium to be just the “end point” of such dynamic evolution. So it\nis to the general account of non-equilibrium we must turn if we want a\nfuller understanding of how this probabilistic theory is functioning in\nphysics. \n\nBoltzmann provided an equation for the evolution of the distribution\nof the velocities of particles from a non-equilibrium initial state for\ndilute gases, the Boltzmann equation. A number of subsequent equations\nhave been found for other types of systems, although generalizing to,\nsay, dense gases has proven intractable. All of these equations are\ncalled kinetic equations. \n\nHow may they be justified and explained? In the discussions\nconcerning the problem of irreversibility that ensued after Boltzmann's\nwork, attention was focussed on a fundamental assumption he made: the\nhypothesis with regard to collision numbers. This time-asymmetrical\nassumption posited that the motions of the molecules in a gas were\nstatistically uncorrelated prior to the molecules colliding. In\nderiving any of the other kinetic equations a similar such posit must\nbe made. Some general methods for deriving such equations are the\nmaster equation approach and an approach that relies upon\ncoarse-graining the phase space of points representing the micro-states\nof the system into finite cells and assuming fixed transition\nprobabilities from cell to cell (Markov assumption). But such an\nassumption was not derived from the underlying dynamics of the system,\nand, for all they knew so far, might have been inconsistent with that\ndynamics. \n\nA number of attempts have been made to do without such an assumption\nand to derive the approach to equilibrium out of the underlying\ndynamics of the system. Since that dynamics is invariant under time\nreversal and the kinetic equations are time asymmetric, time asymmetry\nmust be put into the explanatory theory somewhere. \n\nOne approach to deriving the kinetic equations relies upon work\nwhich generalizes ergodic theory. Relying upon the instability of\ntrajectories, one tries to show that a region of phase points\nrepresenting the possible micro-states for a system prepared in a\nnon-equilibrium condition will, if the constraints are changed,\neventually evolve into a set of phase points that is “coarsely” spread\nover the entire region of phase space allowed by the changed\nconstraints. The old region cannot “finely” cover the new region by a\nfundamental theorem of dynamics (Liouville's theorem). But, in a manner\nfirst described by Gibbs, it can cover the region in a coarse-grained\nsense. To show that a collection of points will spread in such a way\n(in the infinite time limit at least) one tries to show the system\npossessed of an appropriate “randomization” property. In order of\nincreasing strength such properties include weak-mixing, mixing, being\na K system or being a Bernoulli system. Other, topological as opposed\nto measure-theoretic, approaches to this problem exist as well. \n\nAs usual, many caveats apply. Can the system really be shown to have\nsuch a randomizing feature (in the light of the KAM theorem, for\nexample)? Are infinite time limit results relevant to our physical\nexplanations? If the results are finite time, are they relativized in\nthe sense of saying that they only hold for some coarse partitionings\nof the system rather than to those of experimental interest? \n\nMost importantly, mixing and its ilk cannot be the whole story. All\nthe results of this theory are time symmetric. To get time asymmetric\nresults, and to get results that hold in finite times and which show\nevolution in the manner described by the kinetic equation over those\nfinite times, requires an assumption as well about how the probability\nis to be distributed over the region of points allowed as representing\nthe system at the initial moment. \n\nWhat must that probability assumption look like and how may it be\njustified? These questions were asked, and partly explored, by N.\nKrylov. Attempts at rationalizing this initial probability assumption\nhave ranged from Krylov's own suggestion that it is the result of a\nnon-quantum “uncertainty” principle founded physically on the modes by\nwhich we prepare systems, to the suggestion that it is the result of an\nunderlying stochastic nature of the world described as in the\nGhirardi-Rimini-Weber approach to understanding measurement in quantum\nmechanics. The status and explanation of the initial probability\nassumption remains the central puzzle of non-equilibrium statistical\nmechanics. \n\nThere are other approaches to understanding the approach to\nequilibrium at variance with the approaches that rely on mixing\nphenomena. O. Lanford, for example, has shown that for an idealized\ninfinitely dilute gas one can show, for very small time intervals, an\noverwhelmingly likely behavior of the gas according to the Boltzmann\nequation. Here the interpretation of that equation by the Ehrenfests,\nthe interpretation suitable to the mixing approach, is being dropped in\nfavor of the older idea of the equation describing the overwhelmingly\nprobable evolution of a system. This derivation has the virtue of\nrigorously generating the Boltzmann equation, but at the cost of\napplying only to one severely idealized system and then only for a very\nshort time (although the result may be true, if unproven, for longer\ntime scales). Once again an initial probability distribution is still\nnecessary for time asymmetry. \n\nThe thermodynamic principles demand a world in which physical\nprocesses are asymmetric in time. Entropy of an isolated system may\nincrease spontaneously into the future but not into the past. But the\ndynamical laws governing the motion of the micro-constituents are, at\nleast on the standard views of those laws as being the usual laws of\nclassical or quantum dynamics, time reversal invariant. Introducing\nprobabilistic elements into the underlying theory still does not by\nitself explain where time asymmetry gets into the explanatory account.\nEven if, following Maxwell, we take the Second Law of thermodynamics to\nbe merely probabilistic in its assertions, it remains time\nasymmetric. \n\nThroughout the history of the discipline suggestions have often been\nmade to the effect that some deep, underlying dynamical law itself\nintroduces time asymmetry into the motion of the\nmicro-constituents. \n\nOne approach is to deny the time asymmetry of the dynamics governing\nthe micro-constituents and to look for a replacement law that is\nitself time asymmetric.  A modern version of this looks to an\ninterpretation of quantum mechanics that seeks to explain the\nnotorious “collapse of the wave packet” upon measurement.  Ghirardi,\nRimini and Weber (GRW) have posited the existence of a purely\nstochastic process deeper than that of the usual quantum evolution.\nThis pure chance process will quickly drive macroscopic systems into\nnear eigenfunctions of position while leaving isolated micro-systems\nin superposition states.  The stochastic process is asymmetric in time\n(as is the collapse of the wave function upon measurement).  D. Albert\nhas suggested that such a GRW process, if real, might also be invoked\nto account for the time asymmetry of the dynamics of systems that\nneeds to be accounted for in thermodynamics.  The time asymmetry of\nthe GRW collapse might work by directly affecting the dynamics of the\nsystem, or it might do its job by appropriately randomizing the\ninitial states of isolated systems.  Little has yet been done to fill\nin the details to see if the posited GRW processes could, if real,\naccount for the know thermodynamic asymmetries.  And, of course, there\nis much skepticism that the GRW processes are even real. \n\nOther proposals take the entropic change of a system to be mediated\nby an actually uneliminable “interference” into the system of random\ncausal influences from outside the system. It is impossible, for\nexample, to genuinely screen the system from subtle gravitational\ninfluences from the outside. The issue of the role of external\ninterference in the apparently spontaneous behavior of what is\nidealized as an isolated system has been much discussed. Here the\nexistence of special systems (such as spin echo systems encountered in\nnuclear magnetic resonance) plays a role in the arguments. For these\nsystems seem to display spontaneous approach to equilibrium when\nisolated, yet can have their apparent entropic behavior made to “go\nbackward” with an appropriate impulse from out side the system. This\nseems to show entropic increase without the kind of interference from\nthe outside that genuinely destroys the initial order implicit in the\nsystem. In any case, it is hard to see how outside interference would\ndo the job of introducing time asymmetry unless such asymmetry is put\nin “by hand” in characterizing that interference. \n\nIt was Boltzmann who first proposed a kind of “cosmological”\nsolution to the problem. As noted above he suggested a universe overall\nclose to equilibrium with “small” sub-regions in fluctuations away from\nthat state. In such a sub-region we would find a world far from\nequilibrium. Introducing the familiar time-symmetric probabilistic\nassumptions, it becomes likely that in such a region one finds states\nof lower entropy in one time direction and states of higher entropy in\nthe other. Then finish the solution by introducing the other Boltzmann\nsuggestion that what we mean by the future direction of time is fixed\nas that direction of time in which entropy is increasing. \n\nCurrent cosmology sees quite a different universe than that posited\nby Boltzmann. As far as we can tell the universe as a whole is in a\nhighly non-equilibrium state with parallel entropic increase into the\nfuture everywhere. But the structure of the cosmos as we know it allows\nfor an alternative solution to the problem of the origin of time\nasymmetry in thermodynamics. The universe seems to be spatially\nexpanding, with an origin some tens of billions of years ago in an\ninitial singularity, the Big Bang. Expansion, however, by itself does\nnot provide the time asymmetry needed for thermodynamics, for an\nexpanding universe with static or decreasing entropy is allowed by\nphysics. Indeed, in some cosmological models in which the universe\ncontracts after expanding, it is usually, though not always, assumed\nthat even in contraction entropy continues to increase. \n\nThe source of entropic asymmetry is sought, rather, in the physical\nstate of the world at the Big Bang. Matter “just after” the Big Bang is\nusually posited to be in a state of maximum entropy – to be in\nthermal equilibrium. But this does not take account of the structure of\n“space itself,” or, if you wish, of the way in which the matter is\ndistributed in space and subject to the universal gravitational\nattraction of all matter for all other matter. A world in which matter\nis distributed with uniformity is one of low entropy. A high entropy\nstate is one in which we find a clustering of matter into dense regions\nwith lots of empty space separating these regions. This deviation from\nthe usual expectation – spatial uniformity as the state of\nhighest entropy – is due to the fact that gravity, unlike the\nforces governing the interaction of molecules in a gas for example, is\na purely attractive force. \n\nOne can then posit an initial “very low entropy” state for the Big\nBang, with the spatial uniformity of matter providing an “entropic\nresevoir.” As the universe expands, matter goes from a uniformly\ndistributed state with temperature also uniform to one in which matter\nis highly clumped into hot stars in an environment of cold empty space.\nOne then has the universe as we know it, with its thermally highly\nnon-equilibrium condition. “Initial low entropy,” then, will be a state\nin the past not (as far as we know) matched by any singularity of any\nkind, much less one of low entropy, in the future. If one\nconditionalizes on that initial low entropy state one then gets, using\nthe time symmetric probabilities of statistical mechanics, a prediction\nof a universe whose entropy increased in time. \n\nBut it is not, of course, the entropy of the whole universe with which\nthe Second Law is concerned, but, rather, that of “small” systems\ntemporarily energetically isolated from their environments. One can\nargue, in a manner tracing back to H. Reichenbach, that the entropic\nincrease of the universe as a whole will lead, again using the usual\ntime symmetric probabilistic posits, to a high probability that a\nrandom “branch system” will show entropic increase parallel to that of\nthe universe and parallel to that of other branch systems. Most of the\narguments in the literature that this will be so are flawed, but the\ninference is reasonable nonetheless.  It has also been suggested that\nif one invokes some underlying statistical dynamic law (such as the\nGRW law noted above), one need not posit a branch system hypothesis in\naddition to initial low entropy to derive the thermodynamic\nresults. \n\nPositing initial low entropy for the Big Bang gives rise to its own\nset of “philosophical” questions: Given the standard probabilities in\nwhich high entropy is overwhelmingly probable, how could we explain the\nradically “unexpected” low entropy of the initial state? Indeed, can we\napply probabilistic reasoning appropriate for systems in the universe\nas we know it to an initial state for the universe as a whole? The\nissues here are reminiscent of the old debates over the teleological\nargument for the existence of God. \n\nIt comes as no surprise that the relationship of the older\nthermodynamic theory to the new statistical mechanics on which it is\n“grounded” is one of some complexity. \n\nThe older theory had no probabilistic qualifications to its laws.\nBut as Maxwell was clearly aware, it could not then be “exactly” true\nif the new probabilistic theory correctly described the world. One can\neither keep the thermodynamic theory in its traditional form and\ncarefully explicate the relationship its principles bear to the newer\nprobabilistic conclusions, or one can, as has been done in deeply\ninteresting ways, generate a new “statistical thermodynamics” that\nimports into the older theory probabilistic structure. \n\nConceptually the relationship of older to newer theory is quite\ncomplex. Concepts of the older theory (volume, pressure, temperature,\nentropy) must be related to the concepts of the newer theory (molecular\nconstitution, dynamical concepts governing the motion of the molecular\nconstituents, probabilistic notions characterizing either the states of\nan individual system or distributions of states over an imagined\nensemble of systems subject to some common constraints). \n\nA single term of the thermodynamic theory such as\n‘entropy’ will be associated with a wide variety of\nconcepts defined in the newer account. There is, for example, Boltzmann\nentropy which is the property of a single system defined in terms of\nthe spatial and momentum distribution of its molecules. On the other\nhand there are the Gibbs'entropies, definable out of the probability\ndistribution over some Gibbsian ensemble of systems. Adding even more\ncomplications there is, for example, Gibbs' fine grained entropy which\nis defined by the ensemble probability alone and is very useful in\ncharacterizing equilibrium states and Gibbs' coarse grained entropy\nwhose definition requires some partitioning of the phase space into\nfinite cells as well as the original probability distribution and which\nis a useful concept in characterizing approach to equilibrium from the\nensemble perspective. In addition to these notions which are measure\ntheoretic in nature, there are topological notions which can play the\nrole of a kind of entropy as well. \n\nNothing in this complexity stands in the way of claiming that\nstatistical mechanics describes the world in a way that explains why\nthermodynamics works and works as well as it does. But the complexity\nof the inter-relationship between the theories should make the\nphilosopher cautious in using this relationship as a well understood\nand simple paradigm of inter-theoretic reduction. \n\nIt is of some philosophical interest that the relationship of\nthermodynamics to statistical mechanics shows some similarity to\naspects uncovered in functionalist theories of the mind-body\nrelationship. Consider, for example, the fact that systems of very\ndifferent physical constitutions (say a gas made up of molecules\ninteracting by means of forces on the one hand and on the other hand\nradiation whose components are energetically coupled wave lengths of\nlight) can share thermodynamic features. They can, for example, be at\nthe same temperature. Physically this means that the two systems, if\ninitially in equilibrium and then energetically coupled, will retain\ntheir original equilibrium conditions. The parallel with the claim that\na functionally defined mental state (a belief, say) can be instantiated\nin a wide variety of physical devices is clear. \n\nWe have noted that it was Boltzmann who first suggested that our\nvery concept of the future direction of time was fixed by the direction\nin time in which entropy was increasing in our part of the universe.\nNumerous authors have followed up this suggestion and the “entropic”\ntheory of time asymmetry remains a much debated topic in the philosophy\nof time. \n\nWe must first ask what the theory is really claiming. In a sensible\nversion of the theory there is no claim being made to the effect that\nwe find out the time order of events by checking the entropy of systems\nand taking the later event as the one in which some system has its\nhigher entropy. The claim is, rather, that it is the facts about the\nentropic asymmetry of systems in time that “ground” the phenomena that\nwe usually think of as marking out the asymmetrical nature of time\nitself. \n\nWhat are some features whose intuitive temporal asymmetry we think\nof as, perhaps, “constituting” the asymmetrical nature of time? There\nare asymmetries of knowledge: We have memories and records of the past,\nbut not of the future. There are asymmetries of determination: We think\nof causation as going from past through present to future, and not of\ngoing the other way round. There are asymmetries of concern: We may\nregret the past, but we anxiously anticipate the future. There are\nalleged asymmetries of “determinateness” of reality: It is sometimes\nclaimed that past and present have determinate reality, but that the\nfuture, being a realm of mere possibilities, has no such determinate\nbeing at all. \n\nThe entropic theory in its most plausible formulation is a claim to\nthe effect that we can explain the origin of all of these intuitive\nasymmetries by referring to fact about the entropic asymmetry of the\nworld. \n\nThis can be best understood by looking at the very analogy used by\nBoltzmann: the gravitational account of up and down. What do we mean by\nthe downward direction at a spatial location? All of the phenomena by\nwhich we intuitively identify the downward direction (as the direction\nin which rocks fall, for example) receive an explanation in terms of\nthe spatial direction of the local gravitational force. Even our\nimmediate awareness of which direction is down is explainable in terms\nof the effect of gravity on the fluid in our semi-circular canals. It\ncomes as no shock to us at all that “down” for Australia is in the\nopposite direction as “down” for Chicago. Nor are we dismayed to be\ntold that in outer space, far from a large gravitating object such as\nthe Earth, there is no such thing as the up-down distinction and no\ndirection of space which is the downward direction. \n\nSimilarly the entropic theorist claims that it is the entropic\nfeatures that explain the intuitive asymmetries noted above, that in\nregions of the universe in which the entropic asymmetry was\ncounter-directed in time the past-future directions of time would be\nopposite, and that in a region of the universe without an entropic\nasymmetry neither direction of time would count as past or as\nfuture. \n\nThe great problem remains in trying to show that the entropic\nasymmetry is explanatorily adequate to account for all the other\nasymmetries in the way that the gravitational asymmetry can account for\nthe distinction of up and down. Despite many interesting contributions\nto the literature on this, the problem remains unresolved.","contact.mail":"lsklar@umich.edu","contact.domain":"umich.edu"}]
