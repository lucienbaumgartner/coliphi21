[{"date.published":"2006-07-06","date.changed":"2015-04-02","url":"https://plato.stanford.edu/entries/logic-intensional/","author1":"Melvin Fitting","author1.info":"http://comet.lehman.cuny.edu/fitting","entry":"logic-intensional","body.text":"\n\n\n\nThere is an obvious difference between what a term designates and what\nit means. At least it is obvious that there is a difference. In some\nway, meaning determines designation, but is not synonymous with it.\nAfter all, “the morning star” and “the evening\nstar” both designate the planet Venus, but don’t have the same\nmeaning. Intensional logic attempts to study both designation and\nmeaning and investigate the relationships between them. \n\n\n\nIf you are not skilled in colloquial astronomy, and I tell you that\nthe morning star is the evening star, I have given you\ninformation—your knowledge has changed. If I tell you the\nmorning star is the morning star, you might feel I was wasting your\ntime. Yet in both cases I have told you the planet Venus was\nself-identical. There must be more to it than this. Naively, we might\nsay the morning star and the evening star are the same in one way, and\nnot the same in another. The two phrases, “morning star”\nand “evening star” may designate the same object, but they\ndo not have the same meaning. Meanings, in this sense, are often\ncalled intensions, and things designated,\nextensions. Contexts in which extension is all that matters\nare, naturally, called extensional, while contexts in which\nextension is not enough are intensional. Mathematics is\ntypically extensional throughout—we happily write\n“\\(1+4=2+3\\)” even though the two terms involved may\ndiffer in meaning (more about this later). “It is known\nthat…” is a typical intensional context—“it\nis known that \\(1+4 = 2 + 3\\)” may not be correct when the\nknowledge of small children is involved. Thus mathematical pedagogy\ndiffers from mathematics proper. Other examples of intensional\ncontexts are “it is believed that…”, “it is\nnecessary that…”, “it is informative\nthat…”, “it is said that…”, “it\nis astonishing that…”, and so on. Typically a context\nthat is intensional can be recognized by a failure of the\nsubstitutivity of equality when naively applied. Thus, the morning\nstar equals the evening star; you know the morning star equals the\nmorning star; then on substituting equals for equals, you know the\nmorning star equals the evening star. Note that this knowledge arises\nfrom purely logical reasoning, and does not involve any investigation\nof the sky, which should arouse some suspicion. Substitution of\nco-referring terms in a knowledge context is the problematic\nmove—such a context is intensional, after all. Admittedly this\nis somewhat circular. We should not make use of equality of extensions\nin an intensional context, and an intensional context is one in which\nsuch substitutivity does not work. \n\nThe examples used above involve complex terms, disguised definite\ndescriptions. But the same issues come up elsewhere as well, often in\nways that are harder to deal with formally. Proper names constitute\none well-known area of difficulties. The name “Cicero”\nand the name “Tully” denote the same person, so\n“Cicero is Tully” is true. Proper names are generally\nconsidered to be rigid, once a designation has been specified it does\nnot change. This, in effect, makes “Cicero is Tully” into\na necessary truth. How, then, could someone not know it?\n“Superman is Clark Kent” is even more difficult to deal\nwith, since there is no actual person the names refer to. Thus while\nthe sentence is true, not only might one not know it, but one might\nperfectly well believe Clark Kent exists, that is “Clark\nKent” designates something, while not believing Superman exists.\nExistence issues are intertwined, in complex ways, with intensional\nmatters. Further, the problems just sketched at the ground level\ncontinue up the type hierarchy. The property of being an equilateral\ntriangle is coextensive with the property of being an equiangular\ntriangle, though clearly meanings differ. Then one might say,\n“it is trivial that an equilateral triangle is an equilateral\ntriangle,” yet one might deny that “it is trivial that an\nequilateral triangle is an equiangular triangle”. \n\nIn classical first-order logic intension plays no role. It is\nextensional by design since primarily it evolved to model the\nreasoning needed in mathematics. Formalizing aspects of natural\nlanguage or everyday reasoning needs something richer. Formal systems\nin which intensional features can be represented are generally\nreferred to as intensional logics. This article discusses\nsomething of the history and evolution of intensional logics. The aim\nis to find logics that can formally represent the issues sketched\nabove. This is not simple and probably no proposed logic has been\nentirely successful. A relatively simple intensional logic that can\nbe used to illustrate several major points will be discussed in some\ndetail, difficulties will be pointed out, and pointers to other, more\ncomplex, approaches will be given. \n\nRecognition that designating terms have a dual nature is far from\nrecent. The Port-Royal Logic used terminology that translates as\n“comprehension” and “denotation” for\nthis. John Stuart Mill used “connotation” and\n“denotation.” Frege famously used “Sinn” and\n“Bedeutung,” often left untranslated, but when translated,\nthese usually become “sense” and “reference.”\nCarnap settled on “intension” and “extension.”\nHowever expressed, and with variation from author to author, the\nessential dichotomy is that between what a term means, and\nwhat it denotes. “The number of the planets”\ndenotes the number 9 (ignoring recent disputes about the status of\nbodies at the outer edges of the solar system), but it does not have\nthe number 9 as its meaning, or else in earlier times scientists might\nhave determined that the number of planets was 9 through a process of\nlinguistic analysis, and not through astronomical observation. Of the\nmany people who have contributed to the analysis of intensional\nproblems several stand out. At the head of the list is Gottlob Frege. \n\nThe modern understanding of intensional issues and problems begins\nwith a fundamental paper of Gottlob Frege, (Frege 1892). This paper\nopens with a recital of the difficulties posed by the notion of\nequality. In his earlier work, Frege notes, he had taken equality to\nrelate names, or signs, of objects, and not objects themselves. For\notherwise, if \\(a\\) and \\(b\\) designate the same object,\nthere would be no cognitive difference between \\(a = a\\)\nand \\(a = b\\), yet the first is analytic while the\nsecond generally is not. Thus, he once supposed, equality relates\nsigns that designate the same thing. But, he now realizes, this cannot\nbe quite correct either. The use of signs is entirely arbitrary,\nanything can be a sign for anything, so in considering \\(a =\nb\\) we would also need to take into account the mode of\npresentation of the two signs—what it is that associates\nthem with the things they designate. Following this line of thought,\nequality becomes a relation between signs, relative to their modes of\npresentation. Of course the notion of a mode of presentation is\nsomewhat obscure, and Frege quickly shifts attention elsewhere. \n\nA sign has both a reference, and what Frege calls a sense\n—we can think of the sense as being some kind of embodiment of\nthe mode of presentation. From here on in his paper, sense is under\ndiscussion, and modes of presentation fade into the background. A name\nexpresses its sense, and designates its\nreference. Thus, “morning star” and “evening\nstar” have the same designation, but express different senses,\nrepresenting different modes of presentation—one is a celestial\nbody last seen in the morning before the sun obscures it, the other is\na celestial body first seen in the evening after the sun no longer\nobscures it. Frege goes on to complicate matters by introducing the\nidea associated with a sign, which is distinct from its sense\nand its reference. But the idea is subjective, varying from person to\nperson, while both sense and denotation are said by Frege to be not\ndependent in this way. Consequently the idea also fades into the\nbackground, while sense and reference remain central. \n\nGenerally when a sign appears as part of a declarative sentence, it is\nthe reference of the sign that is important. Both “Venus”\nand “the morning star” designate the same object. The\nsentence “The morning star is seen in the sky near\nsunrise” is true, and remains true when “the morning\nstar” is replaced with “Venus”. Substitution of\nequi-designating signs preserves truth. But not always; there are\ncontexts in which this does not happen, indirect reference\ncontexts. As a typical example, “George knows that the morning\nstar is seen in the sky near sunrise” may be true while\n“George knows that Venus is seen in the sky near sunrise”\nmay be false. Besides knowledge contexts, indirect reference arises\nwhen a sentence involves “I believe that…”,\n“I think that…”, “It seems to me\nthat…”, “It is surprising that…”,\n“It is trivial that…”, and so on. In such contexts,\nFrege concludes, not designation but sense is central. Then, since\n“George knows that…” is an indirect reference\ncontext, senses are significant. The signs “the morning\nstar” and “Venus” have different senses, we are not\nreplacing a sense by a sense equal to it, and so should not expect\ntruth to be preserved. \n\nFrege notes that an expression might have a sense, but not a\nreference. An example he gives is, “the least rapidly convergent\nseries.” Of course an object might have several signs that\ndesignate it, but with different senses. Frege extends the\nsense/reference dichotomy rather far. In particular, declarative\nsentences are said to have both a sense and a reference. The sense is\nthe proposition it expresses, while the reference is its truth value.\nThen logically equivalent sentences have the same designation, but may\nhave different senses. In indirect contexts sense, and not\ndesignation, matters and so we may know the well-ordering principle\nfor natural numbers, but not know the principle of mathematical\ninduction because, while they are equivalent in truth value, they have\ndifferent senses. \n\nNo formal machinery for dealing with sense, as opposed to reference,\nis proposed in Frege 1892. But Frege defined the terms under which\nfurther discussion took place. There are two distinct but related\nnotions, sense and reference. Equality plays a fundamental role, and a\ncentral issue is the substitutivity of equals for equals. Names,\nsigns, expressions, can be equal in designation, but not equal in\nsense. There are both direct or extensional, and indirect or\nintensional contexts, and reference matters for the first while sense\nis fundamental for the second. \n\nFrege gave the outline of a theory of intensionality, but no\nintensional logic in any formal sense. There have been attempts to\nfill in his outline. Alonzo Church (1951) went at it quite directly.\nIn this paper there is a formal logic in which terms have both senses\nand denotations. These are simply taken to be different sorts, and\nminimal requirements are placed on them. Nonetheless the logic is\nquite complex. The formal logic that Frege had created for his work on\nthe foundations of mathematics was type free. Russell showed his\nfamous paradox applied to Frege’s system, so it was inconsistent. As a\nway out of this problem, Russell developed the type theory that was\nembodied in Principia Mathematica. Church had given an\nelegant and precise formulation of the simple theory of types (Church\n1940), and that was incorporated into his work on intensionality,\nwhich is one of the reasons for its formal complexity. \n\nChurch uses a notion he calls a concept, where anything that\nis the sense of a name for something can serve as a concept of that\nsomething. There is no attempt to make this more precise—indeed\nit is not really clear how that might be done. It is explicit that concepts\nare language independent, and might even be uncountable. There is a\ntype \\(\\omicron_{0}\\) of the two truth values. Then, there is a\ntype \\(\\omicron_{1}\\) of concepts of members of\n\\(\\omicron_{0}\\), which are called propositional\nconcepts. There is a type \\(\\omicron_{2}\\) of concepts of\nmembers of \\(\\omicron_{1}\\), and so on. There a type\n\\(\\iota_{0}\\) of individuals, a type \\(\\iota_{1}\\) of\nconcepts of members of \\(\\iota_{0}\\), a type \\(\\iota_{2}\\)\nof concepts of members of \\(\\iota_{1}\\), and so on. And finally,\nfor any two types \\(\\alpha\\) and \\(\\beta\\) there is a type\n\\((\\alpha\\,\\beta )\\) of functions from items of type \\(\\beta\\) to\nitems of type \\(\\alpha\\). Church makes a simplifying assumption\nconcerning functional types. In order to state it easily he introduces\nsome special notation: if \\(\\alpha\\) is a type symbol, for example\n\\(((\\iota_{3}\\,\\omicron_{2})(\\omicron_{5}\\,\\iota_{4}))\\),\nthen \\(\\alpha_{1}\\) is the result of increasing each subscript by\n1, in our example we get\n\\(((\\iota_{4}\\,\\omicron_{3})(\\omicron_{6}\\,\\iota_{5}))\\). (There\nis a similar definition for \\(\\alpha_{n}\\) for each\npositive integer \\(n\\), but we will not need it here.) Church’s\nassumption is that the concepts of members of the functional type\n\\((\\alpha\\,\\beta )\\) are the members of the type\n\\((\\alpha_{1}\\,\\beta_{1})\\). With this assumption,\nuniformly the concepts of members of any type \\(\\alpha\\) are the members\nof type \\(\\alpha_{1}\\). \n\nQuantification and implication are introduced, or rather versions\nappropriate for the various types are introduced.\n\\(\\lambda\\) abstraction notation is present. And finally, for each\ntype \\(\\alpha\\) it is assumed there is a relation that holds\nbetween a concept of something of type \\(\\alpha\\) and the thing\nitself; this is a relation between members of type\n\\(\\alpha_{1}\\) and members of type \\(\\alpha\\). This\nis denoted \\(\\Delta\\), with appropriate type-identifying subscripts. \n\nA fundamental issue for Church is when two names, lambda terms, have\nthe same sense. Three alternatives are considered. Common to all three\nalternatives are the assumptions that sense is unchanged under the\nrenaming of bound variables (with the usual conditions of freeness),\nand under \\(\\beta\\) reduction. Beyond these, Alternative 0 is\nsomewhat technical and is only briefly mentioned, Alternative 1 is fine\ngrained, making senses distinct as far as possible, while Alternative 2\nmakes two terms have the same sense whenever equality between them is a\nlogical validity. The proper definition of the alternatives is\naxiomatic, and altogether various combinations of some 53 axiom schemes\nare introduced, with none examined in detail. Clearly Church was\nproposing an investigation, rather than presenting full results. \n\nAs noted, the primary reference for this work is Church 1951, but\nthere are several other significant papers including Church 1973,\nChurch 1974, and the Introduction to Church 1944, which contains an\ninformal discussion of some of the ideas. In addition, the expository\npapers of Anderson are enlightening (Anderson 1984, 1998). It should\nbe noted that there are relationships between Church’s work and that\nof Carnap, discussed below. Church’s ideas first appeared in an\nabstract (Church 1946), then Carnap’s book appeared (Carnap 1947). A\nfew years later Church’s paper expanded his abstract in Church 1951.\nThe second edition of Carnap’s book appeared in 1956. Each man had an\ninfluence on the other, and the references between the two authors are\nthoroughly intertwined. \n\nChurch simply(!) formalized something of how intensions behaved,\nwithout saying what they were. Rudolf Carnap took things further with\nhis method of intension and extension, and provided a semantics in\nwhich quite specific model-theoretic entities are identified with\nintensions (Carnap 1947). Indeed, the goal was to supply intensions\nand extensions for every meaningful expression, and this was done in a\nway that has heavily influenced much subsequent work. \n\nAlthough Carnap attended courses of Frege, his main ideas are based on\nWittgenstein 1921. In the Tractatus, Wittgenstein\nintroduced a precursor of possible world semantics. There are\nstates of affairs, which can be identified with the set of\nall their truths, “(1.13) The facts in logical space are the\nworld.” Presumably these facts are atomic, and can be varied\nindependently, “(1.21) Each item can be the case or not the case\nwhile everything else remains the same.” Thus there are many\npossible states of affairs, among them the actual one, the real\nworld. Objects, in some way, involve not only the actual state of\naffairs, but all possible ones, “(2.0123) If I know an object I\nalso know all its possible occurrences in states of affairs. (Every\none of these possibilities must be part of the nature of the object.)\nA new possibility cannot be discovered later.” It is from these\nideas that Carnap developed his treatment. \n\nCarnap begins with a fixed formal language whose details need not\nconcern us now. A class of atomic sentences in this language,\ncontaining exactly one of \\(A\\) or \\(\\neg A\\) for each\natomic sentence, is a state-description. In each\nstate-description the truth or falsity of every sentence of the\nlanguage is determined following the usual truth-functional\nrules—quantifiers are treated substitutionally, and the language\nis assumed to have ‘enough’ constants. Thus truth is\nrelative to a state-description. Now Carnap introduces a stronger\nnotion than truth, L-truth, intended to be “an\nexplicatum for what philosophers call logical or necessary or analytic\ntruth.” Initially he presents this somewhat informally, “a\nsentence is \\(L\\)-true in a state description \\(S\\) if it is\ntrue in \\(S\\) in such a way that its truth can be established on\nthe basis of the semantical rules of the system \\(S\\) alone,\nwithout any reference to (extra-linguistic) facts.” But this is\nquickly replaced by a more precise semantic version, “A sentence is\n\\(L\\)-true if it holds in every state-description.” \n\nOne can recognize in \\(L\\)-truth a version of necessary truth\nusing possible world semantics. There is no accessibility relation, so\nwhat is being captured is more like S5 than like other modal\nlogics. But it is not S5 semantics either, since there is a fixed set\nof state-descriptions determined by the language itself. (If\n\\(P\\) is any propositional atom, some state-description\nwill contain \\(P\\), and so \\(\\Diamond P\\) will\nbe validated.) Nonetheless, it is a clear anticipation of possible\nworld semantics. But what concerns us here is how Carnap treats\ndesignating terms in such a setting. Consider predicates\n\\(P\\) and \\(Q\\). For Carnap these are intensionally\nequivalent if \\(\\forall x(Px \\equiv Qx)\\) is an \\(L\\)-truth, that is, in each\nstate-description \\(P\\) and \\(Q\\) have the same\nextension. Without being quite explicit about it, Carnap is proposing\nthat the intension of a predicate is an assignment of an extension for\nit to each state-description—intensional identity means identity\nof extension across all state-descriptions and not just at the actual\none. Thus the predicate ‘\\(H\\)’, human, and the\npredicate ‘\\(FB\\)’, featherless biped, have the same\nextension—in the actual state-description they apply to the same\nbeings—but they do not have the same intension since there are\nother state-descriptions in which their extensions can differ. In a\nsimilar way one can model individual expressions, “The extension\nof an individual expression is the individual to which it\nrefers.” Thus, ‘Scott’ and ‘the author of\nWaverly’ have the same extension (in the actual\nstate-description). Carnap proposes calling the intension of an\nindividual expression an individual concept, and\nsuch a thing picks out, in each state-description, the individual to\nwhich it refers in that state description. Then ‘Scott’\nand ‘the author of Waverly’ have different intensions\nbecause, as most of us would happily say, they could have been\ndifferent, that is, there are state-descriptions in which they are\ndifferent. (I am ignoring the problems of non-designation in this\nexample.) \n\nCarnap’s fundamental idea is that intensions, for whatever entities\nare being considered, can be given a precise mathematical embodiment\nas functions on states, while extensions are relative to a single\nstate. This has been further developed by subsequent researchers, of\ncourse with modern possible world semantics added to the mix. The\nCarnap approach is not the only one around, but it does take us quite\na bit of the way into the intensional thicket. Even though it does\nnot get us all the way through, it will be the primary version\nconsidered here, since it is concrete, intuitive, and natural when it\nworks. \n Carnap’s work was primarily semantic, and resulted in a logic that did not\ncorrespond to any of the formal systems that had been studied up to\nthis point. Axiomatically presented propositional modal logics were\nwell-established, so it was important to see how (or if) they could be\nextended to include quantifiers and equality. At issue were decisions\nabout what sorts of things quantifiers range over, and substitutivity\nof equals for equals. Quine’s modal objections needed to be\naddressed. Ruth Barcan Marcus began a line of development in (Marcus\n1946) by formally extending the propositional system S2 of C. I. Lewis\nto include quantification, and developing it axiomatically in the\nstyle of Principia Mathematica. It was clear that other\nstandard modal logics besides S2 could have been used, and S4 was\nexplicitly discussed. The Barcan formula, in the form\n \\(\\Diamond (\\exists \\alpha )\\)A \\(\\supset (\\exists \\alpha )\\Diamond\\)A, made its first\nappearance in (Marcus\n 1946),[1]\n though a full understanding of its significance would have to wait\nfor the development of a possible-world semantics. Especially\nsignificant for the present article, her system was further extended\nin (Marcus 1947) to allow for abstraction and identity. Two versions\nof identity were considered, depending on whether things had the same\nproperties (abstracts) or necessarily had them. In the S2 system the\ntwo versions were shown to be equivalent, and in the S4 system,\nnecessarily equivalent. In a later paper (Marcus 1953) the\nfundamental role of the deduction theorem was fully explored as\nwell.  \n Marcus proved that in her system identity was necessary if true, and\nthe same for distinctness. She argued forcefully in subsequent works,\nprimarily (Marcus 1961), that morning star/evening star problems were\nnonetheless avoided. Names were understood as tags. They\nmight have their designation specified through an initial use of a\ndefinite description, or by some other means, but otherwise names had\nno meaning, only a designation. Thus they did not behave like\ndefinite descriptions, which were more than mere tags. Well, the\nobject tagged by “morning star ” and that tagged by\n“evening star ” are the same, and identity between objects\nis never contingent. \n The essential point had been made. One could develop formal modal\nsystems with quantifiers and equality. The ideas had coherence.\nStill missing was a semantics which would help with the understanding\nof the formalism, but this was around the corner. \n\nCarnap’s ideas were extended and formalized by Richard Montague, Pavel\nTichý, and Aldo Bressan, independently. All made use of some version\nof Kripke/Hintikka possible world semantics, instead of the more specialized\nstructure of Carnap. All treated intensions functionally. \n\nIn part, Bressan wanted to provide a logical foundation for physics. The connection\nwith physics is this. When we say something has such-and-such a mass,\nfor instance, we mean that if we had conducted certain experiments, we\nwould have gotten certain results. This does not assume we did conduct\nthose experiments, and thus alternate states (or cases, as Bressan\ncalls them) arise. Hence there is a need for a rich modal language,\nwith an ontology that includes numbers as well as physical objects. In\nBressan 1972, an elaborate modal system was developed, with a full\ntype hierarchy including numbers as in Principia Mathematica. \n\nMontague’s work is primarily in Montague 1960 and 1970, and has natural\nlanguage as its primary motivation. The treatment is\nsemantic, but in Gallin 1975 an axiom system is presented. The\nlogic Gallin axiomatized is a full type-theoretic system,\nwith intensional objects of each type. Completeness is proved relative\nto an analog of Henkin models, familiar for higher type classical\nlogics. \n\nTichý created a system of intensional logic very similar to\nthat of Montague, beginning, in English, in Tichý 1971, with a\ndetailed presentation in Tichý 1988. Unfortunately his work did\nnot become widely known. Like Montague’s semantics,\nTichý’s formal work is based on a type hierarchy with\nintensions mapping worlds to extensions at each type level, but it\ngoes beyond Montague in certain respects. For one thing, intensions\ndepend not only on worlds, but also on times. For another, in addition\nto intensions and extensions Tichý also\nconsiders constructions. The idea is that expressions\ndetermine intensions and extensions, and this itself is a formal\nprocess in which compound expressions act using the simpler\nexpressions that go into their making; compositionality at the level\nof constructions, in other words. Using this formal machinery,\n“\\(1+4\\)” and “\\(2+3\\)” prescribe different\nconstructions; their meaning is not simply captured by their\nintensional representation as discussed here. \n\nAs has been noted several times earlier, formal intensional logics\nhave been developed with a full hierarchy of higher types, Church, \nMontague, Bressan, Tichý for instance. Such logics can be rather\nformidable, but Carnap’s ideas are often (certainly not always) at the\nheart of such logics, these ideas are simple, and are sufficient to\nallow discussion of several common intensional problems. Somehow,\nbased on its sense (intension, meaning) a designating phrase may\ndesignate different things under different conditions—in\ndifferent states. For instance, “the number of the\nplanets” was believed to designate 6 in ancient times (counting\nEarth). Immediately after the discovery of Uranus in 1781 “the\nnumber of the planets” was believed to designate 7. If we take\nas epistemic states of affairs the universe as conceived by the\nancients, and the universe as conceived just after 1781, in one state\n“the number of the planets” designates 6 and in the other\nit designates 7. In neither state were people wrong about the concept\nof planet, but about the state of affairs constituting the\nuniverse. If we suppress all issues of how meanings are determined,\nhow meanings in turn pick out references, and all issues of what\ncounts as a possible state of affairs, that is, if we abstract all\nthis away, the common feature of every designating term is that\ndesignation may change from state to state—thus it can be\nformalized by a function from states to objects. This bare-bones\napproach is quite enough to deal with many otherwise intractable\nproblems. \n\nIn order to keep things simple, we do not consider a full type\nhierarchy—first-order is enough to get the basics across. The\nfirst-order fragment of the logic of Gallin 1975 would be sufficient,\nfor instance. The particular formulation presented here comes from\nFitting 2004, extending Fitting and Mendelsohn 1998. Predicate\nletters are intensional, as they are in every version of Kripke-style\nsemantics, with interpretations that depend on possible worlds. The\nonly other intensional item considered here is that of individual\nconcept—formally represented by constants and variables that can\ndesignate different objects in different possible worlds. The same\nideas can be extended to higher types, but what the ideas contribute\ncan already be seen at this relatively simple level. Intensional\nlogics often have nothing but intensions—extensions are inferred\nbut are not explicit. However, an approach that is too minimal can\nmake life hard, so consequently here we explicitly allow both objects\nand individual concepts which range over objects. There are two kinds\nof quantification, over each of these sorts. Both extensional and\nintensional objects are first-class citizens. \n\nBasic ideas are presented semantically rather than\nproof-theoretically, though both axiom systems\nand tableau systems exist. Even so, technical details can become baroque,\nso as far as possible, we will separate informal presentation, which\nis enough to get the general idea, from its formal counterpart, which\nis of more specialized interest. A general acquaintance with modal\nlogic is assumed (though there is a very brief discussion to establish\nnotation, which varies some from author to author). It should be noted\nthat modal semantics is used here, and generally, in two different\nways. Often one has a particular Kripke model in mind, though it may\nbe specified informally. For instance, we might consider a Kripke\nmodel in which the states are the present instant and all past ones,\nwith later states accessible from earlier ones. Such a model is\nenlightening when discussing “the King of France” for\ninstance, even though the notion of instant is somewhat vaguely\ndetermined. But besides this use of informally specified concrete\nmodels, there is formal Kripke semantics which is a mathematically\nprecise thing. If it is established that something, say\n\\(\\Box (X \\supset Y) \\supset (\\Box X \\supset \\Box Y)\\), is valid in all formal Kripke models, we can\nassume it will be so in our vaguely specified, intuitive models, no\nmatter how we attempt to make them more precise. Informal models\npervade our discussions—their fundamental properties come from\nthe formal semantics. \n\nA propositional language is built up from\npropositional letters, \\(P, Q,\\ldots\\), using\n \\(\\wedge , \n \\vee , \\supset , \\neg\\) and other\npropositional connectives, and \\(\\Box\\) (necessary) and \\(\\Diamond\\)\n(possible) as modal operators. These operators can be thought of as\nalethic, deontic, temporal, epistemic—it will matter which\neventually, but it does not at the moment. Likewise there could be\nmore than one version of \\(\\Box\\), as in a logic of knowledge with\nmultiple knowers—this too doesn’t make for any essential\ndifferences. \n\nKripke semantics for propositional modal logic is, by now, a very\nfamiliar thing. Here is a quick presentation to establish notation,\nand to point out how one of Frege’s proposals fits in. A more\ndetailed presentation can be found in the article on modal logic in\nthis encyclopedia. \n\nA model consists of a collection of states, some determination of\nwhich states are relevant to which, and also some specification of\nwhich propositional letters hold at which of these states. States could\nbe states of the real world at different times, or states of knowledge,\nor of belief, or of the real world as it might have been had\ncircumstances been different. We have a mathematical abstraction here.\nWe are not trying to define what all these states might\n‘mean,’ we simply assume we have them. Then more complex\nformulas are evaluated as true or false, relative to a state. At each\nstate the propositional connectives have their customary classical\nbehavior. For the modal operators. \\(\\Box X\\), that is,\nnecessarily \\(X\\), is true at a state if \\(X\\) itself\nis true at every state that is relevant to that state (at all\naccessible states). Likewise \\(\\Diamond X\\),\npossibly \\(X\\), is true at a state if \\(X\\) is true at\nsome accessible state. If we think of things epistemically,\naccessibility represents compatibility, and so \\(X\\) is known in a\nstate if \\(X\\) is the case in all states that are compatible with\nthat state. If we think of things alethically, an accessible state can\nbe considered an alternate reality, and so \\(X\\) is necessary in a\nstate if \\(X\\) is the case in all possible alternative states.\nThese are, by now, very familiar ideas. \n\nA frame is a structure \\(\\langle \\bG,\n\\bR\\rangle\\), where \\(\\bG\\) is a non-empty set\nand \\(\\bR\\) is a binary relation on\n\\(\\bG\\). Members of \\(\\bG\\) are states\n(or possible worlds). \\(\\bR\\) is an\naccessibility relation. For \\(\\Gamma , \\Delta \\in \\bG, \\Gamma \\bR \\Delta\\) is read\n“\\(\\Delta\\) is accessible from \\(\\Gamma\\).” A (propositional)\nvaluation on a frame is a mapping, \\(\\bV\\), that\nassigns to each propositional letter a mapping from states of the\nframe to truth values, true or false. For\nsimplicity, we will abbreviate \\(\\bV(P)(\\Gamma )\\)\nby \\(\\bV(P, \\Gamma )\\). A propositional\nmodel is a structure\n \\(\\cM = \\langle \\bG,\n\\bR, \\bV\\rangle\\), where\n\\(\\langle \\bG, \\bR\\rangle\\) is a frame and\n\\(\\bV\\) is a propositional valuation on that frame. \n\nGiven a propositional model \n \\(\\cM = \\langle \\bG,\n\\bR, \\bV\\rangle\\), the notion of\nformula \\(X\\) being true at state \\(\\Gamma\\) will be denoted\n \\(\\cM, \n \\Gamma \\vDash X\\), and is characterized by the following standard rules,\nwhere \\(P\\) is atomic. \n\nSuppose we think about formulas using an intensional/extensional\ndistinction. Given a model \\(\\cM\\), to each\nformula \\(X\\) we can associate a function, call it\n\\(f_{X}\\), mapping states to truth values, where\nwe set \\(f_{X}(\\Gamma )\\) = true just\nin case\n\\(\\cM, \\Gamma \\vDash X\\).\n \n\nThink of the function \\(f_{X}\\) as the intensional meaning of\nthe formula \\(X\\)—indeed, think of it as the\nproposition expressed by the formula (relative to a\nparticular model, of course). At a state \\(\\Gamma\\), \n\\(f_{X}(\\Gamma )\\) is a truth value—think\nof this as the extensional meaning of \\(X\\) at that\nstate. This is a way of thinking that goes back to Frege, who\nconcluded that the denotation of a sentence should be a truth value,\nbut the sense should be a proposition. He was a little vague about\nwhat constituted a proposition—the formalization just presented\nprovides a natural mathematical entity to serve the purpose, and was\nexplicitly proposed for this purpose by Carnap. It should be clear\nthat the mathematical structure does, in a general way, capture some\npart of Frege’s idea. Incidentally we could, with no loss,\nreplace the function \\(f_{X}\\) on states with\nthe set \\(\\{ \\Gamma \\in \\bG \\mid f_{X}(\\Gamma) = \\textit{true}\\}\\).\nThe function \\(f_{X}\\) is simply the\ncharacteristic function of this set. Sets like these are commonly\nreferred to as propositions in the modal logic community. In a\ntechnical sense, then, Frege’s ideas on this particular topic\nhave become common currency. \n\nFirst we discuss some background intuitions, then introduce a\nformal semantics. Intensions will be introduced formally in Section\n3.3. The material discussed here can be found more fully developed in\n(Fitting and Mendelsohn 1998, Hughes and Cresswell 1996), among other\nplaces. \n\nIf we are to think of an intension as designating different things\nunder different circumstances, we need things. At the propositional\nlevel truth values play the role of things, but at the first order\nlevel something more is needed. In classical logic each model has a\ndomain, the things of that model, and quantifiers are understood as\nranging over the members of that domain. It is, of course, left open\nwhat constitutes a thing—any collection of any sort can serve as\na domain. That way, if someone has special restrictions in mind because\nof philosophical or mathematical considerations, they can be\naccommodated. It follows that the validities of classical logic are, by\ndesign, as general as possible—they are true no matter what we\nmight choose as our domain, no matter what our things are. \n\nA similar approach was introduced for modal logics in Kripke 1963.\nDomains are present, but it is left open what they might consist\nof. But there is a complication that has no classical counterpart: in\na Kripke model there are multiple states. Should there be a single\ndomain for the entire model, or separate domains for each state? Both\nhave natural intuitions. \n\nConsider a version of Kripke models in which a separate domain is\nassociated with each state of the model. At each state, quantifiers are\nthought of as ranging over the domain associated with that state. This\nhas come to be known as an actualist semantics. Think of the\ndomain associated with a state as the things that actually exist at\nthat state. Thus, for example, in the so-called real world the Great\nPyramid of Khufu is in the domain, but the Lighthouse of Alexandria is\nnot. If we were considering the world of, say, 1300, both would be in\nthe domain. In an actualist approach, we need to come to some decision\non what to do with formulas containing references to things that exist\nin other states but not in the state we are considering. Several\napproaches are plausible; we could take such formulas to be false, or\nwe could take them to be meaningless, for instance, but this seems to\nbe unnecessarily restrictive. After all, we do say things like\n“the Lighthouse of Alexandria no longer exists,” and we\nthink of it as true. So, the formal version that seems most useful\ntakes quantifiers as ranging over domains state by state, but otherwise\nallows terms to reference members of any domain. The resulting\nsemantics is often called varying domain as well as\nactualist. \n\nSuppose we use the actualist semantics, so each state has an\nassociated domain of actually existing things, but suppose we allow\nquantifiers to range over the members of any domain, without\ndistinction, which means quantifiers are ranging over the same set, at\nevery state. What are the members of that set? They are the things\nthat exist at some state, and so at every state they are the possible\nexistents—things that might exist. Lumping these separate\ndomains into a single domain of quantification, in effect, means we\nare quantifying over possibilia. Thus, a semantics in which there is a\nsingle domain over which quantifiers range, the same for every state,\nis often called possibilist semantics or, of course,\nconstant domain semantics. \n\nPossibilist semantics is simpler to deal with than the actualist\nversion—we have one domain instead of many for quantifiers to\nrange over. And it turns out that if we adopt a possibilist approach,\nthe actualist semantics can be simulated. Suppose we have a single\ndomain of quantification, possibilia, and a special predicate,\n\\(E\\), which we think of as true, at each state, of the things\nthat actually exist at that state. If \\(\\forall\\) is a quantifier\nover the domain of possibilia, we can think of the relativized\nquantifier, \\(\\forall\\)x\\((E(x)\n\\supset \\ldots )\\) as corresponding to actualist\nquantification. (We need to assume that, at each state, \\(E\\) is\ntrue of something—this corresponds to assuming domains are\nnon-empty.) This gives an embedding of the actualist semantics into the\npossibilist one, a result that can be formally stated and proved.\nHere possibilist semantics will be used, and we assume we have an\nexistence predicate \\(E\\) available. \n\nThe language to be used is a straightforward first order extension of\nthe propositional modal language. There is an infinite list of\nobject variables, \\(x, y,\nx_{1}, x_{2},\\ldots\\), and a list\nof relation symbols, \\(R, P,\nP_{1}, P_{2},\\ldots\\), of all\narities. Among these is the one-place symbol \\(E\\) and the\ntwo-place symbol =. Constant and function symbols could be added, but\nlet’s keep things relatively simple, along with simply relative. If\n\\(x_{1}, \\ldots ,x_{n}\\) are\nobject variables and \\(P\\) is an \\(n\\)-place relation\nsymbol, \\(P(x_{1}, \\ldots ,x_{n})\\) is an atomic formula. We’ll write\n\\(x = y\\) in place of = (x,y). More complex\nformulas are built up using propositional connectives, modal\noperators, and quantifiers, \\(\\forall\\) and \\(\\exists\\), in the usual\nway. Free and bound occurrences of variables have the standard\ncharacterization. \n\nA first order model is a structure \\(\\langle \\bG, \\bR,\n\\bD_{O}, \\bI\\rangle\\) where \\(\\langle \\bG, \\bR\\rangle\\) is a frame, as\nin Section 3.1, \\(\\bD_{O}\\) is a non-empty object domain, and\n\\(\\bI\\) is an interpretation that assigns to each \\(n\\)-place\nrelation symbol \\(P\\) a mapping, \\(\\bI(P)\\) from \\(\\bG\\) to subsets of\n\\(\\bD_{O}^{n}\\). We’ll write \\(\\bI(P, \\Gamma )\\) as an\neasier-to-read version of \\(\\bI(P)(\\Gamma )\\). It is required that\n\\(\\bI(=, \\Gamma )\\) is the equality relation on \\(\\bD_{O}\\), for every\nstate \\(\\Gamma\\), and \\(\\bI(E, \\Gamma )\\) is non-empty, for every\n\\(\\Gamma\\). A first-order valuation in a model is a mapping\n\\(v\\) that assigns a member of \\(\\bD_{O}\\) to each variable. Note that\nfirst order valuations are not state-dependent in the way that\ninterpretations are. A first order valuation \\(w\\) is an\n\\(x\\)-variant of valuation \\(v\\) if \\(v\\) and \\(w\\) agree on\nall variables except possibly for \\(x\\). Truth, at a state \\(\\Gamma\\)\nof a model \\(\\cM = \\langle \\bG, \\bR, \\bD_{O}, \\bI\\rangle\\), with\nrespect to a first order valuation \\(v\\), is characterized as follows,\nwhere \\(P(x_{1}, \\ldots ,x_{n})\\) is an atomic formula: \n\nCall a formula valid if it is true at every state of every\nfirst order model with respect to every first-order valuation, as\ndefined above. Among the validities are the usual modal candidates,\nsuch as \\(\\Box (X \\supset Y) \\supset (\\Box X \\supset \\Box Y)\\), and the usual quantificational candidates,\nsuch as \\(\\forall xX \\supset \\exists xX\\). We also have mixed cases such as the\nBarcan and converse Barcan formulas:\n\\(\\forall x\\Box X \\equiv \\Box \\forall xX\\), which are characteristic of\nconstant domain models, as was shown in Kripke 1963. Because of the\nway equality is treated, we have the validity of both\n\\(\\forall x\\forall y(x = y \\supset \\Box x = y)\\) and\n\\(\\forall x\\forall y(x \\ne y \\supset \\Box x \\ne y)\\). Much has been made about the\nidentity of the number of the planets and 9 (Quine 1963), or the\nidentity of the morning star and the evening star (Frege 1892), and\nhow these identities might behave in modal contexts. But that is not\nreally a relevant issue here. Phrases like “the morning\nstar” have an intensional aspect, and the semantics outlined so\nfar does not take intensional issues into account. As a matter of\nfact, the morning star and the evening star are the same object and,\nas Gertrude Stein might have said, “an object is an object is an\nobject.” The necessary identity of a thing and itself should not\ncome as a surprise. Intensional issues will be dealt with shortly. \n\nQuantification is possibilist—domains are constant. But, as was\ndiscussed in Section 3.2.1, varying domains can be brought in\nindirectly by using the existence predicate, \\(E\\), and this\nallows us to introduce actualist quantification definitionally. Let\n\\(\\forall^{E}xX\\) abbreviate\n\\(\\forall x(E(x) \\supset X)\\), and let\n\\(\\exists^{E}xX\\) abbreviate\n\\(\\exists x(E(x)\n \\wedge X)\\). Then, while\n \\(\\forall x\\phi (x) \\supset \\phi (y)\\) is valid, assuming \\(y\\) is free for\n\\(x\\) in \\(\\phi (x)\\), we do not have the validity\nof \\(\\forall^{E}x\\phi (x)\n\\supset \\phi (y)\\). What we have instead is the validity\nof\n\\([\\forall^{E}x\\phi (x)\n \\wedge E(y)] \\supset \\phi (y)\\). \n\nAs another example of possibilist/actualist difference, consider\n\\(\\exists x\\Box P(x) \\supset \\Box \\exists xP(x)\\). With possibilist\nquantifiers, this is valid and reasonable. It asserts that if some\npossibilia has the \\(P\\) property in all alternative states, then\nin every alternative state some possibilia has the \\(P\\)\nproperty. But when possibilist quantification is replaced with\nactualist,\n\\(\\exists^{E}x\\Box P(x)\n\\supset \\Box \\exists^{E}xP(x)\\),\nthe result is no longer valid. As a blatant (but somewhat informal)\nexample, say the actual state is \\(\\Gamma\\) and \\(P\\) is the\nproperty of existing in state \\(\\Gamma\\). Then, at \\(\\Gamma ,\n\\exists^{E}x\\Box P(x)\\)\nsays something that actually exists has, in all alternative states,\nthe property of existing in the state \\(\\Gamma\\). This is true; in fact\nit is true of everything that exists in the state\n\\(\\Gamma\\). But\n\\(\\Box \\exists^{E}xP(x)\\)\nsays that in every alternative state there will be an actually\nexistent object that also exists in the state \\(\\Gamma\\), which need not\nbe the case. \n\nIn Section 3.2 a first order modal logic was sketched, in which\nquantification was over objects. Now a second kind of quantification\nis added, over intensions. As has been noted several times, an\nintensional object, or individual concept, will be modeled by a\nfunction from states to objects, but now we get into the question of\nwhat functions should be allowed. Intensions are supposed to be\nrelated to meanings. If we consider meaning to be a human construct,\nwhat constitutes an intension should probably be restricted. There\nshould not, for instance, be more intensional objects than there are\nsentences that can specify meanings, and this limits intensions to a\ncountable set. Or we might consider intensions as ‘being\nthere,’ and we pick out the ones that we want to think about, in\nwhich case cardinality considerations don’t apply. This is an issue\nthat probably cannot be settled once and for all. Instead, the\nsemantics about to be presented allows for different choices in\ndifferent models—it is not required that all functions from\nstates to objects be present. It should be noted that, while this\nsemantical gambit does have philosophical justification, it also makes\nan axiomatization possible. The fundamental point is the same as in\nthe move from first to second order logic. If we insist that second\norder quantifiers range over all sets and relations, an axiomatization\nis not possible. If we use Henkin models, in which the range of second\norder quantifiers has more freedom, an axiomatization becomes\navailable. \n\nFormulas are constructed more-or-less in the obvious way, with two\nkinds of quantified variables instead of one: extensional and\nintensional. But there is one really important addition to the\nsyntactic machinery, and it requires some discussion. Suppose we have\nan intension, \\(f\\), that picks out an object in each state. For\nexample, the states might be various ways the universe could have been\nconstituted, and at each state \\(f\\) picks out the number of the\nplanets which could, of course, be 0. Suppose \\(P\\) is a one-place\nrelation symbol—what should be meant by \\(P(f)\\)?\nOn the one hand, it could mean that the intension \\(f\\) has the\nproperty \\(P\\), on the other hand it could mean that the object\ndesignated by \\(f\\) has the property \\(P\\). Both versions are\nuseful and correspond to things we say every day. We will allow for\nboth, but the second version requires some cleaning up. Suppose\n\\(P(f)\\) is intended to mean that the object designated\nby \\(f\\) (at a state) has property \\(P\\). Then how do we read\n\\(\\Diamond P(f)\\)? Under what circumstances should\nwe take it to be true at state \\(\\Gamma\\)? It could be understood as\nasserting the thing designated by \\(f\\) at \\(\\Gamma\\) (call it\n\\(f_{\\Gamma })\\) has the ‘possible-\\(P\\)’\nproperty, and so at some alternative state \\(\\Delta\\) we have that\n\\(f_{\\Gamma }\\) has property \\(P\\). This is the\nde re reading, in which a possible property is ascribed to a\nthing. Another way of understanding\n\\(\\Diamond P(f)\\) takes the possibility operator\nas primary: to say the formula is true at \\(\\Gamma\\) means that at some\nalternative state, \\(\\Delta\\), we have \\(P(f)\\), and so at\n\\(\\Delta\\) the object designated by \\(f\\) (call it\n\\(f_{\\Delta })\\) has property \\(P\\). This is the\nde dicto reading, possibility applies to a sentence. Of course\nthere is no particular reason why \\(f_{\\Gamma }\\) and\n\\(f_{\\Delta }\\) should be identical. The de re and\nde dicto readings are different, both need representation, and\nwe cannot manage this with the customary syntax. \n\nAn abstraction mechanism will be used to disambiguate our syntax. The\nde re reading will be symbolized\n\\([\\lambda x\\,\\Diamond P(x)](f)\\)\nand the de dicto will be symbolized\n\\(\\Diamond [\\lambda x\\,P(x)](f)\\). The\n(incomplete) expression \\([\\lambda x\\,X]\\) is\noften called a predicate abstraction; one can think of it as\nthe predicate abstracted from the formula \\(X\\). In\n\\([\\lambda x\\,\\Diamond P(x)](f)\\) we\nare asserting that \\(f\\) has the possible-\\(P\\) property,\nwhile in\n\\(\\Diamond [\\lambda x\\,P(x)](f)\\) we\nare asserting the possibility that \\(f\\) has the \\(P\\)\nproperty. Abstraction disambiguates. What we have said about \\(\\Diamond\\)\napplies equally well to \\(\\Box\\) of course. It should be noted that one\ncould simply think of abstraction as a scope-specifying device, in a\ntradition that goes back to Russell, who made use of such a mechanism\nin his treatment of definite descriptions. Abstraction in modal logic\ngoes back to Carnap 1947, but in a way that ignores the issues\ndiscussed above. The present usage comes from Stalnaker & Thomason\n1968 and Thomason & Stalnaker 1968. \n\nNow the more technical part begins. There are two kinds of variables,\nobject variables as before, and intension variables,\nor individual concept variables, \\(f, g,\ng_{1}, g_{2},\\ldots\\). With two\nkinds of variables present, the formation of atomic formulas becomes a\nlittle more complex. From now on, instead of just being\n\\(n\\)-place for some \\(n\\), a relation symbol will have a\ntype associated with it, where a type is an \\(n\\)-tuple\nwhose entries are members of \\(\\{O, I\\}\\). An atomic\nformula is an expression of the form \\(P(\\alpha_{1},\n\\ldots ,\\alpha_{n})\\) where \\(P\\) is a\nrelation symbol whose type is \\(\\langle t_{1}, \\ldots ,t_{n}\\rangle\\) and, for each \\(i\\), if\n\\(t_{i} = O\\) then\n\\(\\alpha_{i}\\) is an object variable, and if\n\\(t_{i} = I\\) then\n\\(\\alpha_{i}\\) is an intension variable. Among the\nrelation symbols we still have \\(E\\), which now is of type\n\\(\\langle O\\rangle\\), and we have \\(=\\), of type \\(\\langle O,O \\rangle\\). \n\nFormulas are built up from atomic formulas in the usual way, using\npropositional connectives, modal operators, and two kinds of\nquantifiers: over object variables and over intension variables. In\naddition to the usual formula-creating machinery, we have the\nfollowing. If \\(X\\) is a formula, \\(x\\) is an object variable,\nand \\(f\\) is an intension variable, then\n\\([\\lambda x\\,X](f)\\) is a\nformula, in which the free variable occurrences are those of\n\\(X\\) except for \\(x\\), together with the displayed occurrence\nof \\(f\\). \n\nTo distinguish the models described here from those in Section 3.2.2,\nthese will be referred to as FOIL models, standing for\nfirst order intensional logic. They are discussed more fully\nin (Fitting 2004). A FOIL\nmodel is a structure\n \\(\\cM =\n \\langle \\bG, \\bR,\n\\bD_{O},\n\\bD_{i}, \\bI\\rangle\\) where\n\\(\\langle \\bG, \\bR,\n\\bD_{O}, \\bI\\rangle\\) meets\nthe conditions of Section 3.2.2, and in addition,\n\\(\\bD_{i}\\) is a non-empty set of\nfunctions from \\(\\bG\\) to\n\\(\\bD_{O}\\); it is the intension\ndomain. \n\nA first-order valuation in FOIL model \n \\(\\cM\\) is a mapping that assigns to each\n object variable a member of \\(\\bD_{O}\\),\nas before, and to each intension variable a member of\n\\(\\bD_{i}\\). If \\(f\\) is an\nintension variable, we’ll write \\(v(f, \\Gamma )\\) for\n\\(v(f)(\\Gamma )\\). Now, the definition of truth, at a\nstate \\(\\Gamma\\) of a model\n \\(\\cM\\), with respect to a valuation\n \\(v\\), meets the conditions set forth in\nSection 3.2.2 and, in addition, the following: \n\nLet us agree to abbreviate\n\\([\\lambda x\\,[\\lambda y\\,X](g)](f)\\)\nby \\([\\lambda xy\\,X](f, g)\\), when\nconvenient. Suppose \\(f\\) is intended to be the intension of\n“the morning star,” and \\(g\\) is intended to be the\nintension of “the evening star.” Presumably \\(f\\) and\n\\(g\\) are distinct intensions. Even so,\n\\([\\lambda xy\\,x = y]\\)(f, g) is\ncorrect in the real world—both \\(f\\) and \\(g\\) do\ndesignate the same object. \n\nHere is another example that might help make the de re /\nde dicto distinction clearer. Suppose \\(f\\) is the\nintension of “the tallest person,” and \\(g\\) is the\nintension of “the oldest person,” and suppose it happens\nthat, at the moment, these are the same people. Also, let us read\n\\(\\Box\\) epistemically. It is unlikely we would say that\n\\(\\Box [\\lambda xy\\,x = y]\\)(f, g) is\nthe case. We can read \\(\\Box [\\lambda xy\\,x =\ny]\\)(f, g) as saying we know that\n\\(f\\) and \\(g\\) are the same. It asserts that under all\nepistemic alternatives—all the various ways the world could be that\nare compatible with what we know—\\(f\\) and \\(g\\) designate\nthe same object, and this most decidedly does not seem to be the\ncase. However, we do have \\([\\lambda xy\\,\\Box (x\n= y)]\\)(f, g), which we can read as saying we know\nof \\(f\\) and \\(g\\), that is, of their denotations,\nthat they are the same, and this could be the case. It asserts that in\nall epistemic alternative states, what \\(f\\) and \\(g\\)\ndesignate in this one will be the same. In the setup\ndescribed, \\(f\\) and \\(g\\) do designate the same object, and\nidentity of objects carries over across states. \n\nIt should be noted that the examples of designating terms just given\nare all definite descriptions. These pick out different objects in\ndifferent possible worlds quite naturally. The situation with proper\nnames and with mathematics is different, and will be discussed later\nin section 3.6. \n\nHere’s an example to show how the semantics works in a technical\nway. An intension is rigid if it is constant, the same in\nevery state. We might think of a rigid intension as a disguised\nobject, identifying it with its constant value. It should not be a\nsurprise, then, that for rigid intensions, the distinction between\nde re and de dicto disappears. Indeed, something a\nbit stronger can be shown. Instead of rigidity, consider the weaker\nnotion called local rigidity in Fitting and Mendelsohn 1998:\nan intension is locally rigid at a state if it has the same\ndesignation at that state that it has at all accessible ones. To say\n\\(f\\) is locally rigid at a state, then, amounts to asserting the\ntruth of\n\\([\\lambda x\\,\\Box [\\lambda y\\,x\n= y](f)](f)\\) at that state. Local rigidity\nat a state implies the de re /de dicto distinction\nvanishes at that state. To show how the formal semantics works, here\nis a verification of the validity of \n\nIn a similar way one can establish the validity of \n\nand from these two follows the validity of \n\nwhich directly says local rigidity implies the de re /de\ndicto distinction vanishes. \n\nSuppose \\eqref{eq2} were not valid. Then there would be a model\n \\(\\cM = \\langle \\bG, \\bR, \\bD_{O}, \\bD_{i}, \\bI\\rangle\\), a\nstate \\(\\Gamma\\) of it, and a valuation \\(v\\) in it, such that  \n\nFrom \\eqref{eq6} we have the following, where \\(w\\) is the\n\\(x\\)-variant of \\(v\\) such that \\(w(x) = v (f, \\Gamma )\\). \n\nBy \\eqref{eq8} there is some \\(\\Delta \\in \\bG\\) with \\(\\Gamma\n\\bR \\Delta\\) such that we have the following. \n\nThen, as a consequence of \\eqref{eq7} \n\nand hence we have the following, where \\(w'\\) is the \\(x\\)-variant of\n\\(v\\) such that \\(w'(x) = v(f, \\Delta )\\). \n\nNow from \\eqref{eq5}, since \\(w(x) = v(f, \\Gamma)\\), we have and so and hence \n\nwhere \\(w''\\) is the \\(y\\)-variant of \\(w\\) such that \\(w''(y) = w(f,\n\\Delta )\\). \n\nWe claim that valuations \\(w\\) and \\(w'\\) are the same, which means\nthat \\eqref{eq9} and \\eqref{eq11} are contradictory. Since both are \\(x\\)-variants of\n\\(v\\), it is enough to show that \\(w(x) = w'(x)\\), that is, \\(v(f,\n\\Gamma ) = v(f, \\Delta )\\), which is intuitively what local rigidity\nsays. Proceeding formally, \\(v(f, \\Gamma ) = w(x) = w''(x)\\) since\n\\(w''\\) is a \\(y\\)-variant of \\(w\\) and so they agree on \\(x\\). We\nalso have \\(v(f, \\Delta ) = w''(y)\\). And finally, \\(w''(x) = w''(y)\\)\nby \\eqref{eq14}. \n\nHaving reached a contradiction, we conclude that \\eqref{eq2} must be\nvalid. \n\nIn models the domain of intensions is to be some non-empty set of\nfunctions from states to objects. We have deliberately left vague the\nquestion of which ones we must have. There are some conditions we\nmight want to require. Here are some considerations along these lines,\nbeginning with a handy abbreviation. \n(where \\(x\\) and \\(y\\) are distinct object variables). \n\nWorking through the FOIL semantics, \n \\(\\cM, \\Gamma \\vDash_{v} D(f,x)\\) is true just in case\n\\(v(f, \\Gamma ) = v(x)\\). Thus\n\\(D(f, x)\\) says the intension \\(f\\)\ndesignates the object \\(x\\). \n\nThe formula \\(\\forall f\\exists xD(f, x)\\) is valid in FOIL\nmodels as described so far. It simply says intensions always\ndesignate. On the other hand, there is no a priori reason to\nbelieve that every object is designated by some intension, but under\nspecial circumstances we might want to require this. We can do it by\nrestricting ourselves to models in which we have the validity of \n\nIf we require \\eqref{eq16}, quantification over objects is reducible to\nintensional quantification: \n\nMore precisely, the implication \\(\\eqref{eq16} \\supset \\eqref{eq17}\\) is valid in\nFOIL semantics. \n\nWe also might want to require the existence of choice\nfunctions. Suppose that we have somehow associated an object\n\\(d_{\\Gamma }\\) with each state \\(\\Gamma\\) of a model. If\nour way of choosing \\(d_{\\Gamma }\\) can be specified by a\nformula of the language, we might want to say we have specified an\nintension. Requiring the validity of the following formula seems as\nclose as we can come to imposing such an existence condition on\nFOIL models. For each formula \\(\\Phi\\): \n\n“The King of France in 1700” denotes an object, Louis\nXIV, who does not exist, but did. “The present King of\nFrance” does not denote at all. To handle such things, the\nrepresentation of an intension can be generalized from being a total\nfunction from states to objects, to being a partial function.\nWe routinely talk about non-existent objects—we have no problem\ntalking about the King of France in 1700. But there is nothing to be\nsaid about the present King of France—there is no such thing.\nThis will be our guide for truth conditions in our semantics. \n\nThe language stays the same, but intension variables are now\ninterpreted by partial functions on the set of\nstates—functions whose domains may be proper subsets of the set\nof states. Thus \n \\(\\cM = \\langle \\bG,\n\\bR, \\bD_{O},\n\\bD_{i}, \\bI\\rangle\\) is a\npartial FOIL model if it is as in Section 3.4 except\nthat members of \\(\\bD_{i}\\) are partial\nfunctions from \\(\\bG\\) to\n\\(\\bD_{O}\\). Given a partial FOIL\nmodel\n \\(\\cM\\)\n and a valuation \\(v\\) in it, an intension variable \\(f\\)\ndesignates at state \\(\\Gamma\\) of this model with respect to\n\\(v\\) if \\(\\Gamma\\) is in the domain of \\(v(f)\\). \n\nFollowing the idea that nothing can be said about the present King of\nFrance, we break condition \\eqref{cond1} from Section 3.4 into two parts. Given\na partial FOIL model\n \\(\\cM\\)\n and a valuation \\(v\\) in it: \n\nThus designating terms behave as they did before, but nothing can be\ntruly asserted about non-designating terms. \n\nRecall, we introduced a formula \\eqref{eq15} abbreviated by\n\\(D(f,x)\\) to say \\(f\\) designates \\(x\\).\nUsing that, we introduce a further abbreviation. \n\nThis says \\(f\\) designates. Incidentally, we could have used\n\\([\\lambda x\\,x = x](f)\\) just\nas well, thus avoiding quantification. \n\nIt is important to differentiate between existence and designation.\nAs things have been set up here, existence is a property of objects,\nbut designation really applies to terms of the formal language, in a\ncontext. To use a temporal example from Fitting and Mendelsohn 1998,\nin the usual sense “George Washington” designates a person\nwho does not exist, though he once did, while “George\nWashington’s eldest son,” does not designate at all. That an\nintensional variable \\(f\\) designates an existent object is\nexpressed by an abstract,\n\\([\\lambda x\\,E(x)](f)\\). We have to be\na bit careful about non-existence though. That \\(f\\) designates a\nnon-existent is not simply the denial of the previous expression,\n\\(\\neg [\\lambda x\\,E(x)](f)\\). After\nall, \\([\\lambda x\\,E(x)](f)\\)\nexpresses that \\(f\\) designates an existent, so its denial says\neither \\(f\\) does not designate, or it does, but designates a\nnon-existent. To express that \\(f\\) designates, but designates a\nnon-existent, we need\n\\([\\lambda x\\,\\neg E(x)](f)\\). The\nformula\n\\(\\forall f([\\lambda x\\,E(x)](f)\n \\vee \\neg [\\lambda\\)x E\\((x)](f))\\) is valid,\nbut\n \\(\\forall f([\\lambda x\\,E(x)](f)\n \\vee \n [\\lambda x\\,\\neg E(x)](f))\\) is\nnot—one can easily construct partial FOIL models that\ninvalidate it. What we do have is the following important item. \n\nIn words, intensional terms that designate must designate existents or\nnon-existents. \n\nIn earlier parts of this article, among the examples of intensions and\npartial intensions have been “the present King of France,”\n“the tallest person,” and “the oldest person.”\nOne could add to these “the number of people,” and\n“the positive solution of \\(x^{2} - 9 = 0\\).” All have been specified using definite\ndescriptions. In a temporal model, the first three determine\npartial intensions (there have been instants of time with no people);\nthe fourth determines an intension that is not partial; the fifth\ndetermines an intension that is rigid. \n\nSo far we have been speaking informally, but there are two equivalent\nways of developing definite descriptions ideas formally. The approach\nintroduced by Bertrand Russell (Russell 1905, Whitehead and Russell\n1925) is widely familiar and probably needs little explication\nhere. Suffice it to say, it extends to the intensional setting without\ndifficulty. In this approach, a term-like expression,\n\\(\\atoi y\\phi (y)\\),\n is introduced, where \\(\\phi (y)\\) is a formula and \\(y\\) is an\nobject variable. It is read, “the \\(y\\) such that\n\\(\\phi (y)\\).” This expression is given no\nindependent meaning, but there is a device to translate it away in an\nappropriate context. Thus,\n \\([\\lambda x\\,\\psi (x)] \\atoi y\\phi (y))\\)\n is taken to abbreviate the formula\n\\(\\exists y[\\forall z(\\phi (z)\n\\equiv z = y)\n \\wedge \\psi (y)]\\). \n (The standard device has been used of writing \\(\\phi (z)\\) to\nrepresent the substitution instance of \\(\\phi (y)\\) resulting\nfrom replacing free occurrences of \\(y\\) with occurrences of\n\\(z\\), and modifying bound variables if necessary to avoid\nincidental capture of \\(z\\).) The present abstraction notation,\nusing \\(\\lambda\\), is not that of Russell, but he used an equivalent\nscoping device. As he famously pointed out, Russell’s method allows us\nto distinguish between “The present King of France is not\nbald,” which is false because there is no present King of\nFrance, and “It is not the case that the present King of France\nis bald,” which is true because “The present King of\nFrance is bald” is false. It becomes the distinction between\n \\([\\lambda x\\,\\neg\\textit{Bald}(x)](\\atoi y\\textit{King}(y))\\)\n and\n \\(\\neg [\\lambda x\\textit{Bald}(x)](\\atoi y\\textit{King}(y))\\). \n\nAs an attractive alternative, one could make definite descriptions\ninto first-class things. Enlarge the language so that if\n\\(\\phi (y)\\) is a formula where \\(y\\) is an object\nvariable, then\n \\(\\atoi y\\phi (y)\\)\n is an intension term whose free variables are those of\n\\(\\phi (y)\\) except for \\(y\\). Then modify the\ndefinition of formula, to allow these new intension terms to appear in\nplaces we previously allowed intension variables to appear. That leads\nto a complication, since intension terms involve formulas, and formulas\ncan contain intension terms. In fact, formula and term must be defined\nsimultaneously, but this is no real problem. \n\nSemantically we can model definite descriptions by partial\nintensions. We say the term\n \\(\\atoi y\\phi (y)\\)\n designates at state \\(\\Gamma\\) of a partial FOIL model\n \\(\\cM\\) with respect to valuation\n \\(v\\) if \n \\(\\cM, \\Gamma \\vDash_{w} \\phi (y)\\) for exactly one \\(y\\)-variant\n\\(w\\) of \\(v\\). Then the conditions from section 3.5.1 are\nextended as follows. \n\nOne can show that the Russell approach and the approach just sketched\namount to more-or-less the same thing. But with definite descriptions\navailable as formal parts of the language, instead of just as\nremovable abbreviations in context, one can see they determine\nintensions (possibly partial) that are specified by formulas. \n\nA property need not hold of the corresponding definite description,\nthat is,\n \\([\\lambda x\\,\\phi (x)](\\atoi x\\phi (x))\\)\n need not be valid. This is simply because the definite description\nmight not designate. However, if it does designate, it must have its\ndefining property. Indeed, we have the validity of the following: \n\nOne must be careful about the interaction between definite\ndescriptions and modal operators, just as between them and negation.\nFor instance,\n \\(D(\\atoi x\\Diamond \\phi (x)) \\supset \\Diamond D(\\atoi x\\phi (x))\\)\n is valid, but its converse is not. For a more concrete example of\nmodal/description interaction, suppose \\(K(x)\\) is a\nformula expressing that \\(x\\) is King of France. In the present\nstate,\n \\([\\lambda x\\,\\Diamond E(x)](\\atoi xK(x))\\)\n is false, because the definite description has no designation, but\n \\(\\Diamond [\\lambda x\\,E(x)](\\atoi xK(x))\\)\nis true, because there is an alternative (earlier) state in which the\ndefinite description designates an existent object. It was noted that for rigid terms the de re/de dicto distinction\ncollapses. Indeed, if \\(f\\) and \\(g\\) are rigid, \\([\\lambda xy\\\nx=y](f, g)\\), \\({\\square}[\\lambda xy\\ x=y](f, g)\\) and \\([\\lambda xy\\\n{\\square}x=y](f, g)\\) are all equivalent. This is a problem that sets\na limit on what can be handled by the Carnap-style logic as presented\nso far. Two well-known areas of difficulty are mathematics and proper\nnames, especially in an epistemic setting. How could someone not know that \\(1 + 4 = 2 + 3\\)? Yet it happens\nfor small children, and for us bigger children similar, but more\ncomplex, examples of mathematical truths we don’t know can be\nfound. Obviously the designations of “\\(1 + 4\\)” and\n“\\(2 + 3\\)” are the same, so their senses must be\ndifferent. But if we model sense by a function from states to\ndesignations, the functions would be the same, mapping each state to\n5. If it is necessary truth that is at issue, there is no problem; we\ncertainly want that \\(1 + 4 = 2 + 3\\) is a necessary truth. But if\nepistemic issues are under consideration, since we cannot have a\npossible world in which “\\(1 + 4\\)” and “\\(2 +\n3\\)” designate different things, “\\(1 + 4 = 2 + 3\\)”\nmust be a known truth. So again, how could one not know this, or any\nother mathematical truth? One possible solution is to say that for mathematical terms,\nintension is a different thing than it is for definite descriptions\nlike “the King of France.” The expression “\\(1 +\n4\\)” is a kind of miniature computing program. Exactly what\nprogram depends on how we were taught to add, but let us standardize\non: \\(x + y\\) instructs us to start at the number \\(x\\) and count off\nthe next \\(y\\) numbers. Then obviously, “\\(1 + 4\\)” and\n“\\(2 + 3\\)” correspond to different programs with the same\noutput. We might identify the program with the sense, and the output\nwith the denotation. Then we might account for not knowing that \\(1 +\n4 = 2 + 3\\) by saying we have not executed the two programs, and so\ncan’t conclude anything about the output. Identifying the intension of a mathematical term with its\ncomputational content is a plausible thing to do. It does, however,\nclash with what came earlier in this article. Expressions like\n“the King of France” get treated one way, expressions like\n“\\(1 + 4\\)” another. For any given expression, how do we\ndecide which way to treat it? It is possible to unify all this. Here\nis one somewhat simple-minded way. If we think of the sense of\n“\\(1 + 4\\)” as a small program, there are certainly\nstates, possible worlds, in which we have not executed the program,\nand others in which we have. We might, then, think of the intension of\n“\\(1 + 4\\)” as a partial function on states, whose domain\nis the set of states in which the instructions inherent in “\\(1\n+ 4\\)” have been executed, and mapping those states to 5. Then,\nclearly, we can have states of an epistemic possible world model in\nwhich we do not know that “\\(1 + 4\\)” and “\\(2 +\n3\\)” have the same outputs. This can be pushed only so far. We might be convinced by some\ngeneral argument that addition is a total function always\ndefined. Then it is conceivable that we might know “\\(1 +\n4\\)” designates some number, but not know what it is. But this\ncannot be captured using the semantics outlined thus far, assuming\narithmetic terms behave correctly. If at some state we know \\(\\exists\nx([\\lambda y\\ x = y](1 + 4))\\), that is, we know “\\(1 +\n4\\)” designates, then at all compatible states, “\\(1 +\n4\\)” designates, and since arithmetic terms behave correctly, at\nall compatible states “\\(1 + 4\\)” must designate 5, and\nhence we must know \\([\\lambda y\\ 5 = y](1 + 4)\\) at the original\nstate. We cannot know “\\(1 + 4\\)” designates without\nknowing what. It is also possible to address the problem from quite a different\ndirection. One does not question the necessity of mathematical\ntruths—the issue is an epistemic one. And for this, it has long\nbeen noted that a Hintikka-style treatment of knowledge does not deal\nwith actual knowledge, but with potential knowledge—not what we\nknow, but what we are entitled to know. Then familiar logical\nomniscience problems arise, and we have just seen yet another\ninstance of them. A way out of this was introduced in Fagin and\nHalpern 1988, called awareness logic. The idea was to enrich\nHintikka’s epistemic models with an awareness function, mapping\neach state to the set of formulas we are aware of at that state. The\nidea was that an awareness function reflects some bound on the\nresources we can bring to bear. With such semantical machinery, we\nmight know simple mathematical truths but not more complex ones,\nsimply because they are too complex for us. Awareness, in this technical sense, is a blunt instrument. A\nrefinement was suggested in van Benthem 1991: use explicit\nknowledge terms. As part of a project to provide a constructive\nsemantics for intuitionistic logic, a formal logic of explicit proof\nterms was presented in Artemov 2001. Later a possible world semantics\nfor it was created in Fitting 2005. In this logic truths are known for\nexplicit reasons, and these explicit reasons provide a measure of\ncomplexity. The work was subsequently extended to a more general family\nof justification logics, which are logics of knowledge in\nwhich reasons are made explicit. In justification logics, instead of the familiar \\(KX\\) of\nepistemic logic we have \\(t{:}X\\), where \\(t\\) is an explicit\njustification term. The formula \\(t{:}X\\) is read,\n“\\(X\\) is known for reason \\(t\\).” Justification terms\nhave structure which varies depending on the particular justification\nlogic being investigated. Common to all justification logics is the\nfollowing minimal machinery. First there are justification constants,\nintended to be unanalyzed justifications of accepted logical\ntruths. Second, there are justification variables, standing for\narbitrary justifications. And finally there are binary operations,\nminimally \\(\\cdot\\) and \\(+\\). The intention is that if \\(s\\)\njustifies \\(X \\supset Y\\) and \\(t\\) justifies \\(X\\), then \\(s\\cdot\nt\\) justifies \\(Y\\), and also \\(s+t\\) justifies anything that \\(s\\)\njustifies and also anything that \\(t\\) justifies. There are very close\nconnections between justification logics and epistemic logics,\nembodied in Realization Theorems. This is not the appropriate\nplace to go into details; a thorough discussion of justification\nlogics can be found in this encyclopedia's entry on justification logic. If one follows the justification logic approach one might say, of\n\\(1 + 4 = 2 + 3\\) or some more complicated mathematical truth, that it\nis knowable but too hard for us to actually know. That is, the\njustification terms embodying our reasons for this knowledge are too\ncomplex for us. This follows the general idea of awareness logic, but\nwith a specific and mathematically useful measure of the complexity of\nour awareness. Proper names are even more of a problem than mathematical\nexpressions. These days proper names are generally understood to be\nrigid designators but, unlike mathematical terms, they have no\nstructure that we can make use of. Here is a very standard\nexample. Suppose “Hesperus” is used as a name for the\nevening star, and “Phosphorus” for the morning star. It\nshould be understood that “the evening star” is\nconventional shorthand for a definite description, “the first\nheavenly body seen after sunset” and similarly for “the\nmorning star”. Definite descriptions have structure, they pick\nout objects and in different possible worlds they may pick out\ndifferent objects. But proper names are not like that. Once the\ndesignations of “Hesperus” and “Phosphorus” are\nfixed—as it happens they both name the planet Venus—that\ndesignation is fixed across possible worlds and so they are rigid\ndesignators. It follows that while the morning star is the evening\nstar, that identity is not necessary because definite descriptions are\nnot rigid, but Hesperus is Phosphorus and that identity is a necessary\none. How, then, could the identity of Hesperus and Phosphorus not be a\nknown truth, known without doing any astronomical research? There is more than one solution of the dilemma just mentioned. One\nway is very simple indeed. Possible world models can be used to\nrepresent various kinds of modalities. They provide mathematical\nmachinery, but they do not say what the machinery is for. That is up\nto the user. So, we might want to have such a model to represent\nnecessary truth, or we might want to have such a model to represent\nepistemic issues. The argument that proper names are rigid designators\napplies to models representing necessary truth. It does not follow\nthat this is the case for epistemic models too. Here is a quote from\n(Kripke 1980) that sheds some light on the issue. But being put in a situation where we have exactly the same\nevidence, qualitatively speaking, it could have turned out that\nHesperus was not Phosphorus; that is, in a counterfactual world in\nwhich ‘Hesperus’ and ‘Phosphorus’ were not\nused in the way that we use them, as names of this planet, but as\nnames of some other objects, one could have had qualitatively\nidentical evidence and concluded that ‘Hesperus’ and\n‘Phosphorus’ named two different objects. But we, using\nthe names as we do right now, can say in advance, that if Hesperus and\nPhosphorus are one and the same, then in no other possible world can\nthey be different. We use ‘Hesperus’ as the name of a\ncertain body and ‘Phosphorus’ as the name of a certain\nbody. We use them as names of these bodies in all possible worlds. If,\nin fact, they are the same body, then in any other possible\nworld we have to use them as a name of that object. And so in any\nother possible world it will be true that Hesperus is Phosphorus. So\ntwo things are true: first, that we do not know a priori that\nHesperus is Phosphorus, and are in no position to find out the answer\nexcept empirically. Second, this is so because we could have evidence\nqualitatively indistinguishable from the evidence we have and\ndetermine the reference of the two names by the positions of two\nplanets in the sky, without the planets being the same. In short, proper names are rigid designators in models where the\npossible worlds represent logically alternative states. They need not\nbe rigid designators in models where the possible worlds represent\nepistemically alternative states. Hesperus and Phosphorus are the\nsame, necessarily so, but we could have used the names\n“Hesperus” and “Phosphorus” differently\nwithout being able to tell we were doing so—a state in which we\ndid this might be epistemically indistinguishable from the actual\none. There can be necessary identities that we do not know because\nnecessary truth and known truth do not follow the same rules. The formal machinery behind the discussion above traces back to\nideas of Carnap. In this tradition possible worlds are central, and\nsense or intension is a function from possible worlds to\ndenotations. Senses determine denotations, but detailed machinery\naccounting for how this happens is not made concrete (except for\ndefinite descriptions). One need not do things this way. If the Church\napproach is followed, one can simply say that “Hesperus”\nand “Phosphorus” have the same designation rigidly, hence\nnecessarily, but even so they do not have the same sense. This is\npossible because senses are, in effect, independent and not derived\nthings. Senses can determine the same extension across possible worlds\nwithout being identical. A logic breaking the Carnapian mold, that is thorough and fully\ndeveloped, can be found in Zalta 1988. In this a class of abstract\nobjects is postulated, some among them being ordinary. A distinction\nis made between an object exemplifying a property\nand encoding it. For instance, an abstract object might\nperfectly well encode the property of being a round square, but could\nnot exemplify it. A general comprehension principle is assumed, in the\nform that conditions determine abstract individuals that encode (not\nexemplify) the condition. Identity is taken to hold between objects if\nthey are both abstract and encode the same properties, or they are\nboth ordinary and exemplify the same properties. In effect, this deals\nwith problems of substitutivity. The formal theory (more properly,\ntheories) is quite general and includes both logical necessity and\ntemporal operators. It is assumed that encoding is not contingent,\nthough exemplifying may be, and thus properties have both an\nexemplification extension that can vary across worlds, and an encoding\nextension that is rigid. With all this machinery available, a detailed\ntreatment of proper names can be developed, along with much else. Following Frege, the mathematical expressions “\\(1+4\\)”\nand “\\(2+3\\)” have the same denotation but different\nsenses. Frege did not actually say what a sense was, though it was\nclear that, somehow, sense determined denotation. Earlier we talked of\ncomputations associated with “\\(1+4\\)” and\n“\\(2+3\\)”, but what we presented was quite\nsimple-minded. Tichý introduced the idea of\na construction with these two expressions prescribing\ndifferent constructions. A much more formal version of this appears in\na series of papers, (Moschovakis 1994; Moschovakis 2006; Kalyvianaki\nand Moschovakis 2008), all of which trace back to (Moschovakis\n1989). In these is a very sophisticated formalism in which the sense\nor intension of an expression is an algorithm, and algorithm execution\ndetermines denotation. In what follows we sketch the ideas, skimping\non most technical details. To keep things relatively simple we confine our discussion to\nsentences of a formal language for which, again following Frege,\ndenotation is simply a truth value. Both “there are infinitely\nmany primes” and “there are infinitely many even\nnumbers” agree on denotation—both are true—but\nclearly have different senses. All the basic ideas of Moschovakis are\nalready present at the sentence level, though the ideas extend\nbroadly. We quote from (Moschovakis 1994), on which our presentation\nis based. The mathematical results of the paper are about formal languages,\nbut they are meant to apply also to those fragments of natural\nlanguage which can be formalized, much as the results of denotational\nsemantics for formal languages are often applied to fragments of\nnatural language. In addition to the language of predicate logic whose\nsense semantics are fairly simple, the theory also covers languages\nwith description operators, arbitrary connectives and modal operators,\ngeneralized quantifiers, indirect reference and the ability to define\ntheir own truth predicate. If sense is to be identified with algorithm, perhaps the most basic\nquestion is: what is an algorithm. For Moschovakis, as for many\nworking mathematicians, an algorithm is an abstract mathematical\nobject, in the same way that a number is. Of course one uses special\nnotation to work with a number or an algorithm, but notation is\nsyntactic while mathematical objects are semantic (even\nideal). Algorithmic subject matter may vary: an algorithm for baking a\ncake does not operate in the same space as an algorithm for solving\nquadratic equations. Some formalism is needed so that algorithms can\nbe specified, and this machinery should be suitable for all subjects,\nyet as simple as possible. There are several general, but equivalent,\napproaches to algorithmic specification across a range of subject\nmatters. Moschovakis (1994) introduces a very simple, direct mechanism\nwhich he calls the Lower Predicate Calculus with Reflection,\nwhere reflection essentially\nmeans self-reference. Of course not all algorithms terminate,\nand consequently the underlying truth value space needs some\nconsideration, but a solution along Kripke’s lines in his Theory\nof Truth works well. We lead up to a general definition via some (for\nthe time being) informal examples. Suppose we have a structure with a given domain and some given\nrelations of various arities, say \\(\\langle \\bD, \\bR_1,\n\\ldots, \\bR_n\\rangle\\). And suppose we have a first-order\nlanguage formed in the usual way, with relation symbols, \\(R_1\\),\n…, \\(R_n\\) whose arities match those of the given relations. We\nwill generally use the typographical convention that \\(\\bR\\) is\na relation and \\(R\\) is the associated formal symbol interpreted by\nthat relation. In the usual way we can build up a first-order language\nthat talks about the structure, where atomic formulas involve \\(R_1\\),\n…, \\(R_n\\) and \\(=\\). Constants can be simulated by the use of\nunary relations that are true of single things. For example, in\narithmetic we can have a relation \\(\\bZ\\) such that\n\\(\\bZ(x)\\) holds only when \\(x=0\\). In the interests of\nreadability, in such a case we would act as if we had a constant\nsymbol in our language that was interpreted by 0. Such informal\nsimplifications make formula reading a bit easier, while nothing\nsignificant is lost. What is added to the usual first-order machinery is a\n\\(\\textsf{where}\\) construction. We will give a proper definition\nshortly but first, here is a concrete example. Let us assume we have a\nstructure for arithmetic, \\(\\langle \\{0,1,2,\\ldots\\}, \\bS,\n\\bZ\\rangle\\). Here \\(\\bS\\) is the two-place successor\nrelation on the domain, that is, we have \\(\\bS(0,1)\\),\n\\(\\bS(1,2)\\), …. We also assume \\(\\bZ\\) is true\nuniquely of 0 and, in accord with what we said above about relations\nand individual constants, we act as if we had a constant symbol 0 in\nthe formal language. Consider the following formula, where \\(S\\) is a\ntwo-place relation symbol interpreted by \\(\\bS\\), and \\(E\\) and\n\\(O\\) are auxiliary one-place relation symbols. For the time being think of \\(\\simeq\\) as something like “is\ndefined to be”. This will be discussed further later. Think of\n\\(E(x)\\) as representing the ‘output’ relation. It is\ndefined in terms \\(O\\), where \\(O\\) is defined in terms of\n\\(E\\). Mutual recursion is involved. Even at this informal stage it is\nnot hard to see that \\(\\even\\) defines the set of even\nnumbers, in the sense that \\(\\even(x)\\) evaluates to true for\neven \\(x\\) and to false for odd \\(x\\). Here is an informal calculation\nshowing that \\(\\even(2)\\) evaluates to true. In it we use\n\\(\\Leftarrow\\) for reverse implication. Also we write members of the\ndomain (numbers) directly into formulas, rather than using the\nmachinery of valuations assigning numbers to free variables. \nWe used the clause three times, replacing \\(E(2)\\), \\(O(1)\\), and\n\\(E(0)\\). The final line is true because \\(S(1,2)\\), \\(S(0,1)\\), and\n\\(0=0\\) are true. This example is a start, but it is misleadingly simple. The\nmachinery is rich enough to allow formulation of the liar sentence. In\nthe following, \\(P\\) is an auxiliary relation symbol of arity 0, that\nis, a propositional letter. We have written just \\(P\\) instead of\n\\(P()\\).  \nClearly an evaluation attempt of the sort shown above will not\nterminate. A solution to non-termination is familiar from classical\nrecursion theory, and also from work on the theory of truth: allow the\nrelations defined by our formal machinery to be partial. Not\nall instances of a relation have to receive a truth value. But these\nare semantic issues and before getting to them we need to give a\nproper syntactic definition of the language within which our formulas\nwill be written. Above we spoke of a first-order language appropriate for a\nstructure \\(\\langle\\bD, \\bR_1, \\ldots,\n\\bR_n\\rangle\\), enhanced with clauses, but these clauses were\nonly shown via examples. Here is a proper definition. The Lower\nPredicate Calculus with Reflection (LPCR) for \\(\\langle\\bD,\n\\bR_1, \\ldots, \\bR_n\\rangle\\) is the language built up\nusing the machinery of ordinary first-order logic with equality,\ntogether with the following formation clause. If \\(\\phi_0\\),\n\\(\\phi_1\\), …, \\(\\phi_k\\) are formulas and \\(P_1\\), …,\n\\(P_k\\) are (new) auxiliary relation variables, the following is a\nformula. \nIn this each \\(\\bx_i\\) is a sequence of variables whose length\nis the arity of \\(P_i\\). The \\(P_i\\) may appear in the formulas\n\\(\\phi_0\\), …, \\(\\phi_k\\) themselves, and so we have a\nself-referential set of defining equations, with \\(\\phi_0\\) as\n‘output’. Note that with \\eqref{whereformula} added to the\ndefinition of formula, \\(\\textsf{ where }\\) conditions can appear in some of the\n\\(\\phi_i\\), and so means of preventing inappropriate interaction\nbetween nested conditions is needed. This is done through the familiar\nmachinery of free and bound variables. The symbols \\(P_1\\), …,\n\\(P_k\\) are taken to be relation variables, and are\nconsidered to be bound in \\eqref{whereformula}. Likewise the occurrences\nof individual variables in \\(\\bx_i\\) are understood to be bound\nin \\(P_i(\\bx_i) \\simeq \\phi_i\\). In effect, these are local\nvariables. Now the language LPCR has been defined, and we turn to notions of sense and reference. We have been discussing sentences and more generally formulas with\nfree variables. The familiar Tarskian semantics provides a basis for\nunderstanding here, but we need modifications and extensions to deal\nwith the construct. A partial function on a space \\(S\\) is a function that\nassigns values to some, but not necessarily to all, members of\n\\(S\\). Said otherwise, it is a function whose domain is a subset of\n\\(S\\). For a partial function \\(f\\), \\(f(x)\\simeq y\\) means \\(x\\) is\nin the domain of \\(f\\) and \\(f(x) = y\\). (Finally we have a proper\naccounting of our use of \\(\\simeq\\) in the examples\nearlier.) Partial relations are partial functions from\n\\(k\\)-tuples to \\(\\{{\\textsf{t}}, {\\textsf{f}}\\}\\). The given\nrelations of our structures are relations in the usual sense, but it\nis partial relations that we may find ourselves defining. Assume we have a structure \\(\\langle\\bD, \\bR_1,\n\\ldots, \\bR_n\\rangle\\), and suppose we have an LPCR language\nassociated with it. A valuation \\(v\\) in this structure is a\nmapping from individual variables to members of \\(\\bD\\) and\nfrom auxiliary relation symbols to partial relations on\n\\(\\bD\\). We would like to associate with each valuation \\(v\\) a\nmapping \\(T_v\\) from formulas of LPCR to truth values but since things\nlike the liar sentence are formulable, \\(T_v\\) must be a partial\nfunction, and so we must be careful even about familiar things like\npropositional connectives. Various three valued logics have been\ndeveloped; perhaps the most common is Kleene’s strong\nthree-valued logic, motivated by recursion theory and familiar from\nmuch work on the Theory of Truth. The following table says how\nconnectives and quantifiers behave. Cases that are not explicitly\ncovered are understood to be those for which a truth valuation is left\nundefined. (For instance, if the truth value of \\(X\\) is undefined,\nthe same is the case for \\(\\lnot X\\).) This still leaves formulas to deal with. Suppose we have the following. We make two simplifying assumptions to keep our discussion from\nbeing too intricate. We assume no \\(\\phi_i\\) contains a nested\n\\(\\textsf{ where }\\) clause. The basic ideas are amply illustrated with this condition\nimposed, but everything extends to the general case without too much\ndifficulty. It is a general requirement that the variables in\n\\(\\bx_i\\) are ‘local’ to \\(P_i(\\bx_i) \\simeq\n\\phi_i\\), that is, they are considered to be bound in this formula. To\nthis we add another simplifying assumption: the variables in\n\\(\\bx_i\\) are the only variables that may occur free\nin \\(\\phi_i\\). Roughly this means that we have no parameters, only\nlocal variables. This serves to allow us to discuss things with less\nclutter. Again, everything extends to the more general case with no\nfundamental changes. Continuing with \\eqref{Eexample}, consider the following associated set\n\\(E\\) of equations. The difficulty, of course, is that each \\(P_i\\) is allowed to occur\nin one or more \\(\\phi_j\\), possibly even in \\(\\phi_i\\), and so \\(E\\)\nis self-referential. In many computer programming languages one sees\nthings like \\(x = x+1\\). It is explained to beginning programmers that\nthis takes the current value of \\(x\\), adds 1, and calls the result\n\\(x\\) again. Occurrences of \\(x\\) on the right have\n‘before’ values, occurrences on the left have\n‘after’ values. Analogously, let us think of the members\nof \\(E\\) as (simultaneous) assignment statements. Occurrences of\n\\(P_i\\) on the right of \\(\\simeq\\) are current values, occurrences on\nthe left are next values. Taking all of \\(P_1\\), …, \\(P_k\\)\ninto account, we can think of \\(E\\) as defining a functional that maps\n\\(k\\)-tuples of partial relations (‘before’ values of\nthese relation symbols) to \\(k\\)-tuples of partial relations\n(‘after’ values of these relation symbols). Now here are\nthe details a bit more formally. Suppose we have a \\(k\\)-tuple \\(\\langle\\bP_1, \\ldots,\n\\bP_k\\rangle\\) of partial relations, where for each \\(i\\) the\narity of \\(\\bP_i\\) matches that of the partial relation\nvariable \\(P_i\\). This is our input (‘before’ values). For\neach \\(i\\) we want to define an output partial relation which we call\n\\(\\bP'_i\\), of the same arity as \\(\\bP_i\\), so that\n\\(\\langle\\bP'_1, \\ldots, \\bP'_k\\rangle\\) serves as our\noverall output (‘after’ values). To do this we must say\nwhen \\(\\bP'_i(\\bd)\\) maps to \\({\\textsf{t}}\\), when it\nmaps to \\({\\textsf{f}}\\), and when it is undefined, for each\n\\(\\bd\\) with components from \\(\\bD\\). Well, take \\(v\\)\nto be a valuation assigning to each auxiliary relation symbol \\(P_i\\)\nthe corresponding partial relation \\(\\bP_i\\) (this is how\n‘before’ values for our partial relation symbols come in),\nand assigning to the variables in \\(\\bx_i\\) the corresponding\nmembers of \\(\\bd\\). Now, simply let\n\\(\\bP'_i(\\bd)\\simeq T_v(\\phi_i)\\). In this way a new\npartial relation \\(\\bP'_i\\) is specified, and more generally a\nvector of them, \\(\\langle\\bP'_1, \\ldots,\n\\bP'_k\\rangle\\). The set of equations \\(E\\) can be thought of\nas specifying a functional transforming \\(k\\)-tuple\n\\(\\langle\\bP_1, \\ldots, \\bP_k\\rangle\\) into\n\\(\\langle\\bP'_1, \\ldots, \\bP'_k\\rangle\\). Let us call\nthis functional \\([E]\\), and write \\([E](\\langle\\bP_1, \\ldots,\n\\bP_k\\rangle) = \\langle\\bP'_1, \\ldots,\n\\bP'_k\\rangle\\). If we are to have equations \\(E\\) behave well in a logic setting,\neach \\(P_i\\) should have the same valuation no matter where we see\nit—there should be no distinction between what we have been\ncalling left and right sides; \\(\\bP_i\\) and \\(\\bP'_i\\)\nshould be the same. In other words, we would like to have partial\nrelations \\(\\bP_1\\), …, \\(\\bP_k\\) to interpret\n\\(P_1\\), …, \\(P_k\\) so that \\([E](\\langle\\bP_1, \\ldots,\n\\bP_k\\rangle) = \\langle\\bP_1, \\ldots,\n\\bP_k\\rangle\\)—‘before’ and\n‘after’ values agree. This is called a fixed\npoint of \\([E]\\). So, we need to know that \\([E]\\) has a fixed\npoint, and if it has more than one then there is a plausible candidate\nwe can choose as the best one. If \\(f\\) and \\(g\\) are two partial functions from a space \\(S\\) to\n\\(R\\), one writes \\(f\\subseteq g\\) to mean that whenever \\(f(x)\\simeq\nw\\) then also \\(g(x)\\simeq w\\). Then for two partial relations\n\\(\\bP\\) and \\(\\bQ\\) of the same arity,\n\\(\\bP\\subseteq\\bQ\\) means that whenever\n\\(\\bP(\\bd)\\) is defined, so is\n\\(\\bQ(\\bd)\\), and both have the same truth value. We can\nextend this to \\(k\\)-tuples by setting \\(\\langle\\bP_1, \\ldots,\n\\bP_k\\rangle\\subseteq\\langle\\bQ_1, \\ldots,\n\\bQ_k\\rangle\\) if \\(\\bP_i\\subseteq\\bQ_i\\) for\neach \\(i\\). It is not terribly difficult to show that the functional\n\\([E]\\) defined above, and based on \\eqref{Eexample}, has\nthe monotonicity property: if \\(\\langle\\bP_1, \\ldots,\n\\bP_k\\rangle\\subseteq\\langle\\bQ_1, \\ldots,\n\\bQ_k\\rangle\\) then \\([E](\\langle\\bP_1, \\ldots,\n\\bP_k\\rangle)\\subseteq[E](\\langle\\bQ_1, \\ldots,\n\\bQ_k\\rangle)\\). There is a very general theory of monotone\nmappings like this, from which it follows that \\([E]\\) does have a\nfixed point. Moreover, if there are more than one then there is a\nunique one that is least, in the sense that it is in the \\(\\subseteq\\)\nrelation to any other. This least fixed point is precisely the best\ncandidate we mentioned above. It contains the information that any\nfixed point must have. Now we finish saying how to evaluate the formula\n\\eqref{Eexample}. First, construct the associated set of equations,\n\\(E\\). Next, construct the functional \\([E]\\). There is a least fixed\npoint for \\([E]\\), let us say it is \\(\\langle\\bF_1, \\ldots,\n\\bF_k\\rangle\\). Finally, evaluate \\(\\phi_0\\) using\n\\(\\bF_i\\) to interpret \\(P_i\\) for each \\(i\\). The resulting\ntruth value, or undefined, is the value (denotation) associated with\n\\eqref{Eexample}. We have now said how to associate a truth value, or undefined, with\nevery formula of LPCR (under our simplifying assumptions). We have\n(partial) denotations. Each formula of LPCR specifies an algorithm for its evaluation,\nthat is, for the determination of its truth value (if\npossible). Moschovakis identifies the sense of a formula with\nthat algorithm. Two formulas that evaluate to the same result, thus\nhaving the same denotation, may have different senses because the\nassociated algorithms are different. For example, in \\eqref{evenone} we\ngave a formula that defines the even numbers. Here is another such\nformula.  We leave it to you to verify that \\eqref{eventwo} also defines the even\nnumbers. It is intuitively plausible that \\eqref{evenone} and \\eqref{eventwo}\nevaluate using algorithms that differ, and so have different\nsenses. But of course this must be made precise. What is needed is a\nuniform method of comparison between algorithms. Here we just briefly\nsketch the ideas. There is very general machinery, from Moschovakis 1989, called the\nFormal Language of Recursion, FLR. Using it a thorough exploration of\nrecursive definitions and fixpoints is possible. The language that\nconcerns us here, LPCR, embeds into FLR, even allowing nested clauses\nand parameters, something we ignored in our discussion of\ndenotation. In FLR there is a method for converting recursive\ndefinitions into a normal form, which cannot be further reduced. That\nnormal form has a very simple structure, consisting of a set of\nself-referential equations with no nesting present at all. Normal\nforms reveal essential evaluation structure most clearly. When working\nwith a single structure, \\(\\langle\\bD, \\bR_1, \\ldots,\n\\bR_n\\rangle\\), all normal forms will be built from a common\nset of functionals. This makes it easy to compare normal forms. The\nidea is that if two formulas of LPCR, when embedded into FLR, have\ndiffering normal forms, the two formulas have different senses. Of\ncourse this must be taken with some reasonable flexibility. For\ninstance, two sets of equations that differ only by renaming variables\nor switching order of equations do not differ in any fundamental\nway. With this understood, if two LPCR formulas, when embedded into\nFLR, have truly distinct normal forms, the two LPCR formulas are\ndefined to have different senses. This meets all the informal\nconditions one wants a notion of sense to have. Moschovakis even\nproves the important theorem that equality of sense, as just defined,\nis decidable under natural conditions. The word “algorithm” suggests something effective, but\nhere it is being used in a more general sense, as a set of\ninstructions that, for reasons of our finitistic limitations, we may\nnot be able to actually carry out. Consider again the paradigm\nformula, \\eqref{whereformula}. If one of the \\(\\phi_i\\) contains an\nexistential quantifier in a positive position (or a universal\nquantifier in a negative position) it can be thought of as invoking a\nsystematic search through the domain \\(\\bD\\) for a verifying\nwitness. This is plausible for reasonable domains. But if \\(\\phi_i\\)\nshould contain a universal quantifier in a positive position or an\nexistential quantifier in a negative position, something must be\nverified for every member of the domain and unless the domain is\nfinite, this is not a human task. Nonetheless, we generally believe we\nunderstand quantification. What we are dealing with is algorithms\nrelative to that understanding. The problem with quantifiers is inescapable for much that we\nroutinely discuss using sense and reference. Consider Russell’s\ntreatment of definite descriptions. In this “the \\(A\\) has\nproperty \\(B\\)” is replaced by “exactly one thing has\nproperty \\(A\\) and it has property \\(B\\)”. To say that only one\nthing has property \\(A\\) one says that something has property \\(A\\)\nand everything else does not. The first part of this involves an\nexistential quantifier and the second part a universal one. Then if\nthe definite description occurs in a positive location we have a\npositive occurrence of a universal quantifier, and if it occurs in a\nnegative location we have a negative occurrence of an existential\nquantifier. Essential problems arise either way. Moschovakis is not\nclaiming to turn sense and reference into something computable, but\nsimply to provide mathematical machinery that can plausibly formalize\nthe ideas involved using a generalized notion of algorithm. There is a second, related problem where lack of effectiveness\ncomes in. In our discussion of denotation we considered a set \\(E\\) of\nequations \\eqref{equationsE} and a functional \\([E]\\) associated with\nthem. Recall that \\([E]\\) mapped \\(k\\)-tuples of partial relations to\n\\(k\\)-tuples of partial relations. We noted that \\([E]\\) would\nbe monotone, and by very general results such functionals\nalways have least fixed points. There is more than one way of showing\nthis. One well-known argument has a decidedly algorithmic flavor to\nit. It goes as follows. Start with the smallest \\(k\\)-tuple of partial\nrelations—this is the one where every partial relation is always\nundefined. Call this \\(T_0\\). Apply the functional \\([E]\\) to \\(T_0\\),\ngetting \\(T_1\\). Apply the functional \\([E]\\) to \\(T_1\\) getting\n\\(T_2\\), and so on. It is easy to show that \\(T_0\\subseteq\nT_1\\subseteq T_2\\subseteq\\ldots\\). We have that \\(T_0\\subseteq T_1\\)\nbecause \\(T_0\\) is in the \\(\\subseteq\\) relation to every\n\\(k\\)-tuple. By monotonicity we then have \\([E](T_0)\\subseteq\n[E](T_1)\\), but this says \\(T_1 \\subseteq T_2\\). And so on. Continue\nwith this increasing sequence and eventually the least fixed point of\n\\([E]\\) will be reached. But this is very misleading. What does “continue” mean?\nWe have \\(T_0\\), \\(T_1\\), \\(T_2\\), …. None of these may be a\nfixed point. For instance, suppose we carry out this construction with\nthe functional arising from \\eqref{evenone} for \\(\\even(x)\\). Then\n\\(T_0\\) will be \\(\\langle E_0, O_0\\rangle\\), where both \\(E_0\\) and\n\\(O_0\\) are the everywhere undefined 1-place relation. We leave it to\nyou to check that we get successive \\(T_i=\\langle E_i, O_i\\rangle\\)\nwhere we have the following, with cases not displayed being\nundefined.  None of \\(T_0\\), \\(T_1\\), \\(T_2\\), …is a fixed point, but\nthere is a clear notion of a limit, called \\(T_\\omega\\), that\naccumulates the results produced along the way. It is the least fixed\npoint in this example. But iterating and taking a limit may not be sufficient. Consider\nthe following elaboration of \\eqref{evenone}. The set of equations arising from \\eqref{evenmore} has the two members\nof \\eqref{evenone}, and one more for \\(A\\). Using these equations, in\norder to conclude \\(A(1)\\) we must already have one of \\(E(y)\\) or\n\\(O(y)\\) evaluating to \\({\\textsf{t}}\\) for every number \\(y\\). If we\ncarry out the construction outlined above, we won’t have this\nfor \\(E\\) and \\(O\\) until stage \\(\\omega\\), and so we must go one more\nstep, to what is called \\(T_{\\omega+1}\\), before we reach a fixed\npoint. More and more extreme examples can be given. The fixed point\nconstruction may have to be continued to larger and larger transfinite\nordinals. This is a well-known phenomenon, especially in areas like\nthe Theory of Truth. It cannot be avoided. Incidentally, it should be\nnoted that the machinery introduced by Kripke in his treatment of\ntruth has a natural embedding into LPCR, but we do not discuss this\nhere."}]
