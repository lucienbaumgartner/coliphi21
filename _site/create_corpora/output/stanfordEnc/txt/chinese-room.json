[{"date.published":"2004-03-19","date.changed":"2020-02-20","url":"https://plato.stanford.edu/entries/chinese-room/","author1":"David Cole","author1.info":"http://www.d.umn.edu/~dcole/","entry":"chinese-room","body.text":"\n\n\nThe argument and thought-experiment now generally known as the Chinese\nRoom Argument was first published in a 1980 article by American\nphilosopher John Searle (1932– ). It has become one of the best-known\narguments in recent philosophy. Searle imagines himself alone in a\nroom following a computer program for responding to Chinese characters\nslipped under the door. Searle understands nothing of Chinese, and\nyet, by following the program for manipulating symbols and numerals\njust as a computer does, he sends appropriate strings of Chinese\ncharacters back out under the door, and this leads those outside to\nmistakenly suppose there is a Chinese speaker in the room. \n\n\nThe narrow conclusion of the argument is that programming a digital\ncomputer may make it appear to understand language but could not\nproduce real understanding. Hence the “Turing Test” is\ninadequate. Searle argues that the thought experiment underscores the\nfact that computers merely use syntactic rules to manipulate symbol\nstrings, but have no understanding of meaning or semantics. The\nbroader conclusion of the argument is that the theory that human minds\nare computer-like computational or information processing systems is\nrefuted. Instead minds must result from biological processes;\ncomputers can at best simulate these biological processes. Thus the\nargument has large implications for semantics, philosophy of language\nand mind, theories of consciousness, computer science and cognitive\nscience generally. As a result, there have been many critical replies\nto the argument.\n\nWork in Artificial Intelligence (AI) has produced computer programs\nthat can beat the world chess champion, control autonomous vehicles,\ncomplete our email sentences, and defeat the best human players on the\ntelevision quiz show Jeopardy. AI has also produced programs\nwith which one can converse in natural language, including customer\nservice “virtual agents”, and Amazon’s Alexa and\nApple’s Siri. Our experience shows that playing chess or\nJeopardy, and carrying on a conversation, are activities that\nrequire understanding and intelligence. Does computer prowess at\nconversation and challenging games then show that computers can\nunderstand language and be intelligent? Will further development\nresult in digital computers that fully match or even exceed human\nintelligence?\n Alan Turing\n (1950), one of the pioneer theoreticians of computing, believed the\nanswer to these questions was “yes”. Turing proposed what\nis now known as\n ‘The Turing Test’:\n if a computer can pass for human in online chat, we should grant that\nit is intelligent. By the late 1970s some AI researchers claimed that\ncomputers already understood at least some natural language. In 1980\nU.C. Berkeley philosopher John Searle introduced a short and\nwidely-discussed argument intended to show conclusively that it is\nimpossible for digital computers to understand language or think. \nSearle argues that a good way to test a theory of mind, say a theory\nthat holds that understanding can be created by doing such and such,\nis to imagine what it would be like to actually do what the theory\nsays will create understanding. Searle (1999) summarized his Chinese\nRoom Argument (herinafter, CRA) concisely: \nSearle goes on to say, “The point of the argument is this: if\nthe man in the room does not understand Chinese on the basis of\nimplementing the appropriate program for understanding Chinese then\nneither does any other digital computer solely on that basis because\nno computer, qua computer, has anything the man does not\nhave.” \nThirty years after introducing the CRA Searle 2010 describes the\nconclusion in terms of consciousness and\n intentionality: \n“Intentionality” is a technical term for a feature of\nmental and certain other things, namely being about something. Thus a\ndesire for a piece of chocolate and thoughts about real Manhattan or\nfictional Harry Potter all display intentionality, as will be\ndiscussed in more detail in section 5.2 below. \nSearle’s shift from machine understanding to consciousness and\nintentionality is not directly supported by the original 1980\nargument. However the re-description of the conclusion indicates the\nclose connection between understanding and consciousness in\nSearle’s later accounts of meaning and intentionality. Those who\ndon’t accept Searle’s linking account might hold that\nrunning a program can create understanding without necessarily\ncreating consciousness, and conversely a fancy robot might have dog\nlevel consciousness, desires, and beliefs, without necessarily\nunderstanding natural language. \nIn moving to discussion of intentionality Searle seeks to develop the\nbroader implications of his argument. It aims to refute the\n functionalist\n approach to understanding minds, that is, the approach that holds\nthat mental states are defined by their causal roles, not by the stuff\n(neurons, transistors) that plays those roles. The argument counts\nespecially against that form of functionalism known as\n  the Computational Theory of Mind\n that treats minds as information processing systems. As a result of\nits scope, as well as Searle’s clear and forceful writing style,\nthe Chinese Room argument has probably been the most widely discussed\nphilosophical argument in cognitive science to appear since the Turing\nTest. By 1991 computer scientist Pat Hayes had defined Cognitive\nScience as the ongoing research project of refuting Searle’s\nargument. Cognitive psychologist Steven Pinker (1997) pointed out that\nby the mid-1990s well over 100 articles had been published on\nSearle’s thought experiment – and that discussion of it\nwas so pervasive on the Internet that Pinker found it a compelling\nreason to remove his name from all Internet discussion lists.  \nThis interest has not subsided, and the range of connections with the\nargument has broadened. A search on Google Scholar for “Searle\nChinese Room” limited to the period from 2010 through 2019\nproduced over 2000 results, including papers making connections\nbetween the argument and topics ranging from embodied cognition to\ntheater to talk psychotherapy to postmodern views of truth and\n“our post-human future” – as well as discussions of\ngroup or collective minds and discussions of the role of intuitions in\nphilosophy. In 2007 a game company took the name “The Chinese\nRoom” in joking honor of “...Searle’s critique of AI\n– that you could create a system that gave the impression of\nintelligence without any actual internal smarts.” This\nwide-range of discussion and implications is a tribute to the\nargument’s simple clarity and centrality.  \nSearle’s argument has four important antecedents. The first of\nthese is an argument set out by the philosopher and mathematician\nGottfried Leibniz (1646–1716). This argument, often known as\n“Leibniz’ Mill”, appears as section 17 of\nLeibniz’ Monadology. Like Searle’s argument,\nLeibniz’ argument takes the form of a thought experiment.\nLeibniz asks us to imagine a physical system, a machine, that behaves\nin such a way that it supposedly thinks and has experiences\n(“perception”). \nNotice that Leibniz’s strategy here is to contrast the overt\nbehavior of the machine, which might appear to be the product of\nconscious thought, with the way the machine operates internally. He\npoints out that these internal mechanical operations are just parts\nmoving from point to point, hence there is nothing that is conscious\nor that can explain thinking, feeling or perceiving. For Leibniz\nphysical states are not sufficient for, nor constitutive of, mental\nstates. \nA second antecedent to the Chinese Room argument is the idea of a\npaper machine, a computer implemented by a human. This idea is found\nin the work of Alan Turing, for example in “Intelligent\nMachinery” (1948). Turing writes there that he wrote a program\nfor a “paper machine” to play chess. A paper machine is a\nkind of program, a series of simple steps like a computer program, but\nwritten in natural language (e.g., English), and implemented by a\nhuman. The human operator of the paper chess-playing machine need not\n(otherwise) know how to play chess. All the operator does is follow\nthe instructions for generating moves on the chess board. In fact, the\noperator need not even know that he or she is involved in playing\nchess – the input and output strings, such as\n“N–QB7” need mean nothing to the operator of the\npaper machine. \nAs part of the WWII project to decipher German military encryption,\nTuring had written English-language programs for human\n“computers”, as these specialized workers were then known,\nand these human computers did not need to know what the programs that\nthey implemented were doing. \nOne reason the idea of a human-plus-paper machine is important is that\nit already raises questions about agency and understanding similar to\nthose in the CRA. Suppose I am alone in a closed room and follow an\ninstruction book for manipulating strings of symbols. I thereby\nimplement a paper machine that generates symbol strings such as\n“N-KB3” that I write on pieces of paper and slip under the\ndoor to someone ouside the room. Suppose further that prior to going\ninto the room I don’t know how to play chess, or even that there\nis such a game. However, unbeknownst to me, in the room I am running\nTuring’s chess program and the symbol strings I generate are\nchess notation and are taken as chess moves by those outside the room.\nThey reply by sliding the symbols for their own moves back under the\ndoor into the room. If all you see is the resulting sequence of moves\ndisplayed on a chess board outside the room, you might think that\nsomeone in the room knows how to play chess very well. Do I now know\nhow to play chess? Or is it the system (consisting of me, the manuals,\nand the paper on which I manipulate strings of symbols) that is\nplaying chess? If I memorize the program and do the symbol\nmanipulations inside my head, do I then know how to play chess, albeit\nwith an odd phenomenology? Does someone’s conscious states\nmatter for whether or not they know how to play chess? If a digital\ncomputer implements the same program, does the computer then play\nchess, or merely simulate this?  \nBy mid-century Turing was optimistic that the newly developed\nelectronic computers themselves would soon be able to exhibit\napparently intelligent behavior, answering questions posed in English\nand carrying on conversations. Turing (1950) proposed what is now\nknown as the Turing Test: if a computer could pass for human in\non-line chat, it should be counted as intelligent.  \nA third antecedent of Searle’s argument was the work of\nSearle’s colleague at Berkeley, Hubert Dreyfus. Dreyfus was an\nearly critic of the optimistic claims made by AI researchers. In 1965,\nwhen Dreyfus was at MIT, he published a circa hundred page report\ntitled “Alchemy and Artificial Intelligence”. Dreyfus\nargued that key features of human mental life could not be captured by\nformal rules for manipulating symbols. Dreyfus moved to Berkeley in\n1968 and in 1972 published his extended critique, “What\nComputers Can’t Do”. Dreyfus’ primary research\ninterests were in Continental philosophy, with its focus on\nconsciousness, intentionality, and the role of intuition and the\ninarticulated background in shaping our understandings. Dreyfus\nidentified several problematic assumptions in AI, including the view\nthat brains are like digital computers, and, again, the assumption\nthat understanding can be codified as explicit rules.  \nHowever by the late 1970s, as computers became faster and less\nexpensive, some in the burgeoning AI community started to claim that\ntheir programs could understand English sentences, using a database of\nbackground information. The work of one of these, Yale researcher\nRoger Schank (Schank & Abelson 1977) came to Searle’s\nattention.Schank developed a technique called “conceptual\nrepresentation” that used “scripts” to represent\nconceptual relations (related to Conceptual Role Semantics).\nSearle’s argument was originally presented as a response to the\nclaim that AI programs such as Schank’s literally understand the\nsentences that they respond to. \nA fourth antecedent to the Chinese Room argument are thought\nexperiments involving myriad humans acting as a computer. In 1961\nAnatoly Mickevich (pseudonym A. Dneprov) published “The\nGame”, a story in which a stadium full of 1400 math students are\narranged to function as a digital computer (see Dneprov 1961 and the\nEnglish translation listed at Mickevich 1961, Other Internet\nResources). For 4 hours each repeatedly does a bit of calculation on\nbinary numbers received from someone near them, then passes the binary\nresult onto someone nearby. They learn the next day that they\ncollectively translated a sentence from Portuguese into their native\nRussian. Mickevich’s protagonist concludes “We’ve\nproven that even the most perfect simulation of machine thinking is\nnot the thinking process itself, which is a higher form of motion of\nliving matter.” Apparently independently, a similar\nconsideration emerged in early discussion of functionalist theories of\nminds and cognition (see further discussion in section 5.3 below),\nFunctionalists hold that mental states are defined by the causal role\nthey play in a system (just as a door stop is defined by what it does,\nnot by what it is made out of). Critics of functionalism were quick to\nturn its proclaimed virtue of multiple realizability against it. While\nfunctionalism was consistent with a materialist or biological\nunderstanding of mental states (arguably a virtue), it did not\nidentify types of mental states (such as experiencing pain, or\nwondering about OZ) with particular types of neurophysiological\nstates, as “type-type identity theory” did. In contrast\nwith type-type identity theory, functionalism allowed sentient beings\nwith different physiology to have the same types of mental states as\nhumans – pains, for example. But it was pointed out that if\nextraterrestrial aliens, with some other complex system in place of\nbrains, could realize the functional properties that constituted\nmental states, then, presumably so could systems even less like human\nbrains. The computational form of functionalism, which holds that the\ndefining role of each mental state is its role in information\nprocessing or computation, is particularly vulnerable to this\nmaneuver, since a wide variety of systems with simple components are\ncomputationally equivalent (see e.g., Maudlin 1989 for discussion of a\ncomputer built from buckets of water). Critics asked if it was really\nplausible that these inorganic systems could have mental states or\nfeel pain. \nDaniel Dennett (1978) reports that in 1974 Lawrence Davis gave a\ncolloquium at MIT in which he presented one such unorthodox\nimplementation. Dennett summarizes Davis’ thought experiment as\nfollows: \nIn “Troubles with Functionalism”, also published in 1978,\nNed Block envisions the entire population of China implementing the\nfunctions of neurons in the brain. This scenario has subsequently been\ncalled “The Chinese Nation” or “The Chinese\nGym”. We can suppose that every Chinese citizen would be given a\ncall-list of phone numbers, and at a preset time on implementation\nday, designated “input” citizens would initiate the\nprocess by calling those on their call-list. When any citizen’s\nphone rang, he or she would then phone those on his or her list, who\nwould in turn contact yet others. No phone message need be exchanged;\nall that is required is the pattern of calling. The call-lists would\nbe constructed in such a way that the patterns of calls implemented\nthe same patterns of activation that occur between neurons in\nsomeone’s brain when that person is in a mental state –\npain, for example. The phone calls play the same functional role as\nneurons causing one another to fire. Block was primarily interested in\nqualia, and in particular, whether it is plausible to hold that the\npopulation of China might collectively be in pain, while no individual\nmember of the population experienced any pain, but the thought\nexperiment applies to any mental states and operations, including\nunderstanding language. \nThus Block’s precursor thought experiment, as with those of\nDavis and Dennett, is a system of many humans rather than one. The\nfocus is on consciousness, but to the extent that Searle’s\nargument also involves consciousness, the thought experiment is\nclosely related to Searle’s. Cole (1984) tries to pump\nintuitions in the reverse direction by setting out a thought\nexperiment in which each of his neurons is itself conscious, and fully\naware of its actions including being doused with neurotransmitters,\nundergoing action potentials, and squirting neurotransmitters at its\nneighbors. Cole argues that his conscious neurons would find it\nimplausible that their collective activity produced a consciousness\nand other cognitive competences, including understanding English, that\nthe neurons lack. Cole suggests the intuitions of implementing systems\nare not to be trusted.  \nIn 1980 John Searle published “Minds, Brains and Programs”\nin the journal The Behavioral and Brain Sciences. In this\narticle, Searle sets out the argument, and then replies to the\nhalf-dozen main objections that had been raised during his earlier\npresentations at various university campuses (see next section). In\naddition, Searle’s article in BBS was published along\nwith comments and criticisms by 27 cognitive science researchers.\nThese 27 comments were followed by Searle’s replies to his\ncritics. \nIn the decades following its publication, the Chinese Room argument\nwas the subject of very many discussions. By 1984, Searle presented\nthe Chinese Room argument in a book, Minds, Brains and\nScience. In January 1990, the popular periodical Scientific\nAmerican took the debate to a general scientific audience. Searle\nincluded the Chinese Room Argument in his contribution, “Is the\nBrain’s Mind a Computer Program?”, and Searle’s\npiece was followed by a responding article, “Could a Machine\nThink?”, written by philosophers Paul and Patricia Churchland.\nSoon thereafter Searle had a published exchange about the Chinese Room\nwith another leading philosopher, Jerry Fodor (in Rosenthal (ed.)\n1991). \nThe heart of the argument is Searle imagining himself following an\nsymbol processing program written in English (which is what Turing\ncalled “a paper machine”). The English speaker (Searle)\nsitting in the room follows English instructions for manipulating\nChinese symbols, whereas a computer “follows” (in some\nsense) a program written in a computing language. The human produces\nthe appearance of understanding Chinese by following the symbol\nmanipulating instructions, but does not thereby come to understand\nChinese. Since a computer just does what the human does –\nmanipulate symbols on the basis of their syntax alone – no\ncomputer, merely by following a program, comes to genuinely understand\nChinese. \nThis narrow argument, based closely on the Chinese Room scenario, is\nspecifically directed at a position Searle calls “Strong\nAI”. Strong AI is the view that suitably programmed computers\n(or the programs themselves) can understand natural language and\nactually have other mental capabilities similar to the humans whose\nbehavior they mimic. According to Strong AI, these computers really\nplay chess intelligently, make clever moves, or understand language.\nBy contrast, “weak AI” is the much more modest claim that\ncomputers are merely useful in psychology, linguistics, and other\nareas, in part because they can simulate mental abilities. But weak AI\nmakes no claim that computers actually understand or are intelligent.\nThe Chinese Room argument is not directed at weak AI, nor does it\npurport to show that no machine can think – Searle says that\nbrains are machines, and brains think. The argument is directed at the\nview that formal computations on symbols can produce thought. \nWe might summarize the narrow argument as a reductio ad\nabsurdum against Strong AI as follows. Let L be a natural\nlanguage, and let us say that a “program for L” is a\nprogram for conversing fluently in L. A computing system is any\nsystem, human or otherwise, that can run a program. \nThe first premise elucidates the claim of Strong AI. The second\npremise is supported by the Chinese Room thought experiment. The\nconclusion of this narrow argument is that running a program cannot\nendow the system with language understanding. (There are other ways of\nunderstanding the structure of the argument. It may be relevant to\nunderstand some of the claims as counterfactual: e.g. “there is\na program” in premise 1 as meaning there could be a program,\netc. On this construal the argument involves modal logic, the logic of\npossibility and necessity (see Damper 2006 and Shaffer 2009)).  \nIt is also worth noting that the first premise above attributes\nunderstanding to “the system”. Exactly what Strong-AI\nsupposes will acquire understanding when the program runs is crucial\nto the success or failure of the CRA. Schank 1978 has a title that\nclaims their group’s computer, a physical device, understands,\nbut in the body of the paper he claims that the program\n[“SAM”] is doing the understanding: SAM, Schank says\n“...understands stories about domains about which it has\nknowledge” (p. 133). As we will see in the next section (4),\nthese issues about the identity of the understander (the cpu? the\nprogram? the system? something else?) quickly came to the fore for\ncritics of the CRA. Searle’s wider argument includes the claim\nthat the thought experiment shows more generally that one cannot get\nsemantics (meaning) from syntax (formal symbol manipulation). That and\nrelated issues are discussed in section 5: The Larger Philosophical\nIssues. \nCriticisms of the narrow Chinese Room argument against Strong AI have\noften followed three main lines, which can be distinguished by how\nmuch they concede: \n(1) Some critics concede that the man in the room doesn’t\nunderstand Chinese, but hold that nevertheless running the program may\ncreate comprehension of Chinese by something other than the room\noperator. These critics object to the inference from the claim that\nthe man in the room does not understand Chinese to the\nconclusion that no understanding has been created. There might\nbe understanding by a larger, smaller, or different, entity. This is\nthe strategy of The Systems Reply and the Virtual Mind Reply. These\nreplies hold that the output of the room might reflect real\nunderstanding of Chinese, but the understanding would not be that of\nthe room operator. Thus Searle’s claim that he doesn’t\nunderstand Chinese while running the room is conceded, but his claim\nthat there is no understanding of the questions in Chinese, and that\ncomputationalism is false, is denied. \n(2) Other critics concede Searle’s claim that just running a\nnatural language processing program as described in the CR scenario\ndoes not create any understanding, whether by a human or a computer\nsystem. But these critics hold that a variation on the\ncomputer system could understand. The variant might be a computer\nembedded in a robotic body, having interaction with the physical world\nvia sensors and motors (“The Robot Reply”), or it might be\na system that simulated the detailed operation of an entire human\nbrain, neuron by neuron (“the Brain Simulator Reply”).\n \n(3) Finally, some critics do not concede even the narrow point against\nAI. These critics hold that the man in the original Chinese Room\nscenario might understand Chinese, despite Searle’s denials, or\nthat the scenario is impossible. For example, critics have argued that\nour intuitions in such cases are unreliable. Other critics have held\nthat it all depends on what one means by “understand”\n– points discussed in the section on The Intuition Reply. Others\n(e.g. Sprevak 2007) object to the assumption that any system (e.g.\nSearle in the room) can run any computer program. And finally some\nhave argued that if it is not reasonable to attribute understanding on\nthe basis of the behavior exhibited by the Chinese Room, then it would\nnot be reasonable to attribute understanding to humans on the basis of\nsimilar behavioral evidence (Searle calls this last the “Other\nMinds Reply”). The objection is that we should be willing to\nattribute understanding in the Chinese Room on the basis of the overt\nbehavior, just as we do with other humans (and some animals), and as\nwe would do with extra-terrestrial Aliens (or burning bushes or\nangels) that spoke our language. This position is close to\nTuring’s own, when he proposed his behavioral test for machine\nintelligence. \nIn addition to these responses specifically to the Chinese Room\nscenario and the narrow argument to be discussed here, some critics\nalso independently argue against Searle’s larger claim, and hold\nthat one can get semantics (that is, meaning) from syntactic symbol\nmanipulation, including the sort that takes place inside a digital\ncomputer, a question discussed in the section below on Syntax and\nSemantics. \nIn the original BBS article, Searle identified and discussed several\nresponses to the argument that he had come across in giving the\nargument in talks at various places. As a result, these early\nresponses have received the most attention in subsequent discussion.\nWhat Searle 1980 calls “perhaps the most common reply” is\nthe Systems Reply. \nThe Systems Reply (which Searle says was originally associated with\nYale, the home of Schank’s AI work) concedes that the man in the\nroom does not understand Chinese. But, the reply continues, the man is\nbut a part, a central processing unit (CPU), in a larger system. The\nlarger system includes the huge database, the memory (scratchpads)\ncontaining intermediate states, and the instructions – the\ncomplete system that is required for answering the Chinese questions.\nSo the Sytems Reply is that while the man running the program does not\nunderstand Chinese, the system as a whole does. \nNed Block was one of the first to press the Systems Reply, along with\nmany others including Jack Copeland, Daniel Dennett, Douglas\nHofstadter, Jerry Fodor, John Haugeland, Ray Kurzweil and Georges Rey.\nRey (1986) says the person in the room is just the CPU of the system.\nKurzweil (2002) says that the human being is just an implementer and\nof no significance (presumably meaning that the properties of the\nimplementer are not necessarily those of the system). Kurzweil hews to\nthe spirit of the Turing Test and holds that if the system displays\nthe apparent capacity to understand Chinese “it would have to,\nindeed, understand Chinese” – Searle is contradicting\nhimself in saying in effect, “the machine speaks Chinese but\ndoesn’t understand Chinese”. \nMargaret Boden (1988) raises levels considerations.\n“Computational psychology does not credit the brain with seeing\nbean-sprouts or understanding English: intentional states such as\nthese are properties of people, not of brains” (244). “In\nshort, Searle’s description of the robot’s pseudo-brain\n(that is, of Searle-in-the-robot) as understanding English involves a\ncategory-mistake comparable to treating the brain as the bearer, as\nopposed to the causal basis, of intelligence”. Boden (1988)\npoints out that the room operator is a conscious agent, while the CPU\nin a computer is not – the Chinese Room scenario asks us to take\nthe perspective of the implementer, and not surprisingly fails to see\nthe larger picture. \nSearle’s response to the Systems Reply is simple: in principle,\nhe could internalize the entire system, memorizing all the\ninstructions and the database, and doing all the calculations in his\nhead. He could then leave the room and wander outdoors, perhaps even\nconversing in Chinese. But he still would have no way to attach\n“any meaning to the formal symbols”. The man would now\nbe the entire system, yet he still would not understand\nChinese. For example, he would not know the meaning of the Chinese\nword for hamburger. He still cannot get semantics from syntax.  \nIn some ways Searle’s response here anticipates later extended\nmind views (e.g. Clark and Chalmers 1998): if Otto, who suffers loss\nof memory, can regain those recall abilities by externalizing some of\nthe information to his notebooks, then Searle arguably can do the\nreverse: by internalizing the instructions and notebooks he should\nacquire any abilities had by the extended system. And so Searle in\neffect concludes that since he doesn’t acquire understanding of\nChinese by internalizing the external components of the entire system\n(e.g. he still doesn’t know what the Chinese word for hamburger\nmeans), understanding was never there in the partially externalized\nsystem of the original Chinese Room. \nIn his 2002 paper “The Chinese Room from a Logical Point of\nView”, Jack Copeland considers Searle’s response to the\nSystems Reply and argues that a homunculus inside Searle’s head\nmight understand even though the room operator himself does not, just\nas modules in minds solve tensor equations that enable us to catch\ncricket balls. Copeland then turns to consider the Chinese Gym, and\nagain appears to endorse the Systems Reply: “…the\nindividual players [do not] understand Chinese. But there is no\nentailment from this to the claim that the simulation as a whole does\nnot come to understand Chinese. The fallacy involved in moving from\npart to whole is even more glaring here than in the original version\nof the Chinese Room Argument”. Copeland denies that\nconnectionism implies that a room of people can simulate the\nbrain. \nJohn Haugeland writes (2002) that Searle’s response to the\nSystems Reply is flawed: “…what he now asks is what it\nwould be like if he, in his own mind, were consciously to implement\nthe underlying formal structures and operations that the theory says\nare sufficient to implement another mind”. According to\nHaugeland, his failure to understand Chinese is irrelevant: he is just\nthe implementer. The larger system implemented would understand\n– there is a level-of-description fallacy. \nShaffer 2009 examines modal aspects of the logic of the CRA and argues\nthat familiar versions of the System Reply are question-begging. But,\nShaffer claims, a modalized version of the System Reply succeeds\nbecause there are possible worlds in which understanding is an\nemergent property of complex syntax manipulation. Nute 2011 is a reply\nto Shaffer. \nStevan Harnad has defended Searle’s argument against Systems\nReply critics in two papers. In his 1989 paper, Harnad writes\n“Searle formulates the problem as follows: Is the mind a\ncomputer program? Or, more specifically, if a computer program\nsimulates or imitates activities of ours that seem to require\nunderstanding (such as communicating in language), can the program\nitself be said to understand in so doing?” (Note the specific\nclaim: the issue is taken to be whether the program itself\nunderstands.) Harnad concludes: “On the face of it, [the CR\nargument] looks valid. It certainly works against the most common\nrejoinder, the ‘Systems Reply’….” Harnad\nappears to follow Searle in linking understanding and states of\nconsciousness: Harnad 2012 (Other Internet Resources) argues that\nSearle shows that the core problem of conscious “feeling”\nrequires sensory connections to the real world. (See sections below\n“The Robot Reply” and “Intentionality” for\ndiscussion.) \nFinally some have argued that even if the room operator memorizes the\nrules and does all the operations inside his head, the room operator\ndoes not become the system. Cole (1984) and Block (1998) both argue\nthat the result would not be identity of Searle with the system but\nmuch more like a case of multiple personality – distinct persons\nin a single head. The Chinese responding system would not be Searle,\nbut a sub-part of him. In the CR case, one person (Searle) is an\nEnglish monoglot and the other is a Chinese monoglot. The\nEnglish-speaking person’s total unawareness of the meaning of\nthe Chinese responses does not show that they are not understood. This\nline, of distinct persons, leads to the Virtual Mind Reply.  \nThe Virtual Mind reply concedes, as does the System Reply, that the\noperator of the Chinese Room does not understand Chinese merely by\nrunning the paper machine. However the Virtual Mind reply holds that\nwhat is important is whether understanding is created, not whether the\nRoom operator is the agent that understands. Unlike the Systems Reply,\nthe Virtual Mind reply (VMR) holds that a running system may create\nnew, virtual, entities that are distinct from both the system as a\nwhole, as well as from the sub-systems such as the CPU or operator. In\nparticular, a running system might create a distinct agent that\nunderstands Chinese. This virtual agent would be distinct from both\nthe room operator and the entire system. The psychological traits,\nincluding linguistic abilities, of any mind created by artificial\nintelligence will depend entirely upon the program and the Chinese\ndatabase, and will not be identical with the psychological traits and\nabilities of a CPU or the operator of a paper machine, such as Searle\nin the Chinese Room scenario. According to the VMR the mistake in the\nChinese Room Argument is to make the claim of strong AI to be\n“the computer understands Chinese” or “the System\nunderstands Chinese”. The claim at issue for AI should simply be\nwhether “the running computer creates understanding of\nChinese”. \nA familiar model of virtual agents are characters in computer or video\ngames, and personal digital assistants, such as Apple’s Siri and\nMicrosoft’s Cortana. These characters have various abilities and\npersonalities, and the characters are not identical with the system\nhardware or program that creates them. A single running system might\ncontrol two distinct agents, or physical robots, simultaneously, one\nof which converses only in Chinese and one of which can converse only\nin English, and which otherwise manifest very different personalities,\nmemories, and cognitive abilities. Thus the VM reply asks us to\ndistinguish between minds and their realizing systems.  \nMinsky (1980) and Sloman and Croucher (1980) suggested a Virtual Mind\nreply when the Chinese Room argument first appeared. In his\nwidely-read 1989 paper “Computation and Consciousness”,\nTim Maudlin considers minimal physical systems that might implement a\ncomputational system running a program. His discussion revolves around\nhis imaginary Olympia machine, a system of buckets that transfers\nwater, implementing a Turing machine. Maudlin’s main target is\nthe computationalists’ claim that such a machine could have\nphenomenal consciousness. However in the course of his discussion,\nMaudlin considers the Chinese Room argument. Maudlin (citing Minsky,\nand Sloman and Croucher) points out a Virtual Mind reply that the\nagent that understands could be distinct from the physical system\n(414). Thus “Searle has done nothing to discount the possibility\nof simultaneously existing disjoint mentalities”\n(414–5). \nPerlis (1992), Chalmers (1996) and Block (2002) have apparently\nendorsed versions of a Virtual Mind reply as well, as has Richard\nHanley in The Metaphysics of Star Trek (1997). Penrose (2002)\nis a critic of this strategy, and Stevan Harnad scornfully dismisses\nsuch heroic resorts to metaphysics. Harnad defended Searle’s\nposition in a “Virtual Symposium on Virtual Minds” (1992)\nagainst Patrick Hayes and Don Perlis. Perlis pressed a virtual minds\nargument derived, he says, from Maudlin. Chalmers (1996) notes that\nthe room operator is just a causal facilitator, a “demon”,\nso that his states of consciousness are irrelevant to the properties\nof the system as a whole. Like Maudlin, Chalmers raises issues of\npersonal identity – we might regard the Chinese Room as\n“two mental systems realized within the same physical space. The\norganization that gives rise to the Chinese experiences is quite\ndistinct from the organization that gives rise to the demon’s [=\nroom operator’s] experiences”(326). \nCole (1991, 1994) develops the reply and argues as follows:\nSearle’s argument requires that the agent of understanding be\nthe computer itself or, in the Chinese Room parallel, the person in\nthe room. However Searle’s failure to understand Chinese in the\nroom does not show that there is no understanding being created. One\nof the key considerations is that in Searle’s discussion the\nactual conversation with the Chinese Room is always seriously under\nspecified. Searle was considering Schank’s programs, which can\nonly respond to a few questions about what happened in a restaurant,\nall in third person. But Searle wishes his conclusions to apply to any\nAI-produced responses, including those that would pass the toughest\nunrestricted Turing Test, i.e. they would be just the sort of\nconversations real people have with each other. If we flesh out the\nconversation in the original CR scenario to include questions in\nChinese such as “How tall are you?”, “Where do you\nlive?”, “What did you have for breakfast?”,\n“What is your attitude toward Mao?”, and so forth, it\nimmediately becomes clear that the answers in Chinese are not\nSearle’s answers. Searle is not the author of the\nanswers, and his beliefs and desires, memories and personality traits\n(apart from his industriousness!) are not reflected in the answers and\nin general Searle’s traits are causally inert in producing the\nanswers to the Chinese questions. This suggests the following\nconditional is true: if there is understanding of Chinese created by\nrunning the program, the mind understanding the Chinese would not be\nthe computer, whether the computer is human or electronic. The person\nunderstanding the Chinese would be a distinct person from the room\noperator, with beliefs and desires bestowed by the program and its\ndatabase. Hence Searle’s failure to understand Chinese while\noperating the room does not show that understanding is not being\ncreated.  \nCole (1991) offers an additional argument that the mind doing the\nunderstanding is neither the mind of the room operator nor the system\nconsisting of the operator and the program: running a suitably\nstructured computer program might produce answers submitted in Chinese\nand also answers to questions submitted in Korean. Yet the Chinese\nanswers might apparently display completely different knowledge and\nmemories, beliefs and desires than the answers to the Korean questions\n– along with a denial that the Chinese answerer knows any\nKorean, and vice versa. Thus the behavioral evidence would be that\nthere were two non-identical minds (one understanding Chinese only,\nand one understanding Korean only). Since these might have mutually\nexclusive properties, they cannot be identical, and ipso facto, cannot\nbe identical with the mind of the implementer in the room.\nAnalogously, a video game might include a character with one set of\ncognitive abilities (smart, understands Chinese) as well as another\ncharacter with an incompatible set (stupid, English monoglot). These\ninconsistent cognitive traits cannot be traits of the XBOX system that\nrealizes them. Cole argues that the implication is that minds\ngenerally are more abstract than the systems that realize them (see\nMind and Body in the Larger Philosophical Issues section). \nIn short, the Virtual Mind argument is that since the evidence that\nSearle provides that there is no understanding of Chinese was that\nhe wouldn’t understand Chinese in the room, the Chinese\nRoom Argument cannot refute a differently formulated equally strong AI\nclaim, asserting the possibility of creating understanding using a\nprogrammed digital computer. Maudlin (1989) says that Searle has not\nadequately responded to this criticism. \nOthers however have replied to the VMR, including Stevan Harnad and\nmathematical physicist Roger Penrose. Penrose is generally sympathetic\nto the points Searle raises with the Chinese Room argument, and has\nargued against the Virtual Mind reply. Penrose does not believe that\ncomputational processes can account for consciousness, both on Chinese\nRoom grounds, as well as because of limitations on formal systems\nrevealed by Kurt Gödel’s incompleteness proof. (Penrose has\ntwo books on mind and consciousness; Chalmers and others have\nresponded to Penrose’s appeals to Gödel.) In his 2002\narticle “Consciousness, Computation, and the Chinese Room”\nthat specifically addresses the Chinese Room argument, Penrose argues\nthat the Chinese Gym variation – with a room expanded to the\nsize of India, with Indians doing the processing – shows it is\nvery implausible to hold there is “some kind of disembodied\n‘understanding’ associated with the person’s\ncarrying out of that algorithm, and whose presence does not impinge in\nany way upon his own consciousness” (230–1). Penrose\nconcludes the Chinese Room argument refutes Strong AI. Christian\nKaernbach (2005) reports that he subjected the virtual mind theory to\nan empirical test, with negative results. \nThe Robot Reply concedes Searle is right about the Chinese Room\nscenario: it shows that a computer trapped in a computer room cannot\nunderstand language, or know what words mean. The Robot reply is\nresponsive to the problem of knowing the meaning of the Chinese word\nfor hamburger – Searle’s example of something the room\noperator would not know. It seems reasonable to hold that most of us\nknow what a hamburger is because we have seen one, and perhaps even\nmade one, or tasted one, or at least heard people talk about\nhamburgers and understood what they are by relating them to things we\ndo know by seeing, making, and tasting. Given this is how one might\ncome to know what hamburgers are, the Robot Reply suggests that we put\na digital computer in a robot body, with sensors, such as video\ncameras and microphones, and add effectors, such as wheels to move\naround with, and arms with which to manipulate things in the world.\nSuch a robot – a computer with a body – might do what a\nchild does, learn by seeing and doing. The Robot Reply holds that such\na digital computer in a robot body, freed from the room, could attach\nmeanings to symbols and actually understand natural language. Margaret\nBoden, Tim Crane, Daniel Dennett, Jerry Fodor, Stevan Harnad, Hans\nMoravec and Georges Rey are among those who have endorsed versions of\nthis reply at one time or another. The Robot Reply in effect appeals\nto “wide content” or “externalist semantics”.\nThis can agree with Searle that syntax and internal connections in\nisolation from the world are insufficient for semantics, while holding\nthat suitable causal connections with the world can provide content to\nthe internal symbols.  \nAbout the time Searle was pressing the CRA, many in philosophy of\nlanguage and mind were recognizing the importance of causal\nconnections to the world as the source of meaning or reference for\nwords and concepts. Hilary Putnam 1981 argued that a Brain in a Vat,\nisolated from the world, might speak or think in a language that\nsounded like English, but it would not be English – hence a\nbrain in a vat could not wonder if it was a brain in a vat (because of\nits sensory isolation, its words “brain” and\n“vat” do not refer to brains or vats). The view that\nmeaning was determined by connections with the world became\nwidespread. Searle resisted this turn outward and continued to think\nof meaning as subjective and connected with consciousness.  \nA related view that minds are best understood as embodied or embedded\nin the world has gained many supporters since the 1990s, contra\nCartesian solipsistic intuitions. Organisms rely on environmental\nfeatures for the success of their behavior. So whether one takes a\nmind to be a symbol processing system, with the symbols getting their\ncontent from sensory connections with the world, or a non-symbolic\nsystem that succeeds by being embedded in a particular environment,\nthe important of things outside the head have come to the fore. Hence\nmany are sympathetic to some form of the Robot Reply: a computational\nsystem might understand, provided it is acting in the world. E.g\nCarter 2007 in a textbook on philosophy and AI concludes “The\nlesson to draw from the Chinese Room thought experiment is that\nembodied experience is necessary for the development of\nsemantics.”  \nHowever Searle does not think that the Robot Reply to the Chinese Room\nargument is any stronger than the Systems Reply. All the sensors can\ndo is provide additional input to the computer – and it will be\njust syntactic input. We can see this by making a parallel change to\nthe Chinese Room scenario. Suppose the man in the Chinese Room\nreceives, in addition to the Chinese characters slipped under the\ndoor, a stream of binary digits that appear, say, on a ticker tape in\na corner of the room. The instruction books are augmented to use the\nnumerals from the tape as input, along with the Chinese characters.\nUnbeknownst to the man in the room, the symbols on the tape are the\ndigitized output of a video camera (and possibly other sensors).\nSearle argues that additional syntactic inputs will do nothing to\nallow the man to associate meanings with the Chinese characters. It is\njust more work for the man in the room. \nJerry Fodor, Hilary Putnam, and David Lewis, were principle architects\nof the computational theory of mind that Searle’s wider argument\nattacks. In his original 1980 reply to Searle, Fodor allows Searle is\ncertainly right that “instantiating the same program as the\nbrain does is not, in and of itself, sufficient for having those\npropositional attitudes characteristic of the organism that has the\nbrain.” But Fodor holds that Searle is wrong about the robot\nreply. A computer might have propositional attitudes if it has the\nright causal connections to the world – but those are not ones\nmediated by a man sitting in the head of the robot. We don’t\nknow what the right causal connections are. Searle commits the fallacy\nof inferring from “the little man is not the right causal\nconnection” to conclude that no causal linkage would succeed.\nThere is considerable empirical evidence that mental processes involve\n“manipulation of symbols”; Searle gives us no alternative\nexplanation (this is sometimes called Fodor’s “Only Game\nin Town” argument for computational approaches). In the 1980s\nand 1990s Fodor wrote extensively on what the connections must be\nbetween a brain state and the world for the state to have intentional\n(representational) properties, while also emphasizing that\ncomputationalism has limits because the computations are intrinsically\nlocal and so cannot account for abductive reasoning. \nIn a later piece, “Yin and Yang in the Chinese Room” (in\nRosenthal 1991 pp.524–525), Fodor substantially revises his 1980\nview. He distances himself from his earlier version of the robot\nreply, and holds instead that “instantiation” should be\ndefined in such a way that the symbol must be the proximate cause of\nthe effect – no intervening guys in a room. So Searle in the\nroom is not an instantiation of a Turing Machine, and\n“Searle’s setup does not instantiate the machine that the\nbrain instantiates.” He concludes: “…Searle’s\nsetup is irrelevant to the claim that strong equivalence to a Chinese\nspeaker’s brain is ipso facto sufficient for speaking\nChinese.” Searle says of Fodor’s move, “Of all the\nzillions of criticisms of the Chinese Room argument, Fodor’s is\nperhaps the most desperate. He claims that precisely because the man\nin the Chinese room sets out to implement the steps in the computer\nprogram, he is not implementing the steps in the computer program. He\noffers no argument for this extraordinary claim.” (in Rosenthal\n1991, p. 525) \nIn a 1986 paper, Georges Rey advocated a combination of the system and\nrobot reply, after noting that the original Turing Test is\ninsufficient as a test of intelligence and understanding, and that the\nisolated system Searle describes in the room is certainly not\nfunctionally equivalent to a real Chinese speaker sensing and acting\nin the world. In a 2002 second look, “Searle’s\nMisunderstandings of Functionalism and Strong AI”, Rey again\ndefends functionalism against Searle, and in the particular form Rey\ncalls the “computational-representational theory of thought\n– CRTT”. CRTT is not committed to attributing thought to\njust any system that passes the Turing Test (like the Chinese Room).\nNor is it committed to a conversation manual model of understanding\nnatural language. Rather, CRTT is concerned with intentionality,\nnatural and artificial (the representations in the system are\nsemantically evaluable – they are true or false, hence have\naboutness). Searle saddles functionalism with the\n“blackbox” character of behaviorism, but functionalism\ncares how things are done. Rey sketches “a modest mind”\n– a CRTT system that has perception, can make deductive and\ninductive inferences, makes decisions on basis of goals and\nrepresentations of how the world is, and can process natural language\nby converting to and from its native representations. To explain the\nbehavior of such a system we would need to use the same attributions\nneeded to explain the behavior of a normal Chinese speaker. \nIf we flesh out the Chinese conversation in the context of the Robot\nReply, we may again see evidence that the entity that understands is\nnot the operator inside the room. Suppose we ask the robot system\nChinese translations of “what do you see?”, we might get\nthe answer “My old friend Shakey”, or “I see\nyou!”. Whereas if we phone Searle in the room and ask the same\nquestions in English we might get “These same four walls”\nor “these damn endless instruction books and notebooks.”\nAgain this is evidence that we have distinct responders here, an\nEnglish speaker and a Chinese speaker, who see and do quite different\nthings. If the giant robot goes on a rampage and smashes much of\nTokyo, and all the while oblivious Searle is just following the\nprogram in his notebooks in the room, Searle is not guilty of homicide\nand mayhem, because he is not the agent committing the acts.  \nTim Crane discusses the Chinese Room argument in his 1991 book,\nThe Mechanical Mind. He cites the Churchlands’ luminous\nroom analogy, but then goes on to argue that in the course of\noperating the room, Searle would learn the meaning of the Chinese:\n“…if Searle had not just memorized the rules and the\ndata, but also started acting in the world of Chinese people, then it\nis plausible that he would before too long come to realize what these\nsymbols mean.”(127). (Rapaport 2006 presses an analogy between\nHelen Keller and the Chinese Room.) Crane appears to end with a\nversion of the Robot Reply: “Searle’s argument itself begs\nthe question by (in effect) just denying the central thesis of AI\n– that thinking is formal symbol manipulation. But\nSearle’s assumption, none the less, seems to me correct …\nthe proper response to Searle’s argument is: sure,\nSearle-in-the-room, or the room alone, cannot understand Chinese. But\nif you let the outside world have some impact on the room, meaning or\n‘semantics’ might begin to get a foothold. But of course,\nthis concedes that thinking cannot be simply symbol\nmanipulation.” (129) The idea that learning grounds\nunderstanding has led to work in developmental robotics (a.k.a.\nepigenetic robotics). This AI research area seeks to replicate key\nhuman learning abilities, such as robots that are shown an object from\nseveral angles while being told in natural language the name of the\nobject.  \nMargaret Boden 1988 also argues that Searle mistakenly supposes\nprograms are pure syntax. But programs bring about the activity of\ncertain machines: “The inherent procedural consequences of any\ncomputer program give it a toehold in semantics, where the semantics\nin question is not denotational, but causal.” (250) Thus a robot\nmight have causal powers that enable it to refer to a hamburger. \nStevan Harnad also finds important our sensory and motor capabilities:\n“Who is to say that the Turing Test, whether conducted in\nChinese or in any other language, could be successfully passed without\noperations that draw on our sensory, motor, and other higher cognitive\ncapacities as well? Where does the capacity to comprehend Chinese\nbegin and the rest of our mental competence leave off?” Harnad\nbelieves that symbolic functions must be grounded in\n“robotic” functions that connect a system with the world.\nAnd he thinks this counts against symbolic accounts of mentality, such\nas Jerry Fodor’s, and, one suspects, the approach of Roger\nSchank that was Searle’s original target. Harnad 2012 (Other\nInternet Resources) argues that the CRA shows that even with a robot\nwith symbols grounded in the external world, there is still something\nmissing: feeling, such as the feeling of understanding. \nHowever Ziemke 2016 argues a robotic embodiment with layered systems\nof bodily regulation may ground emotion and meaning, and Seligman 2019\nargues that “perceptually grounded” approaches to natural\nlanguage processing (NLP) have the “potential to display\nintentionality, and thus after all to foster a truly meaningful\nsemantics that, in the view of Searle and other skeptics, is\nintrinsically beyond computers’ capacity.”  \nConsider a computer that operates in quite a different manner than the\nusual AI program with scripts and operations on sentence-like strings\nof symbols. The Brain Simulator reply asks us to suppose instead the\nprogram simulates the actual sequence of nerve firings that occur in\nthe brain of a native Chinese language speaker when that person\nunderstands Chinese – every nerve, every firing. Since the\ncomputer then works the very same way as the brain of a native Chinese\nspeaker, processing information in just the same way, it will\nunderstand Chinese. Paul and Patricia Churchland have set out a reply\nalong these lines, discussed below.  \nIn response to this, Searle argues that it makes no difference. He\nsuggests a variation on the brain simulator scenario: suppose that in\nthe room the man has a huge set of valves and water pipes, in the same\narrangement as the neurons in a native Chinese speaker’s brain.\nThe program now tells the man which valves to open in response to\ninput. Searle claims that it is obvious that there would be no\nunderstanding of Chinese. (Note however that the basis for this claim\nis no longer simply that Searle himself wouldn’t understand\nChinese – it seems clear that now he is just facilitating the\ncausal operation of the system and so we rely on our Leibnizian\nintuition that water-works don’t understand (see also Maudlin\n1989).) Searle concludes that a simulation of brain activity is not\nthe real thing.  \nHowever, following Pylyshyn 1980, Cole and Foelber 1984, Chalmers\n1996, we might wonder about hybrid systems. Pylyshyn writes:  \nThese cyborgization thought experiments can be linked to the Chinese\nRoom. Suppose Otto has a neural disease that causes one of the neurons\nin my brain to fail, but surgeons install a tiny remotely controlled\nartificial neuron, a synron, along side his disabled neuron. The\ncontrol of Otto’s neuron is by John Searle in the Chinese Room,\nunbeknownst to both Searle and Otto. Tiny wires connect the artificial\nneuron to the synapses on the cell-body of his disabled neuron. When\nhis artificial neuron is stimulated by neurons that synapse on his\ndisabled neuron, a light goes on in the Chinese Room. Searle then\nmanipulates some valves and switches in accord with a program. That,\nvia the radio link, causes Otto’s artificial neuron to release\nneuro-transmitters from its tiny artificial vesicles. If\nSearle’s programmed activity causes Otto’s artificial\nneuron to behave just as his disabled natural neuron once did, the\nbehavior of the rest of his nervous system will be unchanged. Alas,\nOtto’s disease progresses; more neurons are replaced by synrons\ncontrolled by Searle. Ex hypothesi the rest of the world will not\nnotice the difference; will Otto?  If so, when? And why? \nUnder the rubric “The Combination Reply”, Searle also\nconsiders a system with the features of all three of the preceding: a\nrobot with a digital brain simulating computer in its cranium, such\nthat the system as a whole behaves indistinguishably from a human.\nSince the normal input to the brain is from sense organs, it is\nnatural to suppose that most advocates of the Brain Simulator Reply\nhave in mind such a combination of brain simulation, Robot, and\nSystems Reply. Some (e.g. Rey 1986) argue it is reasonable to\nattribute intentionality to such a system as a whole. Searle agrees\nthat it would indeed be reasonable to attribute understanding to such\nan android system – but only as long as you don’t know how\nit works. As soon as you know the truth – it is a computer,\nuncomprehendingly manipulating symbols on the basis of syntax, not\nmeaning – you would cease to attribute intentionality to it. \n(One assumes this would be true even if it were one’s spouse,\nwith whom one had built a life-long relationship, that was revealed to\nhide a silicon secret. Science fiction stories, including episodes of\nRod Serling’s television series The Twilight Zone, have\nbeen based on such possibilities (the face of the beloved peels away\nto reveal the awful android truth); however, Steven Pinker (1997)\nmentions one episode in which the android’s secret was known\nfrom the start, but the protagonist developed a romantic relationship\nwith the android.) \nOn its tenth anniversary the Chinese Room argument was featured in the\ngeneral science periodical Scientific American. Leading the\nopposition to Searle’s lead article in that issue were\nphilosophers Paul and Patricia Churchland. The Churchlands agree with\nSearle that the Chinese Room does not understand Chinese, but hold\nthat the argument itself exploits our ignorance of cognitive and\nsemantic phenomena. They raise a parallel case of “The Luminous\nRoom” where someone waves a magnet and argues that the absence\nof resulting visible light shows that Maxwell’s electromagnetic\ntheory is false. The Churchlands advocate a view of the brain as a\nconnectionist system, a vector transformer, not a system manipulating\nsymbols according to structure-sensitive rules. The system in the\nChinese Room uses the wrong computational strategies. Thus they agree\nwith Searle against traditional AI, but they presumably would endorse\nwhat Searle calls “the Brain Simulator Reply”, arguing\nthat, as with the Luminous Room, our intuitions fail us when\nconsidering such a complex system, and it is a fallacy to move from\npart to whole: “… no neuron in my brain understands\nEnglish, although my whole brain does.” \nIn his 1991 book, Microcognition. Andy Clark holds that\nSearle is right that a computer running Schank’s program does\nnot know anything about restaurants, “at least if by\n‘know’ we mean anything like\n‘understand’”. But Searle thinks that this would\napply to any computational model, while Clark, like the Churchlands,\nholds that Searle is wrong about connectionist models. Clark’s\ninterest is thus in the brain-simulator reply. The brain thinks in\nvirtue of its physical properties. What physical properties of the\nbrain are important? Clark answers that what is important about brains\nare “variable and flexible substructures” which\nconventional AI systems lack. But that doesn’t mean\ncomputationalism or functionalism is false. It depends on what level\nyou take the functional units to be. Clark defends\n“microfunctionalism” – one should look to a\nfine-grained functional description, e.g. neural net level. Clark\ncites William Lycan approvingly contra Block’s absent qualia\nobjection – yes, there can be absent qualia, if the functional\nunits are made large. But that does not constitute a refutation of\nfunctionalism generally. So Clark’s views are not unlike the\nChurchlands’, conceding that Searle is right about Schank and\nsymbolic-level processing systems, but holding that he is mistaken\nabout connectionist systems. \nSimilarly Ray Kurzweil (2002) argues that Searle’s argument\ncould be turned around to show that human brains cannot understand\n– the brain succeeds by manipulating neurotransmitter\nconcentrations and other mechanisms that are in themselves\nmeaningless. In criticism of Searle’s response to the Brain\nSimulator Reply, Kurzweil says: “So if we scale up\nSearle’s Chinese Room to be the rather massive\n‘room’ it needs to be, who’s to say that the entire\nsystem of a hundred trillion people simulating a Chinese Brain that\nknows Chinese isn’t conscious? Certainly, it would be correct to\nsay that such a system knows Chinese. And we can’t say that it\nis not conscious anymore than we can say that about any other process.\nWe can’t know the subjective experience of another\nentity….”  \nRelated to the preceding is The Other Minds Reply: “How do you\nknow that other people understand Chinese or anything else? Only by\ntheir behavior. Now the computer can pass the behavioral tests as well\nas they can (in principle), so if you are going to attribute cognition\nto other people you must in principle also attribute it to\ncomputers.” \nSearle’s (1980) reply to this is very short: \nCritics hold that if the evidence we have that humans understand is\nthe same as the evidence we might have that a visiting\nextra-terrestrial alien understands, which is the same as the evidence\nthat a robot understands, the presuppositions we may make in the case\nof our own species are not relevant, for presuppositions are sometimes\nfalse. For similar reasons, Turing, in proposing the Turing Test, is\nspecifically worried about our presuppositions and chauvinism. If the\nreasons for the presuppositions regarding humans are pragmatic, in\nthat they enable us to predict the behavior of humans and to interact\neffectively with them, perhaps the presupposition could apply equally\nto computers (similar considerations are pressed by Dennett, in his\ndiscussions of what he calls the Intentional Stance). \nSearle raises the question of just what we are attributing in\nattributing understanding to other minds, saying that it is more than\ncomplex behavioral dispositions. For Searle the additional seems to be\ncertain states of consciousness, as is seen in his 2010 summary of the\nCRA conclusions. Terry Horgan (2013) endorses this claim: “the\nreal moral of Searle’s Chinese room thought experiment is that\ngenuine original intentionality requires the presence of internal\nstates with intrinsic phenomenal character that is inherently\nintentional…” But this tying of understanding to\nphenomenal consciousness raises a host of issues.  \nWe attribute limited understanding of language to toddlers, dogs, and\nother animals, but it is not clear that we are ipso facto attributing\nunseen states of subjective consciousness – what do we know of\nthe hidden states of exotic creatures? Ludwig Wittgenstein (the\nPrivate Language Argument) and his followers pressed similar points.\nAltered qualia possibilities, analogous to the inverted spectrum,\narise: suppose I ask “what’s the sum of 5 and 7” and\nyou respond “the sum of 5 and 7 is 12”, but as you heard\nmy question you had the conscious experience of hearing and\nunderstanding “what is the sum of 10 and 14”, though you\nwere in the computational states appropriate for producing the correct\nsum and so said “12”. Are there certain conscious states\nthat are “correct” for certain functional states?\nWittgenstein’s considerations appear to be that the subjective\nstate is irrelevant, at best epiphenomenal, if a language user\ndisplays appropriate linguistic behavior. Afterall, we are taught\nlanguage on the basis of our overt responses, not our qualia. The\nmathematical savant Daniel Tammet reports that when he generates the\ndecimal expansion of pi to thousands of digits he experiences colors\nthat reveal the next digit, but even here it may be that\nTennant’s performance is likely not produced by the colors he\nexperiences, but rather by unconscious neural computation. The\npossible importance of subjective states is further considered in the\nsection on Intentionality, below.  \nIn the 30 years since the CRA there has been philosophical interest in\nzombies – creatures that look like and behave just as normal\nhumans, including linguistic behavior, yet have no subjective\nconsciousness. A difficulty for claiming that subjective states of\nconsciousness are crucial for understanding meaning will arise in\nthese cases of absent qualia: we can’t tell the difference\nbetween zombies and non-zombies, and so on Searle’s account we\ncan’t tell the difference between those that really understand\nEnglish and those that don’t. And if you and I can’t tell\nthe difference between those who understand language and Zombies who\nbehave like they do but don’t really, than neither can any\nselection factor in the history of human evolution – to\npredators, prey, and mates, zombies and true understanders, with the\n“right” conscious experience, have been indistinguishable.\nBut then there appears to be a distinction without a difference. In\nany case, Searle’s short reply to the Other Minds Reply may be\ntoo short.  \nDescartes famously argued that speech was sufficient for attributing\nminds and consciousness to others, and infamously argued that it was\nnecessary. Turing was in effect endorsing Descartes’ sufficiency\ncondition, at least for intelligence, while substituting written for\noral linguistic behavior. Since most of us use dialog as a sufficient\ncondition for attributing understanding, Searle’s argument,\nwhich holds that speech is a sufficient condition for attributing\nunderstanding to humans but not for anything that doesn’t share\nour biology, an account would appear to be required of what\nadditionally is being attributed, and what can justify the additional\nattribution. Further, if being con-specific is key on Searle’s\naccount, a natural question arises as to what circumstances would\njustify us in attributing understanding (or consciousness) to\nextra-terrestrial aliens who do not share our biology? Offending\nET’s by withholding attributions of understanding until after\ndoing a post-mortem may be risky. \nHans Moravec, director of the Robotics laboratory at Carnegie Mellon\nUniversity, and author of Robot: Mere Machine to Transcendent\nMind, argues that Searle’s position merely reflects\nintuitions from traditional philosophy of mind that are out of step\nwith the new cognitive science. Moravec endorses a version of the\nOther Minds reply. It makes sense to attribute intentionality to\nmachines for the same reasons it makes sense to attribute them to\nhumans; his “interpretative position” is similar to the\nviews of Daniel Dennett. Moravec goes on to note that one of the\nthings we attribute to others is the ability to make attributions of\nintentionality, and then we make such attributions to ourselves. It is\nsuch self-representation that is at the heart of consciousness. These\ncapacities appear to be implementation independent, and hence possible\nfor aliens and suitably programmed computers. \nAs we have seen, the reason that Searle thinks we can disregard the\nevidence in the case of robots and computers is that we know that\ntheir processing is syntactic, and this fact trumps all other\nconsiderations. Indeed, Searle believes this is the larger point that\nthe Chinese Room merely illustrates. This larger point is addressed in\nthe Syntax and Semantics section below. \nMany responses to the Chinese Room argument have noted that, as with\nLeibniz’ Mill, the argument appears to be based on intuition:\nthe intuition that a computer (or the man in the room) cannot think or\nhave understanding. For example, Ned Block (1980) in his original BBS\ncommentary says “Searle’s argument depends for its force\non intuitions that certain entities do not think.” But, Block\nargues, (1) intuitions sometimes can and should be trumped and (2)\nperhaps we need to bring our concept of understanding in line with a\nreality in which certain computer robots belong to the same natural\nkind as humans. Similarly Margaret Boden (1988) points out that we\ncan’t trust our untutored intuitions about how mind depends on\nmatter; developments in science may change our intuitions. Indeed,\nelimination of bias in our intuitions was precisely what motivated\nTuring (1950) to propose the Turing Test, a test that was blind to the\nphysical character of the system replying to questions. Some of\nSearle’s critics in effect argue that he has merely pushed the\nreliance on intuition back, into the room. \nFor example, one can hold that despite Searle’s intuition that\nhe would not understand Chinese while in the room, perhaps he is\nmistaken and does, albeit unconsciously. Hauser (2002) accuses Searle\nof Cartesian bias in his inference from “it seems to me quite\nobvious that I understand nothing” to the conclusion that I\nreally understand nothing. Normally, if one understands English or\nChinese, one knows that one does – but not necessarily. Searle\nlacks the normal introspective awareness of understanding – but\nthis, while abnormal, is not conclusive.  \nCritics of the CRA note that our intuitions about intelligence,\nunderstanding and meaning may all be unreliable. With regard to\nmeaning, Wakefield 2003, following Block 1998, defends what Wakefield\ncalls “the essentialist objection” to the CRA, namely that\na computational account of meaning is not analysis of ordinary\nconcepts and their related intuitions. Rather we are building a\nscientific theory of meaning that may require revising our intuitions.\nAs a theory, it gets its evidence from its explanatory power, not its\naccord with pre-theoretic intuitions (however Wakefield himself argues\nthat computational accounts of meaning are afflicted by a pernicious\nindeterminacy (pp. 308ff)).  \nOther critics focusing on the role of intuitions in the CRA argue that\nour intuitions regarding both intelligence and understanding may also\nbe unreliable, and perhaps incompatible even with current science.\nWith regard to understanding, Steven Pinker, in How the Mind\nWorks (1997), holds that “… Searle is merely\nexploring facts about the English word understand….\nPeople are reluctant to use the word unless certain stereotypical\nconditions apply…” But, Pinker claims, nothing\nscientifically speaking is at stake. Pinker objects to Searle’s\nappeal to the “causal powers of the brain” by noting that\nthe apparent locus of the causal powers is the “patterns of\ninterconnectivity that carry out the right information\nprocessing”. Pinker ends his discussion by citing a science\nfiction story in which Aliens, anatomically quite unlike humans,\ncannot believe that humans think when they discover that our heads are\nfilled with meat. The Aliens’ intuitions are unreliable –\npresumably ours may be so as well. \nClearly the CRA turns on what is required to understand language.\nSchank 1978 clarifies his claim about what he thinks his programs can\ndo: “By ‘understand’, we mean SAM [one of his\nprograms] can create a linked causal chain of conceptualizations that\nrepresent what took place in each story.” This is a nuanced\nunderstanding of “understanding”, whereas the Chinese Room\nthought experiment does not turn on a technical understanding of\n“understanding”, but rather intuitions about our ordinary\ncompetence when we understand a word like “hamburger”.\nIndeed by 2015 Schank distances himself from weak senses of\n“understand”, holding that no computer can\n“understand when you tell it something”, and that\nIBM’s WATSON “doesn’t know what it is saying”.\nSchank’s program may get links right, but arguably does not know\nwhat the linked entities are. Whether it does or not depends on what\nconcepts are, see section 5.1. Furthermore it is possible that when it\ncomes to attributing understanding of language we have different\nstandards for different things – more relaxed for dogs and\ntoddlers. Some things understand a language “un poco”.\nSearle (1980)concedes that there are degrees of understanding, but\nsays that all that matters that there are clear cases of no\nunderstanding, and AI programs are an example: “The computer\nunderstanding is not just (like my understanding of German) partial or\nincomplete; it is zero.”  \nSome defenders of AI are also concerned with how our understanding of\nunderstanding bears on the Chinese Room argument. In their paper\n“A Chinese Room that Understands” AI researchers Simon and\nEisenstadt (2002) argue that whereas Searle refutes “logical\nstrong AI”, the thesis that a program that passes the Turing\nTest will necessarily understand, Searle’s argument\ndoes not impugn “Empirical Strong AI” – the thesis\nthat it is possible to program a computer that convincingly satisfies\nordinary criteria of understanding. They hold however that it is\nimpossible to settle these questions “without employing a\ndefinition of the term ‘understand’ that can provide a\ntest for judging whether the hypothesis is true or false”. They\ncite W.V.O. Quine’s Word and Object as showing that\nthere is always empirical uncertainty in attributing understanding to\nhumans. The Chinese Room is a Clever Hans trick (Clever Hans was a\nhorse who appeared to clomp out the answers to simple arithmetic\nquestions, but it was discovered that Hans could detect unconscious\ncues from his trainer). Similarly, the man in the room doesn’t\nunderstand Chinese, and could be exposed by watching him closely.\n(Simon and Eisenstadt do not explain just how this would be done, or\nhow it would affect the argument.) Citing the work of Rudolf Carnap,\nSimon and Eisenstadt argue that to understand is not just to exhibit\ncertain behavior, but to use “intensions” that determine\nextensions, and that one can see in actual programs that they do use\nappropriate intensions. They discuss three actual AI programs, and\ndefend various attributions of mentality to them, including\nunderstanding, and conclude that computers understand; they learn\n“intensions by associating words and other linguistic structure\nwith their denotations, as detected through sensory stimuli”.\nAnd since we can see exactly how the machines work, “it is, in\nfact, easier to establish that a machine exhibits understanding that\nto establish that a human exhibits understanding….” Thus,\nthey conclude, the evidence for empirical strong AI is\noverwhelming. \nSimilarly, Daniel Dennett in his original 1980 response to\nSearle’s argument called it “an intuition pump”, a\nterm he came up with in discussing the CRA with Hofstader. Sharvy 1983\nechoes the complaint. Dennett’s considered view (2013) is that\nthe CRA is “clearly a fallacious and misleading argument\n….” (p. 320). Paul Thagard (2013) proposes that for every\nthought experiment in philosophy there is an equal and opposite\nthought experiment. Thagard holds that intuitions are unreliable, and\nthe CRA is an example (and that in fact the CRA has now been refuted\nby the technology of autonomous robotic cars). Dennett has elaborated\non concerns about our intuitions regarding intelligence. Dennett 1987\n(“Fast Thinking”) expressed concerns about the slow speed\nat which the Chinese Room would operate, and he has been joined by\nseveral other commentators, including Tim Maudlin, David Chalmers, and\nSteven Pinker. The operator of the Chinese Room may eventually produce\nappropriate answers to Chinese questions. But slow thinkers are\nstupid, not intelligent – and in the wild, they may well end up\ndead. Dennett argues that “speed … is ‘of the\nessence’ for intelligence. If you can’t figure out the\nrelevant portions of the changing environment fast enough to fend for\nyourself, you are not practically intelligent, however complex you\nare” (326). Thus Dennett relativizes intelligence to processing\nspeed relative to current environment.  \nTim Maudlin (1989) disagrees. Maudlin considers the time-scale problem\npointed to by other writers, and concludes, contra Dennett, that the\nextreme slowness of a computational system does not violate any\nnecessary conditions on thinking or consciousness. Furthermore,\nSearle’s main claim is about understanding, not intelligence or\nbeing quick-witted. If we were to encounter extra-terrestrials that\ncould process information a thousand times more quickly than we do, it\nseems that would show nothing about our own slow-poke ability to\nunderstand the languages we speak. \nSteven Pinker (1997) also holds that Searle relies on untutored\nintuitions. Pinker endorses the Churchlands’ (1990)\ncounterexample of an analogous thought experiment of waving a magnet\nand not generating light, noting that this outcome would not disprove\nMaxwell’s theory that light consists of electromagnetic waves.\nPinker holds that the key issue is speed: “The thought\nexperiment slows down the waves to a range to which we humans no\nlonger see them as light. By trusting our intuitions in the thought\nexperiment, we falsely conclude that rapid waves cannot be light\neither. Similarly, Searle has slowed down the mental computations to a\nrange in which we humans no longer think of it as understanding (since\nunderstanding is ordinarily much faster)” (94–95). Howard\nGardiner, a supporter of Searle’s conclusions regarding the\nroom, makes a similar point about understanding. Gardiner addresses\nthe Chinese Room argument in his book The Mind’s New\nScience (1985, 171–177). Gardiner considers all the\nstandard replies to the Chinese Room argument and concludes that\nSearle is correct about the room: “…the word understand\nhas been unduly stretched in the case of the Chinese room\n….” (175). \nThus several in this group of critics argue that speed affects our\nwillingness to attribute intelligence and understanding to a slow\nsystem, such as that in the Chinese Room. The result may simply be\nthat our intuitions regarding the Chinese Room are unreliable, and\nthus the man in the room, in implementing the program, may understand\nChinese despite intuitions to the contrary (Maudlin and Pinker). Or it\nmay be that the slowness marks a crucial difference between the\nsimulation in the room and what a fast computer does, such that the\nman is not intelligent while the computer system is (Dennett). \nSearle believes the Chinese Room argument supports a larger point,\nwhich explains the failure of the Chinese Room to produce\nunderstanding. Searle argued that programs implemented by computers\nare just syntactical. Computer operations are “formal” in\nthat they respond only to the physical form of the strings of symbols,\nnot to the meaning of the symbols. Minds on the other hand have states\nwith meaning, mental contents. We associate meanings with the words or\nsigns in language. We respond to signs because of their meaning, not\njust their physical appearance. In short, we understand. But, and\naccording to Searle this is the key point, “Syntax is not by\nitself sufficient for, nor constitutive of, semantics.” So\nalthough computers may be able to manipulate syntax to produce\nappropriate responses to natural language input, they do not\nunderstand the sentences they receive or output, for they cannot\nassociate meanings with the words. \nSearle (1984) presents a three premise argument that because syntax is\nnot sufficient for semantics, programs cannot produce minds. \nThe Chinese Room thought experiment itself is the support for the\nthird premise. The claim that syntactic manipulation is not sufficient\nfor meaning or thought is a significant issue, with wider implications\nthan AI, or attributions of understanding. Prominent theories of mind\nhold that human cognition generally is computational. In one form, it\nis held that thought involves operations on symbols in virtue of their\nphysical properties. On an alternative connectionist account, the\ncomputations are on “subsymbolic” states. If Searle is\nright, not only Strong AI but also these main approaches to\nunderstanding human cognition are misguided. \nAs we have seen, Searle holds that the Chinese Room scenario shows\nthat one cannot get semantics from syntax alone. In a symbolic logic\nsystem, a kind of artificial language, rules are given for syntax. A\nsemantics, if any, comes later. The logician specifies the basic\nsymbol set and some rules for manipulating strings to produce new\nones. These rules are purely syntactic – they are applied to\nstrings of symbols solely in virtue of their syntax or form. A\nsemantics, if any, for the symbol system must be provided separately.\nAnd if one wishes to show that interesting additional relationships\nhold between the syntactic operations and semantics, such as that the\nsymbol manipulations preserve truth, one must provide sometimes\ncomplex meta-proofs to show this. So on the face of it, semantics is\nquite independent of syntax for artificial languages, and one cannot\nget semantics from syntax alone. “Formal symbols by themselves\ncan never be enough for mental contents, because the symbols, by\ndefinition, have no meaning (or interpretation, or semantics) except\ninsofar as someone outside the system gives it to them” (Searle\n1989, 45). \nSearle’s identification of meaning with interpretation in this\npassage is important. Searle’s point is clearly true of the\ncausally inert formal systems of logicians. A semantic interpretation\nhas to be given to those symbols by a logician. When we move from\nformal systems to computational systems, the situation is more\ncomplex. As many of Searle’s critics (e.g. Cole 1984, Dennett\n1987, Boden 1988, and Chalmers 1996) have noted, a computer running a\nprogram is not the same as “syntax alone”. A computer is\nan enormously complex electronic causal system. State changes in the\nsystem are physical. One can interpret the physical states,\ne.g. voltages, as syntactic 1’s and 0’s, but the intrinsic\nreality is electronic and the syntax is “derived”, a\nproduct of interpretation. The states are syntactically specified by\nprogrammers, but when implemented in a running machine they are\nelectronic states of a complex causal system embedded in the real\nworld. This is quite different from the abstract formal systems that\nlogicians study. Dennett notes that no “computer program by\nitself” (Searle’s language) – e.g. a program lying\non a shelf – can cause anything, even simple addition, let alone\nmental states. The program must be running. Chalmers (1996) offers a\nparody in which it is reasoned that recipes are syntactic, syntax is\nnot sufficient for crumbliness, cakes are crumbly, so implementation\nof a recipe is not sufficient for making a cake. Implementation makes\nall the difference; an abstract entity (recipe, program) determines\nthe causal powers of a physical system embedded in the larger causal\nnexus of the world.  \nDennett (1987) sums up the issue: “Searle’s view, then,\ncomes to this: take a material object (any material object) that does\nnot have the power of causing mental phenomena; you cannot turn it in\nto an object that does have the power of producing mental phenomena\nsimply by programming it – reorganizing the conditional\ndependencies of transitions between its states.” Dennett’s\nview is the opposite: programming “is precisely what could give\nsomething a mind”. But Dennett claims that in fact it is\n“empirically unlikely that the right sorts of programs can be\nrun on anything but organic, human brains” (325–6). \nA further related complication is that it is not clear that computers\nperform syntactic operations in quite the same sense that a human does\n– it is not clear that a computer understands syntax or\nsyntactic operations. A computer does not know that it is manipulating\n1’s and 0’s. A computer does not recognize that its binary\ndata strings have a certain form, and thus that certain syntactic\nrules may be applied to them, unlike the man inside the Chinese Room.\nInside a computer, there is nothing that literally reads input data,\nor that “knows” what symbols are. Instead, there are\nmillions of transistors that change states. A sequence of voltages\ncauses operations to be performed. We humans may choose to interpret\nthese voltages as binary numerals and the voltage changes as syntactic\noperations, but a computer does not interpret its operations as\nsyntactic or any other way. So perhaps a computer does not need to\nmake the move from syntax to semantics that Searle objects to; it\nneeds to move from complex causal connections to semantics.\nFurthermore, perhaps any causal system is describable as\nperforming syntactic operations – if we interpret a light square\nas logical “0” and a dark square as logical\n“1”, then a kitchen toaster may be described as a\ndevice that rewrites logical “0”s as logical\n“1”s. But there is no philosophical problem about getting\nfrom syntax to breakfast.  \nIn the 1990s, Searle began to use considerations related to these to\nargue that computational views are not just false, but lack a clear\nsense. Computation, or syntax, is “observer-relative”, not\nan intrinsic feature of reality: “…you can assign a\ncomputational interpretation to anything” (Searle 2002b, p. 17),\neven the molecules in the paint on the wall. Since nothing is\nintrinsically computational, one cannot have a scientific theory that\nreduces the mental, which is not observer-relative, to computation,\nwhich is. “Computation exists only relative to some agent or\nobserver who imposes a computational interpretation on some\nphenomenon. This is an obvious point. I should have seen it ten years\nago, but I did not.” (Searle 2002b, p.17, originally published\n1993). \nCritics note that walls are not computers; unlike a wall, a computer\ngoes through state-transitions that are counterfactually described by\na program (Chalmers 1996, Block 2002, Haugeland 2002). In his 2002\npaper, Block addresses the question of whether a wall is a computer\n(in reply to Searle’s charge that anything that maps onto a\nformal system is a formal system, whereas minds are quite different).\nBlock denies that whether or not something is a computer depends\nentirely on our interpretation. Block notes that Searle ignores the\ncounterfactuals that must be true of an implementing system. Haugeland\n(2002) makes the similar point that an implementation will be a causal\nprocess that reliably carries out the operations – and they must\nbe the right causal powers. Block concludes that Searle’s\narguments fail, but he concedes that they “do succeed in\nsharpening our understanding of the nature of intentionality and its\nrelation to computation and representation” (78). \nRey (2002) also addresses Searle’s arguments that syntax and\nsymbols are observer-relative properties, not physical. Searle infers\nthis from the fact that syntactic properties (e.g. being a logical\n“1”)are not defined in physics; however Rey holds that it\ndoes not follow that they are observer-relative. Rey argues that\nSearle also misunderstands what it is to realize a program. Rey\nendorses Chalmers’ reply to Putnam: a realization is not just a\nstructural mapping, but involves causation, supporting\ncounterfactuals. “This point is missed so often, it bears\nrepeating: the syntactically specifiable objects over which\ncomputations are defined can and standardly do possess a semantics;\nit’s just that the semantics is not involved in the\nspecification.” States of a person have their semantics in\nvirtue of computational organization and their causal relations to the\nworld. Rey concludes: Searle “simply does not consider the\nsubstantial resources of functionalism and Strong AI.” (222) A\nplausibly detailed story would defuse negative conclusions drawn from\nthe superficial sketch of the system in the Chinese Room. \nJohn Haugeland (2002) argues that there is a sense in which a\nprocessor must intrinsically understand the commands in the programs\nit runs: it executes them in accord with the specifications.\n“The only way that we can make sense of a computer as executing\na program is by understanding its processor as responding to the\nprogram prescriptions as meaningful” (385). Thus operation\nsymbols have meaning to a system. Haugeland goes on to draw a\ndistinction between narrow and wide system. He argues that data can\nhave semantics in the wide system that includes representations of\nexternal objects produced by transducers. In passing, Haugeland makes\nthe unusual claim, argued for elsewhere, that genuine intelligence and\nsemantics presuppose “the capacity for a kind of commitment in\nhow one lives” which is non-propositional – that is, love\n(cp. Steven Spielberg’s 2001 film Artificial Intelligence:\nAI).  \nTo Searle’s claim that syntax is observer-relative, that the\nmolecules in a wall might be interpreted as implementing the Wordstar\nprogram (an early word processing program) because “there is\nsome pattern in the molecule movements which is isomorphic with the\nformal structure of Wordstar” (Searle 1990b, p. 27), Haugeland\ncounters that “the very idea of a complex syntactical token\n… presupposes specified processes of writing and\nreading….” The tokens must be systematically producible\nand retrievable. So no random isomorphism or pattern somewhere (e.g.\non some wall) is going to count, and hence syntax is not\nobserver-relative. \nWith regard to the question of whether one can get semantics from\nsyntax, William Rapaport has for many years argued for\n“syntactic semantics”, a view in which understanding is a\nspecial form of syntactic structure in which symbols (such as Chinese\nwords) are linked to concepts, themselves represented syntactically.\nOthers believe we are not there yet. AI futurist (The Age of\nSpiritual Machines) Ray Kurzweil holds in a 2002 follow-up book\nthat it is red herring to focus on traditional symbol-manipulating\ncomputers. Kurzweil agrees with Searle that existent computers do not\nunderstand language – as evidenced by the fact that they\ncan’t engage in convincing dialog. But that failure does not\nbear on the capacity of future computers based on different\ntechnology. Kurzweil claims that Searle fails to understand that\nfuture machines will use “chaotic emergent methods that are\nmassively parallel”. This claim appears to be similar to that of\nconnectionists, such as Andy Clark, and the position taken by the\nChurchlands in their 1990 Scientific American article. \nApart from Haugeland’s claim that processors understand program\ninstructions, Searle’s critics can agree that computers no more\nunderstand syntax than they understand semantics, although, like all\ncausal engines, a computer has syntactic descriptions. And while it is\noften useful to programmers to treat the machine as if it performed\nsyntactic operations, it is not always so: sometimes the characters\nprogrammers use are just switches that make the machine do something,\nfor example, make a given pixel on the computer display turn red, or\nmake a car transmission shift gears. Thus it is not clear that Searle\nis correct when he says a digital computer is just “a device\nwhich manipulates symbols”. Computers are complex causal\nengines, and syntactic descriptions are useful in order to structure\nthe causal interconnections in the machine. AI programmers face many\ntough problems, but one can hold that they do not have to get\nsemantics from syntax. If they are to get semantics, they must get it\nfrom causality. \nTwo main approaches have developed that explain meaning in terms of\ncausal connections. The internalist approaches, such as Schank’s\nand Rapaport’s conceptual representation approaches, and also\nConceptual Role Semantics, hold that a state of a physical system gets\nits semantics from causal connections to other states of the same\nsystem. Thus a state of a computer might represent “kiwi”\nbecause it is connected to “bird” and\n“flightless” nodes, and perhaps also to images of\nprototypical kiwis. The state that represents the property of being\n“flightless” might get its content from a\nNegation-operator modifying a representation of “capable of\nairborne self-propulsion”, and so forth, to form a vast\nconnected conceptual network, a kind of mental dictionary. \n\n Externalist\n approaches developed by Dennis Stampe, Fred Dretske, Hilary Putnam,\nJerry Fodor, Ruth Millikan, and others, hold that states of a physical\nsystem get their content through causal connections to the external\nreality they represent. Thus, roughly, a system with a KIWI concept is\none that has a state it uses to represent the presence of kiwis in the\nexternal environment. This kiwi-representing state can be any state\nthat is appropriately causally connected to the presence of kiwis.\nDepending on the system, the kiwi representing state could be a state\nof a brain, or of an electrical device such as a computer, or even of\na hydraulic system. The internal representing state can then in turn\nplay a causal role in the determining the behavior of the system. For\nexample, Rey (1986) endorses an indicator semantics along the lines of\nthe work of Dennis Stampe (1977) and Fodor’s\nPsychosemantics. These semantic theories that locate content\nor meaning in appropriate causal relations to the world fit well with\nthe Robot Reply. A computer in a robot body might have just the causal\nconnections that could allow its inner syntactic states to have the\nsemantic property of representing states of things in its\nenvironment. \nThus there are at least two families of theories (and marriages of the\ntwo, as in Block 1986) about how semantics might depend upon causal\nconnections. Both of these attempt to provide accounts that are\nsubstance neutral: states of suitably organized causal systems can\nhave content, no matter what the systems are made of. On these\ntheories a computer could have states that have meaning. It is not\nnecessary that the computer be aware of its own states and know that\nthey have meaning, nor that any outsider appreciate the meaning of the\nstates. On either of these accounts meaning depends upon the (possibly\ncomplex) causal connections, and digital computers are systems\ndesigned to have states that have just such complex causal\ndependencies. It should be noted that Searle does not subscribe to\nthese theories of semantics. Instead, Searle’s discussions of\nlinguistic meaning have often centered on the notion of\nintentionality. \n\n Intentionality\n is the property of being about something, having content. In the 19th\nCentury, psychologist Franz Brentano re-introduced this term from\nMedieval philosophy and held that intentionality was the “mark\nof the mental”. Beliefs and desires are intentional states: they\nhave propositional content (one believes that p, one desires\nthat p, where sentences that represent propositions substitute\nfor “p”). Searle’s views regarding\nintentionality are complex; of relevance here is that he makes a\ndistinction between the original or intrinsic intentionality of\ngenuine mental states, and the derived intentionality of language. A\nwritten or spoken sentence only has derivative intentionality insofar\nas it is interpreted by someone. It appears that on Searle’s\nview, original intentionality can at least potentially be conscious.\nSearle then argues that the distinction between original and derived\nintentionality applies to computers. We can interpret the states of a\ncomputer as having content, but the states themselves do not have\noriginal intentionality. Many philosophers endorse this intentionality\ndualism, including Sayre (1986) and even Fodor (2009), despite\nFodor’s many differences with Searle. \nIn a section of her 1988 book, Computer Models of the Mind,\nMargaret Boden notes that intentionality is not well-understood\n– reason to not put too much weight on arguments that turn on\nintentionality. Furthermore, insofar as we understand the brain, we\nfocus on informational functions, not unspecified causal powers of the\nbrain: “…from the psychological point of view, it is not\nthe biochemistry as such which matters but the information-bearing\nfunctions grounded in it.” (241) Searle sees intentionality as a\ncausal power of the brain, uniquely produced by biological processes.\nDale Jacquette 1989 argues against a reduction of intentionality\n– intentionality, he says, is an “ineliminable,\nirreducible primitive concept.” However most AI sympathizers\nhave seen intentionality, aboutness, as bound up with information, and\nnon-biological states can bear information as well as can brain\nstates. Hence many responders to Searle have argued that he displays\nsubstance chauvinism, in holding that brains understand but systems\nmade of silicon with comparable information processing capabilities\ncannot, even in principle. Papers on both sides of the issue appeared,\nsuch as J. Maloney’s 1987 paper “The Right Stuff”,\ndefending Searle, and R. Sharvy’s 1983 critique, “It\nAin’t the Meat, it’s the Motion”. AI proponents such\nas Kurzweil (1999, see also Richards 2002) have continued to hold that\nAI systems can potentially have such mental properties as\nunderstanding, intelligence, consciousness and intentionality, and\nwill exceed human abilities in these areas. \nOther critics of Searle’s position take intentionality more\nseriously than Boden does, but deny his dualistic distinction between\noriginal and derived intentionality. Dennett (1987, e.g.) argues that\nall intentionality is derived, in that attributions of intentionality\n– to animals, other people, and even ourselves – are\ninstrumental and allow us to predict behavior, but they are not\ndescriptions of intrinsic properties. As we have seen, Dennett is\nconcerned about the slow speed of things in the Chinese Room, but he\nargues that once a system is working up to speed, it has all that is\nneeded for intelligence and derived intentionality – and derived\nintentionality is the only kind that there is, according to Dennett. A\nmachine can be an intentional system because intentional explanations\nwork in predicting the machine’s behavior. Dennett also suggests\nthat Searle conflates intentionality with awareness of intentionality.\nIn his syntax-semantic arguments, “Searle has apparently\nconfused a claim about the underivability of semantics from syntax\nwith a claim about the underivability of the consciousness of\nsemantics from syntax” (336). The emphasis on consciousness\nforces us to think about things from a first-person point of view, but\nDennett 2017 continues to press the claim that this is a fundamental\nmistake if we want to understand the mental.  \nWe might also worry that Searle conflates meaning and interpretation,\nand that Searle’s original or underived intentionality is just\nsecond-order intentionality, a representation of what an intentional\nobject represents or means. Dretske and others have seen\nintentionality as information-based. One state of the world, including\na state in a computer, may carry information about other states in the\nworld, and this informational aboutness is a mind-independent feature\nof states. Hence it is a mistake to hold that conscious attributions\nof meaning are the source of intentionality.  \nOthers have noted that Searle’s discussion has shown a shift\nover time from issues of intentionality and understanding to issues of\nconsciousness. Searle links intentionality to awareness of\nintentionality, in holding that intentional states are at least\npotentially conscious. In his 1996 book, The Conscious Mind,\nDavid Chalmers notes that although Searle originally directs his\nargument against machine intentionality, it is clear from later\nwritings that the real issue is consciousness, which Searle holds is a\nnecessary condition of intentionality. It is consciousness that is\nlacking in digital computers. Chalmers uses thought experiments to\nargue that it is implausible that one system has some basic mental\nproperty (such as having qualia) that another system lacks, if it is\npossible to imagine transforming one system into the other, either\ngradually (as replacing neurons one at a time by digital circuits), or\nall at once, switching back and forth between flesh and silicon. \nA second strategy regarding the attribution of intentionality is taken\nby critics who in effect argue that intentionality is an intrinsic\nfeature of states of physical systems that are causally connected with\nthe world in the right way, independently of interpretation (see the\npreceding Syntax and Semantics section). Fodor’s semantic\nexternalism is influenced by Fred Dretske, but they come to different\nconclusions with regard to the semantics of states of computers. Over\na period of years, Dretske developed an historical account of meaning\nor mental content that would preclude attributing beliefs and\nunderstanding to most machines. Dretske (1985) agrees with Searle that\nadding machines don’t literally add; we do the adding,\nusing the machines. Dretske emphasizes the crucial role of natural\nselection and learning in producing states that have genuine content.\nHuman built systems will be, at best, like Swampmen (beings that\nresult from a lightning strike in a swamp and by chance happen to be a\nmolecule by molecule copy of some human being, say, you) – they\nappear to have intentionality or mental states, but do not, because\nsuch states require the right history. AI states will generally be\ncounterfeits of real mental states; like counterfeit money, they may\nappear perfectly identical but lack the right pedigree. But\nDretske’s account of belief appears to make it distinct from\nconscious awareness of the belief or intentional state (if that is\ntaken to require a higher order thought), and so would apparently\nallow attribution of intentionality to artificial systems that can get\nthe right history by learning. \nHoward Gardiner endorses Zenon Pylyshyn’s criticisms of\nSearle’s view of the relation of brain and intentionality, as\nsupposing that intentionality is somehow a stuff “secreted by\nthe brain”, and Pylyshyn’s own counter-thought experiment\nin which one’s neurons are replaced one by one with integrated\ncircuit workalikes (see also Cole and Foelber (1984) and Chalmers\n(1996) for exploration of neuron replacement scenarios). Gardiner\nholds that Searle owes us a more precise account of intentionality\nthan Searle has given so far, and until then it is an open question\nwhether AI can produce it, or whether it is beyond its scope. Gardiner\nconcludes with the possibility that the dispute between Searle and his\ncritics is not scientific, but (quasi?) religious. \nSeveral critics have noted that there are metaphysical issues at stake\nin the original argument. The Systems Reply draws attention to the\nmetaphysical problem of the relation of mind to body. It does this in\nholding that understanding is a property of the system as a whole, not\nthe physical implementer. The Virtual Mind Reply holds that minds or\npersons – the entities that understand and are conscious –\nare more abstract than any physical system, and that there could be a\nmany-to-one relation between minds and physical systems. (Even if\neverything is physical, in principle a single body could be shared by\nmultiple minds, and a single mind could have a sequence of bodies over\ntime.) Thus larger issues about personal identity and the relation of\nmind and body are in play in the debate between Searle and some of his\ncritics. \nSearle’s view is that the problem the relation of mind and body\n“has a rather simple solution. Here it is: Conscious states are\ncaused by lower level neurobiological processes in the brain and are\nthemselves higher level features of the brain” (Searle 2002b, p.\n9). In his early discussion of the CRA, Searle spoke of the causal\npowers of the brain. Thus his view appears to be that brain states\ncause consciousness and understanding, and “consciousness is\njust a feature of the brain” (ibid). However, as we have seen,\neven if this is true it begs the question of just whose consciousness\na brain creates. Roger Sperry’s split-brain experiments suggest\nthat perhaps there can be two centers of consciousness, and so in that\nsense two minds, implemented by a single brain. While both display at\nleast some language comprehension, only one (typically created by the\nleft hemisphere) controls language production. Thus many current\napproaches to understanding the relation of brain and consciousness\nemphasize connectedness and information flow (see e.g. Dehaene 2014).\n \nConsciousness and understanding are features of persons, so it appears\nthat Searle accepts a metaphysics in which I, my conscious self, am\nidentical with my brain – a form of mind-brain identity theory.\nThis very concrete metaphysics is reflected in Searle’s original\npresentation of the CR argument, in which Strong AI was described by\nhim as the claim that “the appropriately programmed computer\nreally is a mind” (Searle 1980). This is an identity claim, and\nhas odd consequences. If A and B are identical, any property of A is a\nproperty of B. Computers are physical objects. Some computers weigh 6\nlbs and have stereo speakers. So the claim that Searle called Strong\nAI would entail that some minds weigh 6 lbs and have stereo speakers.\nHowever it seems to be clear that while humans may weigh 150 pounds;\nhuman minds do not weigh 150 pounds. This suggests that neither bodies\nnor machines can literally be minds. Such considerations support the\nview that minds are more abstract that brains, and if so that at least\none version of the claim that Searle calls Strong AI, the version that\nsays that computers literally are minds, is metaphysically untenable\non the face of it, apart from any thought-experiments. \nSearle’s CR argument was thus directed against the claim that a\ncomputer is a mind, that a suitably programmed digital computer\nunderstands language, or that its program does. Searle’s thought\nexperiment appeals to our strong intuition that someone who did\nexactly what the computer does would not thereby come to understand\nChinese. As noted above, many critics have held that Searle is quite\nright on this point – no matter how you program a computer, the\ncomputer will not literally be a mind and the computer will not\nunderstand natural language. But if minds are not physical objects\nthis inability of a computer to be a mind does not show that running\nan AI program cannot produce understanding of natural\nlanguage, by something other than the computer (See section 4.1\nabove.)  \n\n Functionalism\n is a theory of the relation of minds to bodies that was developed in\nthe two decades prior to Searle’s CRA. Functionalism is an\nalternative to the identity theory that is implicit in much of\nSearle’s discussion, as well as to the dominant behaviorism of\nthe mid-Twentieth Century. If functionalism is correct, there appears\nto be no intrinsic reason why a computer couldn’t have mental\nstates. Hence the CRA’s conclusion that a computer is\nintrinsically incapable of mental states is an important consideration\nagainst functionalism. Julian Baggini (2009, 37) writes that Searle\n“came up with perhaps the most famous counter-example in history\n– the Chinese room argument – and in one intellectual\npunch inflicted so much damage on the then dominant theory of\nfunctionalism that many would argue it has never recovered.” \nFunctionalists hold that a mental state is what a mental\nstate does – the causal (or “functional”)\nrole that the state plays determines what state it is. A functionalist\nmight hold that pain, for example, is a state that is typically caused\nby damage to the body, is located in a body-image, and is aversive.\nFunctionalists distance themselves both from behaviorists and identity\ntheorists. In contrast with the former, functionalists hold that the\ninternal causal processes are important for the possession of\nmental states. Thus functionalists may agree with Searle in rejecting\nthe Turing Test as too behavioristic. In contrast with identity\ntheorists (who might e.g. hold “pain is identical with C-fiber\nfiring”), functionalists hold that mental states might be had by\na variety of physical systems (or non-physical, as in Cole and Foelber\n1984, in which a mind changes from a material to an immaterial\nimplementation, neuron by neuron). Thus while an identity theorist\nwill identify pain with certain neuron firings, a functionalist will\nidentify pain with something more abstract and higher level, a\nfunctional role that might be had by many different types of\nunderlying system.  \nFunctionalists accuse identity theorists of substance chauvinism.\nHowever, functionalism remains controversial: functionalism is\nvulnerable to the Chinese Nation type objections discussed above, and\nfunctionalists notoriously have trouble explaining qualia, a problem\nhighlighted by the apparent possibility of an inverted spectrum, where\nqualitatively different states might have the same functional role\n(e.g. Block 1978, Maudlin 1989, Cole 1990). \n\n Computationalism\n is the sub-species of functionalism that holds that the important\ncausal role of brain processes is information processing. Milkowski\n2017 notes that computational approaches have been fruitful in\ncognitive science; he surveys objections to computationalism and\nconcludes that the majority target a strawman version. However Jerry\nFodor, an early proponent of computational approaches, argues in Fodor\n2005 that key mental processes, such as inference to the best\nexplanation, which depend on non-local properties of representations,\ncannot be explained by computational modules in the brain. If Fodor is\nright, understanding language and interpretation appear to involve\nglobal considerations such as linguistic and non-linguistic context\nand theory of mind and so might resist computational explanation. If\nso, we reach Searle’s conclusion on the basis of different\nconsiderations.  \nSearle’s 2010 statement of the conclusion of the CRA has it\nshowing that computational accounts cannot explain consciousness.\nThere has been considerable interest in the decades since 1980 in\ndetermining what does explain consciousness, and this has been an\nextremely active research area across disciplines. One interest has\nbeen in the neural correlates of consciousness. This bears directly on\nSearle’s claim that consciousness is intrinsically biological\nand not computational or information processing. There is no\ndefinitive answer yet, though some recent work on anesthesia suggests\nthat consciousness is lost when cortical (and cortico-thalamic)\nconnections and information flow are disrupted (e.g.Hudetz 2012, a\nreview article).  \nIn general, if the basis of consciousness is confirmed to be at the\nrelatively abstract level of information flow through neural networks,\nit will be friendly to functionalism, and if it is turns out to be\nlower and more biological (or sub-neuronal), it will be friendly to\nSearle’s account. \nThese controversial biological and metaphysical issues bear on the\ncentral inference in the Chinese Room argument. From the intuition\nthat in the CR thought experiment he would not understand Chinese by\nrunning a program, Searle infers that there is no understanding\ncreated by running a program. Clearly, whether that inference is valid\nor not turns on a metaphysical question about the identity of persons\nand minds. If the person understanding is not identical with the room\noperator, then the inference is unsound. \nIn discussing the CRA, Searle argues that there is an important\ndistinction between simulation and duplication. No one would mistake a\ncomputer simulation of the weather for weather, or a computer\nsimulation of digestion for real digestion. Searle concludes that it\nis just as serious a mistake to confuse a computer simulation of\nunderstanding with understanding. \nOn the face of it, there is generally an important distinction between\na simulation and the real thing. But two problems emerge. It is not\nclear that the distinction can always be made. Hearts are biological\nif anything is. Are artificial hearts simulations of hearts? Or are\nthey functional duplicates of hearts, hearts made from different\nmaterials? Walking is normally a biological phenomenon performed using\nlimbs. Do those with artificial limbs walk? Or do they simulate\nwalking? Do robots walk? If the properties that are needed to be\ncertain kind of thing are high-level properties, anything sharing\nthose properties will be a thing of that kind, even if it differs in\nits lower level properties. Chalmers (1996) offers a principle\ngoverning when simulation is replication. Chalmers suggests that,\ncontra Searle and Harnad (1989), a simulation of X can be an\nX, namely when the property of being an X is an\norganizational invariant, a property that depends only on the\nfunctional organization of the underlying system, and not on any other\ndetails. \nCopeland (2002) argues that the Church-Turing thesis does not entail\nthat the brain (or every machine) can be simulated by a universal\nTuring machine, for the brain (or other machine) might have primitive\noperations that are not simple clerical routines that can be carried\nout by hand. (An example might be that human brains likely display\ngenuine low-level randomness, whereas computers are carefully designed\nnot to do that, and so computers resort to pseudo-random numbers when\napparent randomness is needed.) Sprevak 2007 raises a related point.\nTuring’s 1938 Princeton thesis described such machines\n(“O-machines”). O-machines are machines that include\nfunctions of natural numbers that are not Turing-machine computable.\nIf the brain is such a machine, then, says Sprevak,: “There is\nno possibility of Searle’s Chinese Room Argument being\nsuccessfully deployed against the functionalist hypothesis that the\nbrain instantiates an O-machine….” (120).  \nCopeland discusses the simulation / duplication distinction in\nconnection with the Brain Simulator Reply. He argues that Searle\ncorrectly notes that one cannot infer from X simulates\nY, and Y has property P, to the conclusion\nthat therefore X has Y’s property P\nfor arbitrary P. But Copeland claims that Searle himself\ncommits the simulation fallacy in extending the CR argument from\ntraditional AI to apply against computationalism. The contrapositive\nof the inference is logically equivalent – X simulates\nY, X does not have P therefore Y\ndoes not – where P is understands Chinese. The faulty\nstep is: the CR operator S simulates a neural net N,\nit is not the case that S understands Chinese, therefore it\nis not the case that N understands Chinese. Copeland also\nnotes results by Siegelmann and Sontag (1994) showing that some\nconnectionist networks cannot be simulated by a universal Turing\nMachine (in particular, where connection weights are real\nnumbers). \nThere is another problem with the simulation-duplication distinction,\narising from the process of evolution. Searle wishes to see original\nintentionality and genuine understanding as properties only of certain\nbiological systems, presumably the product of evolution. Computers\nmerely simulate these properties. At the same time, in the Chinese\nRoom scenario, Searle maintains that a system can exhibit behavior\njust as complex as human behavior, simulating any degree of\nintelligence and language comprehension that one can imagine, and\nsimulating any ability to deal with the world, yet not understand a\nthing. He also says that such behaviorally complex systems might be\nimplemented with very ordinary materials, for example with tubes of\nwater and valves. \nThis creates a biological problem, beyond the Other Minds problem\nnoted by early critics of the CR argument. While we may\npresuppose that others have minds, evolution makes no such\npresuppositions. The selection forces that drive biological evolution\nselect on the basis of behavior. Evolution can select for the ability\nto use information about the environment creatively and intelligently,\nas long as this is manifest in the behavior of the organism. If there\nis no overt difference in behavior in any set of circumstances between\na system that understands and one that does not, evolution cannot\nselect for genuine understanding. And so it seems that on\nSearle’s account, minds that genuinely understand meaning have\nno advantage over creatures that merely process information, using\npurely computational processes. Thus a position that implies that\nsimulations of understanding can be just as biologically adaptive as\nthe real thing, leaves us with a puzzle about how and why systems with\n“genuine” understanding could evolve. Original\nintentionality and genuine understanding become epiphenomenal. \nAs we have seen, since its appearance in 1980 the Chinese Room\nargument has sparked discussion across disciplines. Despite the\nextensive discussion there is still no consensus as to whether the\nargument is sound. At one end we have Julian Baggini’s (2009)\nassessment that Searle “came up with perhaps the most famous\ncounter-example in history – the Chinese room argument –\nand in one intellectual punch inflicted so much damage on the then\ndominant theory of functionalism that many would argue it has never\nrecovered.” Whereas philosopher Daniel Dennett (2013, p. 320)\nconcludes that the Chinese Room argument is “clearly a\nfallacious and misleading argument”. Hence there is no consensus\nas to whether the argument is a proof that limits the aspirations of\nArtificial Intelligence or computational accounts of mind.  \nMeanwhile work in artificial intelligence and natural language\nprocessing has continued. The CRA led Stevan Harnad and others on a\nquest for “symbol grounding” in AI. Many in philosophy\n(Dretske, Fodor, Millikan) worked on naturalistic theories of mental\ncontent. Speculation about the nature of consciousness continues in\nmany disciplines. And computers have moved from the lab to the pocket\nand the wrist. \nAt the time of Searle’s construction of the argument, personal\ncomputers were very limited hobbyist devices. Weizenbaum’s\n‘Eliza’ and a few text ‘adventure’ games were\nplayed on DEC computers; these included limited parsers. More advanced\nparsing of language was limited to computer researchers such as\nSchank. Much changed in the next quarter century; billions now use\nnatural language to interrogate and command virtual agents via\ncomputers they carry in their pockets. Has the Chinese Room argument\nmoderated claims by those who produce AI and natural language systems?\nSome manufacturers linking devices to the “internet of\nthings” make modest claims: appliance manufacturer LG says the\nsecond decade of the 21st century brings the “experience of\nconversing” with major appliances. That may or may not be the\nsame as conversing. Apple is less cautious than LG in describing the\ncapabilities of its “virtual personal assistant”\napplication called ‘Siri’: Apple says of Siri that\n“It understands what you say. It knows what you mean.” IBM\nis quick to claim its much larger ‘Watson’ system is\nsuperior in language abilities to Siri. In 2011 Watson beat human\nchampions on the television game show ‘Jeopardy’, a feat\nthat relies heavily on language abilities and inference. IBM goes on\nto claim that what distinguishes Watson is that it “knows what\nit knows, and knows what it does not know.” This appears to be\nclaiming a form of reflexive self-awareness or consciousness for the\nWatson computer system. Thus the claims of strong AI now are hardly\nchastened, and if anything some are stronger and more exuberant. At\nthe same time, as we have seen, many others believe that the Chinese\nRoom Argument showed once and for all that at best computers can\nsimulate human cognition. \nThough separated by three centuries, Leibniz and Searle had similar\nintuitions about the systems they consider in their respective thought\nexperiments, Leibniz’ Mill and the Chinese Room. In both cases\nthey consider a complex system composed of relatively simple\noperations, and note that it is impossible to see how understanding or\nconsciousness could result. These simple arguments do us the service\nof highlighting the serious problems we face in understanding meaning\nand minds. The many issues raised by the Chinese Room argument may not\nbe settled until there is a consensus about the nature of meaning, its\nrelation to syntax, and about the biological basis of consciousness.\nThere continues to be significant disagreement about what processes\ncreate meaning, understanding, and consciousness, as well as what can\nbe proven a priori by thought experiments.","contact.mail":"dcole@d.umn.edu","contact.domain":"d.umn.edu"}]
