[{"date.published":"1997-09-04","date.changed":"2019-04-02","url":"https://plato.stanford.edu/entries/prisoner-dilemma/","author1":"Steven Kuhn","author1.info":"http://explore.georgetown.edu/people/kuhns/?PageTemplateID=189","entry":"prisoner-dilemma","body.text":"\n\n\nTanya and Cinque have been arrested for robbing the Hibernia Savings\nBank and placed in separate isolation cells. Both care much more about\ntheir personal freedom than about the welfare of their accomplice. A\nclever prosecutor makes the following offer to each: “You may\nchoose to confess or remain silent. If you confess and your accomplice\nremains silent I will drop all charges against you and use your\ntestimony to ensure that your accomplice does serious time. Likewise,\nif your accomplice confesses while you remain silent, they will go\nfree while you do the time. If you both confess I get two convictions,\nbut I'll see to it that you both get early parole. If you both remain\nsilent, I'll have to settle for token sentences on firearms possession\ncharges. If you wish to confess, you must leave a note with the jailer\nbefore my return tomorrow morning.”\n\n\nThe “dilemma” faced by the prisoners here is that,\nwhatever the other does, each is better off confessing than remaining\nsilent. But the outcome obtained when both confess is worse for each\nthan the outcome they would have obtained had both remained silent. A\ncommon view is that the puzzle illustrates a conflict between\nindividual and group rationality. A group whose members pursue\nrational self-interest may all end up worse off than a group whose\nmembers act contrary to rational self-interest. More generally, if the\npayoffs are not assumed to represent self-interest, a group whose\nmembers rationally pursue any goals may all meet less success than if\nthey had not rationally pursued their goals individually. A closely\nrelated view is that the prisoner's dilemma game and its multi-player\ngeneralizations model familiar situations in which it is difficult to\nget rational, selfish agents to cooperate for their common good. Much\nof the contemporary literature has focused on identifying conditions\nunder which players would or should make the “cooperative”\nmove corresponding to remaining silent. A slightly different\ninterpretation takes the game to represent a choice between selfish\nbehavior and socially desirable altruism. The move corresponding to\nconfession benefits the actor, no matter what the other does, while\nthe move corresponding to silence benefits the other player no matter\nwhat that other player does. Benefiting oneself is not always wrong,\nof course, and benefiting others at the expense of oneself is not\nalways morally required, but in the prisoner's dilemma game both\nplayers prefer the outcome with the altruistic moves to that with the\nselfish moves. This observation has led David Gauthier and others to\ntake the prisoner's dilemma to say something important about the\nnature of morality.\n\n\nHere is another story. Bill has a blue cap and would prefer a red one,\nwhile Rose has a red cap and would prefer a blue one. Both prefer two\ncaps to any one and either of the caps to no cap at all. They are each\ngiven a choice between keeping the cap they have or giving it to the\nother. This “exchange game” has the same structure as the\nstory about the prisoners. Whether Rose keeps her cap or gives to\nBill, Bill is better off keeping his and she is better off if he gives\nit to her. Whether Bill keeps his cap or gives it to Rose, Rose is\nbetter off keeping hers and he is better off if she gives it to him.\nBut both are better off if they exchange caps than if they both keep\nwhat they have. The new story suggests that the prisoner's dilemma\nalso occupies a place at the heart of our economic system. It would\nseem that any market designed to facilitate mutually beneficial\nexchanges will need to overcome the dilemma or avoid it. \n\n\nPuzzles with the structure of the prisoner's dilemma were discussed by\nMerrill Flood and Melvin Dresher in 1950, as part of the Rand\nCorporation's investigations into game theory (which Rand pursued\nbecause of possible applications to global nuclear strategy). The\ntitle “prisoner's dilemma” and the version with prison\nsentences as payoffs are due to Albert Tucker, who wanted to make\nFlood and Dresher's ideas more accessible to an audience of Stanford\npsychologists. More recently, it has been suggested (Peterson, p1)\nthat Tucker may have been discussing the work of his famous graduate\nstudent John Nash, and Nash 1950 (p. 291) does indeed contain a game\nwith the structure of the prisoner's dilemma as the second in a series\nof six examples illustrating his technical ideas. Although Flood and\nDresher (and Nash) didn't themselves rush to publicize their ideas in\nexternal journal articles, the puzzle has since attracted widespread\nand increasing attention in a variety of disciplines. Donninger\nreports that “more than a thousand articles” about it were\npublished in the sixties and seventies. A Google Scholar\nsearch for “prisoner's dilemma” in 2018 returns 49,600\nresults.\n\n\nThe sections below provide a variety of more precise characterizations\nof the prisoner's dilemma, beginning with the narrowest, and survey\nsome connections with similar games and some applications in\nphilosophy and elsewhere. Particular attention is paid to iterated and\nevolutionary versions of the game. In the fomer, the prisoner's\ndilemma game is played repeatedly, opening the possibility that a\nplayer can use its current move to reward or punish the other's play\nin previous moves in order to induce cooperative play in the future.\nIn the latter, members of a population play one another repeatedly in\nprisoner's dilemma games and those who get higher payoffs\n“reproduce” more rapidly than those who get lower payoffs.\n‘Prisoner's dilemma’ is abbreviated as\n‘PD’.\n\nIn its simplest form the PD is a game described by the payoff\nmatrix: \nsatisfying the following chain of inequalities: \nThere are two players, Row and Column. Each has two possible moves,\n“cooperate” (\\(\\bC\\)) or “defect” (\\(\\bD\\)),\ncorresponding, respectively, to the options of remaining silent or\nconfessing in the illustrative anecdote above. For each possible pair\nof moves, the payoffs to Row and Column (in that order) are listed in\nthe appropriate cell. \\(R\\) is the “reward” payoff that\neach player receives if both cooperate. \\(P\\) is the\n“punishment” that each receives if both defect. \\(T\\) is\nthe “temptation” that each receives as sole defector and\n\\(S\\) is the “sucker” payoff that each receives as sole\ncooperator. We assume here that the game is symmetric, i.e., that the\nreward, punishment, temptation and sucker payoffs are the same for\neach player, and payoffs have only ordinal significance, i.e., they\nindicate whether one payoff is better than another, but tell us\nnothing about how much better. It is now easy to see that we have the\nstructure of a dilemma like the one in the story. Suppose Column\ncooperates. Then Row gets \\(R\\) for cooperating and \\(T\\) for\ndefecting, and so is better off defecting. Suppose Column defects.\nThen Row gets \\(S\\) for cooperating and \\(P\\) for defecting, and so is\nagain better off defecting. The move \\(\\bD\\) for Row is said to\nstrictly dominate the move \\(\\bC\\): whatever Column does, Row\nis better off choosing \\(\\bD\\) than \\(\\bC\\). By symmetry \\(\\bD\\) also\nstrictly dominates \\(\\bC\\) for Column. Thus two “rational”\nplayers will defect and receive a payoff of \\(P\\), while two\n“irrational” players can cooperate and receive greater\npayoff \\(R\\). In standard treatments, game theory assumes rationality\nand common knowledge. Each player is rational, knows the other is\nrational, knows that the other knows he is rational, etc. Each player\nalso knows how the other values the outcomes. But since \\(\\bD\\)\nstrictly dominates \\(\\bC\\) for both players, the argument for dilemma\nhere requires only that each player knows his own payoffs. (The\nargument remains valid, of course, under the stronger standard\nassumptions.) It is also worth noting that the outcome \\((\\bD, \\bD)\\)\nof both players defecting is the game's only strict nash equilibrium,\ni.e., it is the only outcome from which each player could only do\nworse by unilaterally changing its move. Flood and Dresher's interest\nin their dilemma seems to have stemmed from their view that it\nprovided a counterexample to the claim that the nash equilibria of a\ngame constitute its natural “solutions”. \nIf there can be “ties” in rankings of the payoffs,\ncondition PD1 can be weakened without destroying the nature of the\ndilemma. For suppose that one of the following conditions obtains: \nThen, for each player, although \\(\\bD\\) does not strictly dominate\n\\(\\bC\\), it still weakly dominates in the sense that each\nplayer always does at least as well, and sometimes better, by playing\n\\(\\bD\\). Under these conditions it still seems rational to play\n\\(\\bD\\), which again results in the payoff that neither player\nprefers. Let us call a game that meets PD2 a weak PD. Note\nthat in a weak PD that does not satisfy PD1 mutual defection is no\nlonger a nash equilibrium in the strict sense defined above. It is\nstill, however, the only nash equilibrium in the weaker sense, that\nneither player can improve its position by unilaterally changing its\nmove. Again, one might suppose that if there is a unique nash\nequilibrium of this weaker variety, rational self-interested players\nwould reach it. \nWithout assuming symmetry, the PD can be represented by using\nsubscripts \\(r\\) and \\(c\\) for the payoffs to Row and Column. \nIf we assume that the payoffs are ordered as before for each player,\ni.e., that \\(T_i \\gt R_i \\gt P_i \\gt S_i\\) when \\(i=r,c\\), then, as\nbefore, \\(\\bD\\) is the strictly dominant move for both players, but\nthe outcome \\((\\bD, \\bD)\\) of both players making this move is worse\nfor each than \\((\\bC, \\bC)\\). The force of the dilemma can now also be\nfelt under weaker conditions, however. Consider the following three\npairs of inequalities: \nIf these conditions all obtain the argument for dilemma goes through\nas before. Defection strictly dominates cooperation for each player,\nand \\((\\bC,\\bC)\\) is strictly preferred by each to \\((\\bD,\\bD)\\). If\none of the two \\(\\gt\\) signs in each of the conditions\n\\(a\\)–\\(c\\) is replaced by a weak inequality sign (\\(\\ge\\)) we\nhave a weak PD. \\(\\bD\\) weakly dominates \\(\\bC\\) for each player\n(i.e., \\(\\bD\\) is as good as \\(\\bC\\) in all cases and better in some)\nand \\((\\bC,\\bC)\\) weakly better than \\((\\bD,\\bD)\\) (i.e., it is at\nleast as good for both players and better for one). Since none of the\nclauses requires comparisons between \\(r\\)'s payoffs and \\(c\\)'s, we\nneed not assume that \\(\\gt\\) has any “interpersonal”\nsignificance. \nNow suppose we drop the first inequality of either \\(a\\) or \\(b\\) (but\nnot both). A game that meets the resulting conditions might be termed\na common knowledge PD. As long as each player knows that the\nother is rational and each knows the other's ordering of payoffs, we\nstill feel the force of the dilemma. For suppose a holds. Then \\(\\bD\\)\nis the dominant move for Row. Column, knowing that Row is rational,\nknows that Row will defect, and so, by the remaining inequality in\n\\(b\\), will defect himself. Similarly, if \\(b\\) holds Column will\ndefect, and Row, realizing this, will defect herself. By \\(c\\), the\nresulting \\((\\bD,\\bD)\\) is again worse for both than\n\\((\\bC,\\bC)\\). \nIf the game specifies absolute (as opposed to relative) payoffs, then\nuniversal cooperation may not be a pareto optimal outcome even in the\ntwo person PD. For under some conditions both players do better by\nadopting a mixed strategy of cooperating with probability\n\\(p\\) and defecting with probability \\((1-p)\\). This point is\nillustrated in the graphs below. \nFigure 1 \nHere the \\(x\\) and \\(y\\) axes represent the utilities of Row and\nColumn. The four outcomes entered in the matrix of the second section\nare represented by the labeled dots. Conditions PD3a and PD3b (see\n above)\n ensure that \\((\\bC,\\bD)\\) and \\((\\bD,\\bC)\\) lie northwest and\nsoutheast of \\((\\bD,\\bD)\\), and PD3c is reflected in the fact that\n\\((\\bC,\\bC)\\) lies northeast of \\((\\bD,\\bD)\\). Suppose first that\n\\((\\bD,\\bD)\\) and \\((\\bC,\\bC)\\) lie on opposite sides of the line\nbetween \\((\\bC,\\bD)\\) and \\((\\bD,\\bC)\\), as in the graph on the left.\nThen the four points form a convex quadrilateral, and the payoffs of\nthe feasible outcomes of mixed strategies are represented by all the\npoints on or within this quadrilateral. Of course a player can really\nonly get one of four possible payoffs each time the game is played,\nbut the points in the quadrilateral represent the expected\nvalues of the payoffs to the two players. If Row and Column\ncooperate with probabilities \\(p\\) and \\(q\\) (and defect with\nprobabilities \\(p^*=1-p\\) and \\(q^*=1-q\\)), for example, then the\nexpected value of the payoff to Row is \\(p^*qT+pqR+p^*q^*P+pq^*S\\). A\nrational self-interested player, according to a standard view, should\nprefer a higher expected payoff to a lower one. In the graph on the\nleft the payoff for universal cooperation (with probability one) is\npareto optimal among the payoffs for all mixed strategies. In the\ngraph on the right, however, where both \\((\\bD, \\bD)\\) and \\((\\bC,\n\\bC)\\) lie southwest of the line between \\((\\bC, \\bD)\\) and \\((\\bD,\n\\bC)\\), the story is more complicated. Here the payoffs of the\nfeasible outcome lie within a figure bounded on the northeast by three\ndistinct curve segments, two linear and one concave. Notice that\n\\((\\bC, \\bC)\\) is now in the interior of the region bounded by solid\nlines, indicating that there are mixed strategies that provide both\nplayers a higher expected payoff than \\((\\bC, \\bC)\\). It is important\nto note that we are talking about independent mixed strategies here.\nRow and Column use private randomizing devices and have no\ncommunication. If they were able to correlate their mixed strategies,\nso as to ensure, say \\((\\bC, \\bD)\\) with probability \\(p\\) and \\((\\bD,\n\\bC)\\) with probability \\(p^*\\), the set of feasible solutions would\nextend up to (and include) the dotted line between \\((\\bC, \\bD)\\) and\n\\((\\bD, \\bC)\\). The point here is that, even confined to independent\nstrategies, there are some games satisfying PD3 in which both players\ncan both do better than they do with universal cooperation. A PD in\nwhich universal cooperation is pareto optimal may be called a pure PD.\n(This phenomenon is identified in Kuhn and Moresi and applied to moral\nphilosophy in Kuhn 1996.) A pure PD is characterized by adding to PD3\nthe following condition. \nIn a symmetric game \\(P\\) reduces to the simpler condition  \n(named after the authors Rapoport, Chammah and Axelrod who employed\nit). \nSpeaking generally, one might say that a PD is a game in which a\n“cooperative” outcome obtainable only when every player\nviolates rational self-interest is unanimously preferred to the\n“selfish” outcome obtained when every player adheres to\nrational self-interest. We can characterize the selfish outcome either\nas the result of each player pursuing its dominant (strongly dominant)\nstrategy, or as the unique weak (strong) nash equilibrium. In a two\nmove game the two characterizations come to the same thing—a\ndominant move pair is a unique equilibrium and a unique equilibrium is\na dominant move pair. As the payoff matrix below shows, however, the\ntwo notions diverge in a game with more than two moves. \nHere each player can choose “cooperate”, (\\(\\bC\\))\n“defect” (\\(\\bD\\) ), or “neither”\n(\\(\\bN\\)),and the payoffs are ordered as before. Defection is no\nlonger dominant, because each player is better off choosing \\(\\bC\\)\nthan \\(\\bD\\) when the other chooses \\(\\bN\\). Nevertheless \\((\\bD,\n\\bD)\\) is still the unique equilibrium. Let us label a game like this\nin which the selfish outcome is the unique equilibrium an\nequilibrium PD, and one in which the selfish outcome is a\npair of dominant moves a dominance PD. As will be seen below,\nattempts to “solve” the PD by allowing conditional\nstrategies can create multiple-move games that are themselves\nequilibrium PDs. \nThree-move games with a slightly different structure have received\nattention under the label “optional PD.” See, for example,\nKitcher (2011), Kitcher (1993), Batali and Kitcher, Szabó and\nHauert, Orbell and Dawes (1993), and Orbell and Dawes (1991). The\nfirst three sources take optional games also to allow players to\nsignal willingness to engage (i.e., play \\(\\bC\\) or \\(\\bD\\) against)\nparticular opponents. The simple three-move games without signaling\ndiscussed in this section are called “semi-optional” in\nBatali and Kitcher. \\(S,R,P\\) and \\(T\\) payoffs are ordered as before,\nbut the payoff matrix now contains, in addition, an\n“opt-out” value, \\(O\\), that lies between \\(P\\) and\n\\(R\\). \nIn this version of the game, defection is no longer a dominant move\nand mutual defection is no longer an equilibrium outcome. If Column\ncooperates, Row does best by defecting; if Column defects, Row does\nbest by playing \\(\\bN\\); and if Column plays \\(\\bN\\), then Row does\nequally well by playing any move. From the outcome of mutual \\(\\bD\\)\neither player can benefit by unilaterally switching to \\(\\bN\\). But\nfrom the outcome of mutual \\(\\bN\\), neither party can benefit by\nunilaterally changing moves. So the optional PD is a weak equilibrium\nPD, with \\(\\bN\\) playing the role of defection. Orbell and Dawes (1991\nand 1993) add the additional condition that the opt-out payoff \\(O\\)\nis equal to zero. In an optional PD, a rational player will engage\n(i.e., play either \\(\\bC\\) or \\(\\bD\\)) if and only if she expects her\nopponent to cooperate. For, if her opponent does cooperate, she will\nbe guaranteed at least \\(R\\) by engaging and exactly \\(O\\) by not\nengaging, whereas if her opponent does not cooperate she will be\nguaranteed at most \\(P\\) by engaging and exactly \\(O\\) by not\nengaging. This feature becomes especially salient when \\(O\\) is zero,\nfor then the payoff for engaging is positive if and only if one's\nopponent cooperates. \nThe description of the “neither” move and\n“opt-out” payoffs varies somewhat in accounts of the\noptional PD. For Kitcher they frequently represent a choice to\n“go solo.” For example, a baboon, rather than thoroughly\nor sloppily grooming a partner in exchange for being groomed\nthoroughly or sloppily by its partner, may choose to groom itself.\nOften, on the other hand, it is suggested is that \\(\\bN\\) represents a\nchoice to “sit out” the game, perhaps in order to obtain a\nmore suitable partner with whom to play later. The significance of\nthis difference, if any, will emerge in iterated and evolutionary\nversions of the game. (See sections 11–17 below.) Those who\nwrite about the optional PD often express the hope that it might\nprovide a suitable model to investigate the idea that cooperation can\nbe achieved if agents select the partners with whom they interact.\nThat idea is modeled somewhat differently, and perhaps more directly,\nin Social Network Games discussed in\n section 19\n below. Further discussion of the idea is left to that section. \nOrbell and Dawes are particularly concerned with an explanation for\ncooperative behavior that rests on the empirically supported\nhypothesis that individuals often base expectations about behavior of\nothers on their knowledge of their own behavior and tendencies. This\nhypothesis suggests that a cooperator is more likely than a defector\nto expect others to cooperate and therefore, if he is rational, more\nlikely to engage in the optional PD. Orbell and Dawes (1991)\ndemonstrate that, if a cooperator is substantially more likely than a\ndefector to expect his opponent to cooperate, then (provided the odds\nof his opponent cooperating are sufficiently high), a cooperator can\nactually expect a higher return than a defector in the optional PD.\nOrbell and Dawes (1993) present experimental evidence that\nparticipants in an optional PD do receive higher average payouts than\nthose in the corresponding PD lacking the \\(\\bN\\) move. They provide\nclever statistical arguments to support the following hypotheses:\nintending cooperators (those who cooperate when they must engage) do\nbetter in the optional PD than in the corresponding PD; intending\ndefectors generally do worse in the optional PD; under some conditions\nthese gains and losses are sufficient to make the intending\ncooperators better off than the intending defectors (as might be\npredicted by the theoretical result of the previous paper); and,\nfinally, those who expect cooperation from others (as evidenced by\ntheir engagement) do so on the basis of their own tendency to\ncooperate rather rather than any direct discernment of the character\nof their opponent. (See\n Transparency\n below.) \nMost of those who maintain that the PD illustrates something important\nabout morality seem to believe that the basic structure of the game is\nreflected in situations that larger groups, perhaps entire societies,\nface. The most obvious generalization from the two-player to the\nmany-player game would pay each player the reward (\\(R\\)) if all\ncooperate, the punishment (\\(P\\)) if all defect, and, if some\ncooperate and some defect, it would pay the cooperators the sucker\npayoff (\\(S\\)) and the defectors the temptation (\\(T\\)). But it is\nunlikely that we face many situations of this structure. \nA common view is that a multi-player PD structure is reflected in what\nGarret Hardin popularized as “the tragedy of the commons.”\nEach member of a group of neighboring farmers prefers to allow his cow\nto graze on the commons, rather than keeping it on his own inadequate\nland, but the commons will be rendered unsuitable for grazing if more\nthan some threshold number use it. More generally, there is some\nsocial benefit \\(B\\) that each member can achieve if sufficiently many\npay a cost \\(C\\). We might represent the payoff matrix as follows: \nThe cost \\(C\\) is assumed to be a negative number. The\n“temptation” here is to get the benefit without the cost,\nthe reward is the benefit with the cost, the punishment is to get\nneither and the sucker payoff is to pay the cost without realizing the\nbenefit. So the payoffs are ordered \\(B \\gt (B+C) \\gt 0 \\gt C\\). As in\nthe two-player game, it appears that \\(\\bD\\) strongly dominates\n\\(\\bC\\) for all players, and so rational players would choose \\(\\bD\\)\nand achieve 0, while preferring that everyone would choose \\(\\bC\\) and\nobtain \\(C+B\\). \nUnlike the more straightforward generalization, this matrix does\nreflect, in a highly idealized way, common social choices —\nbetween depleting and conserving a scarce resource, between using\npolluting and non-polluting means of manufacture or disposal, and\nbetween participating and not participating in a group effort towards\nsome common goal. When the number of players is small, it represents a\nversion of what has been called the “volunteer dilemma”. A\ngroup needs a few volunteers, but each member is better off if others\nvolunteer. (Notice, however, that in a true volunteer dilemma, where\nonly one volunteer is needed, \\(n\\) is zero and the top right outcome\nis impossible. Under these conditions \\(\\bD\\) no longer dominates\n\\(\\bC\\) and the game loses its PD flavor.) A particularly vexing\nmanifestation of this game occurs when a vaccination known to have\nserious risks is needed to prevent the outbreak of a fatal disease. If\nenough of her neighbors get the vaccine, each person may be protected\nwithout assuming therisks. \nOne idealizion here of the situations described is that the costs and\nbenefits of cooperation are assumed to be independent of the number of\nthose who cooperate. Until the threshold of cooperation is exceeded,\nnobody gets the benefit. Afterwards, everyone does. They are, in the\nterminology of Frolich et al, lumpy. This is not true of, say, a lake\nmade clean when residents refrain from dumping waste into it, or a gas\nsupply maintained by users' conservation. We will consider relaxing\nthis idealization later. For now, note that a situation more closely\nmirrored by the matrix is faced by the supporters of a particular\npolitical candidate or proposition who face the choice of whether to\nvote in a majority-rule election. Once enough supporters to constitute\na majority choose to vote, additional votes will not increase their\nbenefit. For this reason we might call the game described by the\nmatrix above a voting game.  \nThe voting game, as characterized above, has a somewhat different\ncharacter than the two-player PD. First, even if each player's moves\nare entirely independent of the others, the alternatives represented\nby the columns in the commons matrix above are no longer independent\nof the alternatives represented by the rows. My choosing \\(\\bC\\)\nnecessarily increases the chances that more than \\(n\\) people will\nchoose \\(\\bC\\). To ensure independence we should really redraw the\nmatrix as follows: \nBut now we see that move \\(\\bD\\) does not dominate \\(\\bC\\),\nas it does in the 2-player prisoner's dilemma. When we are at the\nthreshold of adequate cooperation, where exactly \\(n\\) others choose\n\\(\\bC\\), I am better off cooperating. Similarly, whereas mutual\ndefection is the only nash equilibrium in the original PD, this game\nhas two equilibria. One is universal defection, since any player\nunilaterally departing from that outcome will move from payoff 0 to\n\\(C\\). But a second is the state of minimally effective\ncooperation, where the number of cooperators is just sufficient to\nobtain the benefit. A defector who unilaterally departs from that\noutcome will move from \\(B\\) to \\(B+C\\) and a cooperator who\nunilaterally departs will move from \\(B+C\\) to 0. Finally, in the\norginal PD, every outcome except universal defecton is pareto\noptimal--i.e., as long as at least one of the players cooperates,\nthere is no outcome in which in which each player is at least as well\noff and one is better off. In the voting game, on the other hand, only\nthe states of minimally effective cooperation are pareto optimal. If\nthe number of cooperators exceeds the threshold by one or more, a new\ndefector will benefit himself while hurting no others.  \nIn view of these properties, it may seem that the voting game presents\nfar less of a dilemma than the PD. There are, after all, equilibria\nthat are pareto-optimal outcomes. In practice, however, it is\ndifficult to see how these equilibria could be attained and the\nall-defect equilibrium could be avoided. When n is large, defection\n“almost dominates” cooperation. In the voting case, for\nexample, a player might plausibly reason: if few of my fellow\nsupporters vote, my vote will be futile, if many of them do it will be\nunneccessary. Even if a group were in the unlikely situation of being\njust below the threshold of minimally effective cooperation, a\nprospective voter would have no way of knowing this. In the pollution\nand conservation examples moves should really not be modeled as\nsimultaneous (see Asynchronous Moves below), so we may perhaps be a\nlittle more optimistic. By observing the actions of those who have\nmoved previously a player might know whether at his turn the threshold\nof minimally effective cooperation is near. In most real-world\nsituations, however, a player can deduce this only by observing the\neffects of those actions, and often these effects manifest themselves\nonly after his move is made. A conspicuous example of this delay\neffect might be the succession of carbon-emitting activities leading\nto climate change. \nIn examples philosophers discuss as instances of prisoner's dilemma,\nit is taken to be obvious that universal cooperation is the most\nsocially desirable outcome. In the voters dilemma, since minimally\neffective cooperation is pareto superior, one might think that we\nshould aim instead for that outcome. But this seems to depend on the\nnature of the choices involved. In the medical example it may seem best\nto vaccinate everyone. In the agricultural example, however, it seems\nfoolish to stipulate that nobody use the commons. Someone who avoids\nvaccination in the former case is seen as a “free rider”.\nAn underused commons in the latter seems to exemplify “surplus\ncooperation.” All these cases seem to raise questions of\nfairness. If t+1 is the size of a minimally effective collection of\nplayers, then any profile in which exactly t+1 players cooperate is a\npareto optimal equilibrium. If there is no reason to prefer one such\nprofile over another, it is possible that fairness would dictate\nchoosing the inferior outcome of universal cooperation.  \nThe two-person version of the tragedy of the commons game (with\nthreshold of one) produces a matrix presenting considerably less of a\ndilemma. \nThis game captures David Hume's example of a boat with one oarsman on\nthe port side and another on the starboard (provided we assume that\nHume's oarsmen must make their choices between rest (D) and exertion\n(\\(\\bC\\)) simultaneously). If either rows alone, she exerts herself to\nno good effect, which is worse than had she merely rested. Mutual\ncooperation here is identical to minimally effective cooperation and\ntherefore is both an equilibrium outcome and a pareto optimal outcome.\nGames of this sort are discussed in section 8 below,\n under the label “stag hunt.” \nThe above representations of the tragedy of the commons make the\nsimplifying assumptions that the costs and benefits of cooperation are\nthe same for each player, that the cost of cooperation is independent\nof the number of players who cooperate, and that the size of the\nbenefit (\\(0\\) or \\(B)\\)) depends only on whether the number of\ncooperators exceeds the threshold. A somewhat more general account\nwould replace \\(C\\) and \\(B\\) by functions \\(C(i,j)\\) and \\(B(i,j)\\),\nrepresenting the cost of cooperation to player \\(i\\) when he is one of\nexactly \\(j\\) players who cooperate and the benefit to player \\(i\\)\nwhen exactly \\(j\\) players cooperate. We suppose that there is some\nthreshold \\(t\\) for minimally effective cooperation so that \\(B(i,j)\\)\nis not defined unless \\(j \\gt t\\). We may also assume additional\ncooperation never reduces the benefit \\(i\\) gets from effective\ncooperation, i.e., \\(B(i,j+1) \\ge B(i,j)\\) when \\(j \\gt t\\) and that\nadditional defection never reduces the cost \\(i\\) bears in\ncooperating, i.e., \\(C(i,j+1) \\ge C(i,j)\\). Now suppose, in addition,\nthat, once the threshold of effective cooperation has been exceeded,\nany benefit one gets from from the presence of an additional\ncooperator is exceeded by one's cost of cooperation and that the costs\nof ineffective cooperation are genuine, i.e., for all players \\(i\\),\n\\(B(i,j) \\gt ( B(i,j+1)+C(i,j+1) )\\) when \\(j\\) is greater than \\(t\\)\nand \\(0 \\gt C(i,j)\\) when \\(j\\) is less than or equal to \\(t\\).\nFinally, suppose that the benefits to each player \\(i\\), of effective\ncooperation exceed the costs, i.e., for \\(j \\gt t\\), \\(B(i,j)+C(i,j)\n\\gt 0\\). We then have a tragedy of the commons game, which presents a\nfamiliar dilemma: defection benefits an individual in every\ncircumstance (except the one where exactly \\(t\\) others cooperate) but\neverybody is better off in any state of effective cooperation than in\nany state without it. This account could be easily be modified to\nallow threshold of minimally effective cooperation to differ from one\nindividual to another (\\(i\\)'s clean water requirements might be more\nstringent than \\(j\\)'s for example) or to allow \\(B\\) to be defined\neverywhere (thus eliminating the threshold, so that we always benefit\nfrom another's cooperation). The resulting game would still have its\nPD flavor. \nPhillip Pettit has pointed out that examples that might be represented\nas many-player PDs come in two flavors. The examples discussed above\nmight be classified as free-rider problems. My temptation is to enjoy\nsome benefits brought about by burdens shouldered by others. The other\nflavor is what Pettit calls “foul dealer” problems. My\ntemptation is to benefit myself by hurting others. Suppose, for\nexample, that a group of people are applying for a single job, for\nwhich they are equally qualified. If all fill out their applications\nhonestly, they all have an equal chance of being hired. If one lies,\nhowever, he can ensure that he is hired while, let us say, incurring a\nsmall risk of being exposed later. If everyone lies, they again have\nan equal chance for the job, but now they all incur the risk of\nexposure. Thus a lone liar, by reducing the others' chances of\nemployment from slim to none, raises his own chances from slim to\nsure. As Pettit points out, when the minimally effective level of\ncooperation is the same as the size of the population, there is no\nopportunity for free-riding (everyone's cooperation is needed), and so\nthe PD must be of the foul-dealing variety. But (Pettit's contrary\nclaim notwithstanding) not all foul-dealing PDs seem to have this\nfeature. Suppose, for example, that two applicants in the story above\nwill be hired. Then everyone gets the benefit (a chance of employment\nwithout risk of exposure) unless two or more players lie.\nNevertheless, the liars seem to be foul dealers rather than free\nriders. A better characterization of the foul-dealing dilemma might be\nthat every defection from a generally cooperative state strictly\nreduces the payoffs to the cooperators, i.e., for every player \\(i\\)\nand every \\(j\\) greater than the threshold, \\(B(i,j+1)+ C(i,j+1) \\gt\nB(i,j)+ C(i,j)\\). A free-rider's defection benefits himself but does\nnot, by itself, hurt the cooperators. A foul-dealer's defection\nbenefits himself and hurts the cooperators. \nThe game labeled a many-person PD in Schelling, in Molander 1992, and\nelsewhere requires that the payoff to each co-operator and defector\nincreases strictly with the number of cooperators and that the sum of\nthe payoffs to all parties increases with the number of cooperators\n(so that one party's switching from defection to cooperation always\nraises the sum). Neither of these conditions is met by the formulation\nabove, and one may question whether they are appropriate for the\nexamples given. The margin of victory would not seem to raise the\nvalue of winning an election. Natural filtering systems may allow a\nbody of water to absorb a certain amount of waste with zero harmful\neffects. It is often plausible, however, to maintain that they hold\n“locally,” i.e., for \\(j\\) close to the threshold \\(t\\)\nfor minimally effective cooperation, it may be reasonable to assume\nthat: \nBy requiring that cooperation of others always strictly benefits each\nplayer, the Schelling and Molander formulations of the \\(n\\)-person PD\nfail to model the surplus cooperation/free rider phenomenon that seems\nto infuse many of the tragedy-of-commons examples. Their conditions\nmight, however be a plausible model for certain public good\ndilemmas. It is not unreasonable to suppose that any\ncontribution towards public health, national defense, highway safety,\nor clean air is valuable to all, no matter how little or how much we\nalready have, but that the cost to each for his own contribution to\nthose goods always exceeds the benefit that he derives from that\ncontribution. A particularly simple game meeting the conditions above\nis the public goods game. Each player may choose to\ncontribute either nothing or a fixed utility C to a common store.\nContributions to the store are added together, mutiplied by some\nfactor greater than one, and divided equally among the members of the\ngroup. In this way a player benefits by same amount from the\ncontributions of others whether she contributes herself or not, and\nloses by the same (smaller) amount from her own contribution whether\nothers contribute or not. This is not true of PD's in general, though\nit is true of the exchange game mentioned in the introduction. \nThe formulations of Schelling and Per Molander and the public goods\ngame have the advantage of focusing attention on the PD quality of the\ngame. Defection dominates cooperation, while universal cooperation is\nunanimously preferred to universal defection. Michael Taylor goes even\nfurther in this direction. His version of the many-person PD requires\nonly the two PD-conditions just mentioned and the one additional\ncondition that defectors are always better off when some cooperate\nthan when none do. (Taylor's main concern is with the iterated version\nof this game, a topic that will not be addressed here.) \nThese ideas can be made more perspicuous by some pictures, which\nsuggest additional refinements and extensions. Figure 2 below\nillustrates the voting game. In graph 2(a), twenty five\nsupporters are choosing whether to vote in a majority-rule\nelection. Utility to a player i is plotted against the number of those\nother than i who vote. Dark disks represent cooperators (voters) and\ncircles represent defectors (non-voters). When the number of other\nvoters is fewer than twelve or greater than twelve then defection\nbeats cooperation. But when exactly twelve others vote it benefits i\nto vote.  \nFigure 2 \nIn figure 2(b) smooth curves are drawn through the lines\nand circles to illustrate a more general form of the voting game. The\nutilities to cooperators and defectors are represented by two S-shaped\ncurves. The curves intersect in two places. Now, instead of a single\npoint of minimally effective cooperation, we have a small region\nbetween the two curves where cooperation beats defection. In terms of\nthe polluted lake example, we might suppose that to the left of the\nfirst intersection, pollution is so bad that my additional\ncontribution makes it no worse, and to the right of the second\nintersection, the lake is so healthy that it can handle my refuse with\nno ill effects. The intersection points are both equilibria, the\npolluting and fastitidious residents both lose by changing behavior.\nIn terms of the voting example, we might suppose that the behavior of\nnon-supporters is uncertain and the region between the curves\nrepresents the situations in which my vote increases the odds of\nwinning in a way that exceeds my cost of voting.  \nThe more general voting game satisfies the Schelling/Molander\ncondition that utility of each player increases strictly with the\nlevel of cooperation only near the region where cooperation is\neffective. In figure 3 below the S-curves are bent so that this\ncondition is met everywhere. In 3(a) the two curves\nstill intersect twice. Bovens, which contains a very illuminating\ntaxonomy of n-player games, labels this form the voting game\nand argues that it best represents situations described in the\nliterature as tragedies of the commons. Note that if there is a\nvalue of x at which both curves lie above the equilibria, as there\nmust be if the curves are upward sloping, then the equilibria here\ncannot be pareto optimal (as the lone equilibrium was in the simplest\nversion of what was called the voting game above). Hence the tragedy.\nIn graph 3(b) there are no intersections between the two\ncurves. Thus the second of the Schelling/Molander conditions for a PD\nis also met: defection dominates cooperation. The final condition,\nthat cooperation always raises the sum of utilities, is not so easily\npictured, but, because the slopes of the two curves are positive, we\ncan be sure that it will be met if the population is sufficiently\nlarge.  \nFigure 3 \nBenefits are somewhat less lumpy in these two games than the previous\ntwo. Lumpiness can by further reduced by further flattening the\ncurves. At the limit, we get the public goods game shown in the first\ngraph of figure 4. Here the curves are straight lines. Each additional\ncooperator provides both defectors and cooperators with the same\nadditional benefit of mc/n where c is the cost of donation, m is the\nstipulated multiplier and n is the number of players in the game. If\nthe curves are sufficiently flat, they can intersect at most\nonce. Altogether there are three possibilities: the game pictured in\nfigure 4(a), where the two curves do not intersect, the one pictured\nin 4(b), where cooperators' utility is above the defectors'\nto the left of the intersection and below it to the right, and the one\npictured in 4(c), where the defectors' utility starts above that of\nthe cooperators' and ends up below it. In the 4(b), one\nbenefits by cooperating when few of the others do and defecting when\nmost of the others cooperate. Bovens plausibly suggests that this\nshould be regarded as a many-player version of the game of chicken: go\nstraight if your opponent swerves and swerve if your opponent goes\nstraight. In 4(c), one benefits by defecting when most others do\nand cooperating when most others do. As Bovens suggests, this might be\nregarded as a many-person version of the stag hunt: hunt together or\nseparately if your opponent does likewise.  (Stag hunt is further\ndiscussed in\n section 8 below).\n The first possibility, as we have seen, meets conditions plausibly\nassociated with the PD.  \nFigure 4 \nThe PD is usually thought to illustrate conflict between individual\nand collective rationality, but the multiple player form (or something\nvery similar) has also been interpreted as demonstrating problems\nwithin standard conceptions of individual rationality. One such\ninterpretation, elucidated in Quinn, derives from an example of\nParfit's. A medical device enables electric current to be applied to a\npatient's body in increments so tiny that there is no perceivable\ndifference between adjacent settings. You are attached to the device\nand given the following choice every day for ten years: advance the\ndevice one setting and collect a thousand dollars, or leave it where\nit is and get nothing. Since there is no perceivable difference\nbetween adjacent settings, it is apparently rational to advance the\nsetting each day. But at the end of ten years the pain is so great\nthat a rational person would sacrifice all his wealth to return to the\nfirst setting. \nWe can view the situation here as a multi-player PD in which each\n“player” is the temporal stage of a single person. So\nviewed, it has at least two features that were not discussed in\nconnection with the multi-player examples. First, the moves of the\nplayers are sequential rather than simultaneous (and each player has\nknowledge of preceding moves). Second, there is the matter of\ngradation. Increases in electric current between adjacent settings are\nimperceptible, and therefore irrelevant to rational decision-making,\nbut sums of a number such increases are noticeable and highly\nrelevant. Neither of these features, however, is peculiar to\none-person examples. Consider, for example, the choice between a\npolluting and non-polluting means of waste disposal. Each resident of\na lakeside community may dump his or her garbage in the lake or use a\nless convenient landfill. It is reasonable to suppose that each acts\nin the knowledge of how others have acted before. (See\n“Asynchronous Moves” below.) It is also reasonable to\nsuppose that addition of one can of garbage to the lake has no\nperceptible effect on water quality, and therefore no effect on the\nwelfare of the residents. The fact that the dilemma remains suggests\nthat PD-like situations sometimes involve something more than a\nconflict between individual and collective rationality. In the\none-person example, our understanding that we care more about our\noverall well-being than that of our temporal stages does not (by\nitself) eliminate the argument that it is rational to continue to\nadjust the setting. Similarly, in the pollution example, a decision to\nlet collective rationality override individual rationality may not\neliminate the argument for excessive dumping. It seems appropriate,\nhowever, to separate this issue from that raised in the standard PD.\nGradations that are imperceptible individually, but weighty en masse\ngive rise to intransitive preferences. This is a challenge to standard\naccounts of rationality whether or not it arises in a PD-like\nsetting. \nA second one-person interpretation of the PD is suggested in Kavka,\n1991. On Kavka's interpretation, the prisoners are not temporal\nstages, but rather “subagents” reflecting different\ndesiderata that I might bring to bear on a decision. Let us imagine\nthat I am hungry and considering buying a snack. The options open to\nme are: \nMy health-conscious side, “Arnold,” orders these options\nin the following order: \\(c\\), \\(b\\), \\(d\\), \\(a\\). My taste-conscious\nside, “Eppie,” ranks them: \\(a\\), \\(b\\), \\(d\\), \\(c\\).\nSuch inner conflict among preferences might often be resolved in ways\nconsistent with standard views about individual choice. My overall\npreference ordering, for example, might be determined from a weighted\naverage of the utilities that Arnold and Eppie assign to each of the\noptions. It is also possible, Kavka suggests, that my inner conflicts\nare resolved as if they were a result of strategic interaction among\nrational subagents. In this case, Arnold and Eppie can each choose\neither to insist on getting their way \\((\\bI)\\) or to\nacquiesce to a compromise \\((\\bA)\\). The interaction between\nsubagents can then be represented by the following payoff matrix,\nwhere Arnold plays row and Eppie plays column. \nExamination of the table and preference orderings confirms that we\nagain have an intrapersonal PD. Kavka argues that a story like this\nmight “provide a psychologically plausible picture of how\ninternal conflict can lead to suboptimal action.” It also\nundermines a standard view that choices reflect values in favor of one\nthat they partially reflect, “the structure of inner\nconflict.” \nOne controversial argument that it is rational to cooperate in a PD\nrelies on the observation that my partner in crime is likely to think\nand act very much like I do. (See, for example, Davis 1977 and 1985\nfor a sympathetic presentation of one such argument and Binmore 1994,\nchapters 3.4 and 3.5, for a reformulation and extended rebuttal.) In\nthe extreme case, my accomplice is an exact replica of me who\nis wired just as I am so that, of necessity, we do the same thing. It\nwould then seem that the only two possible outcomes are where both\nplayers cooperate and where both players defect. Since the reward\npayoff exceeds the punishment payoff, I should cooperate. More\ngenerally, even if my accomplice is not a perfect replica, the odds of\nhis cooperating are greater if I cooperate and the odds of his\ndefecting are greater if I defect. When the correlation between our\nbehaviors is sufficiently strong or the differences in payoffs is\nsufficiently great, my expected payoff (as that term is\nusually understood) is higher if I cooperate than if I defect. The\ncounter argument, of course, is that my action is causally\nindependent of my replica's. Since I can't affect what my\naccomplice does and since, whatever he does, my payoff is greater if I\ndefect, I should defect. These arguments closely resemble the\narguments for two positions on the Newcomb Problem, a puzzle\npopularized among philosophers in Nozick. (The extent of the\nresemblance is made apparent in Lewis.) The Newcomb Problem asks us to\nconsider two boxes, one transparent and one opaque. In the transparent\nbox we can see a thousand dollars. The opaque box may contain either a\nmillion dollars or nothing. We have two choices: take the contents of\nthe opaque box or take the contents of both boxes. We know before\nchoosing that a reliable predictor of our behavior has put a million\ndollars in the opaque box if he predicted we would take the first\nchoice and left it empty if he predicted we would take the second. To\nsee that each player in a PD faces a Newcomb problem, consider the\nfollowing payoff matrix. \nBy “cooperating” (choosing the opaque box), each player\nensures that the other gets a million dollars (and a thousand extra\nfor defecting). By “defecting” (choosing both boxes) each\nplayer ensures that he will get thousand dollars himself (and a\nmillion more if the other cooperates). As long as \\(m \\gt t \\gt 0\\),\nthe structure of this game is an ordinary two-player, two-move PD (and\nany such PD can be represented in this form). Furthermore, the\narguments for “one-boxing” and “two-boxing” in\na Newcomb problem are the same as the arguments for cooperating and\ndefecting in a prisoner's dilemma where there is positive correlation\nbetween the moves of the players. Two boxing is a dominant\nstrategy: two boxes are better than one whether the first one is full\nor empty. On the other hand, if the predictor is reliable, the\nexpected payoff for one-boxing is greater than the expected\npayoff for two-boxing. (See Hurley (1991) and Bermúdez (2015),\nhowever, for arguments that the two puzzles are significantly\ndifferent.) \nThe intuition that two-boxing is the rational choice in a Newcomb\nproblem, or that defection is the rational choice in the PD with\npositive correlation between the players' moves, seems to conflict\nwith the idea that rationality requires maximizing expectation. This\napparent conflict has led some to suggest that standard decision\ntheory needs to be refined in cases in which an agent's actions\nprovide evidence for, without causing, the context\nin which he is acting. In the case of the PD, standard (evidential)\ndecision theory asks Player One to compare his expected utilities of\ncooperation and defection, which can be written as \\(p(\\bC_2 \\mid\n\\bC_1) \\times R + p(\\bD_2 \\mid \\bC_1) \\times S\\) and \\(p(\\bC_2 \\mid\n\\bD_1) \\times T + p(\\bD_2 \\mid \\bD_1) \\times P\\) (where, for example,\n\\(p(\\bC_2 \\mid \\bC_1)\\) is the conditional probability that player Two\ncooperates given that Player One cooperates). If the players' moves\nare strongly correlated then \\(p(\\bC_2 \\mid \\bC_1)\\) and \\(p(\\bD_2\n\\mid \\bD_1)\\) will be close to one and \\(p(\\bC_2 \\mid \\bD_1)\\) and\n\\(p(\\bD_2 \\mid \\bC_1)\\) will be close to zero. On the suggested\nrevision, these conditional probabilities should be replaced by some\nkind of causally conditional probabilities, which might (on some\naccounts) be expressed by phrases like “the probability that if\nOne were to cooperate, Two would also cooperate.” When the moves\nare causality independent this would just be the probability that Two\ncooperates. \nThe rather far-fetched scenario described in Newcomb's Problem\ninitially led some to doubt the importance of the distinction between\ncausal and evidential decision theory. Lewis argues that the link to\nthe PD suggests that situations where the two decisions diverge are\nnot so unusual, and recent writings on causal decision theory contain\nmany examples far less bizarre than Newcomb's problem. (See Joyce, for\nexample.) \nIn recent years technical machinery from the epistemic foundations of\ngame theory literature and various logics of conditionals has been\nemployed to represent arguments for cooperation and defection in\nprisoner's dilemma games between replicas (and for one-boxing and\ntwo-boxing in the Newcomb problem). See Bonanno for one example and a\ndiscussion of several others. These representations make clear some\nsubtle assumptions about the nature of rationality that underly the\narguments. Despite the increasing sophistication of the discussion,\nhowever, there remain people committed to each view. \nIt might be noted that what is here called “PD between\nreplicas” is usually called “PD with twins” in the\nliterature. One reason for the present nomenclature is to distinguish\nthese ideas from an experimental literature reporting on PD games\nplayed with real (identical or fraternal) twins. (See, for example,\nSegal and Hershberger.) It turns out that twins are more\nlikely to cooperate in a PD than strangers, but there seems to be no\nsuggestion that the reasoning that leads them to do so follows the\ncontroversial arguments presented above. \nThe idea mentioned in the introduction that the PD models a problem of\ncooperation among rational agents is sometimes criticized because, in\na true PD, the cooperative outcome is not a nash equilibrium. Any\n“problem” of this nature, the critics contend, would be an\nunsolvable one. (See for example, Sugden or Binmore 2005, chapter\n4.5.) By changing the payoff structure of the PD slightly, so that the\nreward payoff exceeds the temptation payoff, we obtain a game where\nmutual cooperation, as well as mutual defection, is a nash\nequilibrium. This game is known as the stag hunt. It might provide a\nbetter model for situations where cooperation is difficult, but still\npossible, and it may also be a better fit for other roles sometimes\nassigned to the PD. More specifically, a stag hunt is a two player,\ntwo move game with a payoff matrix like that for the PD given in\n section 1\n where the conditions PD1 are replaced by: \nThe fable dramatizing the game and providing its name, gleaned from a\npassage in Rousseau's Discourse on Inequality, concerns a\nhunting expedition rather than a jail cell interrogation. Two hunters\nare are looking to bag a stag. Success is uncertain and, if it comes,\nrequire the efforts of both. On the other hand, either hunter can\nforsake his partner and catch a hare with a good chance of success. A\ntypical payoff matrix is shown below. \nHere the “cooperative” move is hunting stag with one's\npartner and “defection” is hunting hare by oneself. The\n“temptation” payoff in a stag hunt is no longer much of a\ntemptation, but we retain the payoff terminology for ease of\nexposition. In this case the temptation and punishment penalties are\nidentical, perhaps reflecting the fact that my partner's choice of\nprey has no effect on my success in hare-hunting. Alternatively we\ncould have temptation exceeding punishment, perhaps because hunting\nhare is more rewarding together than alone (though still less\nrewarding, of course, than hunting stag together), or we could have\npunishment exceeding temptation, perhaps because a second hare hunter\nrepresents unhelpful competition. Either way, the essence of the Stag\nHunt remains. There are two equilibria, one unanimously preferred to\nthe other. The stag hunt becomes a “dilemma” when\nrationality dictates that both players choose the action leading to\nthe inferior equilibrium. It is clear that if I am certain that my\npartner will hunt stag I should join him and that if I am certain that\nhe will hunt hare I should hunt hare as well. For this reason games\nwith this structure are sometimes called games of\n“assurance” or “trust.” (But these should not\nbe confused with “trust game” versions of the asynchronous\nPD discussed in the following section.) If I do not know what my\npartner will do, standard decision theory tells me to maximize\nexpectation. This requires, however, that I estimate the probability\nof my partner playing \\(\\bC\\) or \\(\\bD\\). If I lack information to\nform any such estimates, then one putative principle of rationality\n(“indifference”) suggests that I ought to treat all\noptions as equally likely. By this criterion I ought to hunt hare if\nand only if the following condition is met: \nWhen SHD obtains, hare hunting is said to be the\n“risk-dominant” equilibrium. Let us call a stag hunt game\nwhere this condition is met a stag hunt dilemma. The matrix\nabove provides one example. \nAnother proposed principle of rationality (“maximin”)\nsuggests that I ought to consider the worst payoff I could obtain\nunder any course of action, and choose that action that maximizes this\nvalue. Since the sucker payoff is the worst payoff in a stag hunt,\nthis principle suggests that any stag hunt presents a\ndilemma. Maximin, however, makes more sense as a principle of\nrationality for zero sum games, where it can be assumed that a\nrational opponent is trying to minimize my score, than for games like\nstag hunt, where a rational opponent may be quite happy to see me do\nwell, as long as he does so as well. \nThe stag hunt can be generalized in the obvious way to accommodate\nasymmetric and cardinal payoffs. The quadrilateral formed by the\ngames' graphical representation is convex, so the pure/impure\ndistinction no longer applies. (In other words, in a stag hunt no\nmixed strategies are ever preferred to mutual cooperation.) The most\nobvious way to generalize the game to many players would retain the\ncondition that there be exactly two equilibria, one unanimously\npreferred to the other. This might be a good model for cooperative\nactivity in which success requires full cooperation. Imagine, for\nexample, that a single polluter would spoil a lake, or a single leak\nwould thwart an investigation. If many agents are involved and, by\nappeal to indifference or for other reasons, we estimate a fifty-fifty\nchance of cooperation from each, then these examples would represent\nstag hunt dilemmas in an extreme form. Everyone would benefit if all\ncooperate, but only a very trusting fool would think it rational to\ncooperate himself. Perhaps some broader generalization to the\nmany-person case would represent the structure of other familiar\nsocial phenomena, but that matter will not be pursued here. \nThe cooperative outcome in the stag hunt can be assured by many of the\nsame means as are discussed here for the PD. As might be expected,\ncooperation is somewhat easier to come by in the two-person stag hunt\nthan in the two-person PD. Details will not be given here, but the\ninterested reader may consult Skyrms 2004, which is responsible for a\nresurgence of interest in this game. \nIt has often been argued that rational self-interested players can\nobtain the cooperative outcome by making their moves conditional on\nthe moves of the other player. Peter Danielson, for example, favors a\nstrategy of reciprocal cooperation: if the other player would\ncooperate if you cooperate and would defect if you don't, then\ncooperate, but otherwise defect. Conditional strategies like this are\nruled out in the versions of the game described above, but they may be\npossible in versions that more accurately model real world situations.\nIn this section and the next, we consider two such versions. In this\nsection we eliminate the requirement that the two players move\nsimultaneously. Consider the situation of a firm whose sole competitor\nhas just lowered prices. Or suppose the buyer of a car has just paid\nthe agreed purchase price and the seller has not yet handed over the\ntitle. We can think of these as situations in which one player has to\nchoose to cooperate or defect after the other player has already made\na similar choice. The corresponding game is an asynchronous\nor extended PD. \nCareful discussion of an asynchronous PD example, as Skyrms (1998) and\nVanderschraaf recently note, occurs in the writings of David Hume,\nwell before Flood and Dresher's formulation of the ordinary PD. Hume\nwrites about two neighboring grain farmers: \nIn deference to Hume, Skyrms and Vanderschraaf refer to this kind of\nasynchronous PD as the “farmer's dilemma.” It is\ninstructive to picture it in a tree diagram. \nFigure 5 \nHere, time flows to the right. The node marked by a square indicates\nPlayer One's choice point, those marked by circles indicate Player\nTwo's. The moves and the payoffs to each player are exactly as in the\nordinary PD, but here Player Two can choose his move according to what\nPlayer One does. Tree diagrams like Figure 5 are said to be\nextensive-form game representations, whereas the payoff\nmatrices given previously are normal-form representations. As\nHume's analysis indicates, making the game asynchronous does not\nremove the dilemma. Player One knows that if he were to choose \\(\\bC\\)\non the first move, Player Two would choose \\(\\bD\\) on the second move\n(since she prefers the temptation to the reward), so he would himself\nend up with the sucker payoff. If Player One were to choose \\(\\bD\\),\nPlayer Two would still choose \\(\\bD\\) (since she prefers the\npunishment to the sucker payoff), and he would end up with the\npunishment payoff. Since he prefers the punishment payoff to the\nsucker payoff, Player One will choose \\(\\bD\\) on the first move and\nboth players will end up with the punishment payoff. This kind of\n“backward” reasoning, in which the players first evaluate\nwhat would happen on the last move if various game histories were\nrealized, and use this to determine what would happen on preceding\nmoves applies quite broadly to games in extensive form, and a more\ngeneral version of it will be discussed under finite\niteration below. \nThe farmer's dilemma can be represented in normal form by\nunderstanding Player One to be choosing between \\(\\bC\\) and \\(\\bD\\)\nand Player Two to be (simultaneously) choosing among four conditional\nmoves: cooperate unconditionally \\((\\bCu)\\), defect unconditionally\n\\((\\bDu)\\), imitate Player One's move \\((\\bI)\\), and do the opposite\nof Player One's move \\((\\bO)\\). The result is a two player game with\nthe following matrix. \nThe reader may note that this game is a (multiple-move) equilibrium\ndilemma. The sole (weak) nash equilibrium results when Player One\nchooses \\(\\bD\\) and Player Two chooses \\(\\bDu\\), thereby achieving for\nthemselves the inferior payoffs of \\(P\\) and \\(P\\). The game is not,\nhowever, a dominance PD. Indeed, there is no dominant move for either\nplayer. It is commonly believed that rational self-interested players\nwill reach a nash equilibrium even when neither player has a dominant\nmove. If so, the farmer's dilemma is still a dilemma. \nTo preserve the symmetry between the players that characterizes the\nordinary PD, we may wish to modify the asynchronous game. Let us take\nextended PD to be played in stages. First each player chooses a first\nmove \\((\\bC \\text{ or } \\bD)\\) and a second move \\((\\bCu, \\bDu, \\bI,\n\\text{ or } \\bO)\\). Next a referee determines who moves first, giving\neach player an equal chance. Finally the outcome is computed in the\nappropriate way. For example, suppose Row plays \\((\\bD, \\bO)\\)\n(meaning that he will defect if he moves first and do the opposite of\nhis opponent if he moves second) and Column plays \\((\\bC, \\bDu)\\).\nThen Row will get \\(P\\) if he goes first and \\(T\\) if he goes second,\nwhich implies that his expected payoff is \\(\\tfrac{1}{2}(P+T)\\).\nColumn will get \\(S\\) if she goes first and \\(P\\) if she goes second,\ngiving her an expected payoff of \\(\\tfrac{1}{2}(P+S)\\). It is\nstraightforward, but tedious, to calculate the entire eight by eight\npayoff matrix. After doing so, the reader may observe that, like the\nfarmer's dilemma, the symmetric form of the extended PD is an\nequilibrium PD, but not a dominance PD. The sole nash equilibrium\noccurs when both players adopt the strategy \\((\\bD, \\bDu)\\), thereby\nachieving the inferior payoffs of \\((P,P)\\). \nSome particularly simple and suggestive variations of on this theme\nhave been studied under the labels “investor game” or\n“trust game” (See, for example, Kreps (1990), Berg (1995)\nand Bicchieri and Suntuoso (2015) and note that the game nomenclature\nis not consistent accross these references.) Player One is given \\(s\\)\nunits of utility. He may choose to pass any number \\(s\\prime \\lt s\\)\nto a “trustee”, who triples that number and passes it to\nPlayer Two. Player Two may then either keep the units that she has or\nreturn some of them to Player One. So formulated, the game has the\nadvantage that one can take the proportion of her utility that a\nplayer surrenders as her degree of cooperativeness. If one\nrestricts the moves so that Player One may give none or \\(s\\), and\nPlayer Two may give none or \\(2s\\) one gets exactly the farmer's\ndilemma).  \nIn the farmer's dilemma and the trust game, unlike the PD, the\nsimilarly-labeled moves of the two players seem to have somewhat\ndifferent flavors. We are more likely to regard Player One's\ncooperation as generous or perhaps calculated (even\nif we regard the calculations involved to be irrational), and Player\nTwo's as fair. The label trusting is appropriate\nonly with regard to Player One's cooperative move, though Player Two's\ncooperation might be thought to show her to be worthy of that\ntrust. \nIt may be worth noting that an asynchronous version of the stag hunt,\nunlike the PD, presents few issues of interest. If the first player\ndoes his part in the hunt for stag on day one, the second should do\nher part on day two. If he hunts hare on day one, she should do\nlikewise on day two. The first player, realizing this, should hunt\nstag on day one. So rational players should have no difficulty\nreaching the cooperative outcome in the asynchronous stag hunt. \nAnother way that conditional moves can be introduced into the PD is by\nassuming that players have the property that David Gauthier has\nlabeled transparency. A fully transparent player is one whose\nintentions are completely visible to others. Nobody holds that we\nhumans are fully transparent, but the observation that we can often\nsuccessfully predict what others will do suggests that we are at least\n“translucent.” Furthermore agents of larger scale, like\nfirms or countries, which may have to publicly deliberate before\nacting, may be more transparent than we are. Thus there may be some\ntheoretical interest in investigations of PDs with transparent\nplayers. Such players could presumably execute conditional strategies\nmore sophisticated than those of the (non-transparent) extended game\nplayers, strategies, for example that are conditional on the\nconditional strategies employed by others. There is some difficulty,\nhowever, in determining exactly what strategies are feasible for such\nplayers. Suppose Row adopted the strategy “do the same as\nColumn” and Column adopted the strategy “do the opposite\nof Row”. There is no way that both these strategies could be\nsatisfied. On the other hand, if each adopted the strategy\n“imitate the other player”, there are two ways the\nstrategies could be satisfied, and there is no way to determine which\nof the two they would adopt. Nigel Howard, who was probably the first\nto study such conditional strategies systematically, avoided this\ndifficulty by insisting on a rigidly typed hierarchy of games. At the\nbase level we have the ordinary PD game, where each player chooses\nbetween \\(\\bC\\) and \\(\\bD\\). For any game \\(G\\) in the hierarchy we\ncan generate two new games \\(RG\\) and \\(CG\\). In \\(RG\\), Column has\nthe same moves as in game \\(G\\) and Row can choose any function that\nassigns \\(\\bC\\) or \\(\\bD\\) to each of Column's possible moves.\nSimilarly in \\(CG\\), Row has the same moves as in \\(G\\) and Column has\na new set of conditional moves. For example, if [PD] is the base level\ngame, then \\(C\\)[PD] is the game in which Column can choose from among\nthe strategies \\(\\bCu\\), \\(\\bDu\\), \\(\\bI\\) and \\(\\bO\\) mentioned\nabove. Howard observed that in the two third level games \\(RC\\)[PD]\nand \\(CR\\)[PD] (and in every higher level game) there is an\nequilibrium outcome giving each player \\(R\\). In particular, such an\nequilibrium is reached when one player plays \\(\\bI\\) and the other\ncooperates when his opponent plays \\(\\bI\\) and defects when his\nopponent plays \\(\\bCu\\), \\(\\bDu\\) or \\(\\bO\\). Notice that this last\nstrategy is tantamount to Danielson's reciprocal cooperation\ndescribed in the last section. \nThe lesson of all this for rational action is not clear. Suppose two\nplayers in a PD were sufficiently transparent to employ the\nconditional strategies of higher level games. How do they decide what\nlevel game to play? Who chooses the imitation move and who chooses\nreciprocal cooperation? To make a move in a higher level game is\npresumably to form an intention observable by the other player. But\nwhy should either player expect the intention to be carried out if\nthere is benefit in ignoring it? \nConditional strategies have a more convincing application when we take\nour inquiry as directed, not towards playing the PD, but as designing\nagents who would play it well with a variety of likely opponents. This\nis the viewpoint of Danielson. (See also J.V. Howard for an earlier\nenlightening discussion of this viewpoint.) A conditional strategy is\nnot an intention that a player forms as a move in a game, but a\ndeterministic algorithm defining a kind of player. Indeed, one of the\nlessons of the PD may be that transparent agents are better off if\nthey can form irrevocable “action protocols” rather than\nalways following the intentions they may form at the time of action.\nDanielson does not limit himself a priori to strategies\nwithin Howard's hierarchy. An agent is simply a computer program,\nwhich can contain lines permitting other programs to read and execute\nit. We could easily write two such programs, each designed to\ndetermine whether its opponent plays \\(\\bC\\) or \\(\\bD\\) and to do the\nopposite. What happens when these two play a PD depends on the details\nof implementation, but it is likely that they will be\n“incoherent,” i.e., they will enter endless loops and be\nunable to make any move at all. To be successful a program\nshould be able to move when paired with a variety of other\nprograms, including copies of itself, and it should be able to get\nvaluable outcomes. Programs implementing \\(\\bI\\) and \\(\\bO\\) in a\nstraightforward way are not likely to succeed because when paired with\neach other they will be incoherent. Programs implementing \\(\\bDu\\) are\nnot likely to succeed because they get only \\(P\\) when paired with\ntheir clones. Those implementing \\(\\bCu\\) are not likely to succeed\nbecause they get only \\(S\\) when paired with programs that recognize\nand exploit their unconditionally cooperative nature. There is some\nvagueness in the criteria of success. In Howard's scheme we could\ncompare a conditional strategy with all the possible alternatives of\nthat level. Here, where any two programs can be paired, that approach\nis senseless. Nevertheless, certain programs seem to do well when\npaired with a wide variety of players. One is a version of the\nstrategy that Gauthier has advocated as constrained\nmaximization. The idea is that a player \\(j\\) should cooperate if\nthe other would cooperate if \\(j\\) did, and defect otherwise. As\nstated, this appears to be a strategy for the \\(RC\\)[PD] or \\(CR\\)[PD]\ngames. It is not clear how a program implementing it would move (if\nindeed it does move) when paired with itself. Danielson is able to\nconstruct an approximation to constrained maximization,\nhowever, that does cooperate with itself. Danielson's program (and\nother implementations of constrained maximization) cannot be\ncoherently paired with everything. Nevertheless it does move and score\nwell against familiar strategies. It cooperates with \\(\\bCu\\) and\nitself and it defects against \\(\\bDu\\). If it is coherently paired it\nseems guaranteed a payoff no worse than \\(P\\). \nA second successful program models Danielson's reciprocal\ncooperation. Again, it is not clear that the strategy (as\nformulated above) allows it to cooperate (or make any move) with\nitself, but Danielson is able to construct an approximation that does.\nThe (approximate) reciprocal cooperation does as well as\n(approximate) constrained maximization against itself,\n\\(\\bDu\\) and constrained maximization. Against \\(\\bCu\\) it\ndoes even better, getting \\(T\\) where constrained\nmaximization got only \\(R\\). \nMany of the situations that are alleged to have the structure of the\nPD, like defense appropriations of military rivals or price setting\nfor duopolistic firms, are better modeled by an iterated version of\nthe game in which players play the PD repeatedly, retaining access at\neach round to the results of all previous rounds. In these iterated\nPDs (hence forth IPDs) players who defect in one round can be\n“punished” by defections in subsequent rounds and those\nwho cooperate can be rewarded by cooperation. Thus the appropriate\nstrategy for rationally self-interested players is no longer obvious.\nThe theoretical answer to this question, it turns out, depends\nstrongly on the definition of IPD employed and the knowledge\nattributed to rational players. \nAn IPD can be represented in extensive form by a tree diagram like the\none for the farmer's dilemma above. \nFigure 6 \nHere we have an IPD of length two. The end of each of the two rounds\nof the game is marked by a dotted vertical line. The payoffs to each\nof the two players (obtained by adding their payoffs for the two\nrounds) are listed at the end of each path through the tree. The\nrepresentation differs from the previous one in that the two nodes on\neach branch within the same division mark simultaneous choices by the\ntwo players. Since neither player knows the move of the other at the\nsame round, the IPD does not qualify as one of the game theorist's\nstandard “games of perfect information.” If the players\nmove in succession rather than simultaneously (which we might indicate\nby removing the dotted vertical lines), the resulting game is an\niterated farmer's dilemma, which does meet the game theorist's\ndefinition and which shares many of the features that make the IPD\ninteresting. \nLike the farmer's dilemma, an IPD can, in theory, be represented in\nnormal form by taking the players' moves to be strategies\ntelling them how to move if they should reach any node at the end of a\nround of the game tree. The number of strategies increases very\nrapidly with the length of the game so that it is impossible in\npractice to write out the normal form for all but the shortest IPD's.\nEvery pair of strategies determines a “play” of the game,\ni.e., a path through the extensive-form tree. \nIn a game like this, the notion of nash equilibrium loses some of its\nprivileged status. Recall that a pair of moves is a nash equilibrium\nif each is a best reply to the other. Let us extend the notation used\nin the discussion of the asynchronous PD and let \\(\\bDu\\) be the\nstrategy that calls for defection at every node of an IPD. It is easy\nto see that \\(\\bDu\\) and \\(\\bDu\\) form a nash equilibrium. But against\n\\(\\bDu\\), a strategy that calls for defection unless the other player\ncooperated at, say, the fifteenth node, would determine the same play\n(and therefore the same payoffs) as \\(\\bDu\\) itself does. The\ncomponents that call for cooperation never come into play, because the\nother player does not cooperate on the fifteenth (or any other) move.\nSimilarly, a strategy calling for cooperation only after the second\ncooperation by itself does equally well. Thus these strategies and\nmany others form nash equilibria with \\(\\bDu\\). There is a sense in\nwhich these strategies are clearly not equally rational. Although they\nyield the same payoffs at the nodes along the path representing the\nactual play, they would not yield the same payoffs if other nodes had\nbeen reached. If Player One had cooperated in the past, that\nwould still provide no good reason for him to cooperate now. A nash\nequilibrium requires only that the two strategies are best replies to\neach other as the game actually develops. A stronger solution concept\nfor extensive-form games requires that the two strategies would still\nbe best replies to each other no matter what node on the game tree\nwere reached. This notion of subgame-perfect equilibrium is\ndefined and defended in Selten 1975. It can be expressed by saying\nthat the strategy-pair is a nash equilibrium for every subgame of the\noriginal game, where a subgame is the result of taking a node of the\noriginal game tree as the root, pruning away everything that does not\ndescend from it. \nGiven this new, stronger solution concept, we can ask about the\nsolutions to the IPD. There is a significant theoretical difference on\nthis matter between IPDs of fixed, finite length, like the one\npictured above, and those of infinite or indefinitely finite length.\nIn games of the first kind, one can prove by an argument known as\nbackward induction that \\(\\bDu\\), \\(\\bDu\\) is the only\nsubgame perfect equilibrium. Suppose the players know the game will\nlast exactly \\(n\\) rounds. Then, no matter what node have been\nreached, at round \\(n-1\\) the players face an ordinary\n(“one-shot”) PD, and they will defect. At round \\(n-2\\)\nthe players know that, whatever they do now, they will both defect at\nthe next round. Thus it is rational for them to defect now as well. By\nrepeating this argument sufficiently many times, the rational players\ndeduce that they should defect at every node on the tree. Indeed,\nsince at every node defection is a best response to any move, there\ncan be no other subgame-perfect equilibria. \nIn practice, there is not a great difference between how people behave\nin long fixed-length IPDs (except in the final few rounds) and those\nof indeterminate length. This suggests that some of the rationality\nand common knowledge assumptions used in the backward induction\nargument (and elsewhere in game theory) are unrealistic. There is a\nconsiderable literature attempting to formulate the argument\ncarefully, examine its assumptions, and to see how relaxing\nunrealistic assumptions might change the rationally acceptable\nstrategies in the PD and other games of fixed length. (For a small\nsample, see Bovens, Kreps and Wilson, Pettit and Sugden, Sobel 1993\nand Binmore 1997). \nPlayer One's belief that there is a slight chance that Two might\npursue an “irrational” strategy other than continual\ndefection could make it rational for her to cooperate frequently\nherself. Indeed, even if One were certain of Two's rationality, One's\nbelief that there was some chance that Two believed she harbored such\ndoubts could have the same effect. Thus the argument for continual\ndefection in the IPD of fixed length depends on complex iterated\nclaims of certain knowledge of rationality. An even more unrealistic\nassumption, noted by Rabinowicz and others, is that each player\ncontinue to believe that the other will choose rationally on the next\nmove even after evidence of irrational play on previous moves. For\nexample, it is assumed that, at the node reached after a long series\nof moves (\\(\\bC\\), \\(\\bC\\)), …,(\\(\\bC\\), \\(\\bC\\)), Player One\nwill choose \\(\\bD\\) despite never having done so before. \nSome have used these kinds of observation to argue that the backward\ninduction argument shows that standard assumptions about rationality\n(with other plausible assumptions) are inconsistent or self-defeating.\nFor (with plausible assumptions) one way to ensure that a rational\nplayer will doubt one's own rationality is to behave irrationally. In\nthe fixed-length IPD, for example, Player One may be able to deduce\nthat, if she were to follow an appropriate “irrational”\nstrategy, Player Two would rationally react so that they can achieve\nmutual cooperation in almost all rounds. So our assumptions seem to\nimply both that Player One should continually defect and that she\nwould do better if she didn't. (See Skyrms 1990, pp. 125–139 and\nBicchieri 1989.) \nMany of the issues raised by the fixed-length IPD can be raised in\neven starker form by a somewhat simpler game. Consider a PD in which\nthey punishment payoff is zero. Now iterate the asynchronous version\nof this game a fixed number times. Imagine that both players are\nrestricted to highly “punitive” strategies according to\nwhich, they must always defect against a player who has ever defected.\n(One important strategy of this variety is discussed below under the\nlabel GRIM.) The result is a centipede game.\nA particularly nice realization is given by Sobel 2005. A stack of\n\\(n\\) one-dollar bills lies on a table. Players take turns taking\nmoney from the stack, one or two bills per turn. The game ends when\nthe stack runs out or one of the players takes two bills (whichever\ncomes first). Both players keep what they have taken to that point.\nThe extensive form of the game with for \\(n=4\\) is pictured below. \nFigure 7 \nPresumably the true centipede would contain 100 “legs” and\nthe general form discussed here should really be called the\n“\\(n\\)-tipede.” The game appears to be discussed first in\nRosenthal. \nAs in the fixed-length PD, a backward induction argument easily\nestablishes that a rational player should take two bills on his first\nmove, giving her a payoff of two or three dollars, depending on\nwhether she moves first or second, and leaving the remainder of the\n\\(n\\) dollars undistributed. In more technical terms, the only nash\nequilibria of the game are those where the first player takes two\ndollars on the first move and the only subgame perfect equilibrium is\nthe one in which both players take two dollars on any turn they should\nget. Again, common sense and experimental evidence suggest that real\nplayers rarely act in this way and this leads to questions about\nexactly what assumptions this kind of argument requires and whether\nthey are realistic. (In addition to the sample mentioned in the\nsection on finitely iterated PDs, see, for example, Aumann 1998,\nSelten 1978, and Rabinowicz.) The centipede also raises some of the\nsame questions about cooperation and socially desirable altruism as\ndoes the PD and it is a favorite tool in empirical investigations of\ngame playing. \nOne way to avoid the dubious conclusion of the backward induction\nargument without delving too deeply into conditions of knowledge and\nrationality is to consider infinitely repeated PDs. No human agents\ncan actually play an infinitely repeated game, of course, but the\ninfinite IPD has been considered an appropriate way to model a series\nof interactions in which the participants never have reason to think\nthe current interaction is their last. In this setting a pair of\nstrategies determines an infinite path through of the game tree. If\nthe payoffs of the one-shot game are positive, their total along any\nsuch path is infinite. This makes it somewhat awkward to compare\nstrategies. In many cases, the average payoff per round approaches a\nlimit as the number of rounds increases, and so that limit can\nconveniently serve as the payoff. (See Binmore 1992, page 365 for\nfurther justification.) For example, if we confine ourselves to those\nstrategies that can be implemented by mechanical devices (with finite\nmemories and speeds of computation), then the sequence of payoffs to\neach player will always, after a finite number of rounds, cycle\nrepeatedly through a particular finite sequence of payoffs. The limit\nof the average payoff per round will then be the average payoff in the\ncycle. In recent years, Press and Dyson have shown that for many\npurposes, investigation of the infinite IPD can be confined to the\n“memory-one” strategies, in which the probability of\ncooperating in any round depends only on what happened in the previous\nmeeting between the strategies. The average payoff per round is again\nalways well-defined in the limit. The ideas of Press and Dyson have\ninspired much new work on the infinite IPD. (See\n Zero-Determinant Strategies\n below.) Since there is no last round, it is obvious that backward\ninduction does not apply to the infinite IPD. \nMost contemporary investigations the IPD take it to be neither\ninfinite nor of fixed finite length but rather of indeterminate\nlength. This is accomplished by including in the game specification a\nprobability \\(p\\) (the “shadow of the future”) such that\nat each round the game will continue with probability \\(p\\).\nAlternatively, a “discount factor” \\(p\\) is applied to the\npayoffs after each round so that nearby payoffs are valued more highly\nthan distant ones. Mathematically, it makes little difference whether\n\\(p\\) is regarded as a probability of continuation or a discount on\npayoffs. The value of cooperation at a given stage in an IPD clearly\ndepends on the odds of meeting one's opponent in later rounds. (This\nhas been said to explain why the level of courtesy is higher in a\nvillage than a metropolis and why customers tend leave better tips in\nlocal restaurants than distant ones.) As \\(p\\) approaches zero, the\nIPD becomes a one-shot PD, and the value of defection increases. As\n\\(p\\) approaches one the IPD becomes an infinite IPD, and the value of\ndefection decreases. It is also customary to insist that the game has\nthe property labeled RCA above, so that (in the symmetric game)\nplayers do better by cooperating on every round than they would do by\n“taking turns” — you cooperate while I defect and\nthen I cooperate while you defect. \nThere is an observation, apparently originating in Kavka 1983, and\ngiven more mathematical form in Carroll, that the backward induction\nargument applies as long as an upper bound to the length of the game\nis common knowledge. For if \\(b\\) is such an upper bound, then, if the\nplayers were to get to stage \\(b\\), they would know that it was the\nlast round and they would defect; if they were to get to stage\n\\(b-1\\), they would know that their behavior on this round cannot\naffect the decision to defect on the next, and so they would defect;\nand so on. It seems an easy matter to compute upper bounds on the\nnumber of interactions in real-life situations. For example, since\nshopkeeper Jones cannot make more than one sale a second and since he\nwill live less than a thousand years, he and customer Smith can\ncalculate (conservatively) that they cannot possibly conduct more than\n\\(10^{12}\\) transactions. It is instructive to examine this argument\nmore closely in order to dramatize the assumptions made in standard\ntreatments of the indefinite IPD and other indefinitely repeated\ngames. Note first that, in an indefinite IPD as described above, there\ncan be no upper bound on the length of the game. There is, instead,\nsome fixed probability \\(p\\) that, at any time in which the game is\nstill being played, it will continue to be played with probability\n\\(p\\). If the interaction of Smith and Jones were modeled as an\nindefinite IPD, therefore, the probability of their interacting in a\nthousand years would not be zero, but rather some number greater than\n\\(p^k\\) where \\(p\\) is the probability of their interacting again now\nand \\(k\\) is the number of seconds in a thousand years. A more\nrealistic way to model the interaction might be to allow the value of\n\\(p\\) to decrease as the game progressed. As long as \\(p\\) always\nremains greater than zero, however, it remains true that there can be\nno upper bound on the number of possible interactions, i.e., no time\nat which the probability of future interactions becomes zero. Suppose,\non the other hand, that there was a number \\(n\\) such that\nthat there was zero probability of the game's continuing to stage\n\\(n\\). Let \\(p_1, \\ldots, p_n\\), be the probabilities that game\ncontinues after \\(\\text{stage } 1, \\ldots, \\text{stage } n\\). Then\nthere must be a smallest \\(i\\) such that \\(p_i\\) becomes \\(0\\). (It\nwould happen at \\(i=n\\) if not sooner.) Given the standard common\nknowledge assumptions that we have been making, the players would know\nthis value of \\(i\\), and the IPD would be one of fixed length, and not\nan indefinite IPD at all. In the case of the shopkeeper and his\ncustomer, we are to suppose that both know today that their last\ninteraction will occur, let's say, at noon on June 10th, 2020. The\nvery plausible idea that we began with, viz., that some upper\nbounds on the number of interactions are common knowledge, even though\nthe smallest upper bound is not, is incompatible with the assumption\nthat we know all the continuation probabilities \\(p_i\\) from the\nstart. \nAs Becker and Cudd astutely observe, we don't need an upper bound on\nthe number of possible iterations to make a backward induction\nargument for defection possible. If the players know all the values of\n\\(p_i\\) from the outset, then, as long as the value of \\(p_i\\) becomes\nand remains sufficiently small, they (and we) can compute a stage\n\\(k\\) at which the risk of future punishment and the chance of future\nreward no longer outweighs the benefit of immediate defection. So they\nknow their opponent will defect at stage \\(k\\), and the induction\nbegins. This modification of the Kavka/Carroll argument, however, only\nfurther exposes the implausibility of its assumptions. Not only are\nSmith and Jones expected to believe that there is non-zero probability\nthat they will be interacting in a thousand years, each is expected to\nbe able to compute the precise day on which future interactions will\nbecome and remain so unlikely that their expected future return is\noutweighed by that day's payoff. Furthermore each is expected to\nbelieve that the other has made this computation, and that the other\nexpects him to have made it, and so on. \nThe iterated version of the PD was discussed from the time the game\nwas devised, but interest accelerated after influential publications\nof Robert Axelrod in the early eighties. Axelrod invited professional\ngame theorists to submit computer programs for playing IPDs. All the\nprograms were entered into a tournament in which each played every\nother (as well as a clone of itself and a strategy that cooperated and\ndefected at random) hundreds of times. It is easy to see that in a\ngame like this no strategy is “best” in the sense that its\nscore would be highest among any group of competitors. If the other\nstrategies never consider the previous history of interaction in\nchoosing their next move, it would be best to defect unconditionally.\nIf the other strategies all begin by cooperating and then\n“punish” any defection against themselves by defecting on\nall subsequent rounds, then a policy of unconditional cooperation is\nbetter. Nevertheless, as in the transparent game, some strategies have\nfeatures that seem to allow them to do well in a variety of\nenvironments. The strategy that scored highest in Axelrod's initial\ntournament, Tit for Tat (henceforth TFT), simply\ncooperates on the first round and imitates its opponent's previous\nmove thereafter. More significant than TFT's initial\nvictory, perhaps, is the fact that it won Axelrod's second tournament,\nwhose sixty three entrants were all given the results of the first\ntournament. In analyzing the his second tournament, Axelrod noted that\neach of the entrants could be assigned one of five\n“representative” strategies in such a way that a\nstrategy's success against a set of others can be accurately predicted\nby its success against their representative. As a further\ndemonstration of the strength of TFT, he calculated\nthe scores each strategy would have received in tournaments in which\none of the representative strategies was five times as common as in\nthe original tournament. TFT received the highest\nscore in all but one of these hypothetical tournaments. \nAxelrod attributed the success of TFT to four\nproperties. It is nice, meaning that it is never the first to\ndefect. The eight nice entries in Axelrod's tournament were the eight\nhighest ranking strategies. It is retaliatory, making it\ndifficult for it to be exploited by the rules that were not nice. It\nis forgiving, in the sense of being willing to cooperate even\nwith those who have defected against it (provided their defection\nwasn't in the immediately preceding round). An unforgiving rule is\nincapable of ever getting the reward payoff after its opponent has\ndefected once. And it is clear, presumably making it easier\nfor other strategies to predict its behavior so as to facilitate\nmutually beneficial interaction. \nSuggestive as Axelrod's discussion is, it is worth noting that the\nideas are not formulated precisely enough to permit a rigorous\ndemonstration of the supremacy of TFT. One doesn't\nknow, for example, the extent of the class of strategies that might\nhave the four properties outlined, or what success criteria might be\nimplied by having them. It is true that if one's opponent is playing\nTFT (and the shadow of the future is sufficiently\nlarge) then one's maximum payoff is obtained by a strategy that\nresults in mutual cooperation on every round. Since\nTFT is itself one such strategy this implies that\nTFT forms a nash equilibrium with itself in the space\nof all strategies. But that does not particularly distinguish\nTFT, for \\(\\bDu\\), \\(\\bDu\\) is also a nash\nequilibrium. Indeed, a “folk theorem” of iterated game\ntheory (now widely published — see, for example, Binmore 1992,\npp. 373–377) implies that, for any \\(p\\), \\(0 \\le p \\le 1\\)\nthere is a nash equilibrium in which \\(p\\) is the fraction of times\nthat mutual cooperation occurs. Indeed TFT is, in\nsome respects, worse than many of these other equilibrium\nstrategies, because the folk theorem can be sharpened to a similar\nresult about subgame perfect equilibria. TFT is, in\ngeneral, not subgame perfect. For, were one\nTFT player (per impossible) to defect\nagainst another in a single round, the second would have done better\nas an unconditional cooperator. \nAfter publication of Axelrod, 1984, a number of strategies commonly\nthought to improve on TFT were identified. (Since\nsuccess in an IPD tournament depends on the other stragies present, it\nis not clear exactly what this claim means or how it might be\ndemonstrated.) The first of these was Nowak and Sigmond's\nPavlov, also known as, Win-Stay\nLose-Shift (WSLS), which conditions each\nnon-initial move on its own previous move as well as its opponent's.\nMore specifically, it cooperates if it and its opponent previously\nmoved alike and it defects if they previously moved differently.\nEquivalently, it repeats its move after success (temptation or reward)\nand changes it after failure (punishment or sucker). Hence the names.\nThis strategy does well in environments like that of Axelrod's\ntournment, but poorly when many unconditional defectors or random\nplayers are present. It is discussed further under the label \\(\\bP_1\\)\nin the sections on error and evolution below. A second family of these\nis Gradual Tit for Tat (henceforth\nGrdTFT). GrdTFT differs from\nTFT in two respects. First, it gradually increases\nthe string of punishing defection responses to each defection by its\nopponent. Second, it apologizes for each string of defections by\ncooperating in the subsequent two rounds. The first property ensures\nthat (unlike TFT) it will defect with increasing\nfrequency against a random player. The second ensures that (unlike\nTFT) it will quickly establish a regime of mutual\ncooperation with suspicious versions of TFT (i.e.,\nversions of TFT that defect on their first move).\nBeaufils et al show that the version of GrdTFT in\nwhich the string of defections is increased by one each time it is\nexploited wins round robin tournaments populated by a selection of\n“good” IPD strategies (including TFT)\nthat the authors chose after an examination of previous tournaments.\nTzafestas (1998) argues that, in making a each move depend on the\nentire prior history of the game, GrdTFT incorporates\nundesirable memory requirements. She suggests that equal success might\nbe obtained with an \"adaptive\" strategy, that tracks a measure of the\nopponent's cooperativeness or responsiveness across a narrow window of\nrecent moves and chooses its move according to whether this measure\n(the \"world\") exceeds some threshold. The critique seems misguided:\nmaintaining a count of prior defections seems no more burdensome than\nupdating the world variable. Nevertheless Tzafestas is able to show\nthat one of the strategies she identifies outperforms both\nTFT and GrdTFT in the very same\nenvironment that Beaufils had constructed. \nIn more recent years enthusiasm about TFT has been\ntempered by increasing skepticism.(See, for example, Binmore 2015 (p.\n30) and Northcott and Alexandrova (pp. 71-78). Evidence has emerged\nthat the striking success of TFT in Axelrod's\ntournaments may be partly due to features particular to Axelrod's\nsetup. Rapoport et al (2015) suggest that, instead of conducting a\nround-robin tournament in which every strategy plays every strategy,\none might divide the initial population of stratgies randomly into\nequal-size groups, conduct round-robin tournaments within each group.\nand then a championship round-robin tournament among the group\nwinners. They find that, with the same initial population of\nstrategies present in Axelrod's first tournament, the strategies\nranked two and six in that tournament both perform considerably better\nthan top-ranked TFT. Kretz (2011) finds that, in\nround-robin tournaments among populations of strategies that can only\ncondition on a small number of prior moves (of which\nTFT is clearly one) relative performance of\nstrategies is sensitive to the payoff values in the PD matrix. (Interestingly,\nthis is so even if the PDs all satisfy or fail to satisfy the condition\nR+P=T+S, characterizing exchange games, and if they all satisfy or fail to satisfy\nthe RCA condition, R>½(T+S).\n  \nEqually telling, perhaps, are the results of a more recent tournament\nusing the same paramters as Axelrod did. To mark the twentieth\nanniversary of the publication of Axelrod's book, a number of similar\ntournaments were staged at the IEEE Congress on Evolutionary Computing\nin Portland in 2004 and the IEEE Symposium on Computational\nIntelligence and Games in Colchester 2005. Kendall et al 2007\ndescribes the tournaments and contains several papers by authors who\nsubmitted winning entries. Most of the tournaments were deliberately\ndesigned to differ significantly from Axelrod's (and some of these are\nbriefly discussed in the section on signaling below). In the one that most closely replicated Axelrod's tournaments. however,\nTFT finished only fourteenth out of the fifty\nstrategies submitted. \nOf Axelrod's five suggested success criteria, the one that seems most\nclearly undermined by the later tournament is\n“clarity”.  Neither of the two highest-scoring\nstrategies, Adaptive Pavlov\n(APavlov) and Omega Tit for Tat\n(ΩTFT, have near the simplicity of Rapoport's\ntit-for-tat. Both are broadly adaptive in the sense of Tzafestas, but\nthe first is more narrowly crafted than Tzafestas's for what its\ndesigner expected the tournament environment to be, and the second\nreplaces Tzafestas's world variable with a pair of measures intended\nto measure \"deadlock\" and randomness. \nLi (2007) says explicitly that the idea behind\nAPavlov was to make an educated guess about what\nstrategies would be entered, find an accurate, low-cost way to\nidentify each during the initial stages of the game and then play an\noptimal strategy against each strategy so identified. For example, the\nstrategies Cu, Du,\nGRIM, RANDOM, TFT,\nTFTT, TTFT, and\nP1, described in a\n supplementary table,\n had all appeared in previous tournaments. By defecting in round one,\ncooperating in round three, and choosing the opposite of one's\nopponent's round-one move in round two, one could identify any\nopposing strategy from among these nine in three moves. This\nidentification process would be costly, however, because, by its first\nmove, it eliminates any opportunity of cooperation with\nGRIM. Li chooses instead to employ\nTFT over the first six rounds as his identifying\nstrategy, reducing cost at the expense of accuracy and range. It is\nworth noting that TFT cannot distinguish any pair of\nstrategies that satisfy Axelrod's niceness condition (never being the\nfirst to defect). This means that it forgoes the chance to exploit\nunconditional cooperators. Li's entry won its tournament only because\nhe guessed correctly that not many unconditional cooperators would be\npresent. The lesson again is to remember that success depends on\nenvironment. \nΩTFT plays TFT unless its\nmeasures of deadlock or randomness exceed specified thresholds. \nThe deadlock measure is intended to check whether\nΩTFT and its opponent are locked into an\nunproductive cycle in which they take turns defecting.  When its\nthreshold is exceeded the strategy cooperates and resets the\nmeasure.  When the randomness measure exceeds its threshold\nΩTFT switches to unconditional defection. \nContrary to what might be expected by its name, randomness grows when\nOmegaTFT is repeatedly exploited by an unconditional defector. \nLike APavlov, however, the strategy cooperates with\nan unconditional cooperator. Details can be found in Slany and\nKienreich (p. 184). \nAll the strategies for IPDs mentioned in this entry are summarized in\nthe Table of Strategies mentioned\nabove. \nIn a survey of the field several years after the publication of the\nresults reported above, Axelrod and Dion, chronicle several successes\nof TFT and modifications of it. They conclude that\n“research has shown that many of Axelrod's findings…can\nbe generalized to settings that are quite different from the original\ntwo-player iterated prisoner's dilemma game.” But in several\nreasonable settings TFT has serious drawbacks. One\nsuch case, noted in the Axelrod and Dion survey, is when attempts are\nmade to incorporate the plausible assumption that players are subject\nto errors of execution and perception. There are a number of ways this\ncan be done. Bendor, for example, considers “noisy\npayoffs.” When a player cooperates while its opponent defects,\nits payoff is \\(S+e\\), where \\(e\\) is a random variable whose expected\nvalue is \\(0\\). Each player infers the other's move from its own\npayoff, and so if \\(e\\) is sufficiently high its inference may be\nmistaken. Sugden (pp. 112–115) considers players who have a\ncertain probability of making an error of execution that is apparent\nto them but not their opponents. Such players can adopt strategies by\nwhich they “atone” for mistaken defections by being more\ncooperative on later rounds than they would be after intended\ndefection. Assuming that players themselves cannot distinguish a\nmistaken move or observation from a real one, however, the simplest\nway to model the inevitability of error is simply to forbid completely\ndeterministic strategies like TFT, replacing them\nwith “imperfect” counterparts, like “imitate the\nother player's last move with 99% probability and oppose it with 1%\nprobability.” Imperfect TFT is much less\nattractive than its deterministic sibling, because when two\nimperfect TFT strategies play each other, an\n“error” by either one will set off a long chain of moves\nin which the players take turns defecting. In a long iterated game\nbetween two imperfect TFT's with any probability\n\\(p\\) of error, \\(0 \\lt p \\lt \\tfrac{1}{2}\\), players will approach\nthe same average payoffs as in a game between two strategies that\nchoose randomly between cooperation and defection, namely\n\\(\\tfrac{1}{4}(R+P+S+T)\\). That is considerably worse than the payoff\nof \\(R\\), that results when \\(p=0\\). \nThe predominant view seems to be that, when imperfection is\ninevitable, successful strategies will have to be more forgiving of\ndefections by their opponents (since those defections might well be\nunintended). Molander 1985 demonstrates that strategies that mix\nTFT with \\(\\bCu\\) do approach a payoff of \\(R\\) as\nthe probability of error approaches zero. When these mixes play each\nother, they benefit from higher ratios of \\(\\bCu\\) to\nTFT, but if they become too generous, they risk\nexploitation by “stingy” strategies that mix\nTFT with defection. Molander calculates that when the\nmix is set so that, following a defection, one cooperates with\nprobability \\(g(R, P, T, S) = \\min\\{1-(T-R)/(R-S), (R-P)/(T-P)\\}\\),\nthe generous strategies will get the highest score with each other\nthat is possible without allowing stingy strategies to do better\nagainst them than TFT does. Following Nowak and\nSigmund, we label this strategy generous TFT, or\nGTFT. When payoffs have the common values 5,3,1,0 (as\nin Axelrod 1984), GTFT cooperates after every\ninstance of an opponent's cooperation and after 25% of an opponent's\ndefections. \nThe idea that the presence of imperfection induces greater forgiveness\nor generosity is only plausible for low levels of imperfection. As the\nlevel of imperfection approaches \\(\\tfrac{1}{2}\\), Imperfect\nTFT becomes indistinguishable from the random strategy, for\nwhich the very ungenerous \\(\\bDu\\) is the best reply. A simulation by\nKollock seems to confirm that at high levels of imperfection, more\nstinginess is better policy than more forgiveness. But Bendor, Kramer\nand Swistak note that the strategies employed in the Kollock\nsimulation are not representative and so the results must be\ninterpreted with caution. \nA second idea is that an imperfect environment encourages strategies\nto observe their opponent's play more carefully. In a tournament\nsimilar to Axelrod's (Donninger) in which each player's moves were\nsubject to a 10% chance of alteration, TFT finished\nsixth out twenty-one strategies. As might have predicted on the\ndominant view, it was beaten by the more generous\nTit-for-Two-Tats, aka TFTT (which\ncooperates unless defected against twice in a row). It was also\nbeaten, however, by two versions of Downing, a\nprogram that bases each new move on its best estimate how responsive\nits opponent has been to its previous moves. In Axelrod's two original\ntournaments, Downing had ranked near the bottom third\nof the programs submitted. Bendor (1987) demonstrates deductively that\nagainst imperfect strategies there are advantages to basing one's\nprobability of defection on longer histories than does\nTFT. \nOne clever implementation of the idea that a strategies in an\nimperfect environment should pay attention to their previous\ninteractions is the family of “Pavlovian” strategies\ninvestigated by Kraines and Kraines. For each natural number \\(n\\),\n\\(n\\)-Pavlov, or \\(\\bP_n\\), adjusts its probability of cooperation in\nunits of \\(\\tfrac{1}{n}\\), according to how well it fared on the\nprevious round. More precisely, if \\(\\bP_n\\) was cooperating with\nprobability \\(p\\) on the last round, then on this round it will\ncooperate with probability \\(p\\,[+]\\tfrac{1}{n}\\) if it received the\nreward payoff on the previous round, \\(p\\,[-]\\tfrac{1}{n}\\) if it\nreceived the punishment payoff, \\(p\\,[+]\\tfrac{2}{n}\\) if it received\nthe temptation payoff, and \\(p\\,[-]\\tfrac{2}{n}\\) if it received the\nsucker payoff. \\([+]\\) and \\([-]\\) are bounded addition and\nsubtraction, i.e., \\(x\\,[+]\\,y\\) is the sum \\(x+y\\) unless that number\nexceeds one, in which case it is one (or as close to one as the\npossibility of error allows), and \\(x\\,[-]\\,y\\) is similarly either\n\\(x-y\\) or close to zero. Strictly speaking, \\(\\bP_n\\) is not fully\nspecified until an initial probability of cooperation is given, but\nfor most purposes the value of that parameter becomes insignificant in\nsufficiently long games and can be safely ignored. It may appear that\n\\(\\bP_n\\) requires far more computational resources to implement than,\nsay, TFT. Each move for the latter depends on only on\nits opponent's last move, whereas each move for \\(\\bP_n\\) is a\nfunction of the entire history of previous moves of both players.\n\\(\\bP_n\\), however, can always calculate its next move by tracking\nonly its current probability of cooperation and its last payoff. As\nits authors maintain, this seems like “a natural strategy in the\nanimal world.” One can calculate that for \\(n \\gt1\\), \\(\\bP_n\\)\ndoes better against the random strategy than does\nTFT. More generally, \\(\\bP_n\\) does as well or better\nthan TFT against the generous unresponsive strategies\nCp that always cooperate with fixed probability \\(p\n\\ge \\tfrac{1}{2}\\) (because an occasional temptation payoff can teach\nit to exploit the unresponsive strategies.) In these cases the\n“slow learner” versions of Pavlov with higher values of\n\\(n\\) do slightly better than the “fast learners” with low\nvalues. Against responsive strategies, like other Pavlovian strategies\nand TFT, \\(\\bP_n\\) and its opponent eventually reach\na state of (almost) constant cooperation. The total payoff is then\ninversely related to the “training time,” i.e., the number\nof rounds required to reach that state. Since training time of\n\\(\\bP_n\\) varies exponentially with \\(n\\), Kraines and Kraines\nmaintain that \\(\\bP_3\\) or \\(\\bP_4\\) are to be preferred to other\nPavlovian strategies, and are close to “ideal” IPD\nstrategies. It should be noted, however, that when (deterministic)\nTFT plays itself, no training time at all is\nrequired, whereas when a Pavlovian strategy plays TFT\nor another Pavlov, the training time can be large. Thus the cogency of\nthe argument for the superiority of Pavlov over\nTFT depends on the observation that its performance\nshows less degradation when subject to imperfections. It is also worth\nremembering that no strategy is best in every environment, and the\ncriteria used in defense of various strategies in the IPD are vague\nand heterogeneous. One advantage of the evolutionary versions of the\nIPD discussed in the next section is that they permit more careful\nformulation and evaluation of success criteria. \nPerhaps the most active area of research on the PD concerns\nevolutionary versions of the game. A population of players employing\nvarious strategies play IPDs among themselves. The lower scoring\nstrategies decrease in number, the higher scoring increase, and the\nprocess is repeated. Thus success in an evolutionary PD (henceforth\nEPD), requires doing well with other successful strategies, rather\nthan doing well with a wide range of strategies. \nThe initial population in an EPD can be represented by a set of pairs\n\\(\\{(p_1, s_1), \\ldots (p_n, s_n)\\}\\) where \\(p_1 \\ldots p_n\\) are the\nproportions of the population playing strategies \\(\\bs_1, \\ldots,\n\\bs_n\\), respectively. The description of EPDs given above does not\nspecify exactly how the population of strategies is to be\nreconstituted after each IPD. The usual assumption, and the most\nsensible one for biological applications, is that a score in any round\nindicates the relative number of “offspring” in the next.\nIt is assumed that the size of the entire population stays fixed, so\nthat births of more successful strategies are exactly offset by deaths\nof less successful ones. This amounts to the condition that the\nproportion \\(p_i^*\\) of each strategy \\(s_i\\) in the successor\npopulation is determined by the equation \\(p_i^* = p_i(V_i / V)\\),\nwhere \\(V_i\\) is the score of \\(s_i\\) in the previous round and \\(V\\)\nis the average of all scores in the population. Thus every strategy\nthat scores above the population average will increase in number and\nevery one that scores below the average will decrease. This kind of\nevolution is referred to as “replicator dynamics” or\nevolution according to the “proportional fitness” rule.\nOther rules of evolution are possible. Bendor and Swistak argue that,\nfor social applications, it makes more sense to think of the players\nas switching from one strategy to another rather than as coming into\nand of existence. Since rational players would presumably switch only\nto strategies that received the highest payoff in previous rounds,\nonly the highest scoring strategies would increase in numbers. Batali\nand Kitcher employ a dynamics in which lowest scoring strategies are\nreplaced by strategies that mix characteristics of the highest scoring\nstrategies. A variety of other possible evolutionary dynamics are\ndescribed and compared in Kuhn 2004. Discussion here, however, will\nprimarily concern EPDs with the proportional fitness rule. \nAxelrod, borrowing from Trivers and Maynard Smith, includes a\ndescription of the EPD with proportional fitness, and a brief analysis\nof the evolutionary version of his IPD tournament. For Axelrod, the\nEPD provides one more piece of evidence in favor of\nTFT: \nAxelrod's EPD tournament, however, incorporated several features that\nmight be deemed artificial. First, it permitted deterministic\nstrategies in a noise-free environment. As noted above,\nTFT can be expected to do worse under conditions that\nmodel the inevitability of error. Second, it began with only the 63\nstrategies from the original IPD tournament. Success against\nstrategies concocted in the ivory tower may not imply success against\nall those that might be found in nature. Third, the only strategies\npermitted to compete at a given stage were the survivors from the\nprevious stage. A more realistic model, one might argue, would allow\nnew “mutant” strategies to enter the game at any stage.\nChanging this third feature might well be expected to hurt\nTFT. For a large growth in the TFT\npopulation would make it possible for mutants employing more naive\nstrategies like \\(\\bCu\\) to regain a foothold, and the presence of\nthese naifs in the population might favor nastier strategies like\n\\(\\bDu\\) over TFT. \nNowak and Sigmund simulated two kinds of tournaments that avoid the\nthree questionable features. The first examined the family of\n“reactive” strategies. For any probabilities \\(y\\), \\(p\\),\nand \\(q\\), \\(\\bR(y,p,q)\\) is the strategy of cooperating with\nprobability \\(y\\) in the first round and thereafter with probability\n\\(p\\) if the other player has cooperated in the previous round, and\nwith probability \\(q\\) if she has defected. This is a broad family,\nincluding many of the strategies already considered. \\(\\bCu\\),\n\\(\\bDu\\), TFT, and Cp are\n\\(\\bR(1,1,1)\\), \\(\\bR(0,0,0)\\), \\(\\bR(1,1,0)\\), and \\(\\bR(p,p,p)\\).\nGTFT, when payoffs are \\(5,3,1,0\\), is\n\\(\\bR(1,1,.25)\\). To capture the inevitability of error, Nowak and\nSigmund exclude the deterministic strategies, where \\(p\\) and \\(q\\)\nare exactly \\(1\\) or \\(0\\), from their tournaments. As before, if the\ngame is sufficiently long (and \\(p\\) and \\(q\\) are not integers), the\nfirst move can be ignored and a reactive strategy can be identified\nwith its \\(p\\) and \\(q\\) values. Particular attention is paid to the\nstrategies close to Molander's GTFT described above,\nwhere \\(p=1\\) and \\(q = \\min \\{1-(T-R)/(R-S), (R-P)/(T-P)\\}\\). The\nfirst series of Nowak and Sigmund's EPD tournaments begin with\nrepresentative samples of reactive strategies. For most such\ntournaments, they found that evolution led irreversibly to \\(\\bDu\\).\nThose strategies \\(\\bR(p,q)\\) closest to \\(\\bR(0,0)\\) thrived while\nthe others perished. When one of the initial strategies is very close\nto TFT, however, the outcome changes. \nOn the basis of their tournaments among reactive strategies, Nowak and\nSigmund conjectured that, while TFT is essential for\nthe emergence of cooperation, the strategy that actually underlies\npersistent patterns of cooperation in the biological world is more\nlikely to be GTFT. \nA second series of simulations with a wider class of strategies,\nhowever, forced them to revise their opinion. The strategies\nconsidered in the second series allowed each player to base its\nprobability of cooperation on its own previous move as well as its\nopponent's. A strategy can now be represented as \\(\\bS(p_1, p_2. p_3,\np_4)\\) where \\(p_1, p_2, p_3, p_4\\) are the probabilities of\ncooperating after outcomes \\((\\bC, \\bC)\\), \\((\\bC, \\bD)\\), \\((\\bD,\n\\bC)\\), and \\((\\bD, \\bD)\\), respectively i.e., after receiving the\nreward, sucker, temptation and punishment payoffs. (Again, we can\nignore the probability of defecting on the first move as long as the\n\\(p_i\\) s are not zero or one.) The initial population in these\ntournaments all play the random strategy \\(\\bS(.5, .5, .5, .5)\\) and\nafter every 100 generations a small amount of a randomly chosen\n(non-deterministic) mutant is introduced, and the population evolves\nby proportional fitness. The results are quite different than before.\nAfter \\(10^7\\) generations, a state of steady mutual cooperation was\nreached in \\(90\\%\\) of the simulation trials. But less than \\(8.3\\%\\)\nof these states were populated by players using TFT\nor GTFT. The remaining \\(91.7\\%\\) were dominated by\nstrategies close to \\(\\bS(1,0,0,1)\\). This is the just the Pavlovian\nstrategy \\(\\bP_1\\) of Kraines and Kraines, which replays its last move\nafter receiving \\(R\\) or \\(T\\) and changes to the other move after\nreceiving \\(P\\) or \\(S\\). Kraines and Kraines had been somewhat\ndismissive of \\(\\bP_1\\). They recall that Rapoport and Chammah, who\nidentified it early in the history of game theory had labeled it\n“simpleton” and remark that “the appellation is well\ndeserved”. Indeed, \\(\\bP_1\\) has the unfortunate characteristic\nof trying to cooperate with \\(\\bDu\\) on every other turn, and against\nTFT it can get locked into the inferior repeating\nseries of payoffs \\(T, P, S, T, P, S, \\ldots\\). But Nowak and Sigmund\nrename the strategy “win-stay lose-shift” and trumpet its\nadvantages. Their simulations suggest that the defects mentioned here\ndo not matter very much in evolutionary contexts. One reason may be\nthat \\(\\bP_1\\) helps to make its environment unsuitable for its\nenemies. \\(\\bDu\\) does well in an environment with generous\nstrategies, like \\(\\bCu\\) or GTFT.\nTFT, as we have seen, allows these strategies to\nflourish, which could pave the way for \\(\\bDu\\). Thus, although\nTFT fares less badly against \\(\\bDu\\) than \\(\\bP_1\\)\ndoes, \\(\\bP_1\\) is better at keeping its environment free of\n\\(\\bDu\\). \nSimulations in a universe of deterministic strategies yield results\nquite different than those of Nowak and Sigmund. Bruce Linster (1992\nand 1994) suggests that natural classes of strategies and realistic\nmechanisms of evolution can be defined by representing strategies as\nsimple Moore machines. For example, \\(\\bP_1\\) is represented\nby the machine pictured below. \nFigure 8 \nThis machine has two states, indicated by circles. It begins in the\nleftmost state. The \\(\\bC\\) in the left circle means that the machine\ncooperates on the first move. The arrow leading from the left to the\nright circle indicates that machine defects (enters the \\(\\bD\\)) after\nit has cooperated (been in the \\(\\bC\\) state) and its opponent has\ndefected (the arrow is labeled by \\(d\\)). Linster has conducted\nsimulations of evolutionary PD's among the strategies that can be\nrepresented by two-state Moore machines. It turns out that these are\nexactly the deterministic versions of the \\(\\bS\\) strategies of Nowak\nand Sigmund. Since the strategies are deterministic, we must\ndistinguish between the versions that cooperate on the first round and\nthose that defect on the first round. Among the first round\ncooperators, \\(\\bS(1,1,1,1), \\bS(1,1,1,0), \\bS(1,1,0,1)\\) and\n\\(\\bS(1,1,0,0)\\) all represent the strategy \\(\\bCu\\) of unconditional\ncooperation. Similarly, four of the first-round defectors all\nrepresent \\(\\bDu\\). Each of the other \\(\\bS(p_1, p_2, p_3, p_4)\\),\nwhere \\(p_1,p_2,p_3,p_4\\) are either zero or one represent unique\nstrategies, and each comes in two varieties according to whether it\ncooperates or defects in round one. By deleting the six duplicates\nfrom these thirty-two deterministic versions of Nowak and Sigmund's\nstrategies, we obtain the twenty-six “two-state”\nstrategies considered by Linster. \nLinster simulated a variety of EPD tournaments among the two-state\nstrategies. Some used “uniform mutation” in which each\nstrategy in the population has an equal probability \\(m\\) of mutating\ninto any of the other strategies. Some used “stylized\nmutation” in which the only mutations permitted are those that\ncan be understood as the result of a single “broken link”\nin the Moore machine diagrams. In some, mutations were assumed to\noccur to a tiny proportion of the population at each generation; in\nothers the “mutants” represented an invading force\namounting to one percent of the original population. In some, a\npenalty was levied for increased complexity in the form of reduced\npayoffs for machines requiring more states or more links. As one might\nexpect, results vary somewhat depending on conditions. There are some\nstriking differences, however, between all of Linster's results and\nthose of Nowak and Sigmund. In Linster's tournaments, no single\nstrategy ever dominated the surviving populations in the way that\n\\(\\bP_1\\) and GTFT did in Nowak and Sigmund's. The\none strategy that did generally come to comprise over fifty percent of\nthe population was the initially-cooperating version of\n\\(\\bS(1,0,0,0)\\). This is a strategy whose imperfect variants seem to\nhave been remarkably uncompetitive for Nowak and Sigmund. It has been\nfrequently discussed in the game theory literature under the label\nGRIM or TRIGGER. It cooperates until\nits opponent has defected once, and then defects for the rest of the\ngame. According to Skyrms (1998) and Vanderschraaf, both Hobbes and\nHume identified it as the strategy that underlies our cooperative\nbehavior in important PD-like situations. The explanation for the\ndiscrepancy between GRIM's strong performance for\nLinster and its poor performance for Nowak and Sigmund probably has to\ndo with its sharp deterioration in the presence of error. In a match\nbetween two imperfect GRIMs, an\n“erronious” defection by either leads to a long string of\nmutual defections. Thus, in the long run imperfect\nGRIM does poorly against itself. The other strategies\nthat survived (in lesser numbers) Linster's tournaments are\nTFT, \\(\\bP_1\\), \\(\\bCu\\), and the\ninitially-cooperative \\(\\bS(1,0,1,1)\\). (Note that imperfect\nGRIM is also likely to do poorly against imperfect\nversions of these.) The observation that evolution might lead to a\nstable mix of strategies (perhaps each serving to protect others\nagainst particular types of invaders) rather than a single dominant\nstrategy is suggestive. Equally suggestive is the result obtained\nunder a few special conditions in which evolution leads to a recurring\ncycle of population mixes. \nOne might expect it to be possible to predict the strategies that will\nprevail in EPDs meeting various conditions, and to justify such\npredictions by formal proofs. Until recently, however, mathematical\nanalyses of the EPD have been plagued by conceptual confusions about\n“evolutionary stability,” the condition under which, as\nNowak and Sigmund say, “evolution stops”. Axelrod and\nAxelrod & Hamilton claim to show that TFT is\nevolutionarily stable. Selten 1983, includes an example of a game with\nno evolutionarily stable strategy, and Selten's argument that there is\nno such strategy clearly applies to the EPD and other\nevolutionary games. Boyd and Lorberbaum and Farrell and Ware present\nstill different proofs demonstrating that no strategies for the\nEPD are evolutionarily stable. Unsurprisingly, the\nparadox is resolved by observing that the three groups of authors each\nemploy slightly different conceptions of evolutionary stability. The\nconceptual tangle is unraveled in a series of papers by Bendor and\nSwistak. Two central stability concepts are described and applied to\nthe EPD below. Readers who wish to compare these with some others that\nappear in the literature may consult the following brief guide: \nA strategy \\(\\bs\\) for an evolutionary game has universal strong\nnarrow stability (“usn-stability”) if a population\nplaying strategy \\(\\bs\\) will, under any rule of evolution, drive to\nextinction any sufficiently small group of invaders all of which play\nthe same strategy. An evolutionary game has usn-stability just in case\nit meets a simple condition on payoffs identified by Maynard\nSmith: \n(Here, and in what follows, the notation \\(V(\\bi,\\bj)\\) indicates the\npayoff to strategy \\(\\bi\\) when \\(\\bi\\) plays \\(\\bj\\).) MS says that\nany invaders do strictly worse against the natives than the natives\nthemselves do against the natives or else they get exactly the same\npayoff against the natives as the natives themselves do, but the\nnative does better against the invader than the invader himself\ndoes. \nFor any strategy \\(\\bi\\) in the IPD (or indeed in any iterated finite\ngame), however, there are strategies \\(\\bj\\) different from \\(\\bi\\)\nsuch that \\(\\bj\\) mimics the way \\(\\bi\\) plays when it plays against\n\\(\\bi\\) or \\(\\bj\\). The existence of these “neutral\nmutants” implies that MS cannot be satisfied and so no EPD has\nusn-stability. This argument, of course, uses the assumption that any\nstrategy in the iterated game is a possible invader. There may be good\nreason to restrict the available strategies. For example, if the\nplayers are assumed to have no knowledge of previous interactions,\nthen it may be appropriate to restrict available strategies to the\nunconditional ones. Since a pair of players then get the same payoffs\nin every round of an iterated game, we may as well take each round of\nthe evolutionary game to be one-shot games between every pair of\nplayers rather than iterated games. Indeed, this is the kind of\nevolutionary game that Maynard Smith himself considered. In this\nframework, any strategy \\(\\bS\\) such that \\((\\bS,\\bS)\\) is a strict\nnash equilibrium in the underlying one-shot game (including\nunconditional defection in the PD) meets the MS condition. Thus MS and\nusn-stability are non-trivial conditions in some contexts. \nA strategy \\(\\bs\\) has restricted weak broad stability)\n(rwb-stability) if, when evolution proceeds according to the\nproportional fitness rule and the native population is playing\n\\(\\bs\\), any (possibly heterogeneous) group of invaders of\nsufficiently small size will fail to drive the natives to extinction.\nThis condition turns out to be equivalent to a weakened version of MS\nidentified by Bendor and Swistak. \nBS and rwb-stability are non-trivial conditions in the more general\nevolutionary framework: strategies for the EPD that satisfy\nrwb-stability do exist. This does not particularly vindicate any of\nthe strategies discussed above, however. Bendor and Swistak prove a\nresult analogous to the folk theorem mentioned previously: If the\nshadow of the future is sufficiently large, there are rwb-stable\nstrategies supporting any degree of cooperation from zero to one. One\nway to distinguish among the strategies that meet BS is by the size of\nthe invasion required to overturn the natives, or, equivalently, by\nthe proportion of natives required to maintain stability. Bendor and\nSwistak show that this number, the minimal stabilizing\nfrequency, never exceeds 1/2: no population can resist every\ninvading group as large as itself. They maintain that this result does\nallow them to begin to provide a theoretical justification for\nAxelrod's claims. They are able to show that, as the shadow of the\nfuture approaches one, any strategy that is nice (meaning that it is\nnever first to defect) and retaliatory (meaning that it always defects\nimmediately after it has been defected against) has a minimal\nstabilizing frequency approaching one half. TFT has\nboth these properties and, in fact, they are the first two of the four\nproperties Axelrod cited as instrumental to TFT's\nsuccess. There are, of course, many other nice and retaliatory\nstrategies, and there are strategies (like \\(\\bP_1\\)) that are not\nretaliatory but still satisfy rwb-stability. But Bendor and Swistak\nare at least able to show that any “maximally robust”\nstrategy, i.e., any strategy whose minimum stabilizing frequency\napproaches one half, chooses cooperation on all but a finite number of\nmoves in an infinitely repeated PD. \nBendor and Swistak's results must be interpreted with some care.\nFirst, one should keep in mind that no probabilistic or\nnoise-sensitive strategies can fit the definitions of either\n“nice” or “retaliatory” strategies.\nFurthermore, imperfect versions of TFT do not satisfy\nrwb-stability. They can be overthrown by arbitrarily small invasions\nof deterministic TFT or, indeed, by arbitrary small\ninvasions of any less imperfect TFT. Second, one must\nremember that the results about minimal stabilizing frequencies only\nconcern weak stability. If the number of generations is large compared\nwith the original population (as it often is in biological\napplications), a population that is initially composed entirely of\nplayers employing the same maximally robust strategy, could well admit\na sequence of small invading groups that eventually reduces the\noriginal strategy to less than half of the population. At that point\nthe original strategy could be overthrown. \nIt is likely that both of these caveats play some role in explaining\nan apparent discrepancy between the Bendor/Swistak results and the\nNowak/Sigmund simulations. One would expect Bendor/Swistak's minimal\nstabilizing frequency to provide some indication of the length of time\nthat a population plays a particular strategy. A strategy requiring a\nlarge invasion to overturn is likely to prevail longer than a strategy\nrequiring only a small invasion. A straightforward calculation reveals\nthat \\(\\bP_1\\) has a relatively low minimum stabilizing frequency. It\nis overturned by invasions of unconditional defectors exceeding\n\\(10\\%\\) of the population. Yet in the Nowak/Sigmund simulations,\n\\(\\bP_1\\)-like strategies predominate over TFT-like\nstrategies. Since the simulations required imperfection and since they\ngenerated a sequence of mutants vastly larger than the original\npopulation, there is no real contradiction here. Nevertheless the\ndiscrepancy suggests that we do not yet have a theoretical\nunderstanding of EPDs sufficient to predict the strategies that will\nemerge under various plausible conditions. \nLike usn-stability, the concept of rwb-stability can be more\ndiscriminating if it is relativized to a particular set of strategies.\nMolander's 1992 investigation of Schelling's many-person version of\nthe PD, for example, restricts attention to the family\n\\(\\{\\bS_1,\\ldots,\\bS_n\\}\\) of TFT-like stratgies. A\nplayer adopting \\(\\bS_i\\) cooperates on the first round and on every\nsubsequent round after at least \\(i\\) others cooperate. By construing\nstability as resistance to invasions by other family members, Molander\nis able to show that there are conditions under which a particular mix\nof two of the \\(\\bS_i\\)'s (one equivalent to \\(\\bDu\\)) is uniquely\nstable. The significance of results like these, however, depends on\nthe plausibility of such limitations on the set of permissible\nstrategies. \nIn iterated and evolutionary versions of the optional PD, we imagine\nthat players from some population are repeatedly paired off and given\nthe opportunity to play the PD (choosing either \\(\\bC\\) or \\(\\bD\\)) or\nto opt out (choosing \\(\\bN\\)). In choosing \\(\\bN\\), a player forgoes\nthe opportunity of receiving the reward or temptation payoffs until\nthe next pairing. In most human interactions that come to mind, a\nrefusal to engage with a particular partner does not represent quite\nthe same loss of opportunity to engage with another as a choice to\nengage does. If I buy a car from an unscrupulous dealer, I'll have to\nwait a long time before my next car purchase to do better; but if I\nrefuse to engage with her I can immediately begin negotiating with a\nneighboring dealer. Nevertheless, there may be situations among people\n(and more likely among non-human animals or among nations or\ncorporations) that are appropriately modeled by evolutionary versions\nof the optional PD. \nWe can represent the strategies for the evolutionary optional PD that\ndo not require memory of previous interaction as triples \\(\\langle\np,q,r \\rangle\\), where \\(p\\), \\(q\\) and \\(r\\) are real numbers adding\nto one, representing the probability of playing \\(\\bC\\), \\(\\bD\\) and\n\\(\\bN\\). None of these strategies meets the BS condition, and so no\nstrategy is rwb-stable within this family. If all the members of a\ngroup unconditionally refuse to engage (adopting \\(\\langle 0,0,1\n\\rangle\\)), then they can be infiltrated and eventually replaced by\nsmall invasions of more cooperative strategies. The cooperative\nstrategies, in turn, will be overthrown by defecting strategies, and,\nwhen the concentration of defectors is sufficiently great, the\n“loners” who refuse to engage can again take over. Adding\nthe option of not-playing to the evolutionary PD does permit escape\nfrom from the unhappy state of universal defection, but leads to an\nonly slightly less undesirable outcome in which a population cycles\nrepeatedly through states of universal non-engagement. (Szabó\nand Hauert contains a nice picture of the phenomenon.) \nAmong strategies that do allow dependence on previous interaction,\nBatali and Kitcher like an analog of GRIM that they\ncall Discriminating Altruist (henceforth\nDA). DA cooperates with any player\nthat has never defected against it, and otherwise refuses to engage.\nThey show that, among a group of five simple strategies, there will be\na cyclic pattern like that described above in which\n“anti-social” (defecting) strategies are replaced by\n“asocial” (non-engaging) strategies, which are replaced in\nturn by “social” (DA) strategies, which are replaced again\nby “anti-social” strategies. Their analysis, however,\nleads them to conclude that if members of a population are restricted\nto these five strategies, evolution will lead them to spend\n“most of their time in states of high cooperation,”\n(though not as much as would be the case in a “fully\noptional” game where, in each round, only those who accurately\nsignal a willingness to engage are paired). Simulations among agents\nwho are permitted any strategies where a move depend on the two\nprevious moves of its opponent are said to provide rough\ncorroboration. Some caution is in order here. There is little analysis\nof which strategies underlie the cooperating populations in the\nsimulations and, indeed, DA is not an option for an\nagent whose memory goes back only two games. Oddly, slightly\nless cooperativity is reported for the fully optional version\nof the game than for the semi-optional (though in each case, as would\nbe expected, cooperativity is significantly greater tha n for the\nordinary PD). The evolutionary dynamics employed and the measures of\ncooperativity employed are sufficiently idiosyncratic to make\ncomparisons with other work difficult. Despite all these caveats, it\nseems safe to conclude that taking engagement to be optional can\nprovide another explanation for the fact that universal, unrelenting\ndefection is rarely seen in patterns of interaction sometimes modeled\nas evolutionary PDs. \nWhen Kendall et al began organizing their IPD tournaments to mark the\n20th anniversary of the publication of Axelrod's influential book,\nthey received an innocent-seeming inquiry: could one entrant make\nmultiple submissions? If they did not immediately realize the\nsignificance of this question, they must surely have done so when a\ngroup from the Technical University of Graz attempted to enter more\nthan 10,000 individually named strategies to the first tournament.\nMost of these aspiring entries were disallowed. As it turned out,\nhowever, the winning strategy came from a group from the University of\nSouthhampton who themselves submitted over half of the 223 strategies\nthat were allowed. As the groups from Graz and Southhampton realized\n(and participants in Axelrod's earlier tournament apparently did not),\na good way to win a round-robin IPD is to accompany one's entrant with\nan army of “enablers” that boost its score relative to\nothers. In extreme form, the master strategy and its enablers begin by\nplaying a short code sequence of \\(\\bC\\) and \\(\\bD\\) moves by which\neach can be identified by either. Thereafter the enablers always\ncooperate against the master (allowing themselves to be exploited) and\ndefect against all others (thereby lowering scores of the master's\ncompetitors). The master defects against the enablers and plays a\nreasonable strategy (like TFT) against all others.\nUnder these circumstances the score of the master depends on only two\nfactors: the size of its enabling army, and the accuracy and cost of\nthe identifying code sequence. Accuracy is less than perfect if an\noutside strategy “accidentally” begins a game with the\ncode sequence. Cost is the payoff value lost by using early moves to\nsignal one's identity rather than to follow a more productive\nstrategy. Longer codes produce greater accuracy at greater cost.\nComing to a better appreciation of these ideas, Kendall et al\norganized additional tournaments in 2005, one restricting each author\nto a single entry and another restricting each author to a team of\ntwenty entries (though, as Slany and Klienrich observe, such\nrestrictions are difficult or impossible to enforce.) \nOne may well wonder whether this sort of signaling and team play has\nany importance beyond showing competitive scholars how to win\nround-robin IPD tournaments. In an evolutionary setting armies of\nenablers would rapidly head towards extinction, leaving a master\nstrategy to face its high-scoring competitors alone. There are, of\ncourse, examples among both animal species and human societies of\nsuccessful “teams” among which large numbers are exploited\nfor the benefit of few. Presumably, in these cases the exploiters\ntransfer enough to the exploited to ensure the latter's continued\navailability. Perhaps such payoff transfers within teams should be\npermitted in IPD tournaments intended to explore these issues. Even\nwithout such rule changes, however, there are less extreme forms of\nteam play that would perform better in an evolutionary setting. If one\nallows enablers to recognize and cooperate with one another, they will\ngain considerably with no loss to their master, except when an enabler\nwrongly identified an outsider as one of its kind. If one allowed them\nto play reasonable strategies against outsiders they would gain still\nmore, though the risk to their master (through outsiders' gain) would\nbe considerably greater. Even without allowing themselves to be\nexploited by a master, teams could benefit by playing \\(\\bC\\) among\nthemselves and \\(\\bD\\) with outsiders, or \\(\\bC\\) among themselves and\na reasonable strategy against outsiders. Slany and Kienreich (the Graz\ngroup) label these approaches EW, EP, DW, and DP and observe (among\nother properties) that for teams of equal and sufficiently large size\nthis order mirrors the order of the best-performing member of the team\nfrom best to worst.  \nThe possibility of error raises special difficulties for team play\nwith signaling of this kind: an incorrect signal could be accidentally\nsent, or a correct signal could be misintepreted. Rogers et al (the\nSouthampton group) realized that the problem of sending and receiving\nsignals when error is possible is a well-studied problem in computer\nscience: reliable communication over a noisy channel. In both 2004 and\n2005 one of the IPD tournaments organized by Kendall et al introduced\nnoise to simulate the possibility of error. By employing some of the\nstandard error-correcting codes designed to deal with communication\nover a noisy channel as their signaling protocol, the Southampton\ngroup won both with a comfortable margin.  \nIn IPD tournaments like those of Axelrod and Kendall et al, players\nknow nothing about each other except their moves in the game, and so\nthey can use no other information to signal their membership in a\ngroup. In the real world it would seem much more likely that other\navenues of communication would be available. The notion that\ncooperative outcomes might be facilitated by such communication among\nplayers is an old idea in game theory. Santos et al show how this\nmight be possible. Their work borrows from an influential paper by\nArthur Robson (1990). Imagine an evolutionary game, whose underlying\npayoff structure may be a stag hunt or a PD, in which all players can\nchoose only between (unconditioned) moves \\(\\bC\\) and \\(\\bD\\). If the\nunderlying game is a PD the population will stablize with universal\ndefection. (If it is stag hunt it might stabilize either with everyone\nplaying \\(\\bD\\) (the inferior equilibrium) or everyone playing \\(\\bC\\)\n(the superior equilibrium). To illustrate the beneficial possibilities\nof communication, let us suppose the former.) Now suppose a small\ngroup of mutants enter the population who make a signal (the\n“secret handshake”) and play \\(\\bC\\) against those who\nsignal and \\(\\bD\\) against all others. Since these players do as well\nas the originals against ousiders and better against themselves, they\nwill soon take over the population. (And this would not be true of the\nother possible mutants with similar resources, like those signaling\nand playing \\(\\bD\\) against themselves and \\(\\bC\\) against outsiders.)\nSo communications does seem to facilitate cooperation. If the\nunderlying game is a PD, however, once the new uniform cooperating\npopulation has taken over, it is itself vulnerable. It can be invaded\nand supplanted by a “deceiving”) invader who signals and\ndefects against signallers. The resulting population can then be\ninfiltrated (but not supplanted) by other, non-signaling, defectors.\nSo Robson concluded that signaling could move a population from the\ninferior equilibrium to the superior one in an evolutionary stag hunt,\nbut could only delay the establishment of universal defection in a PD.\nSantos et al observe, however, that, if a second signal is available,\nthe universally defecting population could be supplanted by a small\ngroup of mutants using it as new secret handshake. Of course this\ngroup is itself vulnerable to mutants who mimic the second signal\nwhile defecting against all. In this case however, the resulting dark\nage may no longer be permanent. If a mutant group of signal-one\nhandshakers re-emerges before any signal-one defectors have drifted\ninto the population, they will again take over, and the cycle will be\nrepeated. (If a third signal were available, of course, the return of\ncooperation would be even easier.) The story is no longer one where\n“evolution stops”: rather the population cycles between\nstates of universal defection and universal cooperation. The time\nspent in each state depends on the payoffs of the PD and the number of\navailable signals. Santos et al demonstrate, however, that, for finite\npopulations with sufficiently slow mutation rates and large numbers of\navailable signals cooperation predominates in EPDs with signaling.\n \nA previous section discussed a controversial argument that cooperation\nis rational in a PD when each player knows that the other is enough\nlike himself to make it likely that they will choose the same move. An\nanalog of this argument in the evolutionary context is more obviously\ncogent. If agents are not paired at random, but rather are more likely\nto play others employing similar strategies, then cooperative behavior\nis more likely to emerge. \nThere are at least three mechanisms by which this kind of\n“association” among players can be achieved. One such\nmechanism in evolutionary PDs has been widely studied under the label\n“spatial PD.” Players are arranged in some\n“geographical” arrangement. This may be an array with a\nrectangular boundary, for example, or a circle, or surface of a sphere\nor surface of a torus with no boundary. From the geographical\narrangement two (possibly identical) kinds of neighborhoods are\nidentified for each player. Agents meet only those in their\n“interaction” neighborhood, and the evolutionary dynamics\nconsiders only the payoffs to those in their “comparison”\nneighborhood. Generally, the evolutionary dynamics employed is one of\n“winner imitation” within the interaction neighborhood.\n(This can model either the idea that each player is invaded by its\nmost successful neighbor or the idea that each player adopts the most\nsuccessful strategy that it sees.) Because both evolution and\ninteraction are “local,” players are more likely (after\nthe first round) to meet those playing strategies like their own in an\nSPD than they would be in an ordinary evolutionary game. In addition\nto the “association” effects, one should also keep in mind\nthat the outcome of SPDs may be influenced by winner imitation\ndynamics, which may drive to extinction strategies that might\nsurvive—and eventually predominate—with the replicator\ndynamics more commonly employed in ordinary EPDs. \nAs usual, the impetus for looking at spatial SPDs seems to come from\nAxelrod. Four copies of each of the 63 strategies submitted to\nAxelrod's tournament were arranged on a grid with a spherical geometry\nso that each cell had four neighbors for both interaction and\ncomparison. For every initial random distribution, the resulting SPD\neventually reached a state where the strategy in every cell was\ncooperating with all its neighbors, at which point no further\nevolution is possible. In these end-states only about ten of the 63\noriginal strategies remained. The remaining strategies were no longer\nrandomly distributed, but segregated into clumps of various sizes.\nAxelrod also showed that under special conditions evolution in an SPD\ncan create successions of complex symmetrical patterns that do not\nappear to reach any steady-state equilibrium. \nTo get an idea of why cooperative behavior might spread in this and\nsimilar frameworks, consider two agents on either side of a frontier\nbetween cooperating and non-cooperating subpopulations. The\ncooperative agent sees a cooperative neighbor whose four neighbors all\ncooperate, and who therefore gets four times the reward payoff after\nplaying them all. So he will imitate this neighbor's strategy and\nremain cooperative. The non-cooperating agent, on the other hand, sees\nhis cooperative counterpart, who gets three reward payoffs from his\ncooperative neighbors and one sucker payoff. He compares this to the\npayoffs of his non-cooperatiave neighbors. The best these can do is to\nget three punishments and a temptation. So, as long as \\(3R+S\\)\nexceeds \\(3P+T\\), the non-coperative agent on the frontier will adopt\nthe strategy of his cooperative neighbor. Axelrod's payoffs of 5, 3, 1\nand 0 for \\(T\\), \\(R\\), \\(P\\), and \\(S\\), do meet this condition. \nNowak and May have investigated in greater detail SPDs in which the\nonly permitted strategies are \\(\\bCu\\) and \\(\\bDu\\). (These are the\nstrategies appropriate among individuals lacking memory or recognition\nskills.) They find that, for a variety of spatial configurations and\ndistributions of strategies, evolution depends on relative payoffs in\na uniform way. When the temptation payoff is sufficiently high,\nclusters of \\(\\bDu\\) grow and those of \\(\\bCu\\) shrink; when it is\nsufficiently low, the \\(\\bDu\\) clusters shrink and the \\(\\bCu\\)\nclusters grow. For a narrow range of intermediate values, we get\nsuccessions of complex patterns like those noted by Axelrod. The\nevolving patterns exhibit great variety. For a given spatial\nconfiguration, however, the ratio \\(\\bCu\\) to \\(\\bDu\\) strategies\nseems to approach the same constant value for all initial\ndistributions of strategies and all temptation payoffs within the\nspecial range. The idea that these simulations partially explain the\npersistence of cooperation in nature has been questioned on the\ngrounds that they assume deterministic (error-free) moves and updates.\nBut the authors report similar phenomena under a variety of\nerror-conditions, although lower relative temptation values are then\nrequired for the survival of the cooperators, and the level of error\ncannot exceed a certain threshold. (See Mukherjii et al, and the reply\nby Nowak et al that immediately follows it.) \nGrim, Mar and St Denis report a number of SPD simulations with a\ngreater variety of initial strategies. In general their observations\nconfirm the plausible conjecture that cooperative outcomes are more\ncommon in SPDs than ordinary EPDs. Simulations starting with all of\nthe pure reactive strategies of Nowak and Sigmund (i.e., all of the\nstrategies \\(\\bR(y,p,q)\\) described above where \\(y\\), \\(p\\), and\n\\(q\\) are either 0 or 1.), all ended with\nTFT—i.e., with \\(\\bR(1,1,0)\\)—as the only\nsurvivor (though other outcomes—including one in which \\(\\bDu\\)\nis the sole survivor and ones in which \\(\\bCu\\) and\nTFT are intermixed—are clearly possible.)\nSimulations starting with all of the 64 possible pure strategies in\nwhich a move may depend on the opponent's previous two moves,\nended with mixed populations of survivors employing a variety of\nTFT-like strategies. They all defect after a double\ndefection (\\(\\bD\\)\\(\\bD\\)), though not necessarily after a single\ndefection \\(\\bC\\)\\(\\bD\\) or \\(\\bD\\)\\(\\bC\\)); they all cooperate after\na double cooperation, though not necessarily after a single\ncooperation; they all cooperate in the second round of the game,\nthough not necessarily in the first). (Again, other outcomes are\npossible.) Simulations starting with many (viz., 100) evenly\ndistributed samples of Nowak and Sigmunds mixed reactive strategies,\ntended to be taken over by \\(\\bR(.99,.1)\\), which is a version of\ngenerous TFT with less than half the generosity of\nGTFT. Simulations beginning with a random selection\nof a few (viz., 8) of these strategies tended to evolve to a mixed\nstable or cyclic pattern dominated by a single version of generous\nTFT with considerably more generosity than\nGTFT. \\(\\bR(.99,.6)\\), which is more than twice as\ngenerous as GTFT, seems to have been a frequent\nvictor. \nSzabó and Hauert have investigated spatial versions of the\noptional PD in some detail. Among their findings is that, for a\nparticular (intermediate) range of payoffs, a population of agents\nplaying “pure” strategies on a square lattice will evolove\ntowards a unique equilibrium in which all three strategies are\npresent. This contrasts with the continuous cycles for the\nnon-spatialized versions of the evolutionary optional PD's discussed\nabove. Like the earlier observations, it may help to explain how a\ngroup might achieve a state other than universal defection, but not\nhow it might achieve a state of universal cooperation. \nThe “geographical” aspect of SPD's need not be taken too\nliterally. For social applications, and probably even for many\nbiological ones, there seems to be no motivation for any particular\ngeometrical arrangement. (Why not a “honeycomb,” for\nexample, where each agent has six neighbors, rather than a grid where\neach agent has four or eight?) The interest in SPDs presumably lies in\nthe insight that my “neighborhood” of interaction and my\n“neighborhood” of comparison, is much smaller than the\npopulation as a whole even if it turns out not to be limited by\ndetails of physical geography. Nevertheless SPD models of the\nevolution of cooperation in particular geometrical arrangements have\ngiven us some suggestive and pretty pictures to contemplate. Several\nexamples are accessible through the links at the end of this\nentry. \nOne way to make the idea of local interaction more realistic for some\napplications is to let agents choose the partners with whom\nto interact, based on payoffs in past interactions. Skyrms 2004\nconsiders iterated PDs among a population of unconditional cooperators\nand defectors. Initially, as usual, each agent chooses a partner at\nrandom from the remaining members of the population. For subsequent\ninteractions, however, the odds of choosing that partner are adjusted,\naccording to either the payoffs from previous times that partner was\nchosen, or (more realistically) the payoffs from previous times that\nthere has been interaction with that partner (regardless of which one\nwas “chooser”). In a typical PD, where the payoffs for\ntemptation, reward, punishment and sucker are 3, 2, 1 and 0, both\ncooperators and defectors eventually choose only cooperators. Since\nthe cooperators are chosen by both cooperators and defectors, they\nplay more often than the defectors who play only when they are doing\nthe choosing. If we assume that there is an equal division between\ncooperators and defectors, then cooperators can expect a return of one\nreward payoff when they are the chooser, and a fifty/fifty mix of of\nreward and sucker payoffs when they are the chosen. So their expected\npayoff for each interaction will be \\((3R+S)/2\\). Defectors can expect\na return of one temptation payoff per play, but they play half as\noften. With payoff structure indicated, \\(3R+S \\gt T\\), and so\ncooperators do better, even with this “one-way”\nassociation.  \nThe story may unfold somewhat differently in what Skyrms calls an\n“attenuated” PD, where the payoffs are, let us say, 2.01,\n2, 1.98, and 0. (We might think of this as the “Just Don't Be a\nSucker” game.) Here, as before, the cooperators quickly learn\nnot to choose defectors as partners. The defectors get roughly the\nsame payoffs whether they choose cooperators or defectors as partners.\nSince they rapidly cease being chosen by cooperators, however, their\nreturns from interactions with cooperators will be less than returns\nfrom defectors and they will soon limit their choices to other\ndefectors. (It is important to understand here that the learning\nalgorithm that determines the probability I will interact with agent\n\\(\\ba\\) depends on total returns from interacting with\n\\(\\ba\\)(or total recent returns from interacting with\n\\(\\ba\\)) rather than average returns from interacting with\n\\(\\ba\\).) So in the attenuated game we end up with perfect\nassociation: defectors play defectors and cooperators play\ncooperators. Since the reward payoff slightly exceeds the punishment\npayoff, the cooperators again do better than the defectors. \nThe social network games considered above are not really evolutionary\nPDs in the sense described above. The patterns of interaction evolve,\nbut the strategy profile of the population remains fixed. It is\nnatural to allow both strategies and probabilities of interaction to\nevolve simultaneously as payoffs are distributed. Whether cooperation\nor defection (or neither) comes to dominate the population under such\nconditions depends on a multitude of factors: the values of the\npayoffs, the initial distribution of strategies, the relative speed of\nthe adjustments in strategy and interaction probabilities, and other\nproperties of those two evolutionary dynamics. Skyrms 2004 contains a\ngeneral discussion and a number of suggestive examples, but it does\nnot provide (or aim to provide) a comprehensive account of social\nnetwork PDs or a careful analysis of precise formulations to properly\nmodel particular phenomena. Much remains unknown. \nIn a social network game, agents choose from a population of potential\nopponents; in the version of the IPD that interested Axelrod, agents\nmust play every other member of the population of which they are a\npart. The original description of the IPD by Dresher and Flood,\nhowever, concerned a single pair of players who repeatedly play the\nsame PD game. In a brief, but influential, paper a pair of\ndistinguished physicists, William Press and Freeman Dyson, recently\nreturned attention to this original version of the IPD, or rather to\nthe infinitely repeated version of it. \nLet's call this version of the game the (infinite) two-player IPD, or\n2IPD. In other versions of the IPD, where pairs from a larger\npopulation come together repeatedly to play the game, a successful\nstrategy is one that scores well. “Well” may mean (as in\nthe case of evolution under the replicator dynamic) a score at least\nhigh as the average score in the population, or (as in the case of the\nevolution under the imitation dynamic) at least as high as the scores\nof the most successful agents in the population. Under those\nconditions, it is much more important, in a particular round of the\ngame, to raise your own score than to lower your opponent's. Axelrod\nrepeatedly (and with cause) advised participants in his tournaments\nnot to be envious. In the 2IPD, however, the population size is two.\nIn that case it is as valuable to lower your opponent's payoff as to\nraise your own, and it can even be beneficial to lower your own\npayoff, if doing so lowers your opponent's more than yours. \nAnother noteworthy feature of the 2IPD, proved rigorously in Press and\nDyson (Appendix A), is that a long memory is unnecessary to play well.\nSuppose I adopt a memory-one strategy i.e., I condition each move only\non our last interaction. Then Press and Dyson show that you can't\nbenefit by using a longer memory: whatever strategy you adopt, there\nis an equivalent memory-one strategy you could have adopted that would\nnet us both the same scores. By adopting a memory-one strategy myself,\nI ensure that a longer memory will be of no benefit to you. Thus we\ncan, without loss of generality, take the 2IPD game to be a game\nbetween agents with memory-one strategies. \nA 2IPD game between memory-one agents (and indeed any 2-player, 2-move\ngame between memory-one agents) can be represented in a particularly\nperspicuous way. Let \\(\\bO_1, \\bO_2, \\bO_3, \\bO_4\\) be the four\noutcomes \\(\\bC\\bC, \\bC\\bD, \\bD\\bC\\) and \\(\\bD\\bD\\). The memory-one\nstrategies (as noted in the discussion of evolution above) are the\nstrategies \\(\\bS(p_1,p_2,p_3,p_4)\\) of cooperating with probability\n\\(p_1, p_2, p_3, p_4\\) after outcomes \\(\\bO_1, \\bO_2, \\bO_3, \\bO_4\\).\n(If we assume that the game is repeated infinitely many times and that\n\\(0 \\lt p_i \\lt 1\\) for \\(i=1,2,3,4\\), the initial move can be\nignored.). Let \\(\\bS(p_1,p_2,p_3,p_4)\\) and \\(\\bS(q_1,q_2,q_3,q_4)\\)\nbe the strategies of players One and Two. (The subscripts are switched\nfor Player Two, so that \\(p_2\\) and \\(q_2\\) both give the probability\nof cooperation after receiving the sucker payoff and \\(p_3\\) and\n\\(q_3\\) give the probability of cooperation after receiving\ntemptation.) Let \\(p'_i = 1 - p_i\\) and \\(q'_i=1 - q_i\\) (for\n\\(i=1,2\\)) (so that \\(p'_i\\) and \\(q'_i\\) are odds of defection). Then\nwe can represent the 2IPD between One and Two as a “Markov\ntransition matrix,” that displays the odds of moving from any\nstate to any state. \nFor example, the odds of moving from state \\(\\bO_2\\), where One\ncooperates and Two defects to state \\(\\bO_4\\) where both players\ndefect is given in the second row and the fourth column: \\(p'_2\nq'_3\\). \nViewing a game in this way makes it possible apply the machinery of\nmatrix algebra and Markov chains, which led Press and Dyson to the\nidentification of the class of Zero-Determinant (ZD) strategies. (A\nsimpler proof of Press and Dyson's central result, employing more\nmodest mathematical machinery, is given in Appendix A of Hilbe et al.)\nA ZD strategy is a strategy by which a player can ensure a fixed\nlinear relation between his own long-term average payoff and his\nopponent's. For example, TFT \\((= \\bS(1,0,1,0))\\)\nturns out to be such a strategy for any PD. If I adopt\nTFT, then I guarantee that, whatever strategy you\nchoose, we will get the same payoff. If you choose unconditional\ncooperation \\((=\\bS(1,1,1,1))\\) or GTFT \\((= \\bS(1,\n.25,1,.25))\\) we both get the reward payoff, if you choose\nunconditional defection we both approach an average of the punishment.\nFor other choices, you may get a payoff between the punishment and\nreward. Whatever you choose, however, you will still get the same\npayoff as me. \nThere are a variety of such ZD strategies for the IPD (and indeed for\nmost two-player, two-move games). For a standard PD with payoffs 5, 3,\n1, 0, three other representative ZD strategies are the following: \nPress and Dyson emphasize strategies like SET-2 and\nEXTORT-2. If Player One adopts\nSET-2, then Player Two will get a payoff of 2 no\nmatter what strategy she employs. In the memory-one 2IPD a player can\nset his opponent's strategy to any value between the punishment and\nreward payoffs. Hilbe et al. call such strategies\n“equalizer” strategies, but in our context perhaps\n“dictator” would be a better label. If Player One knows\nabout the dictator strategies and knows Player Two to be a naïve\nutility-maximizer, he can trick her into playing a strategy that suits\nhim by raising the level to which he sets her payoff when her recent\nplay satisfies his desires. Of course, a more witting Player Two might\nrealize that the same dictatorial strategies are available to her.\nThese will be of no use, however, unless they lead to a shift in\nPlayer One's behavior. If the existence of the dictator strategies is\ncommon knowledge for the two players, then they might profitably agree\nto set each other's scores to the reward payoff. Since each is\nemploying a dictator strategy, neither can benefit in the short term\nby deviating. If either deviates in hopes of a long term gain, the\nother could detect it by the change in his or her own payoff and take\nretaliatory action. Whether such an agreement is stable, of course,\ndepends on whether the players can make their threats of retaliation\ncredible. \nEXTORT-2 is an example of an\n“extortionary” ZD strategy. If Player One adopts\nEXTORT-2, then his payoff \\(V(\\bone,\\btwo)\\) will\nalways be \\(2V(\\btwo,\\bone)-1\\) (where \\(V(\\btwo,\\bone)\\) is the\npayoff to Player Two). Player Two can, of course, guarantee herself a\nreturn of at least one by constant defection. If she does, Player One\nwill defect with increasing frequency and their average payoffs will\nboth approach the punishment value. Because of the linear relation\nthat links their payoffs, however, if she does better than this, she\nwill necessarily lose to Player One. Indeed, any increment above\npunishment to her own score will be only half the same increment to\nher opponent's. Against a naïve, utility-maximizing opponent,\nEXTORT-2 is even more effective than\nSET-2. No tricks are needed. Whenever the naïve\nopponent scores more than the punishment payoff she loses to the\nextortionist. Whatever she does to increase her own payoff, will, of\nnecessity, increase the extortionist's by double the amount. The best\nthat she can do against EXTORT-2 is to cooperate\nunconditionally. This will result in the pair realizing the outcomes\n\\(\\bC\\bC\\) and \\(\\bD\\bC\\) in a ratio of three to one, giving the her\nan average payoff of 2.25, while the extortionist nets 3.5. Her only\nhope of escape is to abandon utility-maximization and acquire what\nPress and Dyson call a “theory of mind.” If she realizes\nthat her actions might cause the extortionist to abandon his strategy,\nshe might herself adopt an extortionary strategy. This would lower\nboth of their payoffs in the short term, but she might hope for better\nresults in the long term. The upshot, according to Press and Dyson, is\nthat while an extortionist strategy will always defeat a naïve\nutility-maximizer, the 2IPD between an extortionist and a more\nsophisticated agent becomes an ultimatum game. The extortionist\nproposes an unfair division of the joint payoffs, leaving his opponent\nwith the unhappy choice of accepting it or making both players worse\noff. (It is perhaps worth noting that this analysis omits the\npossibility that the extorted party is aware of the payoffs to her\nopponent as well as herself, and, realizing that the IPD is being\nplayed between just two agents, seeks to minimize the difference\nbetween her opponent's payoff and her own. Adopting such an attitude\nwould presumably lead her to a strategy of unconditional defection.\nThe payoffs of both players would then approach the punishment value,\nthe extortionist's from below and the extorted's from above.) \nNeither dictatorial nor extortionary strategies would seem likely to\nfare well in an evolutionary setting with larger populations. By\ndefinition, successful strategies become more commonplace in an\nevolutionary framework, and therefore more likely to face others like\nthemselves. Because dictators and extortionists do not do well against\nthemselves, any success they have in the evolutionary context will be\nself-limiting. Hilbe et al. confirm these intuitions. They show that\nwhen a very small population of general memory-one strategies is\nsubject to mutation and evolution, the time that agents spend\napproximating ZD strategies is reasonably high compared the number of\nsuch strategies that are possible, the relative time spent\napproximating dictator strategies in particular is higher, and the\nrelative time spent approximating extortionary strategies much higher\nstill. As population size increases, however, the proportion of time\nspent approximating all three categories drops rapidly. When the\npopulation exceeds ten, time spent as exemplars of these strategies is\nvirtually zero. At about the same population level, the average\ncomponents \\(x\\) and \\(z\\) of the strategies \\(\\bS(x,y,z,w)\\) of\nsurviving agents (representing the odds of cooperating after receiving\nreward and punishment payoffs) rise rapidly while average components y\nand w decline slowly, so that in larger populations the average of the\nstrategies looks like \\(\\bS(1, .9, .1, .1)\\)—an imperfect\nversion of \\(\\bP_1\\). (As might be expected, as the average of\nstrategies approaches \\(\\bP_1\\), the average payoff increases and\napproaches the reward value. This suggests that in some circumstances\nthe familiar idea that cooperation is more difficult among large\ngroups than small ones gets matters exactly backwards.)  \nAlthough extortionary ZD strategies fare poorly under evolution, Hilbe\net al., suggest that they do play an important evolutionary role, as\n“catalysts” for the evolution of cooperation. They explore\nin some detail the course of evolution among agents restricted to a\nsmall choice of strategies, including unconditional defection, an\nextortionary ZD strategy and the familiar (relatively cooperative)\nstrategy \\(\\bP_1\\). In the absence of extorters, unconditional\ndefection predominates in a population of any size. In the presence of\nextorters, however, \\(\\bP_1\\) becomes more successful as population\nsize increases. In populations larger than fifty, it predominates. The\nsame basic results hold when unconditional cooperation is added as a\nstrategy choice. In the absence of extortionary strategies\nTFT can play a similar catalytic role, allowing\n\\(\\bP_1\\) to predominate over unconditional defection (with or without\nunconditional cooperators present). The short explanation is that\nTFT and the extortionary strategies can both drift\ninto a population of unconditional defectors as neutral mutants, and\nthe proportion of TFT can grow for a time.\nEventually, however, \\(\\bP_1\\) will do better than either. \nIt should be noted that Hilbe et al. defend and employ a\n“pairwise comparison” model of evolution that is markedly\ndifferent from the kinds of evolutionary dynamics discussed previously\nin this entry. At each stage a pair of agents is randomly selected and\nthe first adopts the strategy of the second with a probability that\nincreases with the difference between their payoffs. Under this kind\nof dynamics, if the rate of mutation is sufficiently low, the\npopulation will always move to “fixation,” i.e., a state\nin which every agent employs the same strategy. The time to reach\nfixation increases with population size and, if every strategy gets\nthe same payoff against every other, then the odds of strategy \\(\\bs\\)\nbecoming fixed are proportional to the fraction of the population that\nemploys \\(\\bs\\). These features correspond to familiar properties in\npopulation genetics but they are not true of, for example, the\nreplicator dynamics.  \nIn a commentary on the article introducing ZD strategies, Stewart and\nPlotkin (2012) point out that more generous ZD strategies like\nGEN-2 had been relatively neglected by Press and\nDyson. If Player One adopts GEN-2 in a 2IPD with\ntraditional payoff matrix, then his payoff \\(V(\\bone,\\btwo)\\) will be\n\\(2V(\\btwo,\\bone)-3\\). Player Two then can allow Player One any payoff\nbetween the punishment value of one and the reward value of three,\nwhile ensuring that her own payoff is larger. Her highest paying\nresponse is \\(\\bCu\\), which results in the average long-term payoff of\nthree to both players. If she is willing to adopt a strategy that\nreduces her average payoff below the reward level, she will reduce\nPlayer One's by twice as much. Stewart and Plotkin (2012) report that\na strategy like GEN-2 actually gets the highest score\namong nineteen strategies in a simulated IPD tournament like Axelrod's\nthat includes TFT, GTFT, \\(\\bP_1\\),\nGRIM and other strategies championed in previous\nwritings. A version of EXTORT-2 gets the second\nlowest score. Significantly, the EXTORT-2 version won\nthe second most “head-to-head” contests, and the\nGEN-2 version won the fourth fewest. As Axelrod\nemphasized in connection with TFT, beating one's\nopponents is not the path to success in a PD tournament. Suggestive as\nit is, however, the Stewart and Plotkin commentary leaves open the\nquestion of whether it is the ZD-character of strategies like\nGEN-2 that engenders their success. \nIn more recent work, Stewart and Plotkin (2013) present evidence that\nsupports a qualifiedly affirmative answer to the open question. Their\nwork borrows from a detailed mathematical investigation of the\ninfinite IPD by Ethan Akin. Akin, 2013) focuses on strategies that\nsatisfy exactly the conditions we might want for a morally appropriate\n“solution” to the 2IPD: (1) use by both players ensures\nthe cooperative payoff, (2) use by both players constitutes a nash\nequilibrium, i.e., a strategy-pair giving each player a payoff that he\ncannnot improve upon by deviating from it unilaterally and (3)use by\nboth players prevents exploitation by either—any change in\nstrategy by either player that reduces the payoff of his opponent will\nalso reduce his own. Akin labels such strategies “good”\nand derives a remarkably simple characterization of them. A strategy\n\\(\\bS(p_1,p_2,p_3,p_4)\\) is good if and only if it meets the following\nconditions: \nIt is easy to check that, with standard PD payoffs,\nGRIM, TFT, GTFT and\nGEN-2 all meet these conditions, but\nEXTORT-2, SET-2 and \\(\\bP_1\\) do\nnot. (\\(\\bP_1\\) does meet the conditions when the payoffs satisfy \\(R\n\\gt \\tfrac{1}{2}(T+P)\\).) \nStewart and Plotkin show that the good strategies that are also ZD are\nexactly the generous ZD strategies, i.e., those like\nGEN-2 that concede a greater share of the payoffs\nbetween punishment and reward to the opponent. When the investigations\nof Hilbe et al. are extended to include focus on the good strategies\nand, in particular, to the generous ZD strategies, the patterns\nexhibited turn out to be quite different. In small populations,\nstrategies spend little time near these strategies in these two groups\n(relative to chance) and in larger populations they spend a much\nlarger proportion of their time there. In big populations a very high\nproportion of the strategies most strongly favored by evolution are\nboth good and ZD. The story is not entirely straightforward, however.\nStrategies that are good but not ZD are moderately favored by\nevolution, and a few strategies that are neither ZD nor good, are also\nstrongly favored. \nThere is no doubt that the identification of the ZD strategies has\nnewly energized investigations into simple games and into the IPD in\nparticular. Its lessons for the evolutionary PD and for the emergence\nof cooperation are not yet fully understood. \nA third mechanism by which players can be made more likely to meet\nthose like themselves is to consider a more sophisticated dynamics of\nevolution that operates on groups of players as well as on the\nindividuals within those groups. There has been a heated debate among\nbiologists and philosophers of biology about the appropriate\n“units of selection” on which natural selection operates.\nThe idea that in many cases it makes sense to take these units to be\ngroups of individuals (instead of, or in addition to, genes or\nindividuals) has recently been resuscitated as a respectable and\nplausible viewpoint. (See Sober and Wilson or Wilson and Sober for a\nhistory and impassioned defense of this resuscitation.) For cultural\nevolution, the idea is equally plausible—within-group behavior\nmay be in equilibrium, but the equilibria reached by different groups\nmay be different. Less successful groups may imitate, be replaced by,\nor lose members to, more successful groups. Sober and Wilson sometimes\nwrite as if evolutionary game theory is an alternative\nviewpoint to group selection, but it is important to understand that\nthis is only true of simple evolutionary models like those presented\nabove. More sophisticated evolutionary games are possible. Consider,\nfor example, a simple version of the haystack model\noriginally described by John Maynard Smith. Pairs of players from a\nlarge population pair randomly. Each pair colonizes a single haystack.\nThe pair plays a prisoners dilemma and the payoffs to an individual\ndetermine the number of offspring of that individual in the next\ngeneration. (Parents die when the children are born.) For some fixed\nnumber of generations, members of the colony pair randomly with other\nmembers and play the PD. Then the haystacks are torn down, the\npopulation mixes and random pairs colonize next season's haystacks.\nOne simple way to represent the \\(n\\)-generation haystack PD,\nas we might call it, is to view it as a game between the two initial\nfounders of a haystack with the payoff to a founder being set to the\nnumber of living descendants who are using his strategy. (This idea is\nsuggested in Bergstrom and reported in Skyrms 2004.) For example,\nsuppose \\(n=3\\) and the temptation, reward, punishment and sucker\npayoffs are set to 5, 3, 1, 0. Then, if Player One cooperates and\nplayer Two defects, the payoff to Player One will be 0, because a\ncooperator gets 0 offspring in the second, and any subsequent,\ngeneration. The payoff to player Two will be 5 because the defector\nhas five (like-minded) offspring among the second generation and each\nof these has one in the third generation since there are no\ncooperators left to meet. The full payoff matrix for the four\ngeneration haystack PD with payoffs 3,2,1, and 0 is given by the\nmatrix below. \nAs Skyrms 2004 notes, this matrix characterizes an ordinary stag hunt\ngame, as defined above. In fact, Skyrms' observation is generally\ntrue. For any PD game \\(g\\), if \\(n\\) is sufficiently large, the\n\\(n\\)-generation haystack version of \\(g\\) is a stag hunt. A simple\nargument for this result is given in the following very short\nsupplementary document:","contact.mail":"kuhns@georgetown.edu","contact.domain":"georgetown.edu"}]
