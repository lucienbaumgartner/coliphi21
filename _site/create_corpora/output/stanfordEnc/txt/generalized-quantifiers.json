[{"date.published":"2005-12-05","date.changed":"2019-07-26","url":"https://plato.stanford.edu/entries/generalized-quantifiers/","author1":"Dag Westerståhl","author1.info":"http://www.philosophy.su.se/english/research/our-researchers/faculty/dag-westerst%C3%A5hl-1.165620","entry":"generalized-quantifiers","body.text":"\n\n\nGeneralized quantifiers are now standard equipment in the toolboxes of\nboth logicians and linguists. The purpose of this entry is to describe\nthese tools: where they come from, how they work, and what they can be\nused to do. The description is by necessity sketchy, but several more\ncomprehensive surveys exist in the literature and will be referred to\nwhen appropriate. To fully appreciate the text below, basic\nfamiliarity with elementary set theoretic terminology, and with the\nlanguage of first-order logic will be helpful.\n\n\nThe term “generalized quantifier” reflects that these\nentities were introduced in logic as generalizations of the standard\nquantifiers of modern logic, \\(\\forall\\) and\n \\(\\exists\\).[1]\n In retrospect one may say that \\(\\forall\\) and \\(\\exists\\) have been\nfound to be just two instances of a much more general concept of\nquantifier, making the term “generalized” superfluous.\nToday it is also common to use just “quantifier” for the\ngeneral notion, but “generalized quantifier” is still\nfrequent for historical reasons. This article employs both terms, with\na tendency to insert “generalized” in logical contexts,\nand to drop it in linguistic contexts. \nWe distinguish quantifier expressions from what they signify\nor denote, the (generalized) quantifiers themselves. In logical\nlanguages, quantifier expressions are variable-binding operators.\nThus, \\(\\exists\\) is the familiar operator such that in a formula\n\\(\\exists\n x\\f\\),[2]\n \\(\\exists x\\) binds all free occurrences of x in \\(\\f\\). It\nsignifies the quantifier “there exists”—we’ll\nsee shortly exactly what this object is. Likewise, the symbol \\(Q_0\\)\nis often used as a variable-binding operator signifying “there\nexist infinitely many”. \nIn natural languages a variety of expressions have been seen as\nquantifier expressions, for example, each of the following English\nexpressions:  \neverything, nothing, three books, the ten\nprofessors, John, John and Mary, only John, firemen, every, at least\nfive, most, all but ten, less than half of the, John’s, some\nstudent’s, no…except Mary, more male than female,\nusually, never, each\n other.[3] \nWhat, then, are generalized quantifiers? Before answering that\nquestion, a brief historical prelude is helpful. \nAristotle’s syllogistics can be seen as a formal study of the\nmeaning of the four basic quantifier expressions all,\nno, some, not all, and of their properties. For\nexample, the validity, according to Aristotle, of the syllogism \nshows that he considered, in contrast with modern logical usage, all\nto have existential import, so\nthat All A are B entails that A\nis not an empty term. Likewise, the\nvalidity of the syllogism \nexpresses that some is monotone\nincreasing (as we now put it) in the second argument. Each valid\nsyllogism formalizes part of the meaning of these quantifier\nexpressions, but Aristotle’s study of their properties went\nbeyond the syllogistics. He observed, for example, that some\nand no\nare convertible or, as we might now say, symmetric,\nsince they satisfy the scheme \nin contrast with all and not\nall. Further, he studied how various\nforms of negation combined with quantifier expressions in\n(what was later called) the square of\n opposition.[4]\n Medieval logicians continued in Aristotle’s tradition, but also\nextended syllogistic reasoning to cases where A,B\ncould themselves be quantified expressions, thus\ndealing with premises and conclusions like Some\ndonkey of every man doesn’t run (example\nfrom John Buridan, 14th century). Even though Aristotelian logic falls\nshort of the expressivity and precision of modern logic, the\nsyllogistics certainly was a decisive contribution to the study of\nquantification. In fact, syllogistic systems of various expressive\npower have recently been studied in mathematical logic, precisely\nbecause of their affinity to natural reasoning and their simple\ncomputational properties; see\n section 18\n below. \nEspecially interesting in the present context is the fact that these\nquantifier expressions take two arguments or terms, and thus\ncan be seen as binary relations, both syntactically (as\nAristotle no doubt saw them) and semantically: given that terms\nsignify sets of individuals, the expression some\ncan be taken to signify the relation of overlap,\ni.e., of having non-empty intersection, between two sets, and all\nsignifies the inclusion relation. Note\nthat these are not relations between individuals but between sets of\nindividuals—second-order relations. Indeed, they are\nexactly the generalized quantifiers some and all,\nrespectively (on a given universe). \nThis thread—that quantifier expressions signify second-order\nrelations—was not picked up by any of Aristotle’s medieval\nfollowers (as far as we know). Instead, they picked up on the fact\nthat the two terms have different status: the first combines with the\nquantifier expression to form a noun phrase (as we now say), which is\nthe subject of the sentence, whereas the second is a verb\nphrase constituting the predicate. This led them to focus on\nwhat the subject—all men, some\ndogs, no\nsailors—signified, which conceptually seems to be a\nharder question. One might surmise that all\nmen signifies every man (or the set of men), and that some\ndogs signifies some particular dog, but\nwhat about no sailors? In fact, one\ncan show that approaches like these are doomed to\n failure.[5]\n The modern “solution” is that noun phrases signify\nsets of sets of individuals, so that, for example some\ndogs signifies the set of sets\ncontaining at least one dog—but that appears to require a more\nabstract and mathematical approach to semantics than the idea, which\nis at least implicit in Aristotle, that quantifier phrases signify\nrelations between (the denotations of) terms. \nThe second major historical contribution to the theory of generalized\nquantifiers came from the “inventor” of modern logic,\nGottlob Frege, in the 1870s. In fact, Frege’s contribution is\ntwofold. As every philosophy student knows, he introduced the language\nof predicate logic, with sentential connectives, identity, and the\nvariable-binding operator \\(\\forall\\) (though his 2-dimensional\nlogical notation is no longer used). These are the quantifiers that\nlogicians during the 1950’s began to “generalize”.\nBut Frege also explicitly formulated the abstract notion of a\nquantifier as a second-order relation, or, as he called it, a\nsecond level concept (“Begriff zweiter\nStufe”). He was well aware that the four Aristotelian\nquantifiers were prime examples, but he wanted to avoid the focus on\nsubject-predicate form, which he (with much justification) saw as\nhaving been a major obstacle to the development of logic after\nAristotle. It was therefore an important discovery that these\nquantifiers could all be defined in terms of \\(\\forall\\) and\nsentential operators (replacing all\\((A,B)\\)\nby \\(\\forall x(A(x) \\rightarrow B(x)\\)),\nsome\\((A,B)\\) by \\(\\neg\\forall x(A(x)\n\\rightarrow \\neg B(x)\\)), etc.). \nIn fact, the only significant difference between Frege’s notion\nof a second-level concept and the modern notion of a generalized\nquantifier is that Frege did not have the idea of an\ninterpretation or model, which we now (since the\nadvent of model theory in the 1950s) see as a universe that\nthe quantifiers range over, plus an assignment of suitable semantic\nobjects to the non-logical symbols. Frege’s symbols all had\nfixed meanings, and the only universe he considered was the totality\nof everything. But apart from this, one may well say that it was Frege\nwho discovered generalized quantifiers. This aspect of Frege’s\nlogic, however, remained in the background for a long time, and model\ntheorists in the 50s and 60s seem not to have been aware of it. \nModern predicate logic fixes the meaning of \\(\\forall\\) and\n\\(\\exists\\) with the respective clauses in the truth definition, which\nspecifies inductively the conditions under which a formula\n\\(\\f(x_1,\\ldots,x_n)\\) (with at most \\(x_1,\\ldots,x_n\\) free) is\nsatisfied by corresponding elements \\(a_1,\\ldots,a_n\\) in a\nmodel \\(\\M = (M,I)\\) (where M is the universe and I the\ninterpretation function assigning suitable extensions to non-logical\nsymbols): \\(\\M \\models \\f(a_1,\\ldots,a_n)\\). The clauses are (where\n“iff” as usual stands for “if and only\nif”) \nTo introduce other quantifiers, one needs to appreciate what kind of\nexpressions \\(\\forall\\) and \\(\\exists\\) are. Syntactically, they are\noperators binding one variable in one formula. To see how they work\nsemantically it is useful to rewrite\n (1)\n and\n (2)\n slightly. First, every formula \\(\\p(x)\\) with one free variable\ndenotes in a model \\(\\M\\) a subset of M; the set of individuals\nin M satisfying \\(\\p(x)\\). More generally, if\n\\(\\p(x,x_1,\\ldots,x_n) = \\p(x,\\xbar)\\) has at most the free variables\nshown and \\(\\abar = a_1,\\ldots,a_n\\) are elements of M, let \nbe the extension of \\(\\p(x,\\xbar)\\) in \\(\\M\\) relative to\n\\(a_1,\\ldots,a_n\\). Then we can reformulate\n (1)\n and\n (2)\n as follows: \nThus, the conditions on the right hand side emerge as properties\nof the sets \\(\\p(x,\\abar)\\). In fact, we can think of \\(\\forall\\)\nand \\(\\exists\\) as denoting these properties, i.e., the property of\nbeing identical to the universe, and of being non-empty, respectively.\nAnd now it is easy to think of other properties of sets that can also\nbe treated as quantifiers, for example, the property of containing at\nleast 5, or exactly 3, elements, or of being\n infinite.[6] \nNote that these properties depend only on the universe M, not\non the rest of the model. Extensionally, they are simply sets of\nsubsets of M. This leads to the following definition.\nessentially from Mostowski (1957): \nDefinition 1 \nA generalized quantifier Q of type\n\\({\\langle}1{\\rangle}\\) is \nHere we use the same symbol for the quantifier expression and the\nmapping that it signifies or denotes. Thus, \\(\\forall\\) is now taken\nto denote the universal quantifier, also written \\(\\forall\\), which is\nthe mapping given by  \nfor all M. Similarly, \\(\\exists\\) denotes the mapping defined\nby \nAnd here are some other generalized quantifiers: \nWe now have a precise notion of a generalized quantifier, of which\n\\(\\forall\\) and \\(\\exists\\) are instances, along with infinitely many\nothers. Moreover, we see how to extend first-order logic FO\nto a logic \\(\\FO(Q)\\), by adding the clause\n (5a)\n to the formation rules, and the clause\n (5b-i)\n to the truth definition. Similarly if we add more than one\ngeneralized quantifier: \\(\\FO(Q_1,\\ldots,Q_n)\\). \nIn such a logic one may be a able to say things that are not\nexpressible in FO. For example, it is well-known that in\nFO the notion of finiteness cannot be expressed. Thus there\nis no way to say, of an ordering relation \\(<\\), that each element\nhas only finitely many predecessors, for instance. But this is just\nthe sort of thing one can express in \\(\\FO(Q_0)\\): \nLikewise, one cannot say in FO that a (finite) set A\ncontains exactly half of the elements of the universe M, but\nthat is expressible in \\(\\FO(Q^R)\\): \n(The first conjunct says that \\(|A|\\leq |M-A|\\), and the second that\n\\(|M-A|\\leq |A|\\).) \nFurther generalization is possible. First, we can let Q bind\none variable in two or more formulas. Second, we can let it\nsimultaneously bind two or more variables in (some of) these formulas.\nThe typing of Q indicates this: Q is of type\n\\({\\langle}n_1,\\ldots,n_k{\\rangle}\\) (where each \\(n_i\\) is a natural\nnumber \\(\\geq 1\\)) if it applies to k formulas, and binds\n\\(n_i\\) variables in the ith formula. This explains why the\nquantifiers in the previous section were said to be of type\n\\({\\langle}1{\\rangle}\\). \nIn the general case, one normally chooses distinct variables\n\\(x_{i1},\\)…,\\(x_{in_i} = \\xbar_i\\) for \\(1\\leq i \\leq k\\), so\nthat a formula beginning with Q has the form \nwhere all free occurrences of \\(x_{i1},\\ldots,x_{in_i} \\) in \\(\\f_i\\)\nbecome bound. Now Q associates with each universe M a\nk-ary relation \\(Q_M\\) between relations over M, where\nthe ith argument is an \\(n_i\\)-ary relation between\nindividuals. The corresponding clause in the truth definition\nbecomes \nHere \\(\\p_i(\\xbar_i,\\ybar)\\) is a formula with at most the free\nvariables shown, \\(\\abar\\) is a sequence of elements of M\ncorresponding to \\(\\ybar\\), and \\(\\p_i(\\xbar_i,\\abar)^{\\M,\\xbar_i}\\)\nis the extension of \\(\\p_i(\\xbar_i,\\ybar)\\) in \\(\\M\\) relative to\n\\(\\abar\\), i.e., the set of \\(n_i\\)-tuples \\(\\bbar_i\\) such that\n\\(\\M\\models\\p_i(\\bbar_i,\\abar)\\). \nThis is the official concept of a generalized quantifier in this\narticle. It was introduced by Lindström\n(1966), and these quantifiers are sometimes called\n“Lindström\n quantifiers”.[7]\n If we fix M to the universe containing\n“everything”, we essentially have Frege’s notion of\na second-level\n concept.[8] \nQ is monadic if on each universe M it is a\nrelation between subsets of M, i.e., if its type is\n\\({\\langle}1,\\ldots,1{\\rangle}\\); otherwise it is polyadic.\nFor example, the Aristotelian quantifiers mentioned earlier are of\ntype\n \\({\\langle}1,1{\\rangle}\\):[9] \nHere are some more type \\({\\langle}1,1{\\rangle}\\) \n quantifiers:[10] \nWith monadic quantifiers it is convenient to use just one\nvariable, and let Q bind that same variable in each of the\nformulas. Thus, to say that most As are not B, for\nexample, one may write \nin the corresponding logical language, rather than \\(\\textit{most}\\:\nx,y (A(x),\\neg B(y))\\). \nHere are a few polyadic quantifiers: \nW and \\(Q_0^n\\) come from logic and set theory.\n\\(Res^k(\\textit{most})\\) is the resumption of most\nto k-tuples. Resumption can be applied to any quantifier (in\nthe syntax, this means replacing each individual variable by a\ncorresponding k-tuple of variables); it has logical uses but\nalso, like RECIP, uses in the interpretation of certain\nsentences in natural languages; see\n section 16\n below. \nBoth Mostowski and Lindström had one additional condition in\ntheir definitions of generalized quantifiers: they should not\ndistinguish isomorphic models. Informally, they are\n“topic-neutral”: the truth of a statement of the form \\(\\f\n= Qx,yz(A(x),R(y,z))\\), say, in a model \\(\\M\\) doesn’t depend on\nthe particular individuals M consists of. If the individuals of\nM are mapped in a one-one fashion onto the individuals of\nanother universe \\(M'\\), and if A and R are mapped\naccordingly, one obtains an isomorphic model \\(\\M'\\).\nIsomorphism Closure then says that \\(\\M\\models\\f\\) iff\n\\(\\M'\\models\\f\\). \nMore formally, if \\(\\M = (M,I)\\) and \\(\\M' = (M',I')\\) are models for\nthe same vocabulary V of non-logical symbols, f is an\nisomorphism from \\(\\M\\) to \\(\\M'\\), iff \n\\(\\M\\) and \\(\\M'\\) are isomorphic, in symbols, \nif there is an isomorphism from one to the other. Now if Q is a\ngeneralized quantifier of type \\({\\langle}n_1,\\ldots,n_k{\\rangle}\\),\n\\(P_i\\) is an \\(n_i\\)-ary predicate symbol for \\(1\\leq i\\leq k\\), \\(\\M\n= (M,I)\\) is a model for the vocabulary \\(\\{P_1,\\ldots,P_k\\}\\), and\n\\(R_i = I(P_i)\\), we also write \nThen Q satisfies Isomorphism Closure, or just Isom,\nif the following holds: \nOne easily checks that all the generalized quantifiers exemplified so\nfar are indeed Isom. We did not include this\nrequirement in the definition of generalized quantifiers however,\nsince there are natural language quantifiers that do not satisfy it;\nsee below. But logic is supposed to be topic-neutral, so Isom\nis almost always imposed. Then two important\nthings follow. First, as indicated above, sentences in logical\nlanguages do not distinguish isomorphic models. More precisely, we\nhave the following \nFact 2 \nIf \\(L = \\FO(Q_1,\\ldots,Q_n)\\), each \\(Q_i\\) is Isom,\n\\(\\f\\) is an L-sentence, and \\(\\M\n\\cong \\M'\\), then \\(\\M\\models\\f \\Leftrightarrow \\M'\\models\\f\\). \nSecond, Isom takes a particularly interesting\nform for monadic quantifiers. If \\(\\M = (M,A_1,\\ldots,A_k)\\),\nwhere \\(A_i \\subseteq M\\) for each i, then \\(A_1,\\ldots,A_k\\)\npartition M into \\(2^k\\) pairwise disjoint subsets\n(some of which may be empty); let us call them the parts of\n\\(\\M\\). We illustrate with \\(k=2\\) and \\(\\M = (M,A,B)\\): \nFigure 1 \nNow it is not hard to see that only the sizes of the parts\ndetermine whether two models of this kind are isomorphic or not: \nFact 3 \n\\((M,A_1,\\ldots,A_k) \\cong (M',A'_1,\\ldots,A'_k)\\) iff the\ncardinalities of the corresponding parts are the same. \nThis shows that monadic and Isom generalized\nquantifiers indeed deal only with quantities, i.e., with\nsizes of sets rather than the sets themselves. The list\n\\eqref{ex-qlist3} of type \\({\\langle}1,1{\\rangle}\\) generalized\nquantifiers clearly illustrates this, but also the Aristotelian\nquantifiers can be formulated in terms of cardinalities, \netc., and similarly for the type \\({\\langle}1{\\rangle}\\) examples we\ngave. \nMore generally, under Isom, monadic\nquantifiers can be seen as relations between (cardinal) numbers. For\nexample, if Q is of type \\({\\langle}1{\\rangle}\\), then define\n(using the same symbol Q for the relation between numbers) \nIsom guarantees that this is well-defined, and\nwe have \nEvery statement involving a generalized quantifier Q takes\nplace within some universe M. Sometimes it is useful to be able\nto mirror this relativization to a universe inside M.\nThis means defining a new quantifier with one extra set argument which\nsays that Q behaves on the universe restricted to that argument\nexactly as it behaves on M. Thus, if Q is of type\n\\({\\langle}n_1,\\ldots,n_k{\\rangle}\\), we define \\(Q{^{\\text{rel}}}\\)\nof type \\({\\langle}1,n_1,\\ldots,n_k{\\rangle}\\) as follows: \nwhere \\(R_i \\subseteq M^{n_i}\\) and \\(R_i\\!\\restriction\\! A\\) is the\nrestriction of \\(R_i\\) to A, i.e., the set of\n\\(n_i\\)-tuples in \\(R_i\\cap A^{n_i}\\). \nWe have in fact already seen several examples of relativization: since\none easily verifies (see the lists \\eqref{ex-qlist1} and\n\\eqref{ex-qlist3}) that \nWe described how generalized quantifiers can be added to FO,\nresulting in more expressive logics. A logic in this sense\nroughly consist of a set of sentences, a class of models, and a truth\nrelation (or a satisfaction relation) between sentences and models.\nSuch logics are often called model-theoretic logics, since\nthey are defined semantically in terms of models and truth, rather\nthan proof-theoretically in terms of a deductive system for deriving\n theorems.[11]\n Here we restrict attention to logics of the form\n\\(\\FO(Q_1,Q_2,\\ldots)\\), formed by adding generalized quantifiers to\nFO, where each quantifier comes with a formation rule and a\nsemantic clause for the truth definition as described in\n section 5\n above. \nThere is an obvious way to compare the expressive power of\nmodel-theoretic logics. \\(L_2\\) is at least as expressive as\n\\(L_1\\), in symbols, \nif every \\(L_1\\)-sentence \\(\\f\\) is logically equivalent to\nsome \\(L_2\\)-sentence \\(\\p\\), i.e., \\(\\f\\) and \\(\\p\\) are true in the\nsame models. Also, \\(L_1\\) and \\(L_2\\) have the same expressive\npower, \\(L_1 \\equiv L_2\\), if \\(L_1 \\leq L_2\\) and \\(L_2 \\leq\nL_1\\), and \\(L_2\\) is stronger than \\(L_1\\), \\(L_1 <\nL_2\\), if \\(L_1 \\leq L_2\\) but \\(L_2 \\not\\leq L_1\\). Thus, \\(L_1 <\nL_2\\) if everything that can be said in \\(L_1\\) can also be said in\n\\(L_2\\), but there is some \\(L_2\\)-sentence which is not equivalent to\nany sentence in \\(L_1\\). \nHow does one establish facts about expressive power? It seems as if in\norder to show \\(L_1 \\leq L_2\\) one has to go through all of the\ninfinitely many sentences in \\(L_1\\) and for each one find an\nequivalent in \\(L_2\\). But in practice it suffices to show that the\ngeneralized quantifiers in \\(L_1\\) are definable in \\(L_2\\).\nIf Q is of type \\({\\langle}1,2{\\rangle}\\), say, Q is\ndefinable in \\(L_2\\) if there is an \\(L_2\\)-sentence \\(\\p\\)\nwhose non-logical vocabulary consists exactly of one unary and one\nbinary predicate symbol, such that for all models \\(\\M =\n(M,A,R)\\), \nSimilarly for other types. For example, the quantifier all is\ndefinable in FO, since the following holds:  \nLikewise, \\(Q^R\\) is definable in \\(\\FO(\\textit{most})\\), since  \n(note that all our logics contain the logical apparatus of\nFO, so they are all extensions of FO). The latter is\nan instance of the following observation: \nSuch facts about definability can be easy or hard to\n establish,[12]\n but they suffice to establish positive facts about expressivity,\nsince we have: \nFact 4 \n\\(\\FO(Q_1,\\ldots,Q_n) \\leq L\\) if and only if each \\(Q_i\\) is\ndefinable in L. \nOn the other hand, to prove inexpressibility, i.e., that some\nsentence is not equivalent to any L-sentence, is\nharder. One way that sometimes works is to establish that \\(L_1\\) has\nsome property that \\(L_2\\) lacks; then one might be able to conclude\nthat \\(L_1 \\not\\leq L_2\\). Some properties that are typical of\nFO, but fail for most stronger logics, are: \nThe Löwenheim property: If a sentence is true in some\ninfinite model, it is also true in some countable model. \nThe Tarski property: If a sentence is true in some countably\ninfinite model, it is also true in some uncountable model. \nThe compactness property: If no model makes every element of\nthe set of sentences \\(\\Phi\\) true, then there is a finite subset\n\\(\\Psi\\) of \\(\\Phi\\) such that no model makes every sentence in\n\\(\\Psi\\) true. \nThe completeness property: The set of valid sentences is\nrecursively enumerable (i.e., can be generated by some formal\nsystem). \nFor example, \\(\\FO(Q_0)\\) does not have the compactness\n property.[13]\n This can be seen by looking at the set of sentences  \nwhere \\(\\theta_n\\) is an FO-sentence saying that there are at\nleast n elements in the universe. If you take any finite subset\n\\(\\Phi'\\) of \\(\\Phi\\), and M is a universe whose cardinality is\nthe largest n such that \\(\\theta_n\\) belongs to \\(\\Phi'\\), then\nall sentences in \\(\\Phi'\\) are true in M. But no universe can\nmake all sentences in \\(\\Phi\\) true. And this shows that \\(Q_0\\) is\nnot definable in FO, i.e., that \\(\\FO(Q_0) \\not\\leq \\FO\\),\nsince otherwise we could replace \\(\\Phi\\) by an equivalent set of\nFO-sentences, but FO does have the compactness\nproperty, so that it impossible. \nHowever, this way of proving inexpressibility only works for logics\nwith properties like those above. Moreover, they only work if infinite\nuniverses are allowed, but interesting inexpressibility facts hold\nalso for finite models, for example, the fact that \\(Q^R\\) and\n\\(Q_{\\text{even}}\\) are not definable in FO, or that\nmost = \\((Q^R){^{\\text{rel}}}\\) is not definable in\n\\(\\FO(Q^R)\\). Logicians have developed much more direct and efficient\nmethods of showing undefinability results that work also for finite\n models.[14] \nThe above properties in fact characterize FO, in the\nsense that no proper extension of FO can have (certain\ncombinations of) them. This is the content of a celebrated theorem\nabout model-theoretic logics, Lindström’s Theorem, a\nversion of which is given below. For an accessible proof see, for\nexample, Ebbinghaus, Flum, and Thomas\n(1994). We say that a logic \\(L = \\FO(Q_1,\\ldots,Q_n)\\)\nrelativizes if the “converse” of\n (16)\n holds for each \\(Q_i\\), i.e., if each \\((Q_i){^{\\text{rel}}}\\) is\ndefinable in L. \nTheorem 5 (Lindström) If L is compact and\nhas the Löwenheim property, then \\(L \\equiv \\FO\\). Also, provided\nL relativizes, if L is complete and has the\nLöwenheim property, or if L has both the Löwenheim\nand the Tarski properties, then \\(L \\equiv \\FO\\). \nIn addition to the truth conditions associated with generalized\nquantifiers, one may study the computations required to establish the\ntruth of a quantified statement in a model. Indeed, generalized\nquantifiers turn up in various places in the part of computer science\nthat studies computational complexity. In this context, we\nrestrict attention to finite universes, and assume Isom\nthroughout. So a quantifier is essentially a\nset of finite models; by Isom we can assume\nthat models of cardinality m all have the same domain \\(M =\n\\{1,\\ldots,m\\}\\). Such models can be coded as words,\ni.e. finite strings of symbols. For example, a model \\((M,A)\\) of\ntype \\({\\langle}1{\\rangle}\\) can be seen as a binary word \\(a_1\\ldots\na_m\\), where \\(a_i\\) is 1 if \\(i\\in A\\) and 0 otherwise. Thus \\(|A|\\)\nis the number of 1’s and \\(|M\\!-\\!A|\\) the number of 0’s;\nby Isom, the order in the string doesn’t\nmatter. So Q becomes a set \\(W_Q\\) of words, that is, a\nformal language: a subset of the set of all finite strings of\ncoding\n symbols.[15] \nWe can now ask what it takes to recognize that a word belongs to\n\\(W_Q\\). The abstract notion of an automaton gives an answer;\nautomata are machines that accept or reject words,\nand they are classified according to the complexity of the operations\nthey perform. The language recognized by an automaton is the\nset of words it\n accepts.[16] \nA finite automaton has a finite number of states,\nincluding a start state and at least one accepting state. It starts\nscanning a word at the leftmost symbol in the start state, and at each\nstep it moves one symbol to the right and enters a (possibly) new\nstate, according to a given transition function. If it can\nmove along the whole word ending in an accepting state, the word is\naccepted. The application of automata theory to generalized\nquantifiers was initiated in van Benthem\n(1986) (Ch. 7, “Semantic automata”). It is\neasy to construct a finite automaton recognizing \\(\\forall\\) (or\n\\(\\forall{^{\\text{rel}}}=\\) all), i.e., checking that\nw consists only of 1’s: just remain in the start state =\naccepting state as long as 1’s are encountered, but go to a\nrejecting state as soon as a 0 is scanned, and remain there whatever\nis encountered afterwards. A slightly more complex automaton\nrecognizes \\(Q_{\\text{even}}\\): again there are two states, a start\nstate = the accepting state and a rejecting state, and this time\nremain in the same state when 0’s are scanned, but go to the\nother state when a 1 is scanned. To end in the accepting\nstate it is then necessary and sufficient that there are an even\nnumber of 1’s. This machine essentially uses cycles of\nlength 2, whereas the first example had only 1-cycles. Call an\nautomaton of the latter kind acyclic. Van Benthem showed that\nthe FO-definable quantifiers are exactly the ones accepted by\nfinite automata that are acyclic and permutation\n closed.[17] \nA slightly more complex automaton, the pushdown automaton,\nhas rudimentary memory resources in the form a of stack of symbols\nthat can be pushed or popped from the top, enabling it to keep track\nto some extent of what went on at earlier steps. Another result by van\nBenthem is that the type \\({\\langle}1{\\rangle}\\) quantifiers accepted\nby pushdown automata are precisely those for which the corresponding\nbinary relation between numbers is definable (with first-order means)\nin additive arithmetic, i.e., in the model \\((N,+)\\), where\n\\(N = \\{0,1,2,\\ldots\\}\\). An example is \\(Q^R\\) (or its relativization\nmost): we have \\(Q^R(m,n) \\Leftrightarrow m < n\\), and the\nright hand side is definable in \\((N,+)\\) by \\(\\exists x (x \\neq 0\n\\wedge m + x =\n n)\\).[18] \nThus, an algorithmic characterization is matched with a logical one.\nThis is one prominent direction in the study of algorithmic\ncomplexity. Consider now the most general abstract automata or\ncomputational devices, i.e., Turing machines. One (of many)\ninteresting complexity classes is PTIME: a problem, identified with\nits corresponding set of words, is PTIME if there is a polynomial\n\\(p(x)\\) and a Turing machine accepting W such that whenever\n\\(w \\in W\\) has length n, the accepting computation takes at\nmost \\(p(n)\\) steps. PTIME problems are usually considered\n“tractable”, whereas more complex problems are\n“intractable”, such as EXPTIME ones, where the number of\nsteps required may grow exponentially. An early result by Immerman and\nVardi is that the PTIME sets of (words coding) finite models are\nprecisely those describable by single sentences in \\(\\FO(\\LFP)\\),\nwhich is FO logic with an added mechanism for forming\nleast\n fixed-points.[19]\n Here we need to represent not just monadic models but arbitrary ones.\nFor example, a binary relation on the universe \\(\\{1,\\ldots,m\\}\\) can\nbe represented by a word \\(w_{11}\\cdots w_{1m}\\# \\ldots \\#w_{m1}\\cdots\nw_{mm}\\), where the relation holds of \\((i,j)\\) iff \\(w_{ij} = 1\\).\nBut this time the order does seem to matter, and in fact the Immerman\nand Vardi result just mentioned only holds for models with a given\nlinear order and a binary predicate symbol standing for that\norder. \nLogics like \\(\\FO(\\LFP)\\) can be recast as logics of the form\n\\(\\FO(Q_1,Q_2,\\ldots)\\). Here infinitely many quantifiers may be\nrequired, but in some cases a single one suffices. As to\n\\(\\FO(\\LFP)\\), it suffices to add all the resumptions (see the end of\n section 5\n above) of a single quantifier. More generally, let\n\\(\\FO^*(Q_1,Q_2,\\ldots)\\) be like \\(\\FO(Q_1,Q_2,\\ldots)\\) but with\nmechanisms for making relativizations\n (section 7)\n and for resuming each \\(Q_i\\) to k-tuples for each k.\nThen there is a single quantifier Q such that \\(\\FO(\\LFP) =\n\\FO^*(Q)\\). \nSo generalized quantifiers remain a simple and versatile way of adding\nexpressive power to FO. One natural question was if the\nlogical characterization of PTIME mentioned above could be improved\nusing generalized quantifiers, in particular if one could remove the\nrestriction to ordered structures in this way. The answer, however,\nturned out to be negative, since Hella\n(1989) proved that the PTIME computable properties of arbitrary\nfinite structures cannot be characterized by adding a finite number of\ngeneralized quantifiers to FO, or even to \\(\\FO(\\LFP)\\). The\nquestion of whether PTIME can be characterized by a logic of the form\n\\(\\FO^*(Q)\\) remains open, however (indeed, solving it would be a\nmajor breakthrough in complexity theory). \nIn the late 1960s Richard Montague showed how the semantics of\nsignificant parts of natural languages could be handled with logical\n tools.[20]\n One of his main insights was that noun phrases (NPs) can be\ninterpreted as sets of subsets of the domain, i.e., as (what we now\ncall) type \\({\\langle}1{\\rangle}\\) quantifiers. Montague worked in\ntype theory, but around 1980 a number of linguists and logicians began\nto apply the model-theoretic framework of logics with generalized\nquantifiers to natural language\n semantics.[21]\n Consider the structure of a simple English sentence whose subject is\na quantified\n NP:[22] \nThe (subject) NP consists of a determiner and a noun (N). Both the\nnoun and the verb phrase (VP) have sets as extensions, and so the\ndeterminer is naturally taken to denote a binary relation between\nsets, i.e., a type \\({\\langle}1,1{\\rangle}\\) quantifier. An utterance\nof\n (17)\n has a (discourse) universe in the background (say, the set of people\nat a particular university), but the meaning of most,\nevery, at\nleast five and similar expressions is not\ntied to particular universes. For example, the meaning of all\nin \nhas nothing to do with cats or electrons or numbers or twins or\nHausdorff spaces, nor with the discourse universes that may be\nassociated with the above examples. It simply stands for the inclusion\nrelation, regardless of what we happen to be talking about. Therefore,\nthe generalized quantifier all, which with each universe\nM associates the inclusion relation over M, is eminently\nsuitable to interpret all, and\nsimilarly for other determiners. \nHowever, it is characteristic of sentences of the form\n (17)\n that the noun argument and the VP argument are not on a par. The noun\ncombines with the determiner to form the NP, a separate constituent,\nand this constituent can also be taken to signify a generalized\nquantifier, this time of type \\({\\langle}1{\\rangle}\\). Thus, at\nleast five students denotes the set of\nsubsets of the universe which contain at least five students. This\nquantifier results from freezing the first argument of the\ntype \\({\\langle}1,1{\\rangle}\\) three to the set of students;\nwe write this three\\(^{\\textit{student}}\\). In general, if\nA is a fixed set and Q a type \\({\\langle}1,1{\\rangle}\\)\nquantifier, one may define the type \\({\\langle}1{\\rangle}\\) quantifier\n\\(Q^A\\) by \nfor any M and any \\(B\\subseteq M\\). In a compositional\nsemantics it is natural to take each constituent part of a sentence to\nhave a separate signification or meaning, and the default\nsignifications of noun phrases are type \\({\\langle}1{\\rangle}\\)\nquantifiers. \nThis holds also for some NPs that lack determiners, such as proper\nnames. While the lexical item John is\nassigned some individual j by an interpretation, the\nNP John can be taken to denote the\nquantifier \\(I_j\\), defined, for any M, by \nThis is in fact well motivated, not only because the interpretation of\nNPs becomes more uniform, but also because John\ncan combine with quantified NPs: \nHere it is convenient if John and\nthree professors have the same\nsemantic category. Note that generalized quantifiers—in contrast\nwith individuals!—have a clear Boolean structure; define (here\nin the type \\({\\langle}1{\\rangle}\\) case, but similarly for any other\ntype) \nThen we can take the complex determiner in\n (20)\n to denote \\(I_j \\wedge \\textit{three}^{\\textit{professor}}\\).\nSimilarly, the complex NP in \nsignifies \\(I_j \\wedge I_m\\). \nThe first argument (coming from the noun) of a type\n\\({\\langle}1,1{\\rangle}\\) determiner denotation is often called its\nrestriction, and the second its scope. The\ndifference in syntactic status between these two arguments turns out\nto have a clear semantic counterpart. \nIt was observed early on that type \\({\\langle}1,1{\\rangle}\\)\nquantifiers denoted by determiners in natural languages have the\nfollowing property: \nThis can be seen from sentence pairs such as the following, where it\nis clear that the second sentence is just an awkward way of expressing\nthe first: \nConserv says that only the part of B\nwhich is common to A matters for the truth of \\(Q_M(A,B)\\).\nThat is, the part \\(B-A\\) in Figure 1 doesn’t matter. This\nappears to hold for all determiner denotations, but it fails for\nperfectly natural logical quantifiers, such as MO and\nI from the list \\eqref{ex-qlist3} above. The reason is that it\nis characteristic of determiner denotations that the restriction\nargument restricts the domain of quantification to that\nargument. \nActually, the idea of domain restriction has one further ingredient.\nTo restrict the domain of quantification to a subset A of\nM means not only that \\(B-A\\) is irrelevant but the whole part\nof M that lies outside A, and hence also the part\n\\(M-(A\\cup B)\\) in Figure 1. This in turn is an instance of a more\ngeneral property, applicable to arbitrary generalized quantifiers: \nThat is, nothing happens when the universe is extended, or shrunk, as\nlong as the arguments are not changed. Now recall that for type\n\\({\\langle}1{\\rangle}\\) quantifiers we already provided a logical\nmechanism for restricting the quantification domain to a subuniverse,\nin terms of relativization\n (section 7).\n We can now see (in (b) below) that the combination of Conserv\nand Ext amounts to\nexactly the same thing: \nFact 6 \nAgain, all determiner denotations appear to satisfy Ext.\nAt first sight, nothing in principle would seem\nto prevent a language from containing a determiner, say evso,\nwhich meant every on\nuniverses with less than 10 elements and some on larger\nuniverses. But not only is there in fact no such determiner in any\nlanguage—there couldn’t be, if the noun argument of a\ndeterminer is to restrict the domain of quantification to the\ndenotation of that noun. \nA quantifier such as evso is\nintuitively not constant, in the sense that it doesn’t\nmean the same, or is not interpreted by the same rule, on every\nuniverse. Ext can be seen as a strong\nrequirement of constancy: the rule interpreting Q doesn’t\neven mention the universe. Indeed, many quantifiers from language and\nlogic are Ext. As we saw, all relativized\nquantifiers are Ext, and all the other\nquantifiers in the lists \\eqref{ex-qlist2}–\\eqref{ex-qlist4} as\nwell, except\n W.[24]\n In fact, it seems that all quantifiers taking more than one argument\nthat show up in natural language contexts are Ext.\nAnd many type \\({\\langle}1{\\rangle}\\)\nquantifiers are Ext too, for example,\n\\(\\exists\\), \\(I_j\\), \\(Q^A\\) (when Q is Ext;\nsee \\eqref{QA} above), and all in the list\n\\eqref{ex-qlist1} except \\(Q^R\\). \nBut \\(\\forall\\) and \\(Q^R\\) are not Ext. Yet\none is inclined to say for them too that they mean the same on every\nuniverse. The case of \\(\\forall\\) is particularly interesting since\none might argue that it interprets NPs like everything\nor every\nthing. The crux here is thing.\nIf this expression is seen as a logical constant that always denotes\nthe universe, then these NPs do denote \\(\\forall\\): for all M\nand all \\(B\\subseteq M\\), \nWhen Ext holds, we can usually drop the\nsubscript M and write, for example,  \nrather than \\(Q_M(A,B)\\). That is, a suitable universe can be\npresupposed but left in the background. \nOther properties are not shared by all natural language quantifiers\nbut single out important subclasses. We mentioned two already in\n section 2\n above: symmetry and monotonicity. Typical symmetric\nquantifiers are some, no, at least five, exactly three, an even\nnumber of, infinitely many, whereas all, most, at most\none-third of the are non-symmetric. Another way to express\nsymmetry is to say that the truth-value of \\(Q(A,B)\\) only depends on\nthe set \\(A\\cap B\\). More precisely, call Q\nintersective if for all M and all \\(A,A',B,B'\n\\subseteq M\\): \nOne easily verifies: \nFact 7 \nFor conservative type \\({\\langle}1,1{\\rangle}\\) quantifiers, symmetry\nand intersectivity are\n equivalent.[25] \nWe noted that some of the syllogisms express monotonicity properties.\nIn more succinct notation, a type \\({\\langle}1,1{\\rangle}\\) quantifier\nQ is \nright increasing (right decreasing) iff for all\nM and all \\(A,B \\subseteq B' \\subseteq M\\) (all \\(A,B'\n\\subseteq B \\subseteq M\\)), \\(Q_M(A,B)\\) implies \\(Q_M(A,B')\\). \nSimilarly for left increasing or decreasing, and indeed for\nmonotonicity in any given argument place of a generalized quantifier.\nIn particular, it is clear what it means for a type\n\\({\\langle}1{\\rangle}\\) quantifier to be monotone. Monotonicity is\nubiquitous among natural language quantifiers. It seems that\nsyntactically simple English NPs all denote monotone (increasing or decreasing) type \\({\\langle}1{\\rangle}\\) quantifiers,\nand almost all syntactically simple English determiners denote right\nmonotone\n quantifiers.[26]\n We also have: \nThe Aristotelian all, some, no are monotone in both arguments\n(e.g. all is right increasing and left decreasing), as\nare at least five, no more than ten, infinitely many, whereas\nmost, at least two-thirds of the are right increasing but\nneither increasing nor decreasing in the left argument. Exactly\nthree, between two and seven are non-monotone, though both of\nthese are conjunctions of a (right and left) increasing and a\ndecreasing quantifier (e.g. at least three and at most\nthree), in contrast with an even number of, which is not\na (finite) Boolean combination of monotone quantifiers. \nBoth symmetry and monotonicity have important explanatory roles for\ncertain linguistic phenomena. Symmetry is a feature of (most of) the\nquantifiers allowed in so-called existential there sentences\n(e.g.  There are at least five men in\nthe garden is fine, but There are most\nmen in the garden is not). Monotonicity is crucial for\nexplaining the distribution of polarity items (No\none will ever succeed is fine but Someone\nwill ever succeed is not: negative polarity\nitems such as ever require a\ndecreasing\n environment).[27]\n Furthermore, monotonicity is crucially involved in natural forms\nof reasoning; see\n section 18. \nConsider \nThe expressions John’s, some\nstudent’s, no\n_ except Mary, all _\nexcept a few enthusiastic swimmers, more\nmale than female are quite naturally seen as\ndeterminers: when combined with nouns they form phrases that behave\nlike ordinary NPs. Also, the type \\({\\langle}1,1{\\rangle}\\)\nquantifiers they signify are Conserv and Ext.\nFor example, the sentences in the following\npair are trivially equivalent: \nBut in contrast with the previous examples, they are not Isom,\nsince they involve some fixed individual or\nproperty: if John’s books were stolen, and the number of stolen\nbooks is the same as the number of red pencils (in some discourse\nuniverse), and the number of books that weren’t stolen is the\nsame as the number of pencils that aren’t red, it does\nnot follow that John’s pencils are red, as Isom\nwould have it. \nHowever, just as the non-Isom quantifier\nthree\\(^{\\textit{student}}\\) results by freezing the\nrestriction argument of the Ext quantifier\nthree, the non-Isom quantifiers above\nresult by freezing arguments in more abstract relations, which\nare Isom. We illustrate this with the\npossessive determiner John’s.[28] \nGiven that John denotes an individual\nj, the determiner John’s\ncan be defined, for all M and all \\(A,B\\subseteq M\\),\n by[29] \nwhere \\(R_j = \\{b\\in M\\!: R(j,b)\\}\\) and R is some\n“possessor” relation; it is well-known that this relation\nvaries a lot with the circumstances—one could be talking about\nthe books that John owns, or has written, or borrowed, or bought as a\npresent to Mary, etc. Suppose R is ownership. Then\n (29)\n says that John owns at least one book, and that all of the books he\nowns were stolen. Now consider the more general\n“quantifier” defined, for \\(a\\in M\\), \\(R\\subseteq M^2\\),\nand \\(A,B\\subseteq M\\), by \nWe could say that this is a generalized quantifier of type\n\\({\\langle}0,2,1,1{\\rangle}\\), letting 0 stand for individuals.\n\\(\\mathbf{P}\\) is Isom (extending definition\n\\eqref{ex-isom} in the obvious way to quantifiers of this type), and\nJohn’s results by freezing the\nfirst two arguments to suitable values. \nSimilar constructions work for other cases of quantifier expressions\nin natural languages that denote non-Isom\nquantifiers. For example, the determiner no _\nexcept Mary denotes (given that Mary\nrefers to m) \nThat is,\n (31)\n says that Mary is a professor, that she came to the meeting, and that\nno other professor did. Again, a corresponding Isom\nquantifier of type \\({\\langle}0,1,1{\\rangle}\\)\nis readily defined. So in this way Isom can be\nretrieved for natural language quantifiers. On the other hand,\nassociating type \\({\\langle}1,1{\\rangle}\\) quantifiers with\ndeterminers agrees better with syntax, and allows many generalizations\nconcerning determiner denotations to hold in the non-Isom\ncase as well. \nIsom, i.e., topic neutrality, is standardly\nseen as at least a necessary condition for being a logical\n constant.[30]\n It is possible to distinguish logicality from\nconstancy in the earlier mentioned sense of meaning the same\nover different universes. For one thing, logicality is a property that\nought to be closed under definability, whereas it is not at\nall clear that constancy should be similarly closed. Note, for\nexample, that the class of Ext quantifiers is\nnot closed under first-order definability. More precisely, it is\nclosed under the usual Boolean operations, but not under inner\nnegation and hence not under taking duals, where the\ninner negation of a type \\({\\langle}1{\\rangle}\\) quantifier Q\nis defined by \\((Q\\neg)_M(A) \\Leftrightarrow Q_M(M\\!-\\!A)\\), and the\ndual by \\(Q^d = \\neg(Q\\neg)\\). For example, \\(\\exists^d =\n\\forall\\). \nOne intuition might be that Ext suffices for\nconstancy. But a different intuition is that a quantifier meaning the\nsame on all universes in particular should satisfy Isom,\nwhich forces Q to be the\n“same” on all universes of the same cardinality. These two\nideas are incompatible, since together they would imply that Ext\nimplies Isom, which is\nmanifestly false. Clearly, the vague notion of meaning the same across\ndifferent universes admits of different precisifications. On closer\ninspection, it seems unlikely that there is one precise version that\nwould accommodate all intuitions about sameness. \nIn this situation, a suggestion would be to simply stipulate that\nconstancy amounts to Ext + Isom.\nThis would be a Carnapian explication of\nconstancy. Quantifiers with this combination of properties seem\ncertain to mean the same on all universes. On the other hand, Ext\nbut non-Isom quantifiers\nlike three\\(^{\\textit{student}}\\) or some\nprofessor’s would not have the same meaning across\ndifferent domains, which as we saw accords with one intuition.\nFurthermore, the few natural non-Ext\nquantifiers we have encountered are all definable from Ext\n+ Isom\n quantifiers.[31] \nConsider a typical English sentence where both subject and object are\nquantified: \nThe truth conditions of\n (36)\n can be given in terms of a polyadic quantifier, of type\n\\({\\langle}1,1,2{\\rangle}\\) (omitting M): \n(This is the “narrow scope” reading; the “wide\nscope” reading would be instead \\(\\textit{two}(B,\\{b\\!:\n\\textit{most}(A,(R^{-1})_b))\\).) But this polyadic quantifier results\nfrom two type \\({\\langle}1,1{\\rangle}\\) quantifiers by a ubiquitous\nconstruction that we call iteration. If \\(Q,Q'\\) are of type\n\\({\\langle}1{\\rangle}\\), defined the type \\({\\langle}2{\\rangle}\\)\nquantifier \\(Q\\cdot Q'\\) by \nThen we obtain the iteration of two type \\({\\langle}1,1{\\rangle}\\)\nquantifiers \\(Q_1,Q_2\\) as above with \\(Q_1^A \\cdot Q_2^B\\).\nProperties of iterations are studied in van\nBenthem (1989), Keenan (1992),\nWesterståhl (1994), and Steinert-Threlkeld\nand Icard (2013). \nKeenan thinks of iteration as the Frege boundary. As he and\nothers pointed out, there appear to be many natural language\nquantifiers beyond that boundary, i.e. not definable as\niterations. We give a few examples here; many more can be found in the\nreferences just given. The next sentence may look like expressing an\niteration but in fact doesn’t. \nExample (37) presumably has various interpretations, for example one\nusing the following type \\({\\langle}1,1,2{\\rangle}\\) quantifier: \nThis quantifier is still first-order definable but not an\n iteration.[32]\n Next, consider \nAdverbs like usually, seldom, always,\nnever can be taken to denote generalized quantifiers (an\nobservation originally made in Lewis\n(1975)). For example, Dogs never\nmeow is roughly synonymous with No\ndogs meow. But for\n (38)\n it can be argued that there is a reading where the quantifier applies\nto pairs: among the pairs consisting of a person and a\nfireman who rescues that person, a majority are such that the person\nis grateful. This is just the resumption of most to\npairs, that we defined in \\eqref{ex-qlist4}: \nSo in\n (38b),\n \\(R(a,b)\\) iff \\(a \\in \\textit{person}\\) and \\(b \\in\n\\textit{fireman}\\) and \\(a\\: \\textit{rescued } b\\), and \\(S(a,b)\\) iff\na is grateful to b. It can be shown that for\nmany quantifiers, in particular most, \\(Res^n(Q)\\) is not\ndefinable in \\(\\FO(Q)\\). In fact, \\(Res^2(\\textit{most})\\) is not\ndefinable from any finite number of monadic quantifiers, so it is an\nexample of an irreducibly polyadic\n quantifier.[33] \nNext: \nHere (39a) can have the truth conditions \nwhere RECIP is the type \\({\\langle}1,2{\\rangle}\\) quantifier\ndefined in \\eqref{ex-qlist4}. That is, there is a set of five Boston\npitchers such that if you take any two of those, either they sit next\nto each other, or there is one pitcher, or two, or at most three (all\nin the chosen set), between them. Similarly for\n (39b).\n This is just one of several constructions of polyadic quantifiers\nthat occur in reciprocal\n sentences.[34] \nFinally, consider the sentence \n(40) has been put forward as an example of branching\nquantification, which can be written in a two-dimensional logical\nformat as \nwhere the intended reading is that there is a subset X of\nA containing most of the elements of A, and a similarly\nlarge subset Y of B, such that each pair \\((a,b)\\) where\n\\(a \\in X\\) and \\(b \\in Y\\) belongs to the relation R. More\ngenerally, we have a polyadic quantifier of type\n\\({\\langle}1,1,2{\\rangle}\\) defined for any \\(Q_1,Q_2\\) of type\n\\({\\langle}1,1{\\rangle}\\) by \nQuite plausibly, this gives a reading of\n (40).\n Note that x and y here are independent of each\nother. If one instead would use any one of the linear sentences \nthen either y depends on x or vice versa. The\ntwo-dimensional syntax in\n (41)\n reflects this semantic\n independence.[35] \nIt can be shown that \\(Br(\\textit{most},\\textit{most})\\) is not\nexpressible in \\(\\FO(\\textit{most})\\) alone; indeed not with any\nfinite number of monadic quantifiers (for a proof, see Hella,\nVäänänen, and Westerståhl\n(1997)). On the other hand, branching quantifiers are obtained\nwith a “lifting” operation applied to monadic quantifiers,\nand similarly for resumption. Indeed, although natural language\nexhibits numerous polyadic quantifiers well beyond the Frege boundary,\none might still make a case for the claim that these are all obtained\nfrom monadic quantifiers in systematic ways. \nThe advent of generalized quantifiers had a huge impact on linguistic\nsemantics via Montague’s work in the late 60s, reinforced by the\napplication of model-theoretic methods in the early 80s by Barwise and\nCooper, Keenan and Stavi, and others (see note\n 21).\n In almost all examples in these works, the natural language was\nEnglish. Linguists have since applied and tested the tools and methods\nof “GQ theory” on other languages. The collection Bach\net al. (1995) has, among many other\nthings, seven case studies of quantification in other languages. It\nalso emphasizes the distinction between D-quantification and\nA-quantification. In D-quantification, which most of our\nexamples so far exhibit, the quantifier expression is (usually) a\ndeterminer which applies to a noun. A-quantification is performed by\nother means—A stands for adverbs, auxiliaries, affixes, and\nargument-structure adjusters. Many languages prefer A-quantification,\nsome exclusively. English has both types; recall the adverbs of\nquantification in\n (38).[36] \nMore recently, the volumes Keenan and Paperno\n(2012) and Paperno and Keenan\n(2017) have a separate chapter answering a fixed set of\nquestions about expressing quantification for each of 34 different\nlanguages (different also from those mentioned above), in order to\nmake an extensive inventory of their expressive\n resources.[37]\n The approach is semantic: the questions are of the form “Can\none express X in your language, and if so in what ways?”, which\nallows precise questions about conservativity, monotonicity, polarity\nitems, monadic vs. polyadic quantification, etc. to be put\nto each language. The summary in the last chapter shows that many of\nthe generalizations that hold for English, concerning the existence of\nexpressions denoting certain quantifiers and the properties of those,\nhold in all or most of the other languages studied as well (Keenan and\nPaperno list 25 such generalizations). \nOn the other hand, beginning in the 1990s some linguists have argued\nthat GQ theory is unable to account for a range of important semantic\nphenomena—in English and other languages—related to\nquantification. Szabolcsi (2010) gives a\ndetailed account of these developments. One issue is that GQ theory\nappears to have nothing to say about the compositional meaning of\ncomplex determiners. For example, how is the meaning of more\nthan five derived from the meanings of\nits parts? Or consider most, which is\noften treated as a simple determiner, even though its meaning must\nsomehow come from being a superlative of more. \nAnother problematic phenomenon is scope. While GQ theory in\nprinciple seems to allow all theoretically possible scopings of nested\nquantifier expressions, natural languages have restrictions regulating\nwhich of these are actually allowed. Indeed, scope is a major topic in\nlinguistic syntax and semantics, and a complex one. The problem is\nalso methodological: how to determine if a given sentence S\ncan actually mean Y (where Y corresponds to a\nparticular scoping)? First, one must filter out cases where the\nunavailability of Y depends on facts about the world, not about\nlanguage. Second, whose intuitions should count: the linguist’s,\nor those of native speakers in a test situation, or perhaps\nstatistical evidence should play a role? Still, while it is true that\nmany readings that seem impossible at first sight are in fact\navailable in sufficiently specific contexts, it is plausible that\nlanguages have scope restrictions beyond the reach of GQ\n theory.[38] \nThe “GQ theorist” could reply that her tools were never\nmeant to fully explain scope, or enable compositional analyses of\nevery complex expression. The model-theoretic framework is first of\nall descriptive: it provides mathematical objects that can\nserve as (models of) meaning, and formulate properties of and\nrelations between these objects. Sometimes, facts about the\nmathematical objects reveal insights about the things they are\nmodeling, as in the case of monotonicity and polarity items, or the\ncase of the meaning of conjoined noun phrases. But there is no reason\nto expect this to happen in every case. \nThese are positions in an ongoing debate about the role of formal\nmethods, and in particular of model-theoretic tools, in semantics; a\ndebate which is by no means settled. What seems clear is that the\nphenomena related to quantification in natural languages continue to\nprovide excellent material for that discussion. \nIn recent years there has been an explosion of work connecting\nsemantics, reasoning, and cognition, much of it related to how\nspeakers understand and learn and reason with quantified expressions.\nA major strand of research concerns monotonicity (section\n13). Already Barwise and Cooper (1981)\nnoted the ubiquity of monotone quantifiers in natural languages, and\nsuggested a way of showing that monotone quantifiers are easier to\nprocess than non-monotone ones, and that increasing quantifiers are\neasier than decreasing ones. They also suggested that psychological\nexperiments might be used to test their hypothesis. Their technical\nproposal was developed further in van Benthem\n(1986), which introduced a notion of count complexity\nand showed that, under some assumptions, the quantifiers with minimal\ncount complexity are precisely the ones with a certain strong\nmonotonicity\n property.[39] \nMonotonicity is also involved in what van Benthem has called\n“one-step” reasoning, which appears to be easily available\nto speakers. The monotonicity behavior of basic determiners already\nshows how such reasoning is licensed. Marking right increasing\n(decreasing) type \\({\\langle}1,1{\\rangle}\\) quantifiers with a + (a\n\\(-\\)) to the right, and similarly for left monotonicity, we have, for\nexample: \nwhere \\(\\cdot\\) marks that the position is neither decreasing nor\nincreasing. A nice example is the following inference (from Icard\nand Moss (2014), adapting an example in\nGeurts and Slik (2005)): \nThe premise is a “donkey sentence” with most,\nand it is notoriously hard to pin down the exact\ntruth conditions of these. In fact, several readings are\n possible.[40]\n In spite of this, speakers seem to have no problem making this\ninference, apparently since most is right increasing (the VP\nargument speak it at home is extended to speak it at home\nor at work), regardless of what the subject noun phrase (the same\nin both sentences) exactly means. \nMany other expressions and phrases besides determiners show fixed\nmonotonicity patterns. Beginning with van\nBenthem (1986) this has led to algorithms for how polarity\nmarkers are assigned to the nodes of analysis trees of sentences\n(relative to a given grammar), or how to incorporate such markers\ndirectly in the type notation; see Icard and\nMoss (2014) for an overview and further references. Besides\ntheir role in inference, such marking can also explain, and sometimes\neven predict, the distribution of negative polarity items in languages\n(end of\n section 13).\n Moreover, in many cases no syntactic analysis is necessary:\ninferences can be made directly on surface form, and would in this\nsense be available “on the fly” to speakers; compare\n (43).\n The paper just mentioned also presents a complete axiomatization of a\nformal Monotonicity Calculus, in which many varieties of\nreasoning with monotonicity can be\n expressed.[41] \nA somewhat parallel development has been the formal study of various\nsyllogistic fragments; we noted in\n section 2\n that many syllogisms express monotonicity properties. These\nfragments, most of which were studied by Ian Pratt-Hartmann and above\nall Larry Moss, range from those containing only simple sentences like\nallXY or someXY to ones allowing complements,\nrelative clauses, transitive verbs, non-first-order quantifiers like\nmost, and other features. Here is an example (Moss p.c.) of\nan inference in such a fragment: \nThis illustrates how quite involved reasoning can be expressed in a\nsimple syllogistic-like language. The inference is valid, but one has\nto think a bit to see\n that.[42]\n A main feature of most of these fragments is that, in addition to\nhaving explicit complete axiomatizations, validity in them is\ndecidable, in contrast with first-order logic. This also\nholds for some fragments with quantifiers that are not\nFO-definable. Like the monotonicity calculus, the study of\nsyllogistic fragments is part of the enterprise somewhat loosely\ncalled natural logic, resulting in well-behaved subsystems of\nmore familiar logics, in the sense of being both closer to natural\nlanguage and computationally more tractable; see Moss\n(2015) for a\n survey.[43] \nOn the cognitive side, questions of understanding and learning related\nto quantification and monotonicity have been studied both in\npsychology and neuroscience. Geurts and Slik\n(2005) asked subjects whether certain inferences involving\nmonotonicity were valid or not; the results largely corroborated\nBarwise and Cooper’s earlier hypotheses. The meaning of\nindividual determiners has also been studied empirically; Pietroski\net al. (2009) investigated most,\nwhere the method was to show subjects\na picture with yellow and blue dots for a very short time (to\neliminate counting) and ask, say, if it is true or false that most of\nthe dots are yellow. Variations of this kind of experiment are common\nin the literature; a recent instance is Odic et\nal. (2018), which studies the mass/count distinction in\ncognition and semantics. Both studies involve the human number\nsense and its relation to understanding quantificational\nlanguage. One might entertain a “Whorfian” hypothesis that\nthe latter is a prerequisite for the former. This was tested with\nneurobiological methods (brain-scan methods combined with\npsychological tests with patients suffering from various brain\ndisorders) in Clark and Grossman (2007).\nThey did not find any empirical support for that hypothesis; see also\nClark (2011a) for a description of the\nexperiment and more on research on quantification and number\nsense. \nThere are by now a fair number of empirical studies of how various\nclasses of quantifiers identified by logical or computational means\nare reflected in terms of learning, understanding, cognitive load,\netc. Conversely, linguistic and cognitive facts suggest new\ntheoretical questions. For example, as regards computational\ncomplexity, Sevenster (2006) showed that the branching of\nmost as in\n (40)\n in\n section 9\n is\n intractable.[44]\n Subsequently, Szymanik observed that if the operations of\nresumption and iteration (as in\n (38)\n and\n (36),\n respectively) are applied to PTIME quantifiers, the result is again\nin PTIME, in contrast with branching. Similarly, some forms of\nreciprocal constructions preserve PTIME computability whereas other\ndon’t: “lifting” exactly five with\nRECIP as in\n (39a)\n does, but similarly lifting most as in\n (39b)\n does not. \nIn van Benthem’s semantic automata setting\n (section 9),\n Steinert-Threlkeld and Icard (2013)\nproved that the Frege boundary\n (section 16)\n is robust in the sense that if two Conserv\nand Ext type \\({\\langle}1,1{\\rangle}\\)\nquantifiers are recognizable by finite (or push-down) automata, then\nso is their iteration. Moreover, Steinert-Threlkeld\n(2016) showed that for\nlarge classes of type \\({\\langle}1,1,2{\\rangle}\\) quantifiers, it is\ndecidable whether they are iterations of type\n\\({\\langle}1,1{\\rangle}\\) quantifiers or not. A recent presentation of\nboth theoretical and empirical results around the cognitive aspects of\nquantifier recognition is Szymanik\n(2016). \nComputational models of learning the meaning of quantifiers\nhave been given; for example by Clark (2011a) in the semantic automata\nsetting. In a recent development, Steinert-Threlkeld\nand Szymanik (forthcoming)\nstudies learnability with the technology of neural networks, testing\nwhether certain quantifiers satisfying three commonly proposed\nuniversals—that simple determiner denotations are\nmonotone, Isom, and Conserv,\nrespectively—are easier to learn than quantifiers that do not\nhave these properties. For each universal, the time it takes the\nnetwork to learn a quantifier satisfying it is compared to the time it\ntakes to learn a quantifier that doesn’t. It turns out that\nmonotone and Isom are easier than non-monotone\nand non-Isom ones, whereas there is no\ndetectable difference for Conserv.[45] \nThese are just glimpses of ongoing research. The investigation of how\nspeakers process quantified expressions, combining the basic\nmodel-theoretic analysis with methods from psychology, neuroscience,\nand computer science, is by now a rich area in the study of\ngeneralized quantifiers.","contact.mail":"dag.westerstahl@philosophy.su.se","contact.domain":"philosophy.su.se"}]
