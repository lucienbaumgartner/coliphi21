[{"date.published":"2002-04-12","date.changed":"2019-05-27","url":"https://plato.stanford.edu/entries/scientific-knowledge-social/","author1":"Helen Longino","entry":"scientific-knowledge-social","body.text":"\n\n\nStudy of the social dimensions of scientific knowledge encompasses the\neffects of scientific research on human life and social relations, the\neffects of social relations and values on scientific research, and the\nsocial aspects of inquiry itself. Several factors have combined to\nmake these questions salient to contemporary philosophy of science.\nThese factors include the emergence of social movements, like\nenvironmentalism and feminism, critical of mainstream science;\nconcerns about the social effects of science-based technologies;\nepistemological questions made salient by big science; new trends in\nthe history of science, especially the move away from internalist\nhistoriography; anti-normative approaches in the sociology of science;\nturns in philosophy to naturalism and pragmatism. This entry reviews\nthe historical background to current research in this area and\nfeatures of contemporary science that invite philosophical attention.\n\n\nThe philosophical work can roughly be classified into two camps. One\nacknowledges that scientific inquiry is in fact carried out in social\nsettings and asks whether and how standard epistemology must be\nsupplemented to address this feature. The other treats sociality as a\nfundamental aspect of knowledge and asks how standard epistemology\nmust be modified or reformed from this broadly social perspective.\nConcerns in the supplementing approach include such matters as trust\nand accountability raised by multiple authorship, the division of\ncognitive labor, the reliability of peer review, the challenges of\nprivately funded science, as well as concerns arising from the role of\nscientific research in society. The reformist approach highlights the\nchallenge to normative philosophy from social, cultural, and feminist\nstudies of science while seeking to develop philosophical models of\nthe social character of scientific knowledge and inquiry. It treats\nthe questions of the division of cognitive labor, expertise and\nauthority, the interactions of science and society, etc., from the\nperspective of philosophical models of the irreducibly social\ncharacter of scientific knowledge. Philosophers employ both formal\nmodeling techniques and conceptual analysis in their efforts to\nidentify and analyze epistemologically relevant social aspects of\nscience.\n\nPhilosophers who study the social character of scientific knowledge\ncan trace their lineage at least as far as John Stuart Mill. Mill,\nCharles Sanders Peirce, and Karl Popper all took some type of critical\ninteraction among persons as central to the validation of knowledge\nclaims. \nMill’s arguments occur in his well-known political essay On\nLiberty, (Mill 1859) rather than in the context of his logical and\nmethodological writings, but he makes it clear that they are to apply\nto any kind of knowledge or truth claim. Mill argues from the\nfallibility of human knowers to the necessity of unobstructed\nopportunity for and practice of the critical discussion of ideas. Only\nsuch critical discussion can assure us of the justifiability of the\n(true) beliefs we do have and can help us avoid falsity or the\npartiality of belief or opinion framed in the context of just one\npoint of view. Critical interaction maintains the freshness of our\nreasons and is instrumental in the improvement of both the content and\nthe reasons of our beliefs. The achievement of knowledge, then, is a\nsocial or collective, not an individual, matter. \nPeirce’s contribution to the social epistemology of science is\ncommonly taken to be his consensual theory of truth: “The\nopinion which is fated to be ultimately agreed to by all who\ninvestigate is what we mean by truth, and the object represented is\nthe real.” (Peirce 1878, 133) While often read as meaning that\nthe truth is whatever the community of inquirers converges on in the\nlong run, the notion is interpretable as meaning more precisely either\nthat truth (and “the real”) depends on the agreement of\nthe community of inquirers or that it is an effect of the real that it\nwill in the end produce agreement among inquirers. Whatever the\ncorrect reading of this particular statement, Peirce elsewhere makes\nit clear that, in his view, truth is both attainable and beyond the\nreach of any individual. “We individually cannot hope to attain\nthe ultimate philosophy which we pursue; we can only seek it for the\ncommunity of philosophers.” (Peirce 1868, 40). Peirce puts great\nstock in instigating doubt and critical interaction as means to\nknowledge. Thus, whether his theory of truth is consensualist or\nrealist, his view of the practices by which we attain it grants a\ncentral place to dialogue and social interaction. \nPopper is often treated as a precursor of social epistemology because\nof his emphasis on the importance of criticism in the development of\nscientific knowledge. Two concepts of criticism are found in his works\n(Popper 1963, 1972) and these can be described as logical and\npractical senses of falsification. The logical sense of falsification\nis just the structure of a modus tollens argument, in which a\nhypothesis is falsified by the demonstration that one of its logical\nconsequences is false. This is one notion of criticism, but it is a\nmatter of formal relations between statements. The practical sense of\nfalsification refers to the efforts of scientists to demonstrate the\ninadequacies of one another’s theories by demonstrating\nobservational shortcomings or conceptual inconsistencies. This is a\nsocial activity. For Popper the methodology of science is\nfalsificationist in both its logical and practical senses, and science\nprogresses through the demonstration by falsification of the\nuntenability of theories and hypotheses. Popper’s logical\nfalsificationism is part of an effort to demarcate genuine science\nfrom pseudo science, and has lost its plausibility as a description of\nscientific methodology as the demarcation project has come under\nchallenge from naturalist and historicist approaches in philosophy of\nscience. While criticism does play an important role in some current\napproaches in social epistemology, Popper’s own views are more\nclosely approximated by evolutionary epistemology, especially that\nversion that treats cognitive progress as the effect of selection\nagainst incorrect theories and hypotheses. In contrast to Mill’s\nviews, for Popper the function of criticism is to eliminate false\ntheories rather than to improve them. \nThe work of Mill, Peirce, and Popper is a resource for philosophers\npresently exploring the social dimensions of scientific knowledge.\nHowever, the current debates are framed in the context of developments\nin both philosophy of science and in history and social studies of\nscience following the collapse of the logical empiricist consensus.\nThe philosophers of the Vienna Circle are conventionally associated\nwith an uncritical form of positivism and with the logical empiricism\nthat replaced American pragmatism in the 1940s and 1950s. According to\nsome recent scholars, however, they saw natural science as a potent\nforce for progressive social change. (Cartwright, Cat, and Chang 1996;\nGiere and Richardson, eds., 1996; Uebel 2005) With its grounding in\nobservation and public forms of verification, science for them\nconstituted a superior alternative to what they saw as metaphysical\nobscurantism, an obscurantism that led not only to bad thinking but to\nbad politics. While one development of this point of view leads to\nscientism, the view that any meaningful question can be answered by\nthe methods of science; another development leads to inquiry into what\nsocial conditions promote the growth of scientific knowledge. Logical\nempiricism, the version of Vienna Circle philosophy that developed in\nthe United States, focused on logical, internal aspects of scientific\nknowledge and discouraged philosophical inquiry into the social\ndimensions of science. These came into prominence again after the\npublication of Thomas Kuhn’s Structure of Scientific\nRevolutions (Kuhn 1962). A new generation of sociologists of\nscience, among them Barry Barnes, Steven Shapin, and Harry Collins,\ntook Kuhn’s emphasis on the role of non-evidential community\nfactors in scientific change even further than he had and argued that\nscientific judgment was determined by social factors, such as\nprofessional interests and political ideologies (Barnes 1977, Shapin\n1982, Collins 1983). This family of positions provoked a\ncounter-response among philosophers. These responses are marked by an\neffort to acknowledge some social dimensions to scientific knowledge\nwhile at the same time maintaining its epistemological legitimacy,\nwhich they take to be undermined by the new sociology. At the same\ntime, features of the organization of scientific inquiry compel\nphilosophers to consider their implications for the normative analysis\nof scientific practices. \nThe second half of the twentieth century saw the emergence of what has\ncome to be known as Big Science: the organization of large numbers of\nscientists bringing different bodies of expertise to a common research\nproject. The original model was the Manhattan Project, undertaken\nduring the Second World War to develop an atomic weapon in the United\nStates. Theoretical and experimental physicists located at various\nsites across the country, though principally at Los Alamos, New\nMexico, worked on sub-problems of the project under the overall\ndirection of J. Robert Oppenheimer. While academic and military\nresearch have since been to some degree separated, much experimental\nresearch in physics, especially high energy particle physics,\ncontinues to be pursued by large teams of researchers. Research in\nother areas of science as well, for example the work comprehended\nunder the umbrella of the Human Genome Project, has taken on some of\nthe properties of Big Science, requiring multiple forms of expertise.\nIn addition to the emergence of Big Science, the transition from small\nscale university or even amateur science to institutionalized research\nwith major economic impacts supported by national funding bodies and\nconnected across international borders has seemed to call for new\nethical and epistemological thinking. Moreover, the consequent\ndependence of research on central funding bodies and increasingly,\nprivate foundations or commercial entities, prompts questions about\nthe degree of independence of contemporary scientific knowledge from\nits social and economic context. \nJohn Hardwig (1985) articulated one philosophical dilemma posed by\nlarge teams of researchers. Each member or subgroup participating in\nsuch a project is required because each has a crucial bit of expertise\nnot possessed by any other member or subgroup. This may be knowledge\nof a part of the instrumentation, the ability to perform a certain\nkind of calculation, the ability to make a certain kind of measurement\nor observation. The other members are not in a position to evaluate\nthe results of other members’ work, and hence, all must take one\nanothers’ results on trust. The consequence is an experimental\nresult, (for example, the measurement of a property such as the decay\nrate or spin of a given particle) the evidence for which is not fully\nunderstood by any single participant in the experiment. This leads\nHardwig to ask two questions, one about the evidential status of\ntestimony, and one about the nature of the knowing subject in these\ncases. With respect to the latter, Hardwig says that either the group\nas a whole, but no single member, knows or it is possible to know\nvicariously. Neither of these is palatable to him. Talking about the\ngroup or the community knowing smacks of superorganisms and\ntranscendent entities and Hardwig shrinks from that solution.\nVicarious knowledge, knowing without oneself possessing the evidence\nfor the truth of what one knows, requires, according to Hardwig, too\nmuch of a departure from our ordinary concepts of knowledge. \nThe first question is, as Hardwig notes, part of a more general\ndiscussion about the epistemic value of testimony. Much of what passes\nfor common knowledge is acquired from others. We depend on experts to\ntell us what is wrong or right with our appliances, our cars, our\nbodies. Indeed, much of what we later come to know depends on what we\npreviously learned as children from our parents and teachers. We\nacquire knowledge of the world through the institutions of education,\njournalism, and scientific inquiry. Philosophers disagree about the\nstatus of beliefs acquired in this way. Here is the question: If\nA knows that p on the basis of evidence e,\nB has reason to think A trustworthy and B\nbelieves p on the basis of A’s testimony that\np, does B also know that p? Some\nphilosophers, as Locke and Hume seem to have, argue that only what one\nhas observed oneself could count as a good reason for belief, and that\nthe testimony of another is, therefore, never on its own sufficient\nwarrant for belief. Thus, B does not know simply on the basis\nof A’s testimony but must have additional evidence\nabout A’s reliability. While this result is consistent\nwith traditional philosophical empiricism and rationalism, which\nemphasized the individual’s sense experience or rational\napprehension as foundations of knowledge, it does have the consequence\nthat we do not know most of what we think we know. \nA number of philosophers have recently offered alternative analyses\nfocusing on one or another element in the problem. Some argue that\ntestimony by a qualified expert is itself evidential, (Schmitt 1988),\nothers that the expert’s evidence constitutes good reason for,\nbut is not itself evidential for the recipient of testimony (Hardwig\n1985, 1988), others that what is transmitted in testimony is knowledge\nand not just propositional content and thus the question of the kind\nof reason a recipient of testimony has is not to the point (Welbourne\n1981). \nHowever this dispute is resolved, questions of trust and authority\narise in a particularly pointed way in the sciences, and\nHardwig’s dilemma for the physics experiment is also a specific\nversion of a more general phenomenon. A popular conception of science,\nfed partly by Popper’s falsificationism, is that it is\nepistemically reliable because the results of experiments and\nobservational studies are checked by independent repetition. In\npractice, however, only some results are so checked and many are\nsimply accepted on trust. Not only must positive results be accepted\non trust, but claims of failure to replicate as well as other\ncritiques must be also. Thus, just as in the non-scientific world\ninformation is accepted on trust, so in science, knowledge grows by\ndepending on the testimony of others. What are the implications of\naccepting this fact for our conceptions of the reliability of\nscientific knowledge? \nThe philosopher of biology, David Hull, argued in his (1988) that\nbecause the overall structure of reward and punishment in the sciences\nis a powerful incentive not to cheat, further epistemological analysis\nof the sciences is unnecessary. What scientists have to lose is their\nreputation, which is crucial to their access to grants,\ncollaborations, prizes, etc. So the structure itself guarantees the\nveridicality of research reports. But some celebrated recent episodes,\nsuch as the purported production of “cold fusion” were\ncharacterized by the failure of replication attempts to produce the\nsame phenomenon. And, while the advocates of cold fusion were\nconvinced that their experiments had produced the phenomenon, there\nhave also been cases of outright fraud. Thus, even if the structure of\nreward and punishment is an incentive not to cheat, it does not\nguarantee the veridicality of every research report. \nOn Hull’s view, the scientific community seeks true theories or\nadequate models. Credit, or recognition, accrues to individuals to the\nextent they are perceived as having contributed to that community\ngoal. That is, individual scientists seek reputation and recognition,\nto have their work cited as important and as necessary to further\nscientific progress. Cheating, by misreporting experimental results or\nother misconduct, will be punished by loss of reputation. But this\ndepends on strong guarantees of detection. Absent such guarantees,\nthere is as strong an incentive to cheat, to try to obtain credit\nwithout necessarily having done the work, as not to cheat. \nBoth Alvin Goldman (Goldman, 1995, 1999) and Philip Kitcher (1993)\nhave treated the potential for premature, or otherwise (improperly)\ninterested reporting of results to corrupt the sciences as a question\nto be answered by means of decision theoretic models. The decision\ntheoretic approach to problems of trust and authority treats both\ncredit and truth as utilities. The challenge then is to devise\nformulas that show that actions designed to maximize credit also\nmaximize truth. Kitcher, in particular, develops formulas intended to\nshow that even in situations peopled by non-epistemically motivated\nindividuals (that is, individuals motivated more by a desire for\ncredit than by a desire for truth), the reward structure of the\ncommunity can be organized in such a way as to maximize truth and\nfoster scientific progress. One consequence of this approach is to\ntreat scientific fraud and value or interest infused science as the\nsame problem. One advantage is that it incorporates the motivation to\ncheat into the solution to the problem of cheating. But one may wonder\nhow effective this solution really is. Increasingly, we learn of\nproblematic behavior in science based industries, such as the\npharmaceutical industry. Results are withheld or distorted, authorship\nis manipulated. Hot areas, such as stem cell research, cloning, or\ngene modification, have been subjected to fraudulent research. Thus,\neven if the structure of reward and punishment is an in principle\nincentive not to cheat, it does not guarantee the reliability of every\nresearch report. The decision theoretic model needs to include at\nleast one more parameter, namely the anticipated likelihood of\ndetection within a relevant timeframe. \nCommunity issues have also been addressed under the banners of\nresearch ethics and of peer review. One might think that the only\nethical requirements on scientists are to protect their research\nsubjects from harm and, as professional scientists, to seek truth\nabove any other goals. This presupposes that seeking truth is a\nsufficient guide to scientific decision-making. Heather Douglas, in\nher critical study of the ideal of value-freedom (Douglas 2009),\nrejects this notion. Douglas draws on her earlier study of inductive\nrisk (Douglas 2000) to press the point that countless methodological\ndecisions required in the course of carrying out a single piece of\nresearch are underdetermined by the factual elements of the situation\nand must be guided by an assessment of the consequences of being\nwrong. Science is not value-free, but can be protected from the\ndeleterious effects of values if scientists take steps to mitigate the\ninfluence of inappropriate values. One step is to distinguish between\ndirect and indirect roles of values; another is the articulation of\nguidelines for individual scientists. Values play a direct role when\nthey provide direct motivation to accept or reject a theory; they play\nan indirect role when they play a role in evaluating the consequences\nof accepting or rejecting a claim, thus influencing what will count as\nsufficient evidence to accept or reject. The responsibility of\nscientists is to make sure that values do not play a direct role in\ntheir work and to be transparent about the indirect roles of values. A\nnumber of writers have taken issue with the tenability of\nDouglas’s distinction between direct and indirect. Steel and\nWhyte (2012) examine testing guidelines developed by pharmaceutical\ncompanies to point out that the very same decision may be motivated by\nvalues playing a direct role or playing an indirect role. If the point\nis to prohibit practices such as withholding negative results, then it\nshouldn’t matter whether the practice is motivated by values\nfunctioning directly or indirectly. Elliott (2011) questions whether\nonly harmful consequences should be considered. If science is to be\nuseful to policy makers, then questions of relative social benefit\nshould also be permitted to play a role. Finally the cognitive\nactivities demanded by Douglas’s ethical prescriptions for\nscientists seem beyond the capacities of individual scientists. This\npoint will be pursued below. \nTorsten Wilholt (2013) argues that the research situation is more\ncomplicated than the epistemic vs. nonepistemic tradeoff implied by\nthe decision theoretic approach. In part because of the difficulties\nin achieving the degree of knowledge required to realize\nDouglas’s ethical prescriptions, he argues that the reliance\ncalled for in science extends beyond the veridicality of reported\nresults to the values guiding the investigators relied upon. Most\nresearch involves both results expressed statistically (which requires\nchoice of significance threshold and balancing chances of Type I vs.\nType II error) and multiple steps each requiring methodological\ndecisions. These decisions, Wilholt argues, represent trade-offs among\nthe reliability of positive results, the reliability of negative\nresults, and the power of the investigation. In making these\ntradeoffs, the investigator is per force guided by an evaluation of\nthe consequences of the various possible outcomes of the study.\nWilholt extends the arguments about inductive risk offered originally\nby Richard Rudner and elaborated by Heather Douglas to propose that,\nin relying on another’s results I am relying not only on their\ncompetence and truthfulness, but on their making methodological\ndecisions informed by the same valuations of outcomes as I have. This\nattitude is more than epistemic reliance, but a deeper attitude: one\nof trust that we are guided by the same values in a shared enterprise.\nFor Wilholt, then, scientific inquiry engages ethical norms as well as\nepistemic norms. Formal or mechanical solutions such as those\nsuggested by the application of decision theoretic models are not\nsufficient, if the community must be held together by shared ethical\nvalues. \nPeer review and replication are methods the scientific community,\nindeed the research world in general, employs to assure consumers of\nscientific research that the work is credible. Peer review both of\nresearch proposals and of research reports submitted for publication\nscreens for quality, which includes methodological competence and\nappropriateness as well as for originality and significance, while\nreplication is intended to probe the robustness of results when\nreported experiments are carried out in different laboratories and\nwith slight changes to experimental conditions. Scholars of peer\nreview have noted various forms of bias entering into the peer review\nprocess. In a review of the literature, Lee, Sugimoto, Zhang, and\nCronin (2013) report documented bias along gender, language,\nnationality, prestige, and content as well as such problems as lack of\ninter-reviewer reliability consistency, confirmation bias, and\nreviewer conservatism. Lee (2012) argues that a Kuhnian perspective on\nvalues in science interprets lack of inter-reviewer consistency as\nvariation in interpretation, applicability, and weight assigned to\nshared values by different members of the scientific community. Lee\nand colleagues (2013) argue that journal editors must take much more\naction than is currently taken to require that researchers make their\nraw data and other relevant trial information available to enable peer\nreviewers to conduct their work adequately. \nOne issue that has yet to be addressed by philosophers is the gap\nbetween the ideal of replication resulting in confirmation,\nmodification, or retraction and the reality. This ideal lies behind\nthe assumptions of efficacy of structures of reward and sanction. Only\nif researchers believe that their research reports will be probed by\nefforts at replication will the threat of sanctions against faulty or\nfraudulent research be realistic. John Ioannidis and collaborators\n(Tatsioni, Bonitsis, and Ioannidis 2007; Young, N.S. Ioannidis, and\nAl-Ubaydli 2008) have shown how infrequently attempts to replicate are\nactually made and, even more strikingly, how contradicted results\npersist in the literature. This is an issue that goes beyond\nindividuals and beyond large research collaborators to the scientific\ncommunity in general. It underscores Wilholt’s contention that\nthe scientific community must be held together by bonds of trust, but\nmuch more empirical and philosophical work is needed to address how to\nproceed when such trust is not justified. The demonstration of\nwidespread lack of replicability on studies in psychology and in\nbiomedical research has prompted debate about the causes and the\nseriousness of the alleged crisis (Loken and Gelman 2017; Ioannidis\n2007; Redish, Kummerfeld, Morris, and Love 2018). \nWinsberg, Huebner, and Kukla (2013) draw attention to a different kind\nof supra-empirical, ethical issue raised by the contemporary situation\nof multiple authorship. What they call “radically collaborative\nresearch” involves investigators with different forms of\nexpertise, as in Hardwig’s example, and as is now common across\nmany fields, collaborating to generate an experimental result. For\nWinsberg, Huebner, and Kukla, the question is not merely reliability,\nbut accountability. Who can speak for the integrity of the research\nwhen it has been conducted by researchers with a variety not just of\ninterests, but of methodological standards, most opaque one to\nanother? Winsberg, Huebner, and Kukla argue that a model of the social\ncollaboration is needed as much as a model of the data or of the\ninstruments. They argue further that the laissez-faire Wisdom of\nCrowds model (according to which local differences in methodological\nstandards will cancel each other out), while perhaps adequate if the\nquestion is one of reliability, is not adequate for addressing these\nissues of accountability. They do not themselves, however, offer an\nalternative model. \nWork on the role of science in society encompasses both general models\nof the public authority of science and analysis of particular research\nprograms that have a bearing on public life. In their early work,\nSteve Fuller and Joseph Rouse were both concerned with political\ndimensions of cognitive authority. Rouse, whose (1987) integrated\nanalytic and continental philosophy of science and technology, sought\nto develop what might be called a critical pragmatism. This\nperspective facilitated an analysis of the transformative impact of\nscience on human life and social relations. Rouse emphasized the\nincreased power over individual lives that developments in science\nmake possible. This can only be said to have increased with the\ndevelopment of information technology. Fuller (1988) partially\naccepted the empirical sociologists’ claim that traditional\nnormative accounts of scientific knowledge fail to get a purchase on\nactual scientific practices, but took this as a challenge to relocate\nthe normative concerns of philosophers. These should include the\ndistribution and circulation of knowledge claims. The task of social\nepistemology of science, according to Fuller, should be regulation of\nthe production of knowledge by regulating the rhetorical,\ntechnological, and administrative means of its communication. While\nthere has not been much uptake of Fuller’s proposals as\narticulated, Lee’s work mentioned above begins to make detailed\nrecommendations that take into account the current structures of\nfunding and communication. \nOne key area of socially relevant interdisciplinary science is risk\nassessment, which involves both research on the effects of various\nsubstances or practices and the evaluation of those effects once\nidentified. The idea is to gain an understanding of both positive\neffects and of negative effects and a method of evaluating these. This\ninvolves integrating the work of specialists in the kind of substance\nwhose risks are under assessment (geneticists, chemists, physicists),\nbiomedical specialists, epidemiologists, statisticians, and so on. In\nthese cases, we are dealing not only with the problems of trust and\nauthority among specialists from different disciplines, but also with\nthe effects of introducing new technologies or new substances into the\nworld. The risks studied are generally of harm to human health or to\nthe environment. Interest in applying philosophical analysis to risk\nassessment originated in response to debates about the development and\nexpansion of nuclear power-generating technologies. In addition, the\napplication of cost-benefit analysis and attempts to understand\ndecision-making under conditions of uncertainty became topics of\ninterest as extensions of formal modeling techniques (Giere 1991).\nThese discussions intersect with debates about the scope of rational\ndecision theory and have expanded to include other technologies as\nwell as applications of scientific research in agriculture and in the\nmyriad forms of biological engineering. Essays on the relation between\nscience and social values in risk research collected in the volume\nedited by Deborah Mayo and Rachelle Hollander (1991) attempt to steer\na course between uncritical reliance on cost-benefit models and their\nabsolute rejection. Coming from a slightly different angle, the\nprecautionary principle represents an approach shifting the burden of\nproof in regulatory decisions from demonstration of harm to\ndemonstration of safety of substances and practices. Carl Cranor\n(2004) explores versions of the principle and defends its use in\ncertain decision contexts. Shrader-Frechette (2002) has advocated\nmodels of ethically weighted cost-benefit analysis and greater public\ninvolvement in risk assessment. In particular she (Shrader-Frechette\n1994, 2002) has argued for including members of the public in\ndeliberations about health effects of and reasonable exposure limits\non environmental pollutants, especially radioactive materials.\nPhilosophers of science have also worked to make visible the ways in\nwhich values play a role in the research assessing the effects of\ntechno-scientifically produced substances and practices themselves, as\ndistinct from the challenges of assigning values to identified risks\nand benefits. \nDouglas (2000) is an influential study of toxicological research on\neffects of exposure to dioxins. Douglas set her analysis in the\nframework of inductive risk introduced by Richard Rudner (1953) and\nalso explored by Carl Hempel (1965). The ampliative character of\ninductive inference means that the premises can be true (and even\nstrongly supportive) and the conclusion false. Rudner argued that this\nfeature of inductive inference means that scientists ought to take the\nconsequences of being wrong into account when determining how strong\nthe evidence for a hypothesis needs to be before accepting the\nhypothesis. [But see Jeffrey (1956) for a different view.] Douglas\nproposes that such considerations reach deeper into the scientific\nprocess than the acceptance of a conclusion based on the evidence to\nthe construction of the evidence itself. Scientists must make\ndecisions about levels of statistical significance, how to balance the\nchance of false positives against the chance of false negatives. They\nmust determine protocols for deciding borderline cases in their tissue\nsamples. They must select among possible dose-response models.\nDeciding in one way has one set of social consequences, and in another\nway another, opposing, set of consequences. Douglas claims that\nscientists ought to take these risks into account when making the\nrelevant methodological decisions. Since, even in her examples, public\nhealth considerations point in one direction and economic\nconsiderations point in another, in the end it is not clear just what\nresponsibility can reasonably be assigned to the individual\nscientist. \nIn addition to risk assessment, philosophers have begun thinking about\na variety of research programs and methods that affect human\nwellbeing. Lacey (2005), for example, delineates the contrasting\nvalues informing industrial, conventional agriculture on the one hand\nand small-scale agroecology on the other. Cartwright (2012),\nelaborated in Cartwright and Hardie (2012), is primarily a critical\nanalysis of the reliance on randomized control trials to support\npolicy decisions in economic development, medicine, and education.\nThese fail to take account of variations in contexts of application\nthat will affect the outcome. Cartwright’s focus on a particular\nmethodological approach is an extension of philosophers’\ntraditional engagement in areas of controversy in which philosophical\nanalysis might make a difference. Philip Kitcher’s (1985), which\ntook on sociobiology, and Elliott Sober and David Sloan Wilson’s\n(1998), an extensive argument for group level selection, are examples\nthat focus on content and methodology of extensions of evolutionary\ntheory. \nClimate change research has provoked several quite different kinds of\nanalysis. As a complex interdisciplinary field, its evidential\nstructure leaves it vulnerable to challenge. Opponents of limiting the\nuse of fossil fuels have exploited those vulnerabilities to sow public\ndoubts about the reality and/or causes of climate change (Oreskes and\nConway 2011). Parker 2006, Lloyd 2010, Parker 2010, Winsberg 2012\nhave, respectively, investigated strategies for reconciling apparent\ninconsistencies among climate models, the differences between\nmodel-based projections and strictly inductive projections, methods\nfor assessing and communicating the uncertainties inherent in climate\nmodels. Philosophers have also considered how to interpret the\n(American) public’s susceptibility to the climate change\ndeniers. Philip Kitcher (2012) interprets it as lack of information\namid a plethora of misinformation and proposes methods for more\neffective communication of reputable science to the public. Anderson\n(2011), on the contrary, contends that members of the public are\nperfectly able to evaluate the reliability of contradictory\nassessments by following citation trails, etc., whether on the\ninternet or in hard copies of journals. Her view is that the\nreluctance to accept the reality of climate change is a reluctance to\nabandon familiar ways of life, which is what averting climate-caused\ndisaster requires all to do. Finally, there is an ethical and\npolitical question once the inevitability of climate change is\naccepted: how should the burdens of taking action be distributed? The\nindustrialized West is responsible for most of the carbon pollution up\nto the end of the 20th century, but developing nations trying to\nindustrialize have contributed an increasing share, and will continue\nto do so, in the 21st century. Who bears the burden? And if the\neffects will only be felt by generations in the future, why should\npresent generations take actions whose harms will be felt now and\nwhose benefits lie in the future and will not be experienced by those\nbearing the costs? Broome (2008) explores the intergenerational\nissues, while Raina (2015) explores the global dimensions. \nTwo additional areas of ongoing scientific controversy are the\nbiological reality (or not) of race and the biology of gender\ndifferences. Developments in genetics, and documented racial\ndifferences in health, have thrown doubt on earlier anti-realist views\nof race, such as those articulated by Stephen J. Gould (1981) and\nRichard Lewontin (Lewontin, Rose, and Kamin 1984). Spencer (2012,\n2014) argues for a sophisticated form of biological racial realism.\nGannett (2003) argues that biological populations are not independent\nobjects that can provide data relevant to racial realism, while Kaplan\nand Winther (2013) argue that no claims about race can be read from\nbiological theory or data. The reality and basis of observed gender\ndifferences were the subject of much debate in the late 20th\ncentury(See Fausto-Sterling 1992). These issues have crystallized in\nthe early 21st century in debates about the brain and cognition\ndrawing the attention of philosophers of biology and cognitive\nscientists. Rebecca Jordan-Young (2010), Cordelia Fine (2010), and\nBluhn, Jacobson and Maibom, eds. (2012) all explore, with an aim of\ndebunking, claims of gendered brains.  \nKuhn’s critique of logical empiricism included a strong\nnaturalism. Scientific rationality was to be understood by studying\nactual episodes in the history of science, not by formal analyses\ndeveloped from a priori concepts of knowledge and reason (Kuhn 1962,\n1977). Sociologists and sociologically inclined historians of science\ntook this as a mandate for the examination of the full spectrum of\nscientists’ practices without any prior prejudice as to which\nwere epistemically legitimate and which not. That very distinction\ncame under suspicion from the new social scholars, often labeled\n“social constructivists.” They urged that understanding\nthe production of scientific knowledge required looking at all the\nfactors causally relevant to the acceptance of a scientific idea, not\njust at those the researcher thinks should be relevant. \nA wide range of approaches in social and cultural studies of science\nhas come under the umbrella label of “social\nconstructivism.” Both terms in the label are understood\ndifferently in different programs of research. While constructivists\nagree in holding that those factors treated as evidential, or as\nrationally justifying acceptance, should not be privileged at the\nexpense of other causally relevant factors, they differ in their view\nof which factors are causal or worth examination. Macro-analytic\napproaches, such as those associated with the so-called Strong\nProgramme in the Sociology of Scientific Knowledge, treat social\nrelations as an external, independent factor and scientific judgment\nand content as a dependent outcome. Micro-analyses or laboratory\nstudies, on the other hand, abjure the implied separation of social\ncontext and scientific practice and focus on the social relations\nwithin scientific research programs and communities and on those that\nbind research-productive and research-receptive communities\ntogether. \nResearchers also differ in the degree to which they treat the social\nand the cognitive dimensions of inquiry as independent or interactive.\nThe researchers associated with the macro-analytic Strong Programme in\nthe Sociology of Scientific Knowledge (Barry Barnes, David Bloor,\nHarry Collins, Donald MacKenzie, Andrew Pickering, Steve Shapin) were\nparticularly interested in the role of large scale social phenomena,\nwhether widely held social/political ideologies or group professional\ninterests, on the settlement of scientific controversies. Some\nlandmark studies in this genre include Andrew Pickering’s (1984)\nstudy of competing professional interests in the interpretation of\nhigh energy particle physics experiments, and Steven Shapin and Simon\nShaffer’s (1985) study of the controversy between Robert Boyle\nand Thomas Hobbes about the epistemological relevance of experiments\nwith vacuum pumps. \nThe micro-sociological or laboratory studies approach features\nethnographic study of particular research groups, tracing the myriad\nactivities and interactions that eventuate in the production and\nacceptance of a scientific fact or datum. Karin Knorr Cetina’s\n(1981) reports her year-long study of a plant science laboratory at UC\nBerkeley. Bruno Latour and Steven Woolgar’s (1986) study of\nRoger Guillemin’s neuroendocrinology laboratory at the Salk\nInstitute is another classic in this genre. These scholars argued in\nsubsequent work (Knorr-Cetina 1983; Latour, 1987) that their form of\nstudy showed that philosophical analyses of rationality, of evidence,\nof truth and knowledge, were irrelevant to understanding scientific\nknowledge. Sharon Traweek’s (1988) comparative study of the\ncultures of Japanese and North American high energy physics\ncommunities pointed to the parallels between cosmology and social\norganization but abstained from making extravagant or provocative\nepistemological claims. The efforts of philosophers of science to\narticulate norms of scientific reasoning and judgment were, in the\nview of both macro- and micro-oriented scholars, misdirected, because\nactual scientists relied on quite different kinds of considerations in\nthe practice of science. \nUntil recently, apart from a few anomalous figures like Caroline\nHerschel, Barbara McClintock, and Marie Curie, the sciences were a\nmale preserve. Feminist scholars have asked what bearing the\nmasculinity of the scientific profession has had on the content of\nscience and on conceptions of scientific knowledge and practice.\nDrawing both on work by feminist scientists that exposed and critiqued\ngender biased science and on theories of gender, feminist historians\nand philosophers of science have offered a variety of models of\nscientific knowledge and reasoning intended to accommodate the\ncriticism of accepted science and the concomitant proposal and\nadvocacy of alternatives. Evelyn Keller (1985) proposed a\npsycho-dynamic model of knowledge and objectivity, arguing that a\ncertain psychological profile, facilitated by typical patterns of\nmasculine psychological development, associated knowledge and\nobjectivity with domination. The association of knowledge and control\ncontinues to be a topic of concern for feminist thinkers as it is also\nfor environmentally concerned critics of the sciences. In this\nconnection, see especially Lacey’s (2005) study of the\ncontroversy concerning transgenic crops. Other feminists turned to\nMarxist models of social relations and developed versions of\nstandpoint theory, which holds that the beliefs held by a group\nreflect the social interests of that group. As a consequence, the\nscientific theories accepted in a context marked by divisions of power\nsuch as gender will reflect the interests of those in power.\nAlternative theoretical perspectives can be expected from those\nsystematically excluded from power. (Harding 1986; Rose 1983; Haraway\n1978). \nStill other feminists have argued that some standard philosophical\napproaches to the sciences can be used to express feminist concerns.\nNelson (1990) adopts Quine’s holism and naturalism to analyze\ndebates in recent biology. Elizabeth Potter (2001) adapts Mary\nHesse’s network theory of scientific inference to analyse\ngendered aspects of 17th century physics. Helen Longino (1990)\ndevelops a contextual empiricism to analyze research in human\nevolution and in neuroendocrinology. In addition to the direct role\nplayed by gender bias, scholars have attended to the ways shared\nvalues in the context of reception can confer an a priori\nimplausibility on certain ideas. Keller (1983) argued that this was\nthe fate of Barbara McClintock’s unorthodox proposals of genetic\ntransposition. Stephen Kellert (1993) made a similar suggestion\nregarding the then resistance to so-called chaos theory, that is the\nuse of non-linear dynamics to model processes like climate change. \nWhat the feminist and empirical sociological analyses have in common\nis the view that the social organization of the scientific community\nhas a bearing on the knowledge produced by that community. There are\ndeep differences, however, in their views as to what features of that\nsocial organization are deemed relevant and how they are expressed in\nthe theories and models accepted by a given community. The gender\nrelations focused on by feminists went unrecognized by sociologists\npursuing macro- or microsociological research programs. The feminist\nscientists and scholars further differ from the scholars in empirical\nsocial and cultural studies of science in their call for alternative\ntheories and approaches in the sciences. These calls imply that\nphilosophical concerns with truth and justification are not only\nlegitimate but useful tools in advancing feminist transformative goals\nfor the sciences. As can be seen in their varying treatments of\nobjectivity, however, philosophical concepts are often reworked in\norder to be made applicable to the content or episodes of interest\n(See Anderson 2004, Haraway 1988, Harding 1993, Keller 1985, Longino\n1990, Nelson 1990, Wylie 2005) \nIn addition to differences in analysis of philosophical concepts like\nobjectivity, rationality, or truth, feminist philosophers of science\nhave also debated the proper role of contextual (sometimes called,\n“external” or “social”) values. Some feminists\nargue that, given that values do play a role in scientific inquiry,\nsocially progressive values ought to shape not only decisions about\nwhat to investigate but also the processes of justification.\nPhilosophers of science should incorporate exemplification of the\nright values in their accounts of confirmation or justification.\nOthers are less certain about the identification of the values that\nshould and those that should not inform the conduct of science. These\nphilosophers are dubious that a consensus exists, or is even possible\nin a pluralistic society, on what constitute the values that ought to\nguide inquiry. In an exchange with Ronald Giere, Janet Kourany (2003a,\n2003b) argues that not only science, but philosophy of science ought\nto be concerned with the promotion of socially progressive values.\nGiere (2003) replies that what counts as socially progressive will\nvary among philosophers, and that in a democracy, it is unlikely that\na unanimous or near unanimous consensus regarding the values to inform\nphilosophical analysis or scientific inquiry could be achieved either\nin the larger society or in the smaller social subset of philosophers\nof science. \nSince 1980, interest in developing philosophical accounts of\nscientific knowledge that incorporate the social dimensions of\nscientific practice has been on the increase. Some philosophers see\nattention to the social as a straightforward extension of already\ndeveloped approaches in epistemology. Others, inclined toward some\nform of naturalism, have taken the work in empirical social studies of\nscience discussed above seriously. They have, however, diverged quite\nconsiderably in their treatment of the social. Some understand the\nsocial as biasing or distorting, and hence see the social as opposed\nto or competing with the cognitive or epistemic. These philosophers\nsee the sociologists’ disdain for normative philosophical\nconcerns as part of a general debunking of science that demands a\nresponse and defense. Some philosophers see the social aspects of\nscience as incidental to deep questions about knowledge, but\ninformative about certain tendencies in scientific communities. Others\ntreat the social as instead constitutive of rationality. These\ndifferences in conception of the role and nature of the social inform\ndifferences in the several approaches to modeling the sociality of\ninquiry and knowledge discussed below. \nContemporary philosophers pursue both formal and informal modeling\napproaches in addressing the social character of knowledge. Those\npursuing formal models tend to bracket questions about rationality,\nobjectivity, or justification and concentrate on mathematically\ninvestigating the effects of community structures on features of the\npursuit of knowledge and its diffusion in a community. Those pursuing\ninformal models are more interested in understanding the role of the\ncommunity in enhancing or constituting desired features of inquiry\nsuch as rationality and objectivity and in thinking about the ways\nknowledge is realized \nCommunication and the division of cognitive labor. Among the\nfirst issues to be investigated using formal techniques was the\ndivision of cognitive labor. While big science projects such as\ndiscussed by Hardwig pose a problem of integrating disparate elements\nof the solution to a question, the division of cognitive labor\nconcerns the appropriate or optimal distribution of efforts towards\nsolving a given problem. If everyone follows the same research\nstrategy to solve a problem or answer a question, then a solution\nlying outside that strategy will not be reached. If such a solution is\nbetter than any attainable via the shared strategy, the community\nfails to attain the better solution. But how can it be rational to\nadopt a research strategy other than the one deemed at the time most\nlikely to succeed? Philip Kitcher in his (1993) was concerned to offer\nan alternative to the strong programme’s proposal that\ncontroversy and the persistence of alternative research programs were\na function of the varying social or ideological commitments of\nresearchers. However, he also acknowledged that if researchers\nfollowed only the strategy judged at the time most likely to lead to\ntruth, they would not pursue unorthodox strategies that might lead to\nnew discoveries. He therefore labeled the observed fact that\nresearchers pursued different approaches to the same problem as the\ndivision of cognitive labor and proposed a decision model that\nattributed the pursuit of a nonorthodox (maverick) research strategy\nto a rational calculation about the chances of a positive payoff. This\nchance was calculated on the basis of the likelihood of the maverick\nstrategy being successful (or more successful than the orthodox\napproach), the numbers of peers pursuing orthodox or other maverick\nstrategies, and the anticipated reward of success. A community can\nallocate research resources in such a way as to maintain the balance\nof orthodox and maverick scientists most likely to facilitate\nprogress. Thus, scientific progress can tolerate and indeed benefits\nfrom a certain amount of “impure” motivation. Michael\nStrevens (2003) argued instead that the pursuit of maverick research\nstrategies was to be expected as a consequence of the priority rule.\nThe priority rule refers to the practice of referring to a law or\nobject with the name of the first individual to articulate or perceive\nand identify it. Think of Boyle’s Law, Halley’s comet, the\nPlanck constant, Avogadro’s number, etc. There’s no such\nreward attached to pursuing a research strategy devised by another and\n“merely” adding to what that individual has already\ndiscovered. The rewards of research come from being first. And to be\nfirst requires pursuing a novel problem or strategy. The division of\ncognitive labor, understood as different researchers pursuing\ndifferent research strategies, is a simple effect of the priority\nrule. Muldoon and Weisberg (2011) reject both Kitcher’s and\nStrevens’s accounts as presupposing unrealistically uniform and\nideal agents. In reality, they observe, scientists have at best\nimperfect knowledge of the entire research situation, do not know the\nentirety of the research landscape, and when they do know, know\ndifferent things. They do not have sufficient information to employ\nthe decision methods Kitcher and Strevens attribute to them. Muldoon\nand Weisberg propose agent-based modeling as a means to represent the\nimperfect, non-overlapping, and partial knowledge of the agents\ndeciding what research problems and strategies to pursue.\nSolomon’s advocacy of dissensus discussed below can be\nunderstood as rejecting the premises of the problem. From that point\nof view the aim of scientific organization ought to be to promote\ndisagreement. \nKevin Zollman, following Bala and Goyal (1998), used network theory to\nmodel different possible communication structures. The aim of Zollman\n(2007, 2013) is to investigate what difference communication\nstructures make to the chances of a scientific community settling on a\ncorrect (or incorrect) theory or hypothesis and to the speed by which\nsuch a consensus is reached. Networks consist of nodes and edges that\nconnect them. The nodes can represent individuals or any group that\nhas uniform beliefs. The nodes can have values of believe or not\nbelieve and consensus consists in all nodes in the network taking the\nsame value. Zollman investigates three possible communication\nstructures: the cycle, in which each node is connected only to nodes\non either side of it in the cycle; the wheel, in which there is a\ncentral node to which all other nodes are exclusively connected; and\nthe complete, in which each node is connected to every other node.\nUsing the mathematics of network theory, Zollman proves the somewhat\ncounterintuitive thesis that the network with limited communication,\nthe cycle, has the highest probability of consensus on the correct\nhypothesis, while the network with the densest communication, the\ncomplete, has a non-negligible probability of consensus (from which\ndeparture is not possible) on the incorrect hypothesis. Zollman (2010)\nalso uses this method to investigate the division of labor problem,\nalthough he comes at it from a slightly different point of view that\ndo Kitcher or Strevens. Structures with sparse or limited\ncommunication are more likely to arrive at the correct hypothesis, but\nbecause they take longer to reach consensus, different research\napproaches may persist in such communities. Under the right\ncircumstances, this will prevent foreclosure on the incorrect\nhypothesis. Zollman implicitly blames a dense communication structure\nfor the premature abandonment of the bacterial hypothesis of peptic\nulcers. Diversity is a good thing as long as the evidence is not\ndecisive, and if the acid hypothesis, which held sway until a new\nstaining method showed the presence of Helicobacter pylori, had\nbeen slower to diffuse into the community, the bacterial hypothesis\nmight have been preserved long enough to be better supported. \nWhile Zollman presents his results as an alternative method to the\nreward mechanisms discussed by Kitcher, Strevens, and Muldoon and\nWeisberg, they do not include a mechanism for establishing any of the\nnetwork structures as the preferred communication system for a\nscientific community. Kitcher and the others were concerned with how\nagents might be motivated to pursue a theory or method whose chance of\nsuccess was either unknown or thought unlikely. Funding bodies like\ngovernmental science foundations and private foundations provide or\ncan provide the relevant reward structure. Prize-giving bodies, like\nthe Nobel Foundation or the Kavli Foundation, as well as historical\npractice, entrench the priority rule. Both of these are community\nmethods that can motivate the choice to pursue high risk, high reward\nresearch. It is not clear how communities would select communication\nstructures, nor what kind of system would be able to enforce a\nstructure. Rosenstock, O’Connor, and Bruner (2017) point out in\naddition that Zollman’s results are very sensitive to how\nparameters of the models are set. Adjust the number of nodes or the\nprobabilities assigned to the alternative strategies/hypotheses and\nthe Zollman effect disappears. The probability of consensus on the\nincorrect hypothesis in the densely connected communication structure\nreduces to close to zero with more nodes or greater disparity of\nassigned probabilities to alternatives.  \nO’Connor and other colleagues have used evolutionary game theory\nto model other community phenomena such as the persistence of minority\ndisadvantage in scientific communities (Rubin & O’Connor\n2018), scientific polarization (O’Connor & Weatherall 2017),\ndiversity (O’Connor & Bruner 2017), conservatism in science\n(O’Connor forthcoming). While not necessarily claiming that\nthese game theoretic models are fully descriptive of the phenomena\nthey model, these theorists do claim that given certain initial\nconditions, certain undesirable social situations (like the\ndisadvantage accruing to minority status) are to be expected rather\nthan being understood as perversions of scientific practice. This\nwould suggest that some ways of addressing those undesirable social\noutcomes may not be effective and that alternative measures ought to\nbe sought in case of failure. \nSociality, rationality, and objectivity. Philosophers who treat\nthe social as biasing or distorting tend to focus on the\nconstructivists’ view that there are no universal principles of\nrationality or principles of evidence that can be used to identify in\nany context-independent way which factors are evidential and which\nnot. Reconciliationists tend to argue that what is correct in the\nsociologists’ accounts can be accommodated in orthodox accounts\nof scientific knowledge. The key is sifting the correct from the\nexaggerated or misguided. Integrationists read the relevance of the\nsociologists’ accounts as supporting the development of new\naccounts of rationality or objectivity, rather than as grounds for\nrejecting the cogency of such normative ideals.  \nPhilosophers concerned to defend the rationality of science against\nsociological misrepresentations include Larry Laudan (1984) James\nBrown (1989, 1994), Alvin Goldman (1987, 1995) and Susan Haack (1996).\nThe details of these philosophers’ approaches differ, but they\nagree in holding that scientists are persuaded by what they regard as\nthe best evidence or argument, the evidence most indicative of the\ntruth by their lights, and in holding that arguments and evidence are\nthe appropriate focus of attention for understanding the production of\nscientific knowledge. When evidential considerations have not trumped\nnon-evidential considerations, we have an instance of bad science.\nThey read the sociologists as arguing that a principled distinction\nbetween evidential and nonevidential considerations cannot be drawn\nand devote considerable effort to refuting those arguments. In their\npositive proposals for accommodating the social character of science,\nsociality is understood as a matter of the aggregation of individuals,\nnot their interactions, and public knowledge as simply the additive\noutcome of many individuals making sound epistemic judgments.\nIndividual rationality and individual knowledge are thus the proper\nfocus of philosophers of science. Exhibiting principles of rationality\napplicable to individual reasoning is sufficient to demonstrate the\nrationality of science, at least in its ideal form. \nReconciliationists include Ronald Giere, Mary Hesse, and Philip\nKitcher. Giere (1988) models scientific judgment using decision\ntheory. This permits incorporating scientists’ interests as one\nof the parameters of the decision matrix. He also advocates a\nsatisficing, rather than optimizing, approach to modeling the decision\nsituation, thus enabling different interests interacting with the same\nempirical base to support different selections as long as they are\nconsistent with that base. Mary Hesse (1980) employs a network model\nof scientific inference that resembles W.V.O. Quine’s web of\nbelief in that its constituents are heterogeneous in character, but\nall subject to revision in relation to changes elsewhere in the\nnetwork. She understands the social factors as coherence conditions\noperating in tandem with logical constraints to determine the relative\nplausibility of beliefs in the network. \nThe most elaborate reconciliationist position is that developed in\nPhilip Kitcher’s (1993). In addition to modeling relations of\nauthority and the division of cognitive labor as described above, he\noffers what he terms a compromise between extreme rationalists and\nsociological debunkers. The compromise model appeals to a principle of\nrationality, which Kitcher calls the External Standard. It is deemed\nexternal because it is proposed as holding independently of any\nparticular historical, cultural or social context. Thus, not only is\nit external, but it is also universal. The principle applies to change\nof belief (or shift from one practice to another, in Kitcher’s\nbroader locution), not to belief. It treats a shift (in practice or\nbelief) as rational if and only “the process through which the\nshift was made has a success ratio at least as high as that of any\nother process used by human beings (ever) ...” (Kitcher 1993,\n303). Kitcher’s compromise proposes that scientific ideas\ndevelop over time and benefit from the contributions of many\ndifferently motivated researchers. This is the concession to the\nsociologically oriented scholars. In the end, however, those theories\nthat are rationally accepted are those that satisfy Kitcher’s\nExternal Standard. Kitcher thus joins Goldman, Haack, and Laudan in\nthe view that it is possible to articulate a priori conditions of\nrationality or of epistemic warrant that operate independently of, or,\nperhaps one might say, orthogonally to, the social relations of\nscience. \nA third set of models is integrationist in character. Integrationists\nuse the observations of sociologists of science to develop alternative\naccounts of scientific rationality and objectivity. Nelson (1990)\nfocuses on a slightly different aspect of Quine’s holism than\ndoes Hesse. Nelson uses Quine’s arguments against the\nindependently foundational status of observation statements as the\nbasis for what she calls a feminist empiricism. According to Nelson,\nno principled distinction can be made between the theories,\nobservations, or values of a community. What counts as evidence, in\nher view, is fixed by the entire complex of a community’s\ntheories, value commitments, and observations. There is neither\nknowledge nor evidence apart from such a shared complex. The community\nis the primary knower on this view and individual knowledge is\ndependent on the knowledge and values of the community. \nMiriam Solomon’s social empiricism is focused on scientific\nrationality (Solomon 2001). It, too, involves denying a universal\nprincipled distinction among the causes of belief. Solomon draws on\ncontemporary cognitive science literature to argue that what are\ntraditionally called biases are simply among the kinds of\n“decision vector” that influence belief. They are not\nnecessarily undesirable elements from which science needs to be\nprotected, and can be productive of insight and rational belief.\nSalience and availability (of data, of measurement technologies), also\ncalled cold biases, are decision vectors as much as social ideologies\nor other motivational factors, “hot biases.” The\ndistinctive feature of Solomon’s social empiricism is her\ncontrast between individual and community rationality. Her (2001)\nurges the pluralistic view that a community is rational when the\ntheories it accepts are those that have unique empirical successes.\nIndividuals can persist in beliefs that are (from a panoptic\nperspective) less well supported than others on this view, if the\ntotality of available evidence (or empirical data) is not available to\nthem, or when their favored theory accounts for phenomena not\naccounted for other theories, even when those may have a greater\nquantity of empirical successes. What matters to science, however, is\nthat the aggregated judgments of a community be rational. A community\nis rational when the theories it accepts are those with all or with\nunique empirical successes. It is collectively irrational to jettison\na theory with unique empirical successes. Thus, the community can be\nrational even when its members are, as judged by traditional epistemic\nstandards, individually irrational. Indeed, individual irrationality\ncan contribute to community rationality in that individuals committed\nto a theory that accounts for their data keep that data in the range\nof phenomena any theory accepted by the entire community must\neventually explain. In addition to empirical success, Solomon proposes\nan additional normative criterion. In order to secure appropriate\ndistribution of scientific effort, biases must be appropriately\ndistributed in the community. Solomon proposes a scheme for\nascertaining when a distribution is normatively appropriate. Thus, for\nSolomon, a scientific community is rational when biases are\nappropriately distributed and it accepts only a theory with all or\ntheories with unique empirical successes as the normative\nepistemological condition. Rationality accrues only to a community,\nand not to the individuals constituting the community. As in\nZollman’s network models, consensus just is all members of the\ncommunity assigning the same value (T/F) to a hypothesis or\ntheory. \nFinally, in Longino’s critical contextual empiricism, the\ncognitive processes that eventuate in scientific knowledge are\nthemselves social (Longino 1990, 2002). Longino’s starting point\nis a version of the underdetermination argument: the semantic gap\nbetween statements describing data and statements expressing\nhypotheses or theories to be confirmed or disconfirmed by that data.\nThis gap, created by the difference in descriptive terms used in the\ndescription of data and in the expression of hypotheses, means that\nevidential relations cannot be formally specified and that data cannot\nsupport one theory or hypothesis to the exclusion of all alternatives.\nInstead, such relations are mediated by background assumptions.\nEventually, in the chain of justification, one reaches assumptions for\nwhich no evidence is available. If these are the context in which\nevidential relations are constituted, questions arise concerning how\nthe acceptance of such assumptions can be legitimated. According to\nLongino, the only check against the arbitrary dominance of subjective\n(metaphysical, political, aesthetic) preference in such cases is\ncritical interaction among the members of the scientific community or\namong members of different communities. There is no higher authority\nor transcendent aperspectival position from which it is possible to\nadjudicate among foundational assumptions. Longino takes the\nunderdetermination argument to express in logical terms the point made\nby the sociologically oriented researchers: the individuals\nparticipating in the production of scientific knowledge are\nhistorically, geographically, and socially situated and their\nobservations and reasoning reflect their situations. This fact does\nnot undermine the normative enterprise of philosophy, but requires its\nexpansion to include within its scope the social interactions within\nand between scientific communities. What counts as knowledge is\ndetermined by such interactions. \nLongino claims that scientific communities do institutionalize some\ncritical practices (for example, peer review), but argues that such\npractices and institutions must satisfy conditions of effectiveness in\norder to qualify as objective. She argues, therefore, for the\nexpansion of scientific norms such as accuracy and consistency to\ninclude norms that apply to communities. These are (1) the provision\nof venues in which critical interaction can take place, (2) the uptake\nof critical intervention as demonstrated in change of belief\ndistribution in the community over time in a way that is sensitive to\nthe critical discourse taking place within that community, (3) public\naccessibility of the standards that regulate discourse, and (4)\ntempered equality of intellectual authority. By this latter condition,\nperhaps the most controversial of her proposed norms, Longino means\nthat any perspective has a prima facie capacity to contribute to the\ncritical interactions of a community, though equal standing can be\nlost owing to failure to engage or to respond to criticism. In her\n2002, Longino argues that the cognitive processes of science, such as\nobservation and reasoning, are themselves social processes. Thus the\ninteractions subject to community norms extend not only to discussion\nof assumptions in finished research, but to the constructive processes\nof research as well. \nSolomon and Longino differ on where they locate normativity and on the\nrole and effectiveness of deliberative processes in actual scientific\ninquiry. Solomon attends to the patterns of acceptance and to the\ndistribution of decision vectors, regardless of the interactions among\ncommunity members, while Longino attends to deliberative processes and\ninteractions. They may also differ in their views of what constitutes\nscientific success. \nOne set of issues that has yet to give rise to extended philosophical\nreflection is the question how civilizational differences are\nexpressed in scientific work (See Bala 2008). Here, too, there is a\nmicro- and a macro- version. At the micro level, one might ask how the\ninteractional culture of individual laboratories or theoretical\nsubcommunities is or is not expressed in the outcome of their\nresearch. At the macro level one might be asking how large scale\ncultural features are reflected in the content and practice of science\nin a given cultural formation. For example, Joseph Needham argued that\nfeatures of the culture of ancient China directed their technical and\nintellectual ingenuity into channels that foreclosed the development\nof anything like the science that developed in Western Europe in the\n14th through the 17th centuries. Other cultures developed some aspects\nof what we now think of as a cosmopolitan or global scientific culture\n(for example, the mathematics and astronomy of 10th through 14th\ncentury Islamic and South Asian scholars) independently of the early\nmodern physics developed in Western and Central Europe. The papers in\nHabib and Raina (2001) address aspects of these questions with respect\nto the history of science in India. \nUnity, Plurality and the Aims of Inquiry. The variety of views\non the degree of sociality assignable to the epistemological concepts\nof science lead to different views concerning the ultimate character\nof the outcome of inquiry. This difference can be summarized as the\ndifference between monism and pluralism. Monism, as characterized in\nKellert, Longino, and Waters (2006), holds that the goal of inquiry is\nand should be a unified, comprehensive, and complete account of\nphenomena (whether all phenomena, or the phenomena specific to a\nparticular domain of inquiry). If this is so, then the norms of\nassessment should be informed by this goal and there should be one\nstandard by which theories, models, and hypotheses in the sciences are\nassessed. Deviation from an accepted theoretical framework is\nproblematic and requires explanation, such as the explanations offered\nfor the division of cognitive labor. Monism, with its commitment to\nultimate unity, requires ways to reconcile competing theories or to\nadjudicate controversy so as to eliminate competition in favor of the\none true or best theory. Pluralism, on the other hand, holds that the\nobserved plurality of approaches within a science is not necessarily a\nflaw but rather reflects the complexity of the phenomena under\ninvestigation in interaction with the limitations of human cognitive\ncapacities and the variety of human cognitive as well as pragmatic\ninterests in representations of those phenomena. \nAmong pluralists, a diversity of views is to be found. Suppes (1978)\nemphasized the mutual untranslatability of the descriptive terms\ndeveloped in the course of scientific specialization. Such\nincommensurability will resist evaluation by a common measure.\nCartwright’s (1999) invocation of a dappled world emphasizes the\ncomplexity and diversity of the natural (and social) world. Scientific\ntheories and models are representations of varying degrees of\nabstraction that manage to apply at best partially to whatever\nphenomena they purport to represent. To the extent they are taken to\nrepresent actual process in the real world, they must be hedged by\nceteris paribus clauses. Scientific laws and models attach to patches\nof the world, but not to a seamlessly law-governed whole.\nMitchell’s (2002, 2009) integrative pluralism is a rejection of\nthe goal of unification by either reduction to a single (fundamental)\nlevel of explanation or abstraction to a single theoretical\nrepresentation, in favor of a more pragmatically inflected set of\nexplanatory strategies. The success for any particular investigation\nis answerable to the goals of the investigation, but there may be\nmultiple compatible accounts reflecting both the contingency and\npartiality of the laws/generalizations that can figure in explanations\nand the different goals one may bring to investigation of the same\nphenomenon. The explanations sought in any particular explanatory\nsituation will draw on these multiple accounts as appropriate for the\nlevel of representation adequate to achieve its pragmatic ends.\nMitchell’s defense of integrative pluralism rests on both the\npartiality of representation and the complexity of the phenomena to be\nexplained. \nKellert, Longino, and Waters advance a pluralism that sees\nmultiplicity not only among but within levels of analysis. Furthermore\nthey see no reason to require that the multiple accounts be\ncompatible. The multiplicity of noncongruent empirically adequate\naccounts helps us appreciate the complexity of a phenomenon without\nbeing in a position to generate a single account of that complexity.\nThey do not hold that all phenomena will support ineliminable\npluralism, but that there are some phenomena that will require\nmutually irreducible or incompatible models. Which these are is\ndetermined by examining the phenomena, the models, and the match\nbetween phenomena and models. Like Mitchell, Kellert, Longino, and\nWaters hold that pragmatic considerations (broadly understood) will\ngovern the choice of model to be used in particular circumstances.\nBoth forms of pluralism (compatibilist and noncompatibilist) abandon\nthe notion that there is a set of natural kinds whose causal\ninteractions are the basis for fundamental explanations of natural\nprocesses. The noncompatibilist is open to multiple classification\nschemes answerable to different pragmatic interests in classifying. To\nthis extent the noncompatibilist pluralist embraces a view close to\nthe promiscuous realism articulated by John Dupré (1993). The\ncompatibilist, or integrative pluralist, on the other hand, must hold\nthat there is a way that different classification schemes can be\nreconciled to support the envisioned integration of explanatory\nmodels. \nPluralism receives support from several additional approaches. Giere\n(2006) uses the phenomenon of color vision to support a position he\ncalls perspectival realism. Like the colors of objects, scientific\nrepresentations are the result of interactions between human cognitive\nfaculties and the world. Other species have different visual equipment\nand perceive the world differently. Our human cognitive faculties,\nthen, constitute perspectives. We could have been built differently\nand hence perceived the world differently. Perspectival realism leads\nto pluralism, because perspectives are partial. While van\nFraassen’s (2008) does not take a position on pluralism vs.\nmonism (and as an empiricist and antirealist van Fraassen would not\nhave to), its emphasis on the partiality and perspective dependence of\nmeasurement provides a complementary point of entry to such diversity.\nSolomon (2006) urges a yet more welcoming attitude towards\nmultiplicity. In her view, dissensus is a necessary component of\nwell-functioning scientific communities and consensus can be\nepistemologically pernicious. In an extension of the arguments in\nSolomon (2001) she argues that different models and theoretical\nrepresentations will be associated with particular insights or\nspecific data that are likely to be lost if the aim is to integrate or\notherwise combine the models to achieve a consensus understanding. The\nactivity of integrating two or more models is different from the\nprocess of one model from a set of alternatives coming eventually to\nhave all the empirical successes distributed among the other models.\nIn her examination of consensus conferences called by the United\nStates National Institutes of Health (Solomon 2011), Solomon finds\nthat such conferences do not resolve existing dissent in the\nscientific community. Instead, they tend to take place after a\nconsensus has emerged in the research community and are directed more\nto the communication of such consensus to outside communities (such as\nclinicians, insurers, health policy experts, and the public) than to\nthe assessment of evidence that might warrant consensus. \nResearchers committed to a monist or unified science will see\nplurality as a problem to be overcome, while researchers already\ncommitted to a deeply social view of science will see plurality as a\nresource of communities rather than a problem. The diversity and\npartiality that characterizes both a local and the global scientific\ncommunity characterize the products of those communities as well as\nthe producers. Universalism and unification require the elimination of\nepistemologically relevant diversity, while a pluralist stance\npromotes it and the deeply social conception of knowledge that\nfollows.  \nSociality and the structure of scientific knowledge. Attention\nto the social dimensions of scientific knowledge and the consequent\npotential for plurality has prompted philosophers to rethink the\nstructure of what is known. Many philosophers (including Giere,\nKitcher, and Longino) who advocate forms of pluralism invoke the\nmetaphor of maps to explain how scientific representations can be both\npartial and adequate. Maps only represent those features of the\nterritory mapped that are relevant for the purpose for which the map\nis drawn. Some maps may represent the physical area bounded by state\nboundaries, others may represent the population size, or the relative\nabundance/poverty of natural resources. Winther (forthcoming) explores\nthe variety of kinds of maps used in science and philosophical use of\nthe map metaphor. But the map metaphor is only one of several ways to\nrethink the structure of scientific knowledge. \nOther philosophers draw more heavily on cognitive science. Giere\n(2002) takes a naturalist approach to modeling, not so much the\ndistribution of cognitive labor, but the distribution of cognition.\nThis approach takes a system or interactive community as the locus of\ncognition, rather than the individual agent. Nersessian (2006) extends\ndistributed cognition to model-based reasoning in the sciences. Models\nare artifacts that focus the cognitive activity of multiple\nindividuals in particular settings. Knowledge is distributed across\nthe minds interacting about the artifacts in that setting. Paul\nThagard draws on the increasingly interdisciplinary (and hence social)\nnature of cognitive science itself to argue that not only does\ncognitive science (or certain lines of analysis in cognitive science)\nsupport a conception of cognition as distributed among interacting\nagents, but that this conception can be turned back upon cognitive\nscience itself. (Thagard 2012). Finally Alexander Bird (2010) reflects\non the sense of knowledge required for attributions such as:\n“the biomedical community now knows that peptic ulcers are often\ncaused by the bacterium Helicobacter pylori.” Or\n“There was an explosive growth in scientific knowledge in the\ntwentieth century.” Bird faults other social epistemologists for\nstill making such collective knowledge supervenient on the states of\nindividuals. Instead, he argues, we should understand social knowing\nas a functional analogue of individual knowing. Both are dependent on\nthe existence and proper functioning of the relevant structures:\nreasoning and perception for individuals; libraries and journals and\nother social structures, for collectivities. Scientific knowledge is\nan emergent effect of collective epistemic interactions, concretized\nin the texts that have been designated as vehicles for the\npreservation and communication of that knowledge \nModern science has been regarded as both a model of democratic\nself-governance and an activity requiring and facilitating democratic\npractices in its supporting social context (Popper 1950, Bronowski\n1956). In this perspective, science is seen as embedded in and\ndependent on its supporting social context, but insulated in its\npractices from the influence of that context. As the reach of science\nand science-based technologies has extended further and further into\nthe economy and daily life of industrialized societies, new attention\nis paid to the governance of science. Regardless of one’s views\nabout the social character of knowledge, there are further questions\nconcerning what research to pursue, what social resources to devote to\nit, who should make such decisions, and how they should be made. \nPhilip Kitcher (2001) has opened these questions to philosophical\nscrutiny. While Kitcher largely endorses the epistemological views of\nhis (1993), in the later work he argues that there is no absolute\nstandard of the significance (practical or epistemic) of research\nprojects, nor any standard of the good apart from subjective\npreferences. The only non-arbitrary way to defend judgments concerning\nresearch agendas in the absence of absolute standards is through\ndemocratic means of establishing collective preferences. Kitcher,\nthus, attempts to spell out procedures by which decisions concerning\nwhat research directions to pursue can be made in a democratic manner.\nThe result, which he calls well-ordered science, is a system in which\nthe decisions actually made track the decisions that would be a made\nby a suitably constituted representative body collectively\ndeliberating with the assistance of relevant information (concerning,\ne.g., cost and feasibility) supplied by experts. \nKitcher’s “well-ordered science” has attracted\nattention from other philosophers, from scientists, and from scholars\nof public policy. Winning praise as a first step, it has also elicited\na variety of criticisms and further questions. The criticisms of his\nproposal range from worries about the excessive idealism of the\nconception to worries that it will enshrine the preferences of a much\nsmaller group than those who will be affected by research decisions.\nKitcher’s proposal at best works for a system in which all or\nmost scientific research is publicly funded. But the proportion of\nprivate, corporate, funding of science compared to that of public\nfunding has been increasing, thus calling into question the\neffectiveness of a model that presupposes largely public control\n(Mirowski and Sent 2002, Krimsky 2003). Kitcher’s model, it\nshould be noted, still effects a significant separation between the\nactual conduct of research and decisions concerning the direction of\nresearch and scholars who see a more intimate relation between social\nprocesses and values in the context and those in the conduct of\nresearch will be dissatisfied with it. Kitcher himself (Kitcher 2011)\nseems to relax the separation somewhat. \nThe counterfactual character of the proposal raises questions about\nthe extent to which well-ordered science really is democratic. If the\nactual decisions do not need to be the result of democratic procedures\nbut only to be the same as those that would result from such\nprocedures, how do we know which decisions those are without actually\ngoing through the deliberative exercise? Even if the process is\nactually carried out, there are places, e.g. in choice of experts\nwhose advice is sought, which permit individual preferences to subvert\nor bias the preferences of the whole (Roth 2003). Furthermore, given\nthat the effects of scientific research are potentially global, while\ndemocratic decisions are at best national, national decisions will\nhave an effect well beyond the population represented by the decision\nmakers. Sheila Jasanoff has also commented that even in contemporary\nindustrialized democracies there are quite different science\ngovernance regimes. There is not one model of democratic decision\nmaking, but many, and the differences translate into quite different\npolicies (Jasanoff 2005). \nIn his (2011) Kitcher abandons the counterfactual approach as he\nbrings the ideal of well-orderedness into contact with actual debates\nin and about contemporary science. His concern here is the variety of\nways in which scientific authority has been eroded by what he terms\n“chimeric epistemologies.” It’s not enough to say\nthat the scientific community has concluded that, say, the MMR vaccine\nis safe, or that the climate is changing in a way that requires a\nchange in human activities. In a democratic society, there are many\nother voices claiming authority, whether on presumed evidential\ngrounds or as part of campaigns to manipulate public opinion. Kitcher\nsuggests mechanisms whereby small groups trusted by their communities\nmight develop the understanding of complicated technical issues\nthrough tutoring by members of the relevant research communities and\nthen carry this understanding back to the public. He also endorses\nJames Fishkin’s (2009) experiments in deliberative polling as a\nmeans to bring members of the public committed to different sides of a\ntechnical issue together with the scientific exponents of the issue\nand in a series of exchanges that cover the evidence, the different\nkinds of import different lines of reasoning possess, and the other\nelements of a reasoned discussion, bring the group to a consensus on\nthe correct view. The pluralist and pragmatically inclined\nphilosophers discussed in the previous section might worry that there\nis not a single correct view towards which such an encounter ought to\nconverge, but that a broader discussion that incorporates deliberation\nabout aims and values might produce sufficient (temporary) convergence\nto ground action or policy. \nPhilosophical study of the social dimensions of scientific knowledge\nhas been intensifying in the decades since 1970. Social controversies\nabout the sciences and science based technologies as well as\ndevelopments in philosophical naturalism and social epistemology\ncombine to drive thinking in this area forward. Scholars in a number\nof cognate disciplines continue to investigate the myriad social\nrelations within scientific communities and between them and their\nsocial, economic, and institutional contexts. \nWhile this area first came to prominence in the so-called science wars\nof the 1980s, attending to social dimensions of science has brought a\nnumber of topics to philosophical attention. The phenomenon of Big\nScience has encouraged philosophers to consider the epistemological\nsignificance of such phenomena as trust and cognitive interdependence\nand the division of cognitive labor. The increased economic and social\ndependence on science-based technologies has prompted attention to\nquestions of inductive risk and the role of values in assessing\nhypotheses with social consequences. The controversies over health\nrisks of certain vaccines, over the measurement of environmental\npollution, and over the causes of climate change have expanded\nphilosophy of science from its more accustomed areas of logical and\nepistemological analysis to incorporate concerns about the\ncommunication and uptake of scientific knowledge and the ethical\ndimensions of superficially factual debates. \nPartly in response to the work of scholars in the social studies of\nscience, partly in response to the changing role of scientific inquiry\nthrough the 20th and into the 21st centuries, philosophers have sought\nways to either accommodate the (tenable) results of the sociologists\nand cultural historians or to modify traditional epistemological\nconcepts used in the analysis of scientific knowledge. These\ninvestigations in turn lead to new thinking about the structure and\nlocation of the content of knowledge. While debates within philosophy\nof science between and among adherents to one or another of the models\nof the sociality of knowledge will continue, an important future step\nwill be a fuller encounter between individual-based social\nepistemology with its focus on testimony and disagreement as\ntransactions among individuals and the more fully social\nepistemologies that take social relations or interaction as partially\nconstitutive of empirical knowledge.","contact.mail":"hlongino@stanford.edu","contact.domain":"stanford.edu"}]
