[{"date.published":"2007-05-29","date.changed":"2017-08-04","url":"https://plato.stanford.edu/entries/algebra/","author1":"Vaughan Pratt","entry":"algebra","body.text":"\n\n\n\nAlgebra is a branch of mathematics sibling to geometry, analysis\n(calculus), number theory, combinatorics, etc. Although algebra has\nits roots in numerical domains such as the reals and the complex\nnumbers, in its full generality it differs from its siblings in\nserving no specific mathematical domain. Whereas geometry treats\nspatial entities, analysis continuous variation, number theory integer\narithmetic, and combinatorics discrete structures, algebra is equally\napplicable to all these and other mathematical domains.\n\n\n \nElementary algebra, in use for centuries and taught in\nsecondary school, is the arithmetic of indefinite quantities or\nvariables \\(x, y,\\ldots\\). Whereas the definite sum \\(3+4\\) evaluates\nto the definite quantity 7, the indefinite sum \\(x+y\\) has no definite\nvalue, yet we can still say that it is always equal to \\(y+x\\), or to\n\\(x^2 -y^2\\) if and only if \\(x\\) is either \\(-y\\) or \\(y+1\\).\n\n\n\n\nElementary algebra provides finite ways of managing the infinite.  A\nformula such as \\(\\pi r^2\\) for the area of a circle of radius \\(r\\)\ndescribes infinitely many possible computations, one for each possible\nvaluation of its variables. A universally true law expresses\ninfinitely many cases, for example the single equation \\(x+y = y+x\\)\nsummarizes the infinitely many facts \\(1+2 = 2+1, 3+7 = 7+3\\),\netc. The equation \\(2x = 4\\) selects one number from an infinite set\nof possibilities.  And \\(y = 2x+3\\) expresses the infinitely many\npoints of the line with slope 2 passing through \\((0, 3)\\) with a\nfinite equation whose solutions are exactly those points.\n\n\n\n\nElementary algebra ordinarily works with real or complex values.\nHowever its general methods, if not always its specific operations and\nlaws, are equally applicable to other numeric domains such as the\nnatural numbers, the integers, the integers modulo some integer \\(n\\),\nthe rationals, the quaternions, the Gaussian integers, the \\(p\\)-adic\nnumbers, and so on. They are also applicable to many nonnumeric\ndomains such as the subsets of a given set under the operations of\nunion and intersection, the words over a given alphabet under the\noperations of concatenation and reversal, the permutations of a given\nset under the operations of composition and inverse, etc. Each\nsuch algebraic structure, or simply algebra,\nconsists of the set of its elements and operations on those elements\nobeying the laws holding in that domain, such as the set \\(Z = \\{0,\n\\pm 1, \\pm 2, \\ldots \\}\\) of integers under the integer operations\n\\(x+y\\) of addition, \\(xy\\) of multiplication, and \\(-x\\), negation,\nor the set \\(2^X\\) of subsets of a set \\(X\\) under the set operations\n\\(X\\cup Y\\) of union, \\(X\\cap Y\\) of intersection, and \\(X'\\),\ncomplement relative to \\(X\\).\n\n\n\n\nThe laws are often similar but not identical. For example integer\nmultiplication distributes over addition, \\(x(y+z) = xy+xz\\), but not\nconversely, for example \\(2+(3\\times 5) = 17\\) but \\((2+3)\\times(2+5)\n= 35\\). In the analogy that makes intersection the set theoretic\ncounterpart of multiplication and union that of addition, intersection\ndistributes over union,\n\n\\[\nX\\cap(Y\\cup Z) = (X\\cap Y)\\cup(X\\cap Z),\n\\]\n\n\nas for the integers, but unlike the integers union also distributes\nover intersection: \n\n\\[\nX\\cup(Y\\cap Z) = (X\\cup Y)\\cap(X\\cup Z).\n\\]\n\n\n\nWhereas elementary algebra is conducted in a fixed algebra,\nabstract or modern algebra treats classes of algebras\nhaving certain properties in common, typically those expressible\nas equations. The subject, which emerged during the 19th century, is\ntraditionally introduced via the classes of groups, rings, and fields.\nFor example any number system under the operations of addition and\nsubtraction forms an abelian (commutative) group; one then passes\nto rings by bringing in multiplication, and further to fields with\ndivision. The common four-function calculator provides the four\nfunctions of the field of reals.\n\n\n\n\nThe abstract concept of group in full generality is defined not in\nterms of a set of numbers but rather as an arbitrary set equipped with\na binary operation \\(xy\\), a unary inverse \\(x^{-1}\\)\nof that operation, and a unit \\(e\\) satisfying certain\nequations characteristic of groups. One striking novelty with\ngroups not encountered in everyday elementary algebra is that their\nmultiplication need not be abelian: \\(xy\\) and \\(yx\\)\ncan be different! For example the group \\(S_3\\)\nof the six possible permutations of three things is not abelian,\nas can be seen by exchanging adjacent pairs of letters in the word\ndan. If you exchange the two letters on the left before the\ntwo on the right you get adn and then and, but if\nyou perform these exchanges in the other order you get dna\nand then nda instead of and. Likewise the group of\n43,252,003,274,489,856,000 operations on Rubik’s cube and the\ninfinite group \\(SO(3)\\) of rotations of the sphere are not abelian,\nthough the infinite group \\(SO(2)\\) of rotations of the circle is\nabelian. Quaternion multiplication and matrix multiplication is also\nnoncommutative. Abelian groups are often called additive groups and\ntheir group operation is referred to as addition \\(x+y\\) rather than\nmultiplication \\(xy\\).\n\n\n\n\nGroups, rings and fields only scratch the surface of abstract algebra.\nVector spaces and more generally modules are restricted forms of\nrings in which the operands of multiplication are required to be a\nscalar and a vector. Monoids generalize groups by dropping inverse;\nfor example the natural numbers form a monoid but not a group for\nwant of negation. Boolean algebras abstract the algebra of sets.\nLattices generalize Boolean algebras by dropping complement and\nthe distributivity laws. \n\n\n\n\nA number of branches of mathematics have found algebra such an\neffective tool that they have spawned algebraic subbranches.\nAlgebraic logic, algebraic number theory, and algebraic topology\nare all heavily studied, while algebraic geometry and algebraic\ncombinatorics have entire journals devoted to them.\n\n\n\n\nAlgebra is of philosophical interest for at least two reasons.\nFrom the perspective of foundations of mathematics, algebra is\nstrikingly different from other branches of mathematics in both its\ndomain independence and its close affinity to formal logic. Furthermore\nthe dichotomy between elementary and abstract algebra reflects a\ncertain duality in reasoning that Descartes, the inventor of Cartesian\nDualism, would have appreciated, wherein the former deals with the\nreasoning process and the latter that which is reasoned about, as\nrespectively the mind and body of mathematics.\n\n\n\n\nAlgebra has also played a significant role in clarifying and\nhighlighting notions of logic, at the core of exact philosophy\nfor millennia. The first step away from the Aristotelian logic of\nsyllogisms towards a more algebraic form of logic was taken by Boole\nin an 1847 pamphlet and subsequently in a more detailed treatise,\nThe Laws of Thought, in 1854. The dichotomy between elementary\nalgebra and modern algebra then started to appear in the subsequent\ndevelopment of logic, with logicians strongly divided between the\nformalistic approach as espoused by Frege, Peano, and Russell, and the\nalgebraic approach followed by C. S. Peirce, Schroeder, and Tarski.\n\n\n\n\nElementary algebra deals with numerical terms, namely\nconstants 0, 1, 1.5, \\(\\pi\\), variables \\(x,\ny,\\ldots\\), and combinations thereof built with\noperations such as \\(+\\), \\(-\\), \\(\\times\\) , \\(\\div\\) ,\n\\(\\sqrt{\\phantom{x}}\\), etc. to form such terms as \\(x+1, x\\times y\\) (standardly\nabbreviated \\(xy\\)), \\(x + 3y\\), and \\(\\sqrt{x}\\).  \n\nTerms may be used on their own in formulas such as\n\\(\\pi r^2\\), or in equations serving as laws\nsuch as \\(x+y = y+x\\), or as\nconstraints such as \\(2x^2 -x+3 = 5x+1\\) or \\(x^2 + y^2 = 1\\).\n \n\nLaws are always true; while they have the same form as constraints they\nconstrain only vacuously in that every valuation of their variables\nis a solution. The constraint \\(x^2 +y^2 =\n1\\) has a continuum of solutions forming a shape, in this case\na circle of radius 1. The constraint \\(2x^2 -x+3\n= 5x+1\\) has two solutions, \\(x = 1\\) or 2, and may\nbe encountered in the solution of word problems, or in\nthe determination of the points of intersection of two curves\nsuch as the parabola \\(y = 2x^2 -x+3\\) and the\nline \\(y = 5x+1\\).\n \n\nA formula is a term used in the computation of values by hand or\nmachine. Although some attributes of physical objects lend themselves\nto direct measurement such as length and mass, others such as area,\nvolume, and density do not and must be computed from more readily\nobserved values with the help of the appropriate formula. For example\nthe area of a rectangle \\(L\\) inches long by \\(W\\) inches\nwide is given by the formula \\(LW\\) in units of square inches,\nthe volume of a ball of radius \\(r\\) is \\(4\\pi r^3 /3\\),\nand the density of a solid of mass \\(M\\) and volume \\(V\\)\nis given by \\(M/V\\).\n \n\nFormulas may be combined to give yet more formulas. For example the\ndensity of a ball of mass \\(M\\) and radius \\(r\\) can be\nobtained by substituting the above formula for the volume of a ball\nfor \\(V\\) in the above formula for the density of a solid. The\nresulting formula \\(M/(4\\pi r^3 /3)\\) is then\nthe desired density formula.  \n\nLaws or identities are equations that hold for all applicable values\nof their variables. For example the commutativity law holds for all real values of \\(x\\) and \\(y\\). Likewise the\nassociativity law  holds for all real values of \\(x, y\\) and \\(z\\). On the other\nhand, while the law \\(x/(y/z) = zx/y\\) holds for all numerical values\nof \\(x\\), it holds only for nonzero values of \\(y\\) and \\(z\\) in order\nto avoid the illegal operation of division by zero.  \n\nWhen a law holds for all numerical values of its variables, it also\nholds for all expression values of those variables. Setting \\(x = M, y\n= 4\\pi r^3\\), and \\(z = 3\\) in the last law of the preceding paragraph\nyields \\(M/(4\\pi r^3 /3) = 3M/(4\\pi r^3)\\). The left hand side being\nour density formula from the preceding section, it follows from this\ninstance of the above law that its right hand side is an equivalent\nformula for density in the sense that it gives the same answers as the\nleft hand side. This new density formula replaces one of the two\ndivisions by a multiplication.  \n\nIf Xavier will be three times his present age in four years time,\nhow old is he? We can solve this word problem using algebra\nby formalizing it as the equation \\(3x = x + 4\\) where\n\\(x\\) is Xavier’s present age. The left hand side expresses three\ntimes Xavier’s present age, while the right hand side expresses his age\nin four years’ time.\n \n\nA general rule for solving such equations is that any solution\nto it is also a solution to the equation obtained by applying some\noperation to both sides. In this case we can simplify the equation\nby subtracting \\(x\\) from both sides to give \\(2x = 4\\),\nand then dividing both sides by 2 to give \\(x = 2\\). So Xavier\nis now two years old.\n \n\nIf Xavier is twice as old as Yvonne and half the square of her\nage, how old is each? This is more complicated than the previous\nexample in three respects: it has more unknowns, more equations,\nand terms of higher degree. We may take \\(x\\) for Xavier’s\nage and \\(y\\) for Yvonne’s age. The two constraints may\nbe formalized as the equations \\(x = 2y\\) and\n\\(x = y^2 /2\\), the latter being of degree\n2 or quadratic.\n \n\nSince both right hand sides are equal to \\(x\\) we can infer\n\\(2y = y^2 /2\\). It is tempting to divide\nboth sides by \\(y\\), but what if \\(y = 0\\)? In fact \\(y = 0\\) is one solution,\nfor which \\(x = 2y = 0\\) as well, corresponding to Xavier and Yvonne both\nbeing newborns. Setting that solution to one side we can now look\nfor solutions in which \\(y\\) is not zero by dividing both sides by \\(y\\).\nThis yields \\(y = 4\\), in which case \\(x = 2y = 8\\). So now we have a second\nsolution in which Xavier is eight years old and Yvonne four.\n \n\nIn the absence of any other information, both solutions are legitimate.\nHad the problem further specified that Yvonne was a toddler,\nor that Xavier was older than Yvonne, we could have ruled out the\nfirst solution.\n \n\nLines, circles, and other curves in the plane can be expressed\nalgebraically using Cartesian coordinates, named for\nits inventor Rene Descartes. These are defined with respect to\na distinguished point in the plane called the origin,\ndenoted \\(O\\). Each point is specified by how far it is to\nthe right of and above \\(O\\), written as a pair of numbers.\nFor example the pair (2.1, 3.56) specifies the point 2.1 units to the\nright of \\(O\\), measured horizontally, and 3.56 units above it,\nmeasured vertically; we call 2.1 the \\(x\\) coordinate and 3.56\nthe \\(y\\) coordinate of that point. Either coordinate can be\nnegative: the pair \\((-5, -1)\\) corresponds to the point 5 units to the\nleft of \\(O\\) and 1 unit below it. The point \\(O\\) itself is\ncoordinatized as (0, 0).\n \n\nLines. Given an equation in variables \\(x\\) and\n\\(y\\), a point such as (2, 7) is said to be a solution\nto that equation when setting \\(x\\) to 2 and \\(y\\) to\n7 makes the equation true. For example the equation \\(y = 3x+5\\) has as solutions the points (0, 5), (1, 8), (2, 11),\nand so on. Other solutions include (.5, 6.5), (1.5, 9.5), and so on.\nThe set of all solutions constitutes the unique straight line passing\nthrough (0, 5) and (1, 8). We then call \\(y = 3x+5\\)\nthe equation of that line.\n \n\nCircles. By Pythagoras’s Theorem the square of the\ndistance between two points \\((x, y)\\) and \\((x', y')\\) is given by\n\\((x'-x)^2 +(y'-y)^2\\).  As a special case of this, the square of the\ndistance of the point \\((x, y)\\) to the origin is \\(x^2 +y^2\\). It\nfollows that those point at distance \\(r\\) from the origin are the\nsolutions in \\(x\\) and \\(y\\) to the equation \\(x^2 +y^2 = r^2\\).  But\nthese points are exactly those forming the circle of radius \\(r\\)\ncentered on \\(O\\). We identify this equation with this circle.\n \n\nVarieties The roots of any polynomial in \\(x\\) and \\(y\\) form\na curve in the plane called a one-dimensional variety\nof degree that of the polynomial. Thus lines are of degree 1,\nbeing expressed as polynomials \\(ax+by+c\\), while circles centered on\n\\((x', y')\\) are of degree 2, being expressed as polynomials\n\\((x-x')^2 +(y-y')^2 -r^2\\).  Some varieties may contain no points,\nfor example \\(x^2 +y^2 +1\\), while others may contain one point, for\nexample \\(x^2 +y^2\\) having the origin as its one root. In general\nhowever a two-dimensional variety will be a curve. Such a curve may\ncross itself, or have a cusp, or even separate into two or more\ncomponents not connected to each other.\n \n\nSpace The two-dimensional plane is generalized to\nthree-dimensional space by adding to the variables \\(x\\) and\n\\(y\\) a third variable \\(z\\) corresponding to the third\ndimension. The conventional orientation takes the first dimension to\nrun from west to east, the second from south to north, and the third\nfrom below to above. Points are then triples, for example the point\n\\((2, 5, -3)\\) is 2 units to the east of the origin, 5 units to the north\nof it, and 3 units below it.\n \n\nPlanes and spheres. These are the counterparts in space of\nlines and circles in the plane. An equation such as \\(z = 3x + 2y\\)\ndefines not a straight line but rather a flat plane, in this case the\nunique plane passing through the points (0, 1, 2), (1, 0, 3), and (1,\n1, 5). And the sphere of radius \\(r\\) centered on the origin is given\nby \\(x^2 +y^2 +z^2 = r^2\\). The roots of a polynomial in \\(x, y\\) and\n\\(z\\) form a surface in space called a two-dimensional variety, of\ndegree that of the polynomial, just as for one-dimensional\nvarieties. Thus planes are of degree 1 and spheres of degree 2.\n \n\nThese methods generalize to yet higher dimensions by adding yet more\nvariables. Although the geometric space we experience physically is\nlimited to three dimensions, conceptually there is no limit to the\nnumber of dimensions of abstract mathematical space. Just as a line is\na one-dimensional subspace of the two-dimensional plane, and a plane is\na two-dimensional subspace of three-dimensional space, each specifiable\nwith an equation, so is a hyperplane a three-dimensional\nsubspace of four-dimensional space, also specifiable with an equation\nsuch as \\(w = 2x - 7y + z\\).\n \n\nElementary algebra fixes some domain, typically the reals or complex\nnumbers, and works with the equations holding within that domain.\nAbstract or modern algebra reverses this picture\nby fixing some set \\(A\\) of equations and studying those domains\nfor which those equations are identities. For example if we take the\nset of all identities expressible with the operations of addition,\nsubtraction, and multiplication and constants 0 and 1 that hold\nfor the integers, then the algebras in which those equations hold\nidentically are exactly the commutative rings with identity.\n \n\nHistorically the term modern algebra came from the title of the\nfirst three editions of van der Waerden’s classic text of that name,\nrenamed simply “Algebra” for its fourth edition in 1955. Volume 1\ntreated groups, rings, general fields, vector spaces, well orderings,\nand real fields, while Volume 2 considered mainly linear algebra,\nalgebras (as vector spaces with a compatible multiplication),\nrepresentation theory, ideal theory, integral algebraic elements,\nalgebraic functions, and topological algebra. On the one hand modern\nalgebra has since gone far beyond this curriculum, on the other this\nconsiderable body of material is already more than what can be assumed\nas common knowledge among graduating Ph.D. students in\nmathematics, for whom the typical program is too short to permit\nmastering all this material in parallel with focusing on their area of\nspecialization.\n \n\nA core feature of abstract algebra is the existence of domains where\nfamiliar laws fail to hold. A striking example is commutativity of\nmultiplication, which as we noted in the introduction need not hold\nfor the multiplication of an arbitrary group, even so simple a group as\nthe six permutations of three letters.\n \n\nWe begin with the concept of a binary operation on a set \\(X\\), namely\na function \\(f: X^2 \\rightarrow X\\) such that \\(f(x, y)\\) is an\nelement of \\(X\\) for all elements \\(x, y\\) of \\(X\\).  Such an\noperation is said to be associative when it satisfies\n\\(f(f(x, y), z) = f(x, f(y, z))\\) for all \\(x, y, z\\) in \\(X\\).\n \n\nA semigroup is a set together with an associative operation,\ncalled the multiplication of the semigroup and notated\n\\(xy\\) rather than \\(f(x, y)\\). \n \n\nThe product \\(xx\\) of an element with itself is denoted\n\\(x^2\\). Likewise \\(xxx\\) is denoted \\(x^3\\) and so on.  \n\nConcatenation \\(uv\\) of words \\(u, v\\) is associative because when a\nword is cut into two, the concatenation of the two parts is the\noriginal word regardless of where the cut is made.  The concatenation\nof al and gebra is the same as that\nof algeb and ra, illustrating\nassociativity of concatenation for the case \\(x =\n\\) al, \\(y =\\) geb, \\(z\n=\\) ra.\n \n\nComposition \\(f\\cdot g\\) of two functions \\(f\\) and \\(g\\) is\nassociative via the reasoning\n \n\nfor all \\(x\\) in \\(X\\), whence\n\\(f\\cdot(g\\cdot h) = (f\\cdot g)\\cdot h\\).\n \n\nA semigroup \\(H\\) is a subsemigroup of a semigroup\n\\(G\\) when \\(H\\) is a subset of \\(G\\) and the\nmultiplication of \\(G\\) restricted to \\(H\\) coincides with\nthat of \\(H\\). Equivalently a subsemigroup of \\(G\\) is a\nsubset \\(H\\) of \\(G\\) such that for all \\(x, y\\)\nin \\(H, xy\\) is in \\(H\\).\n \n\nA binary operation is called commutative when it satisfies\n\\(f(x, y) = f(y, x)\\)\nfor all \\(x, y\\) in \\(X\\). A commutative\nsemigroup is a semigroup whose operation is commutative. All the\nexamples so far have been of noncommutative semigroups. The following\nillustrate the commutative case.\n \n\nAn element \\(x\\) of \\(X\\) is a left identity\nfor \\(f\\) when \\(f(x, y) = y\\)\nfor all \\(y\\) in \\(X\\), and a right identity when\n\\(f(y, x) = y\\) for all \\(y\\)\nin \\(X\\). An identity for \\(f\\) is an element\nthat is both a left identity and a right identity for \\(f\\).\nAn operation \\(f\\) can have only one identity, because when\n\\(x\\) and \\(y\\) are identities they are both equal to\n\\(f(x,y)\\).\n \n\nA monoid is a semigroup containing an identity for the\nmultiplication of the semigroup, notated 1.\n \n\nA monoid \\(H\\) is a submonoid of a monoid \\(G\\)\nwhen it is a subsemigroup of \\(G\\) that includes the identity\nof \\(G\\).\n \n\nWhen two elements \\(x, y\\) of a monoid satisfy \\(xy = 1\\) we say that\n\\(x\\) is the left inverse of \\(y\\) and \\(y\\) is the right inverse of\n\\(x\\). An element \\(y\\) that is both a left and right inverse of \\(x\\)\nis called simply an inverse of \\(x\\).\n \n\nA group is a monoid every element of which has an inverse.\n \n\nA subgroup of a group \\(G\\) is a submonoid of \\(G\\)\nclosed under inverses. The monoids of natural numbers and of even\nintegers are both submonoids of the monoid of integers under addition,\nbut only the latter submonoid is a subgroup, being closed under\nnegation, unlike the natural numbers.\n \n\nAn abelian group is a group whose operation is commutative.\nThe group operation of an abelian group is conventionally referred to as\naddition rather than multiplication, and abelian groups are sometimes\ncalled additive groups.\n \n\nA cyclic group is a group \\(G\\) with an\nelement \\(g\\) such that every element of \\(G\\)\nis of the form \\(g^i\\) for some positive\ninteger \\(i\\). Cyclic groups are abelian because\n\\(g^{i}g^j = g^{i + j} = g^j g^{i}\\). The group of integers under addition,\nand the groups of integers mod \\(n\\) for any positive integer\n\\(n\\), all form cyclic groups, with 1 as a generator in every\ncase. All cyclic groups are isomorphic to one of these. There are\nalways other generators when the group is of order 3 or more, for\nexample \\(-1\\), and for groups of prime order every nonzero element is\na generator.\n \n\nA ring is an abelian group that is also a monoid by virtue\nof having a second operation, called the multiplication\nof the ring. Zero annihilates, meaning that \\(0x = x0 = 0\\). Furthermore multiplication distributes over\naddition (the group operation) in both arguments. That is,\n\\(x(y+z) = xy + xz\\) and\n\\((x+y)z = xz + yz\\).\n \n\nIn all but the last example the integers (other than the integer\n\\(n\\) giving the size of the matrices) may be replaced by any\nof the rationals, the reals, or the complex numbers. When replacing the integers with the reals the fourth example becomes simply the ring of reals because even if \\(b\\) is zero \\(a\\) can be any real. However when replacing with the rational numbers the ring includes the rationals, but is more than that because \\(\\sqrt{2}\\)\nis irrational, yet it does not contain for example \\(\\sqrt{3}\\).\n \n\nA field is a ring for which the multiplicative monoid of\nnonzero ring elements is an abelian group. That is, multiplication must\nbe commutative, and every nonzero element \\(x\\) must have a reciprocal\n\\(1/x\\).\n \n\nThe last example does not generalize directly to other moduli.\nHowever for any modulus that is a power \\(p^n\\)\nof a prime, it can be shown that there exists a unique multiplication\nmaking the group \\(Z_{p^n}\\)\na ring in a way that makes the nonzero elements of the ring a\ncyclic (and therefore abelian) group under the multiplication, and\nhence making the ring a field. The fields constructed in this way are\nthe only finite fields.\n \n\nWhy study entire classes? Well, consider for example the set\n\\(Z\\) of integers along with the binary operation of addition\n\\(x+y\\), the unary operation of negation \\(-x\\),\nand the constant 0. These operations and the constant satisfy\nvarious laws such as \\(x+(y+z) =\n(x+y)+z, x+y = y+x, x+0 = x\\), and\n\\(x+(-x) = 0\\). Now consider any other algebra with\noperations that not only have the same names but also satisfy the\nsame laws (and possibly more), called a model of those\nlaws. Such an algebra could serve any of the following purposes.\n \n\n(i) It could tell us to what extent the equational laws holding of the\nintegers characterize the integers. Since the set \\(\\{0, 1\\}\\) of integers\nmod 2 under addition and negation satisfies all the laws that the\nintegers do, we immediately see that no single equational property of\nthe integers tells us that there are infinitely many integers. On the\nother hand any finite model of the equational theory of the integers\nnecessarily satisfies some law that the integers don’t satisfy, in\nparticular the law \\(x+x+\\ldots +x = 0\\) where\nthe number of \\(x\\)\\(s\\) on the left hand side is the\nsize of the model. Since the equational theory of the integers\ncontains no such law we can tell from its theory as a whole that the\nintegers must be an infinite set. On the other hand the rational\nnumbers under addition and negation satisfy exactly the same\nequational properties as the integers, so this theory does not\ncharacterize the algebra of integers under addition and subtraction\nwith sufficient precision to distinguish it from the rationals.  \n\n(ii) It could provide us with a useful new domain that can be\nsubstituted for the integers in any application depending only\non equational properties of the integers, but which differs from\nthe integers in other (necessarily nonequational) useful respects.\nFor example the rationals, which satisfy the same laws as we just\nnoted, differ in having the density property, that between any\ntwo rationals there lies another rational. Another difference is that\nit supports division: whereas the ratio of two integers is usually\nnot an integer, the ratio of two rationals is always a rational.\nThe reals also satisfy the same equations, and like the rationals are\ndense and support division. Unlike the rationals however the reals\nhave the completeness property, that the set of all upper\nbounds of any nonempty set of reals is either empty or has a least\nmember, needed for convergent sequences to have a limit to converge to.\n \n\nThis idea extends to other operations such as multiplication and\ndivision, as with fields. A particularly useful case of such a\ngeneralization is given by the use of complex numbers in Cartesian\ngeometry. When \\(x\\) and \\(y\\) range over the field of\nreals, \\(x^2 +y^2 =1\\) describes the ordinary Euclidean\ncircle in two dimensions, but when the variables range over the\ncomplex numbers this equation describes the complex counterpart of the\ncircle, visualizable as a two-dimensional surface embedded in four\nreal dimensions (regarding the complex plane as having two real\ndimensions). Or if the variables range over the integers mod 7, which\nform a field under the usual arithmetic operations mod 7, the circle\nconsists of eight points, namely \\((\\pm 1, 0), (0, \\pm 1)\\), and\n\\((\\pm 2, \\pm 2)\\). Certain theorems about the Euclidean circle\nprovable purely algebraically remain provable about these other kinds\nof circles because all the equations on which the proof depends\ncontinue to hold in these other fields, for example the theorem that a\nline intersects a circle in at most two points. \n\n(iii) It could help us decide whether some list of equational laws\nintended to axiomatize the integers is complete in the sense\nthat any equation holding of the integers follows from the laws in that\nlist. If some structure satisfies all the axioms in the list, but not\nsome other equation that holds of the integers, then we have a witness\nto the incompleteness of the axiomatization. If on the other hand we\ncan show how to construct any algebra satisfying the axioms from the\nalgebra of integers, limiting ourselves only to certain algebraic\nconstructions, then by a theorem of Birkhoff applicable to those\nconstructions we can infer that the axiomatization is complete.\n \n\n(iv) It could give another of way of defining a class, besides the\nstandard way of listing axioms. In the case at hand, the class of all\nalgebras with a constant, a unary operation, and a binary operation,\nsatisfying all the laws satisfied by the integers, is exactly the\nclass of abelian groups.\n \n\nUniversal algebra is the next level of abstraction after abstract\nalgebra. Whereas elementary algebra treats equational reasoning in a\nparticular algebra such as the field of reals or the field of complex\nnumbers, and abstract algebra studies particular classes of algebras\nsuch as groups, rings, or fields, universal algebra studies classes\nof classes of algebras. Much as abstract algebra numbers groups,\nrings, and fields among its basic classes, so does universal algebra\ncount varieties, quasivarieties, and elementary classes among its\nbasic classes of classes.\n \n\nA model of a theory is a structure for which all the equations\nof that theory are identities. Terms are built up from variables and\nconstants using the operations of the theory. An equation is a pair of\nterms; it is satisfied by an algebra when the two terms are equal under\nall valuations of (assignments of values to) the \\(n\\) variables appearing in the terms,\nequivalently when they denote the same \\(n\\)-ary operation.\nA quasiequation is a pair consisting of a finite set of equations,\ncalled the premises or antecedents, and another equation, the conclusion; it is satisfied\nby an algebra when the two terms of the conclusion are equal under\nall valuations of the \\(n\\) variables appearing in the terms\nsatisfying the premises. A first order formula is a quantified\nBoolean combination of relational terms.\n \n\nA variety is the class of all models of a set of equations.\nA quasivariety is the class of all models of a set of\nquasiequations. An elementary class is the class of all\nmodels of a set of first-order formulas.\n Quasivarieties have received much less attention than either\nvarieties or elementary classes, and we accordingly say little\nabout them here. Elementary classes are treated in sufficient depth\nelsewhere in this encyclopedia that we need not consider them here. \nWe therefore focus in this section on varieties.\n \n\nAbelian groups, groups, rings, and vector spaces over a given field\nall form varieties.\n  A central result in this area is the theorem that a lattice arises\nas the lattice of subalgebras of some algebra if and only if it arises\nas the lattice of congruences on some algebra. Lattices of this sort\nare called algebraic lattices. When the congruences of an\nalgebra permute, its congruence lattice is modular, a strong condition\nfacilitating the analysis of finite algebras in particular.\n \n\nFamiliar theorems of number theory emerge in algebraic form for\nalgebras. An algebra \\(A\\) is called directly irreducible or\nsimple when its lattice of congruences is the two-element\nlattice consisting of \\(A\\) and the one-element algebra,\nparalleling the notion of prime number \\(p\\) as a number whose\nlattice of divisors has two elements \\(p\\) and 1. However the\ncounterpart of the fundamental theorem of arithmetic, that every\npositive integer factors uniquely as a product of primes, requires a\nmore delicate kind of product than direct product. Birkhoff’s notion\nof subdirect product enabled him to prove the Subdirect Representation\nTheorem, that every algebra arises as the subdirect product of its\nsubdirectly irreducible quotients. Whereas there are many subdirectly\nirreducible groups, the only subdirectly irreducible Boolean algebra\nis the initial or two-element one, while the subdirectly irreducible\nrings satisfying \\(x^n = x\\) for some\n\\(n \\gt 1\\) are exactly the finite fields.\n \n\nAnother central topic is duality: Boolean algebras are dual to Stone\nspaces, complete atomic Boolean algebras are dual to sets, distributive\nlattices with top and bottom are dual to partially ordered sets,\nalgebraic lattices are dual to semilattices, and so on. Duality\nprovides two ways of looking at an algebra, one of which may turn out\nto be more insightful or easier to work with than the other depending\non the application.\n \n\nThe structure of varieties as classes of all models of some\nequational theory is also of great interest. The earliest result in\nthis area is Birkhoff’s theorem that a class of algebras is a variety\nif and only if it is closed under formation of quotients (homomorphic\nimages), subalgebras, and arbitrary (including empty and infinite)\ndirect products. This “modern algebra” result constitutes a\ncompleteness theorem for equational logic in terms of its models. Its\nelementary counterpart is the theorem that the equational theories on a\nfree algebra \\(F(V)\\), defined as the deductively closed\nsets of equations that use variables from \\(V\\), are exactly its\nsubstitutive congruences.\n \n\nA locally finite variety is one whose finitely generated free algebras\nare finite, such as pointed sets, graphs (whether of the directed or\nundirected variety), and distributive lattices. A congruence\npermutable variety is a variety all of whose algebras are congruence\npermutable. Maltsev characterized these in terms of a necessary and\nsufficient condition on their theories, namely that \\(F\\)(3)\ncontain an operation \\(t(x, y, z)\\)\nfor which \\(t(x, x, y) = t(y, x, x) = y\\) are in the\ntheory. Analogous notions are congruence distributivity and congruence\nmodularity, for which there exist analogous syntactic\ncharacterizations of varieties of algebras with these properties. A\nmore recently developed power tool for this area is McKenzie’s notion\nof tame congruences, facilitating the study of the structure of finite\nalgebras.  \n\nWithin the algebraic school, varieties have been defined with the\nunderstanding that the operations of a signature form a set. Insights\nfrom category theory, in particular the expression of a variety as a\nmonad, defined as a monoid object in the category\n\\(C^C\\) of endofunctors of a category\n\\(C\\) (Set in the case of ordinary universal algebra) indicate\nthat a cleaner and more general notion of variety is obtained when the\noperations can form a proper class. For example the important classes\nof complete semilattices, CSLat, and complete atomic\n Boolean algebras,\n CABA, form varieties only with this broader notion of\nsignature. In the narrow algebraic sense of variety, the dual of a\nvariety can never be a variety, whereas in the broader monadic notion\nof variety, the variety Set of sets is dual to CABA while CSLat is\nself-dual.\n \n\nAxiom systems. Identities can also be used to transform\nequations to equivalent equations. When those equations are themselves\nidentities for some domain, the equations they are transformed into\nremain identities for that domain. One can therefore start from some\nfinite set of identities and manufacture an unlimited number of new\nidentities from them.\n \n\nFor example if we start from just the two identities\n\\((x+y)+z = x+(y+z)\\)\nand \\(x+y = y+x\\), we can obtain the\nidentity \\((w+x)+(y+z) =\n(w+y)+(x+z)\\) via the following\nseries of transformations. \n\nThis process of manufacturing new identities from old is called\ndeduction. Any identity that can be generated by deduction\nstarting from a given set \\(A\\) of identities is called a\nconsequence of \\(A\\). The set of all consequences of\n\\(A\\) is called the deductive closure of \\(A\\). We\nrefer to \\(A\\) as an axiomatization of its deductive\nclosure. A set that is its own deductive closure is said to be\ndeductively closed. It is straightforward to show that a\nset is deductively closed if and only if it is the deductive closure\nof some set.\n \n\nAn equational theory is a deductively closed set of\nequations, equivalently the set of all consequences of some set\n\\(A\\) of equations. Every theory always has itself as its own\naxiomatization, but it will usually also have smaller\naxiomatizations. A theory that has a finite axiomatization is said to\nbe finitely based or finitely axiomatizable.\n \n\nEffectiveness. Finitely based theories can\nbe effectively enumerated. That is, given a finite set \\(A\\) of\nequations, one can write a computer program that prints consequences\nof \\(A\\) for ever in such a way that every consequence of\n\\(A\\) will appear at some finite position in the infinite list of\nall consequences. The same conclusion obtains when we weaken the\nrequirement that \\(A\\) be finite to merely that it can be\neffectively enumerated. That is, if the axiomatization is effectively\nenumerable so is its deductive closure.\n \n\n(In reconciling the finite with the infinite, bear in mind that if\nwe list all the natural numbers 0, 1, 2, … in order, we obtain an\ninfinite list every member of which is only finitely far from the\nbeginning, and also has a well-defined predecessor (except for 0) and successor. Only\nif we attempt to pad this list out at the “end” with infinite numbers\ndoes this principle break down. \n\nOne way to visualize there being an “end” that could have\nmore elements beyond it is to consider the rationals of the form\n\\(1/n\\) for all nonzero integers \\(n\\), in increasing order. This list\nstarts out \\(-1/1, -1/2, -1/3,\\ldots\\) and after listing infinitely\nmany negative rationals of that form, with no greatest such, switches\nover to positive rationals, with no first such, finally ending with\n1/3, 1/2, 1/1. The entire list is discrete in the sense that every\nrational except the endpoints \\(-1/1\\) and 1/1 has a well-defined\npredecessor and successor in this subset of the rationals, unlike the\nsituation for the set of all rationals between \\(-1/1\\) and\n\\(1/1\\). This would no longer be the case were we to introduce the\nrational 0 “in the middle”, which would have neither a\npredecessor nor a successor.)\n \n\nEquational Logic. Our informal account of\ndeduction can be formalized in terms of five rules for producing new\nidentities from old. In the following, \\(s\\) and \\(t\\) denote\narbitrary terms.\n \n\n“Consistently” in this context means that if a term is\nsubstituted for one occurrence of a given variable, the same term must\nbe substituted for all occurrences of that variable in both \\(s\\) and\n\\(t\\). We could not for example appeal solely to R5 to\njustify substituting \\(u+v\\) for \\(x\\) in the left hand side of \\(x+y\n= y+x\\) and \\(v+u\\) for \\(x\\) in the right hand side, though some\nother rule might permit it.  \n\nAn equational theory as a set of pairs of terms amounts to a binary\nrelation on the set of all terms. Rules R1–R3\ncorrespond to respectively reflexivity, symmetry, and transitivity of\nthis binary relation, \\(i.e\\). these three rules assert\nthat an equational theory is an equivalence relation. Rule\nR4 expresses the further property that this binary relation\nis a congruence. Rule R5 further asserts that the\nrelation is a substitutive congruence. It can be shown that a binary\nrelation on the set of terms is an equational theory if and only if it\nis a substitutive congruence. These five rules therefore completely\naxiomatize equational logic in the sense that every consequence of a\nset \\(A\\) of equations can be produced from \\(A\\) via finitely many\napplications of these five rules.\n \n\nA variety is by definition the class of models of some equational\ntheory. In 1935 Birkhoff provided an equivalent characterization of\nvarieties as any class closed under quotients (homomorphic images),\ndirect products, and subalgebras. These notions are defined as\nfollows.\n \n\nGiven two algebras\n\\((X, f_1 , \\ldots f_k)\\)\nand\n\\((Y, g_1 , \\ldots g_k)\\), a homomorphism\n\\(h:\n(X, f_1 , \\ldots f_k) \\rightarrow \n(Y, g_1 , \\ldots g_k)\\) is a function \\(h: X \\rightarrow Y\\) satisfying\n\\(h(f_i (x_0 , \\ldots ,x_{n_{ i}-1 }))\n= g_i (h(x_0), \\ldots ,h(x_{n_{ i}-1 })))\\)\nfor each \\(i\\) from 1 to \\(k\\) where \\(n_i\\) is the\narity of both \\(f_i\\) and \\(g_i\\).\n \n\nA subalgebra of an algebra is a set of elements of the algebra\nclosed under the operations of the algebra.\n \n\nLet \\(I\\) be an arbitrary set, which may be empty, finite, or\ninfinite. A family \\(\\langle A_{i}\\rangle_{i\\in I}\\) of\nalgebras \\((X_i, f_{1}^i,\\ldots, f_k^i)\\) indexed by \\(I\\) consists of\none algebra \\(A_i\\) for each element \\(i\\) of \\(I\\). We define\nthe direct product \\(\\Pi A_i\\) (or \\(\\Pi_{i\\in I} A_i\\) in\nfull) of such a family as follows.  \n\nThe underlying set of \\(\\Pi A_i\\) is the cartesian product \\(\\Pi X_i\\)\nof the underlying sets \\(X_i\\), and consists of those \\(I\\)-tuples\nwhose \\(i\\)-th element is some element of \\(X_i\\).  (\\(I\\) may even be\nuncountable, but in this case the nonemptiness of \\(\\Pi X_i\\) as a\nconsequence of the nonemptiness of the individual \\(X_i\\)’s is\nequivalent to the axiom of choice. This should be kept in mind for any\nconstructive applications of Birkhoff’s theorem.)\n \n\nThe \\(j\\)-th operation of \\(\\Pi A_i\\), of\narity \\(n_j\\), takes an\n\\(n_j\\)-tuple \\(t\\) of elements of\n\\(\\Pi X_i\\)\nand produces\nthe \\(I\\)-tuple\n\\(\\langle f_{j}^i(t_{1}^i , \\ldots t_{n_{ j} }^i)\\rangle_{i\\in I}\\)\nwhere\n\\(t_k^i\\) is the \\(i\\)-th component of the\n\\(k\\)-th component of \\(t\\) for \\(k\\) from 1 to\n\\(n_j\\).\n \n\nGiven two algebras \\(A\\), \\(B\\) and a homomorphism\n\\(h: A \\rightarrow B\\), the homomorphic image\n\\(h(A)\\) is the subalgebra of \\(B\\) consisting\nof elements of the form \\(h(a)\\) for \\(a\\)\nin \\(A\\).\n \n\nGiven a class \\(C\\) of algebras, we write \\(P(C)\\)\nfor the class of all algebras formed as direct products of families\nof algebras of \\(C, S(C)\\) for the class of\nall subalgebras of algebras of \\(C\\), and \\(H(C)\\)\nfor the class of all homomorphic images of algebras of \\(C\\).\n \n\nIt is relatively straightforward to show that any equation\nsatisfied by all the members of \\(C\\) is also satisfied by all\nthe members of \\(P(C), S(C)\\),\nand \\(H(C)\\). Hence for a variety \\(V,\nP(V) = S(V) = H(V)\\).\n \n\nBirkhoff’s theorem is the converse: for any class \\(C\\)\nsuch that \\(P(C) = S(C)\n= H(C), C\\) is a variety. In fact\nthe theorem is slightly stronger: for any class \\(C\\),\nHSP\\((C)\\) is a variety. That is, to construct all the\nmodels of the theory of \\(C\\) it suffices to close \\(C\\)\nfirst under direct products, then under subalgebras, and finally under\nhomomorphic images; that is, later closures do not compromise earlier\nones provided \\(P, S\\), and \\(H\\) are performed\nin that order.\n \n\nA basic application of Birkhoff’s theorem is in proving the\ncompleteness of a proposed axiomatization of a class \\(C\\).\nGiven an arbitrary model of the axioms, it suffices to show that the\nmodel can be constructed as the homomorphic image of a subalgebra of\na direct product of algebras of \\(C\\).\n \n\nThis completeness technique complements the completeness observed in the\nprevious section for the rules of equational logic.\n \n\nSibling to groups, rings, and fields is the class of vector\nspaces over any given field, constituting the universes of linear\nalgebra. Vector spaces lend themselves to two opposite approaches:\naxiomatic or abstract, and synthetic or concrete. The axiomatic\napproach takes fields (whence rings, whence groups) as a prerequisite;\nit first defines a notion of \\(R\\)-module as an abelian group with\na scalar multiplication over a given ring \\(R\\), and then defines\na vector space to be an \\(R\\)-module for which \\(R\\) is a\nfield. The synthetic approach proceeds via the familiar representation\nof vector spaces over the reals as \\(n\\)-tuples of reals, and of\nlinear transformations from \\(m\\)-dimensional to\n\\(n\\)-dimensional vector spaces as \\(m\\times n\\)\nmatrices of reals. For the full generality of vector spaces including\nthose of infinite dimension, \\(n\\) need not be limited to finite\nnumbers but can be any cardinal.\n \n\nThe abstract approach, as adopted by such classical texts as Mac\nLane and Birkhoff, has a certain purist appeal and is ideally suited to\nmathematics majors. The concrete approach has the benefit of being able\nto substitute calculus or less for groups-rings-fields as a\nprerequisite, suiting it to service courses for scientists and\nengineers needing only finite-dimensional matrix algebra, which enjoys\nenormous practical applicability. Linear algebra over other fields, in\nparticular finite fields, is used in coding theory, quantum computing,\netc., for which the abstract approach tends to be better suited.\n \n\nFor any field \\(F\\), up to isomorphism there is exactly one\nvector space over \\(F\\) of any given finite dimension. This is a\ntheorem in the abstract approach, but is an immediate consequence of\nthe representation in the concrete approach (the theorem is used in\nrelating the two approaches).\n \n\nAnother immediate consequence of the concrete approach is duality for\nfinite-dimensional vector spaces over \\(F\\). To every vector space\n\\(V\\), of any dimension, corresponds its dual space \\(V^*\\) comprised\nof the functionals on \\(V\\), defined as the linear\ntransformations \\(f: V\\rightarrow F\\), viewing the field \\(F\\) as the\none-dimensional vector space. The functionals form a vector space\nunder coordinatewise addition \\((f+g)(u) = f(u)+g(u)\\) and\nmultiplication \\((xf)(u) = x(f(u))\\) by any scalar \\(x\\) in \\(F\\),\nand we take \\(V^*\\) to be that space. This operation on vector spaces\nextends to the linear transformations \\(f: U\\rightarrow V\\) as \\(f^* :\nV^*\\rightarrow U^*\\) defined such that \\(f\\) maps each functional \\(g:\nV\\rightarrow F\\) to \\(g\\cdot f: U\\rightarrow F\\). Repeating this operation\nproduces a vector space that, in the finite-dimensional case, is\nisomorphic to \\(V\\), that is, \\(V \\cong V^{**}\\), making the operation\nan involution. The essence of duality for finite-dimensional vector\nspaces resides in its involutary nature along with the reversal of the\nlinear transformations.\n \n\nThis duality is easily visualized in the concrete approach by viewing\nlinear transformations from \\(U\\) to \\(V\\) as\n\\(m\\times n\\) matrices. The duality\nsimply transposes the matrices while leaving the machinery of matrix\nmultiplication itself unchanged. It is then immediate that this\noperation is an involution that reverses maps—the\n\\(m\\times n\\) matrix linearly\ntransforming an \\(n\\)-dimensional space \\(U\\) to an\n\\(m\\)-dimensional one \\(V\\) transposes to an\n\\(n\\times m\\) matrix linearly\ntransforming the \\(m\\)-dimensional space \\(V^*\\)\nto the \\(n\\)-dimensional space \\(U^*\\).  \n\nThe linear transformations \\(f: V\\rightarrow V\\) on\na vector space \\(V\\) can be added, subtracted, and multiplied by\nscalars, pointwise in each case, and hence form a vector space. When\nthe space has finite dimension \\(n\\), the linear transformations\nare representable as \\(n\\times n\\) matrices.\n \n\nIn addition they can be composed, whence they form a vector space\nequipped with a bilinear associative operation, namely composition. In the\nfinite-dimensional case, composition is just the usual matrix\nproduct. Vector spaces furnished with such a product constitute\nassociative algebras. Up to isomorphism, all associative\nalgebras arise in this way whether of finite or infinite dimension,\nproviding a satisfactory and insightful characterization of the notion\nin lieu of an axiomatic characterization, not given here.\n \n\nWell-known examples of associative algebras are the reals, the\ncomplex numbers, and the quaternions. Unlike\nvector spaces, many nonisomorphic associative algebras of any given\ndimension greater than one are possible.\n \n\nA class of associative algebras of interest to physicists is that of\nthe Clifford algebras. Clifford algebras over the reals (which as\nvector spaces are Euclidean spaces) generalize complex numbers and\nquaternions by permitting any number of formal quantities \\(e\\)\nanalogous to \\(i = \\sqrt{-1}\\) to be adjoined to the field\nof reals. The common feature of these quantities is that each satisfies\neither \\(e^2 = -1\\) or \\(e^2 = 1\\).\nWhereas there are a great many associative algebras of low dimension,\nonly a few of them arise as Clifford algebras. The reals form the only\none-dimensional Clifford algebra, while the hyperbolic plane, defined\nby \\(e^2 = 1\\), and the complex plane, defined by\n\\(e^2 = -1\\), are the two two-dimensional Clifford\nalgebras. The hyperbolic plane is just the direct square of the real\nfield, meaning that its product is coordinatewise, \n\\((a, b)(c, d) = (ac, bd)\\), unlike that of the complex plane where it\ndefined by \\((a, b)(c, d) = (ac - bd, ad+bc)\\).\nThe two four-dimensional Clifford algebras are the \\(2\\times 2\\) matrices\nand the quaternions. Whereas the \\(2\\times 2\\) matrices contain zero\ndivisors (nonzero matrices whose product is zero), and so form only a\nring, the quaternions contain no zero divisors and so form a division\nring. Unlike the complex numbers however the quaternions do not form a\nfield because their multiplication is not commutative. Complex\nmultiplication however makes the complex plane a commutative division\nring, that is, a field.\n \n\nA number of branches of mathematics have benefited from the\nperspective of algebra. Each of algebraic geometry and algebraic\ncombinatorics has an entire journal devoted to it, while algebraic\ntopology, algebraic logic, and algebraic number theory all have strong\nfollowings. Many other more specialized areas of mathematics have\nsimilarly benefited.\n \n\nAlgebraic geometry begins with what we referred to in the\nintroduction as shapes, for example lines \\(y = ax +b\\),\ncircles \\(x^2 +y^2 = r^2\\), spheres\n\\(x^2 +y^2 +z^2 = r^2\\), conic sections \\(f(x,\ny) = 0\\) where \\(f\\) is a quadratic polynomial in\n\\(x\\) and \\(y\\), quadric surfaces \\(f(x,\ny, z) = 0\\) with \\(f\\) again quadratic, and so on.\n \n\nIt is convenient to collect the two sides of these equations on the\nleft so that the right side is always zero. We may then define a\nshape or variety to consist of the roots or zeros of a polynomial, or more generally the common zeros of a set of polynomials. \n\nOrdinary analytical or Cartesian geometry is conducted over the reals.\nAlgebraic geometry is more commonly conducted over the complex numbers,\nor more generally over any algebraically closed field. The varieties\ndefinable in this way are called affine varieties.\n \n\nSometimes however algebraic closure is not desirable, for example\nwhen working at the boundary of algebraic geometry and number theory\nwhere the field may be finite, or the rationals.\n \n\nMany kinds of objects are characterized by what structure their maps\nhold invariant. Posets transform via monotone functions, leaving order\ninvariant. Algebras transform via homomorphisms, leaving the algebraic\nstructure invariant. In algebraic geometry varieties transform via\nregular \\(n\\)-ary functions \\(f: A^n \\rightarrow A\\), defined as functions that are locally rational polynomials\nin \\(n\\) variables. Locally rational means that at each point of\nthe domain of \\(f\\) there exists a neighborhood on which \\(f\\)\nis the ratio of two polynomials, the denominator of which is nonzero in\nthat neighborhood.\n \n\nThis notion generalizes to regular functions \\(f: A^n \\rightarrow A^m\\)\ndefined as \\(m\\)-tuples of regular \\(n\\)-ary functions.\n \n\nGiven two varieties \\(V, V'\\) in\n\\(A^n\\) and \\(A^m\\)\nrespectively, a regular function from \\(A^n\\)\nto \\(A^m\\) whose restriction to \\(V\\) is\na function from \\(V\\) to \\(V'\\) is called a regular\nfunction of varieties. The category of affine varieties is then\ndefined to have as its objects all affine varieties and as its\nmorphisms all regular functions thereof. \n\nPolynomials being continuous, one would expect regular functions between\nvarieties to be continuous also. A difficulty arises with the shapes of\nvarieties, where there can be cusps, crossings, and other symptoms of\nsingularity. What is needed here is a suitable topology by which to\njudge continuity. \n\nThe trick is to work not in affine space but its projective\nspace. To illustrate with Euclidean three-space, its associated\nprojective space is the unit sphere with antipodal points identified,\nforming a two-dimensional manifold. Equivalently this is the space of\nall (unoriented) lines through the origin. Given an arbitrary affine\nspace, its associated projective space is the space of all such lines,\nunderstood as a manifold. \n\nThe topology on projective space appropriate for algebraic geometry\nis the Zariski topology, defined not by its open sets but\nrather by its closed sets, which are taken to be the algebraic sets,\nnamely those sets constituting the common zeros of a set of homogeneous\npolynomials. The crucial theorem is then that regular maps between\naffine varieties are continuous with respect to the Zariski topology. \n\nAlgebraic number theory has adopted these generalizations of\nalgebraic geometry. One class of varieties in particular that has been\nof great importance to number are elliptical curves.\n \n\nA celebrated success of algebraic number theory has\nbeen Andrew Wiles’ proof of Fermat’s so-called “last theorem.” This\nhad remained an open problem for over three and a half centuries.\n \n\nAlgebraic topology analyzes the holes and obstructions in connected\ntopological spaces. A topologist is someone who imagines all objects to\nbe made of unbreakable but very pliable playdough, and therefore does\nnot see the need to distinguish between a coffee cup and doughnut\nbecause either can be turned into the other. Topology is concerned with\nthe similarities and differences between coffee cups with \\(n\\)\nhandles, surfaces with \\(n\\) holes, and more complicated shapes.\nAlgebraic topology expresses the invariants of such shapes in terms of\ntheir homotopy groups and homology groups.\n \n\nAlgebraic logic got off to an early start with Boole’s introduction\nof Boolean algebra in an 1847 pamphlet. The methods of modern algebra\nbegan to be applied to Boolean algebra in the 20th\ncentury. Algebraic logic then broadened its interests to first order\nlogic and modal logic. Central algebraic notions in first order logic\nare ultraproducts, elementary equivalence, and elementary and\npseudoelementary varieties. Tarski’s cylindric algebras constitute a\nparticular abstract formulation of first order logic in terms of\ndiagonal relations coding equality and substitution relations encoding\nvariables. Modal logic as a fragment of first order logic is made\nalgebraic via Boolean modules.\n \n\nGiven any system such as integer arithmetic or real arithmetic, we can\nwrite \\(T\\) for the set of all definite terms such as \\(1 + (2/3)\\)\nbuilt from constants and constituting the definite language, and\n\\(T[V]\\) for the larger indefinite language permitting variables drawn\nfrom a set \\(V\\) in place of some of the constant symbols, with terms\nsuch as \\(x + (2/y)\\). When \\(V\\) contains only a single variable\n\\(“x”\\), \\(T[\\{“x”\\}]\\) is usually\nabbreviated to \\(T[“x”]\\) or just \\(T[x\\)] which is\nusually unambiguous. This convention extends to the algebra \\(\\Phi\\)\nof terms of \\(T\\) together with its list of operation symbols viewed\nas operations for combining terms; we write \\(\\Phi[V\\)] and call it\nthe term algebra on \\(V\\).\n\n \nThis notion of term algebra is a purely syntactic one involving only the operation symbols, constants, and variables of some language. The terms \\(2 + 3\\) and \\(3 + 2\\) are distinct; likewise \\(x + y\\) and \\(y + x\\) are distinct terms. As such they can be considered concrete terms. Now in a universe such as the integers certain concrete terms are equivalent in the sense that they always evaluate to the same element of the universe regardless of the values of their variable, for example \\(x + y\\) and \\(y + x\\). It is convenient to collect equivalent concrete terms into equivalence classes each of which is to be thought of as an abstract term. \nAs a simple example of abstract terms consider linear polynomials of the form \\(ax + by\\) where \\(a\\) and \\(b\\) are nonnegative integers, for example \\(7x + 3y\\). The set of all such polynomials includes 0 and is closed under polynomial addition, an associative and commutative operation. This set together with the operation of addition and the zero polynomial therefore constitutes a commutative monoid. \nThis monoid is an example of a free algebra, namely the free commutative monoid on two generators \\(x\\) and \\(y\\). What makes it free is that it satisfies no laws other than those of a commutative monoid. It is not however a free monoid because it satisfies the commutative law. The free monoid on two generators \\(x\\) and \\(y\\) is instead the set of all finite strings over the two-letter alphabet \\(\\{x,y\\}\\). \nWhen commutativity is introduced as a law, it identifies the\npreviously distinct strings \\(xy\\) and \\(yx\\) as a single polynomial;\nmore generally any two strings with the same number of \\(x\\)s\nand \\(y\\)s are identified.  \nFree monoids and free commutative monoids are examples of free \\(C\\)-algebras where \\(C\\) is a class of algebras. In these two examples the class \\(C\\) is respectively that of monoids and commutative monoids.  \nA free \\(C\\)-algebra is an algebra that lives at the frontier of\nsyntax and semantics. On the semantic side it is a member of \\(C\\). On\nthe syntactic side its elements behave like terms subject to the laws\nof \\(C\\), but no other laws expressible with its\ngenerators. Commutativity \\(xy = yx\\) is expressible with two\ngenerators and so a free monoid on two or more generators cannot be\ncommutative, though the free monoid on one generator, namely the set\nof all finite strings over a one-letter alphabet does form a\ncommutative monoid on one generator.\n \n\nOn the syntactic\nside, the free \\(C\\)-algebra \\(B\\) on a set \\(X\\) arises\nas a quotient of the term algebra formed from \\(X\\) (viewed as\na set of variables) using the operation symbols and constants common\nto the algebras of \\(C\\). The quotient identifies those terms\nthat have the same value for all algebras \\(A\\) of \\(C\\)\nand all valuations assigning values in \\(A\\) to the variables of\n\\(X\\). This performs just enough identifications to satisfy every\nlaw of \\(C\\) (thereby making this quotient a \\(C\\)-algebra)\nwhile still retaining the syntactic essence of the original term\nalgebra in a sense made more precise by the following paragraph.\n \n\n(Since the concept of a term algebra can seem a little circular in\nplaces, a more detailed account may clarify the concept. Given the\nlanguage of \\(C\\), meaning the operation symbols and constant\nsymbols common to the algebras of \\(C\\), along with a set\n\\(X\\) of variables, we first form the underlying set of the\nalgebra, and then interpret the symbols of the language as operations\non and values in that set. The set itself consists of the terms built\nin the usual way from those variables and constant symbols using the\noperation symbols; in that sense these elements are syntactic. But now\nwe change our point of view by treating those elements as semantic, and\nwe look to the constant symbols and operation symbols of the language\nas syntactic entities needing to be interpreted in this semantic domain\n(albeit of terms) in order to turn this set of terms into an algebra of\nterms. We interpret each constant symbol as itself. And we interpret\neach \\(n\\)-ary operation symbol \\(f\\) as the \\(n\\)-ary\noperation that takes any \\(n\\) terms \\(t_1 ,\n\\ldots ,t_n\\) as its \\(n\\) arguments\nand returns the single term \\(f(t_1 , \\ldots ,t_n)\\). Note that this interpretation of\n\\(f\\) only returns a term, it does not actually\nbuild it. All term building was completed when we produced the\nunderlying set of the algebra.)\n \n\nFrom the semantic side, a \\(C\\)-algebra \\(B\\) together with a subset\n\\(X\\) of \\(B\\) thought of as variables is said to be a free\n\\(C\\)-algebra on \\(X\\), or is freely generated by \\(X\\), when, given\nany \\(C\\)-algebra \\(A\\), any valuation in \\(A\\) of the variables in\n\\(X\\) (that is, any function \\(f: X\\rightarrow A\\)) uniquely extends\nto a homomorphism \\(h: B\\rightarrow A\\). (We say that \\(h:\nB\\rightarrow A\\) extends \\(f: X\\rightarrow A\\) when the restriction of\n\\(h\\) to \\(X\\) is \\(f\\).)\n \n\nAs a convenient shorthand a free \\(C\\)-algebra on no generators\ncan also be called an initial \\(C\\)-algebra. An initial\n\\(C\\)-algebra has exactly one homomorphism to every\n\\(C\\)-algebra.\n \n\nBefore proceeding to the examples it is worthwhile pointing out an\nimportant basic property of free algebras as defined from the semantic\nside.\n \n\nTwo free algebras \\(B, B'\\) on respective\ngenerator sets \\(X, Y\\) having the same cardinality are\nisomorphic.\n\n \n\nBy way of proof, pick any bijection \\(f:\nX\\rightarrow Y\\). This, its inverse \\(f':\nY\\rightarrow X\\), and the two identity functions on\nrespectively \\(X\\) and \\(Y\\), form a system of four functions\nclosed under composition. Each of these functions is from a generator\nset to an algebra and therefore has a unique extension to a\nhomomorphism. These four homomorphisms are also closed under\ncomposition. The one from \\(B\\) to itself extends the identity\nfunction on \\(X\\) and therefore must be the identity homomorphism\non \\(B\\) (since the latter exists and its restriction to\n\\(X\\) is the identity function on \\(X)\\). Likewise the\nhomomorphism from \\(G\\) to \\(G\\) is an identity function.\nHence the homomorphisms between \\(B\\) and \\(G\\) compose in\neither order to identities, which makes them isomorphisms. But this is\nwhat it means for \\(B\\) and \\(B'\\) to be\nisomorphic.\n \n\nThis fact allows us to say the free algebra on a given set,\nthinking of isomorphic algebras as being “morally” the same. Were this\nnot the case, our quotient construction would be incomplete as it\nproduces a unique free algebra, whereas the above definition of free\nalgebra allows any algebra isomorphic to that produced by the quotient\nconstruction to be considered free. Since all free algebras on\n\\(X\\) are isomorphic, the quotient construction is as good as any,\nand is furthermore one way of proving that they exist. It also\nestablishes that the choice of set of variables is irrelevant except\nfor its cardinality, as intuition would suggest.\n \n\nTake \\(C\\) to be the class of monoids. The term\nalgebra determined by the binary operation symbol and the constant\nsymbol for identity can be viewed as binary trees with variables and\ncopies of the constant symbol at the leaves. Identifying trees\naccording to associativity has the effect of flattening the trees into\nwords that ignore the order in which the operation was applied (without\nhowever reversing the order of any arguments). This produces words over\nthe alphabet \\(X\\) together with the identity. The identity laws\nthen erase the identities, except in the case of a word consisting only\nof the identity symbol, which we take to be the empty word.\n \n\nThus the monoid of finite words over an alphabet \\(X\\) is the\nfree monoid on \\(X\\).\n \n\nAnother representation of the free monoid on \\(n\\) generators\nis as an infinite tree, every vertex of which has \\(n\\)\ndescendants, one for each letter of the alphabet, with each edge\nlabeled by the corresponding letter. Each vertex \\(v\\) represents\nthe word consisting of the letters encountered along the path from the\nroot to \\(v\\). The concatenation of \\(u\\) and \\(v\\) is\nthe vertex arrived at by taking the subtree whose root is the vertex\n\\(u\\), noticing that this tree is isomorphic to the full tree, and\nlocating \\(v\\) in this subtree as though it were the full\ntree.\n \n\nIf we ignore the direction and labels of the edges in this tree we\ncan still identify the root: it is the only vertex with \\(n\\)\nedges incident on it, all other vertices have \\(n+1\\), namely the\none incoming edge and the \\(n\\) outgoing ones.\n \n\nThe free commutative monoid on a set is that monoid whose\ngenerators behave like letters just as for free monoids (in particular\nthey are still atoms), but which satisfy the additional law \\(uv =\nvu\\). We make further identifications, e.g. of\n“dog” and “dgo”. Order of letters in a word is\nnow immaterial, all that matters is how many copies there are of each\nletter. This information can be represented as an \\(n\\)-tuple of\nnatural numbers where \\(n\\) is the size of the alphabet. Thus the free\ncommutative monoid on \\(n\\) generators is \\(N^n\\), the algebra of\n\\(n\\)-tuples of natural numbers under addition.\n \n\nIt can also be obtained from the tree representation of the free\nmonoid by identifying vertices. Consider the case \\(n = 2\\) of\ntwo letters. Since the identifications do not change word length, all\nidentifications are of vertices at the same depth from the root. We\nperform all identifications simultaneously as follows. At every vertex\n\\(v\\), identify \\(v_{01}\\) and\n\\(v_{10}\\) and their subtrees. Whereas before there were\n\\(2^n\\) vertices at depth \\(n\\), now there are\n\\(n+1\\). Furthermore instead of a tree we have the upper right\nquadrant of the plane, that is, \\(N^2\\), rotated 135 degrees\nclockwise, with every vertex \\(v\\) at the top of a diamond whose\nother vertices are \\(v_0\\) and \\(v_1\\)\nat the next level down, and the identified pair\n\\(v_{01} = v_{10}\\) below both.  \n\nTo form the free group on \\(n\\) generators, first form the free\nmonoid on \\(2n\\) generators, with generators organized into\ncomplementary pairs each the inverse of the other, and then delete all\nadjacent complementary pairs from all words.\n \n\nThis view is not particularly insightful. The group counterpart of the\ntree representation does a better job of presenting a free group.\nConsider the free group on \\(n = 2\\) generators \\(A\\) and\n\\(B\\). We start with the free monoid on 4 generators \\(A,\nB, a, b\\) where \\(a\\) is the inverse of \\(A\\) and\n\\(b\\) that of \\(B\\). Every vertex of this tree has 4\ndescendants. So the root has degree 4 and the remaining vertices have\ndegree 5: every vertex except the root has one edge going in, say the\ngenerator \\(a\\), and four out. Consider any nonroot vertex \\(v\\). The\neffect of deleting adjacent complementary pairs is to identify the\nimmediate ancestor of \\(v\\) with one of the four descendants of\n\\(v\\), namely the one that makes the path from the ancestor to\nthe descendant a complementary pair. For every nonroot vertex\n\\(v\\) these identifications reduce the degree of \\(v\\) from\n5 to 4. The root remains at degree 4.\n \n\nSo now we have an infinite graph every vertex of which has degree 4.\nUnlike the tree for the free monoid on 2 generators, where the root\nis topologically different from the other vertices, the tree for the\nfree group on 2 generators is entirely homogeneous. Thus if we throw\naway the vertex labels and rely only on the edge labels to navigate,\nany vertex can be taken as the identity of the group.\n \n\nThis homogeneity remains the case for the free abelian group on 2\ngenerators, whose vertices are still of degree 4. However the\nadditional identifications turns it from a tree (a graph with no\ncycles) to a grid whose vertices are the lattice points of the plane.\nThat is, the free group on 2 generators is \\(Z^2\\), and\non \\(n\\) generators \\(Z^n\\). The edges are\nthe line segments joining adjacent lattice points. \n\nWith no generators the free monoid, free group, and free ring are\nall the one-element algebra consisting of just the additive identity 0.\nA ring with identity means having a multiplicative identity, that is, a\nword \\(\\varepsilon\\). But this makes \\(\\varepsilon\\) a generator for the additive\ngroup of the ring, and the free abelian group on one generator is the\nintegers. So the free ring with identity on no generators is the\nintegers under addition and now multiplication.\n \n\nThe free ring on one generator \\(x\\) must include \\(x^2 , x^3\\),\netc. by multiplication, but these can be added and subtracted\nresulting in polynomials such as \\(7x^3 -3x^2 +2x\\) but without a\nconstant term, with the exception of 0 itself. The distributivity law\nfor rings means that a term such as \\((7x+x^2 )(2x^3 +x)\\) can be\nexpanded as \\(7x^2 +x^3 +14x^4 +2x^5\\). It should now be clear that\nthese are just ordinary polynomials with no constant term; in\nparticular we are missing the zero-degree polynomial 1 and so this\nring has no multiplicative identity. However it is a commutative ring\neven though we did not specify this. The free ring with identity on\none generator introduces 1 as the multiplicative identity and becomes\nthe ordinary one-variable polynomials since now we can form all the\nintegers. Just as with monoids, the free ring with identity on two\ngenerators is not commutative, the polynomials \\(xy\\) and \\(yx\\)\nbeing distinct. The free commutative ring with identity on\ntwo generators however consists of the ordinary two-variable\npolynomials over the integers.\n \n\nFrom the examples so far one might conclude that all free algebras\non one or more generators are infinite. This is by no means always the\ncase; as counterexamples we may point to a number of classes: sets,\npointed sets, bipointed sets, graphs, undirected graphs, Boolean\nalgebras, distributive lattices, etc. Each of these forms a locally\nfinite variety as defined earlier.\n \n\nA pointed set is an algebra with one constant, say \\(c\\). The\nfree pointed set on \\(x\\) and \\(y\\) has three elements,\n\\(x, y\\), and \\(c\\). A bipointed set is an algebra\nwith two constants \\(c\\) and \\(d\\), and the free bipointed\nset on \\(x\\) and \\(y\\) then has four elements, \\(x,\ny, c\\), and \\(d\\).\n \n\nGraphs, of the oriented kind arising in say automata theory where\nmultiple edges may connect the same two vertices, can be organized as\nalgebras having two unary operations \\(s\\) and \\(t\\) satisfying\n\\(s(s(x)) = t(s(x)) = s(x)\\) and \\(t(t(x)) = s(t(x)) = t(x)\\). The\nfree graph on one generator \\(x\\) has three elements, \\(x, s(x)\\), and\n\\(t(x)\\), constituting respectively an edge and its two endpoints\nor vertices. In this framework the vertices are the elements\nsatisfying \\(s(x) = x\\) (and hence \\(t(x) = x\\) since \\(x = s(x) =\nt(s(x)) = t(x)\\)); all other elements constitute edges. The free graph\non \\(n\\) generators consists of \\(n\\) such edges, all\nindependent. Other graphs arise by identifying elements. There is no\npoint identifying an edge with either another edge or a vertex since\nthat simply absorbs the first edge into the second entity. This leaves\nonly vertices; identifying two vertices yields a single vertex common\nto two edges, or to the same edge in the identification \\(s(x) =\nt(x)\\) creating a self-loop.\n \n\nThe term “oriented” is to be preferred to “directed” because a\ndirected graph as understood in combinatorics is an oriented graph with the\nadditional property that if \\(s(x) = s(y)\\) and \\(t(x) = t(y)\\) then \\(x = y\\); that is, only one\nedge is permitted between two vertices in a given direction.\n \n\nUnoriented graphs are defined as for graphs with an additional unary\noperation \\(g\\) satisfying \\(g(g(x)) = x\\) and \\(s(g(x)) = t(x)\\)\n(whence \\(s(x) = s(g(g(x))) = t(g(x)))\\). The free undirected graph on\n\\(x\\) consists of \\(x, s(x), t(x)\\), and \\(g(x)\\), with the pair \\(x,\ng(x)\\) constituting the two one-way lanes of a two-lane highway\nbetween \\(s(x) = t(g(x))\\) and \\(t(x) = s(g(x)\\)). Identification of\nelements of undirected graphs works as for their oriented\ncounterparts: it is only worth identifying vertices. However there is\none interesting twist here: vertices can be of two kinds, those\nsatisfying \\(x = g(x)\\) and those not. The latter kind of vertex is\nnow asymmetric: one direction of the bidirectional edge is identified\nwith its vertices while the other one forms an oriented loop in the\nsense that its other direction is a vertex. This phenomenon does not\narise for undirected graphs defined as those satisfying “if\n\\(s(x) = s(y)\\) and \\(t(x) = t(y)\\) then \\(x = y\\).”\n \n\nBoolean algebras are traditionally defined axiomatically as\ncomplemented distributive lattices, which has the benefit of showing\nthat they form a variety, and furthermore a finitely axiomatizable one.\nHowever Boolean algebras are so fundamental in their own right that,\nrather than go to the trouble of defining lattice, distributive, and\ncomplemented just for this purpose, it is easier as well as more\ninsightful to obtain them from the initial Boolean algebra. It suffices\nto define this as the two-element set \\(\\{0, 1\\}\\), the constants (zeroary\noperations) 0 and 1, and the \\(2^{2^2} = 16\\) binary operations. A\nBoolean algebra is then any algebra with those 16 operations and two\nconstants satisfying the equations satisfied by the initial Boolean\nalgebra.\n \n\nAn almost-definitive property of the class of Boolean algebras is\nthat their polynomials in the initial Boolean algebra are all the\noperations on that algebra. The catch is that the inconsistent class\nconsisting of only the one-element or inconsistent algebra also has\nthis property. This class is easily ruled out however by adding that\nBoolean algebra is consistent. But just barely—adding any new\nequation to Boolean algebra (without introducing new operations)\naxiomatizes the inconsistent algebra.\n \n\nSheffer has shown that the constants and the 16 operations can be\ngenerated as polynomials in just one constant, which can be 0 or 1,\nand one binary operation, which can be NAND, \\(\\neg(x\\wedge y)\\), or\nNOR, \\(\\neg(x\\vee y)\\). Any such sufficient set is called a\nbasis. Along the same lines Stone has shown that conjunction,\nexclusive-or, and the constant 1 form a basis. The significance of\nStone’s basis over Sheffer’s is that Boolean algebras\norganized with those operations satisfy all the axioms for a\ncommutative ring with identity with conjunction as multiplication and\nexclusive-or as addition, as well as the law \\(x^2 = 1\\). Any ring\nsatisfying this last condition is called a Boolean ring.\nBoolean rings are equivalent to Boolean algebras in the sense that\nthey have the same polynomials.\n \n\nAn atom of a Boolean algebra is an element \\(x\\) such that for\nall \\(y, x\\wedge y\\) is either \\(x\\) or 0. An\natomless Boolean algebra is one with no atoms.\n \n\nThere is exactly one Boolean algebra of cardinality every finite\npower of 2, and it is isomorphic to the Boolean algebra of a power set\n\\(2^X\\) of that cardinality under the set operations of\nunion, intersection, and complement relative to \\(X\\). Hence all\nfinite Boolean algebras have cardinality a power of 2. This situation\nchanges with infinite Boolean algebras; in particular countable Boolean\nalgebras exist. One such is the free Boolean algebra on countably many\ngenerators, which is the only countable atomless Boolean algebra. The\nfinite and cofinite (complement of a finite set) subsets of the set\n\\(N\\) of natural numbers form a subalgebra of the powerset Boolean\nalgebra \\(2^N\\) not isomorphic to the free Boolean\nalgebra, but it has atoms, namely the singleton sets.\n \n\nThe free Boolean algebra \\(F(n)\\) on \\(n\\)\ngenerators consists of all \\(2^2 n\\) \\(n\\)-ary\noperations on the two-element Boolean algebra. Boolean algebras\ntherefore form a locally finite variety.\n \n\nThe equational theory of distributive lattices is obtained from that\nof Boolean algebras by selecting as its operations just the monotone\nbinary operations on the two-element algebra, omitting the constants.\nThese are the operations with the property that if either argument is\nchanged from 0 to 1, the result does not change from 1 to 0. A\ndistributive lattice is any model of those Boolean equations between\nterms built solely with monotone binary operations. Hence every Boolean\nalgebra is a distributive lattice.\n \n\nDistributive lattices can be arbitrarily “thin.” At the extreme, any\nchain (linear or total order, \\(e.g\\). the reals\nstandardly ordered) under the usual operations of max and min forms a\ndistributive lattice. Since we have omitted the constants this includes\nthe empty lattice, which we have not excluded here as an algebra. (Some\nauthors disallow the empty set as an algebra but this proscription\nspoils many good theorems without gaining any useful ones.) Hence there\nexist distributive lattices of every possible cardinality.\n \n\nEvery finite-dimensional vector space is free, being generated by\nany choice of basis. This extends to infinite-dimensional vector spaces\nprovided we accept the Axiom of Choice. Vector spaces over a finite\nfield therefore form a locally finite variety when scalar\nmultiplication is organized as one unary operation for each field\nelement.\n \n\nWe now consider how free algebras are organized from the perspective\nof category theory. We defined the free algebra \\(B\\) generated\nby a subset \\(X\\) of \\(B\\) as having the property that for\nevery algebra \\(A\\) and every valuation \\(f:\nX\\rightarrow A\\), there exists a unique homomorphism\n\\(h: B\\rightarrow A\\). Now every homomorphism\n\\(h: B\\rightarrow A\\) necessarily arises in this way,\nsince its restriction to \\(X\\), as a function from \\(X\\) to\n\\(A\\), is a valuation. Furthermore every function\n\\(f: X\\rightarrow A\\) arises as the\nrestriction to \\(X\\) of its extension to a homomorphism. Hence we\nhave a bijection between the functions from \\(X\\) to \\(A\\)\nand the homomorphisms from \\(B\\) to \\(A\\).\n \n\nNow the typing here is a little casual, so let us clean it up. Since\n\\(X\\) is a set while \\(A\\) is an algebra, \\(f\\) is better typed\nas \\(f: X\\rightarrow U(A)\\) where \\(U(A)\\)\ndenotes the underlying set of \\(A\\). And the relationship of \\(X\\) to\n\\(B\\) is better understood with the notation \\(B = F(X)\\) denoting the free algebra generated by the set\n\\(X\\). So \\(U\\) maps algebras to sets while \\(F\\) maps\nsets to algebras. \\(F\\) and \\(U\\) are not in general inverses\nof each other, but they are nonetheless related in a way we now make\nprecise.\n \n\nFor any category \\(\\mathbf{C}\\), the notation \\(\\mathbf{C}(A, B)\\) is generally used to\ndenote the set of all homomorphisms from object \\(A\\) to object \\(B\\) in category \\(\\mathbf{C}\\). And\nthe set of all functions from the set \\(X\\) to the set \\(Y\\) can be\nunderstood as the particular case \\(\\mathbf{Set}(X, Y)\\) of this\nconvention where \\(\\mathbf{C}\\) is taken to be the class \\(\\mathbf{Set}\\) of all sets,\nwhich we can think of as discrete algebras, that is, algebras\nwith no structure. A class of algebras along with a specified set of\nhomomorphisms between any two of its members is an instance of a\ncategory. The members of the class are called the\nobjects of the category while the homomorphisms are called\nthe morphisms.\n \n\nThe bijection we have just observed can now be stated as\n \n\nSuch a bijection is called an adjunction between \\(\\mathbf{Set}\\) and\n\\(\\mathbf{C}\\). Here \\(F: \\mathbf{Set}\\rightarrow \\mathbf{C}\\) and \\(U:\n\\mathbf{C}\\rightarrow \\mathbf{Set}\\) are respectively the left and right\nadjoints of this adjunction; we say that \\(F\\) is left\nadjoint to (or of) \\(U\\) and \\(U\\) right adjoint to\n\\(F\\).\n \n\nWe have only described how \\(F\\) maps sets to algebras, and\n\\(U\\) maps algebras to sets. However \\(F\\) also maps\nfunctions to homomorphisms, mapping each function \\(f\\) to its unique extension\nas a homomorphism, while \\(U\\) maps homomorphisms to functions,\nnamely the homomorphism itself as a function. Such maps between\ncategories are instances of functors.\n \n\nIn general a category \\(C\\) consists of objects \\(a, b, c\\) and\nmorphisms \\(f: a\\rightarrow b\\), together with an associative\ncomposition law for “composable” morphisms \\(f:\nb\\rightarrow c\\), \\(g: a\\rightarrow b\\) yielding the morphism \\(fg:\na\\rightarrow c\\). Furthermore every object a has an identity element\n\\(1_a : a\\rightarrow a\\) which whenever composable with a morphism\n\\(f\\) (on one side or the other) composes with it to yield \\(f\\). A\nfunctor \\(F: C\\rightarrow D\\) maps objects of \\(C\\) to objects of\n\\(D\\) and morphisms of \\(C\\) to morphisms of \\(D\\), such that \\(F(fg)\n= F(f)F(g)\\) and \\(F(1_a) = 1_{F(a)}\\). That is, functors are\n“homomorphisms of categories,” preserving composition and\nidentities.\n \n\nWith no further qualification such a category is considered an\nabstract category. The categories we have been working with\nare concrete in the sense that they come with a given\nunderlying set or forgetful functor \\(U:\nC\\rightarrow\\)Set. That is, algebras are based on sets, homomorphisms\nare certain functions between these sets, and \\(U\\) simply\n“forgets” the algebraic structure. Such forgetful functors are\nfaithful in the sense that for any two morphisms \\(f,\ng: a\\rightarrow b\\) of \\(C\\), if \\(U(f)\n= U(g)\\) then \\(f = g,\ni.e. U\\) does not identify distinct\nhomomorphisms. In general a concrete category is defined as a category\n\\(C\\) together with a faithful forgetful functor \\(U:\nC\\rightarrow\\)Set.\n \n\nCategories themselves admit a further generalization to 2-categories\nas algebras over two-dimensional graphs, with associative composition\nof 1-cells generalized to the 2-associative pasting of 2-cells. A\nfurther simplification of the free-algebra machinery then obtains,\nnamely via abstract adjunctions as the natural 2-dimensional\ncounterpart of isomorphisms in a category, which in turn is the natural\n1-dimensional counterpart of equality of elements in a set, the\n0-dimensional idea that two points can turn out to be one. This leads\nto a notion of abstract monad as simply the composition of an adjoint\npair of 1-cells, one of which is the 1-cell abstracting the functor\n\\(F\\) that manufactures the free algebra \\(F(V)\\)\nfrom \\(V\\). Ordinary or concrete monads arise as the composition\nof functors as concrete 1-cells of a 2-category of categories.\n","contact.mail":"pratt@cs.stanford.edu","contact.domain":"cs.stanford.edu"}]
